From 098ce6e674729f41409f89471583d45e5775705c Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 31 Oct 2023 07:14:07 -0700
Subject: [PATCH 1/1] linux-v6.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/arch_gicv3.h           |   8 +
 arch/arm64/include/asm/kvm_emulate.h          |  21 +
 arch/arm64/include/asm/kvm_host.h             |  27 +
 arch/arm64/include/asm/kvm_mmu.h              |  29 +
 arch/arm64/kvm/arch_timer.c                   | 709 +++++++++++++
 arch/arm64/kvm/arm.c                          |  56 +
 arch/arm64/kvm/guest.c                        |   4 +
 arch/arm64/kvm/handle_exit.c                  |  27 +
 arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h    |  18 +
 arch/arm64/kvm/hyp/nvhe/mem_protect.c         |   4 +
 arch/arm64/kvm/hyp/pgtable.c                  |  13 +
 arch/arm64/kvm/hyp/vgic-v3-sr.c               |  97 ++
 arch/arm64/kvm/hyp/vhe/sysreg-sr.c            |   8 +
 arch/arm64/kvm/hyp/vhe/timer-sr.c             |   4 +
 arch/arm64/kvm/mmio.c                         |   4 +
 arch/arm64/kvm/mmu.c                          |  39 +
 arch/arm64/kvm/pmu-emul.c                     |  11 +
 arch/arm64/kvm/pvtime.c                       |  35 +
 arch/arm64/kvm/sys_regs.c                     |  49 +
 arch/arm64/kvm/vgic-sys-reg-v3.c              |  17 +
 arch/arm64/kvm/vgic/vgic-init.c               |  34 +
 arch/arm64/kvm/vgic/vgic-irqfd.c              |  44 +
 arch/arm64/kvm/vgic/vgic-its.c                | 161 +++
 arch/arm64/kvm/vgic/vgic-mmio-v3.c            |  19 +
 arch/arm64/kvm/vgic/vgic-v3.c                 |  52 +
 arch/arm64/kvm/vgic/vgic-v4.c                 |  10 +
 arch/arm64/kvm/vgic/vgic.c                    | 234 +++++
 arch/x86/events/core.c                        |   6 +
 arch/x86/include/asm/kvm_host.h               | 256 +++++
 arch/x86/include/asm/nmi.h                    |  22 +
 arch/x86/include/asm/paravirt.h               |  12 +
 arch/x86/include/asm/pvclock.h                |   7 +
 arch/x86/include/asm/x86_init.h               |   9 +
 arch/x86/include/uapi/asm/kvm.h               |  15 +
 arch/x86/kernel/apic/apic.c                   |   4 +
 arch/x86/kernel/cpu/vmware.c                  |  12 +
 arch/x86/kernel/kvm.c                         |   3 +
 arch/x86/kernel/kvmclock.c                    |  56 +
 arch/x86/kernel/nmi.c                         | 152 +++
 arch/x86/kernel/paravirt.c                    |  15 +
 arch/x86/kernel/pvclock.c                     |  10 +
 arch/x86/kernel/smpboot.c                     |   4 +
 arch/x86/kernel/tsc.c                         |  47 +
 arch/x86/kvm/hyperv.c                         |  12 +
 arch/x86/kvm/irq_comm.c                       |   9 +
 arch/x86/kvm/lapic.c                          |  94 ++
 arch/x86/kvm/lapic.h                          |  49 +
 arch/x86/kvm/mmu/mmu.c                        | 302 ++++++
 arch/x86/kvm/mmu/mmu_internal.h               |  36 +
 arch/x86/kvm/mmu/spte.h                       |  25 +
 arch/x86/kvm/mmu/tdp_mmu.c                    |   8 +
 arch/x86/kvm/pmu.c                            | 310 ++++++
 arch/x86/kvm/pmu.h                            | 135 +++
 arch/x86/kvm/vmx/pmu_intel.c                  |  35 +
 arch/x86/kvm/vmx/vmx.c                        |  12 +
 arch/x86/kvm/x86.c                            | 971 ++++++++++++++++++
 arch/x86/kvm/x86.h                            |   7 +
 arch/x86/kvm/xen.c                            |  12 +
 arch/x86/xen/time.c                           |  52 +
 drivers/clocksource/arm_arch_timer.c          | 102 ++
 drivers/clocksource/hyperv_timer.c            |  25 +
 drivers/irqchip/irq-gic-v3-its-pci-msi.c      |   4 +
 drivers/irqchip/irq-gic-v3-its.c              | 114 ++
 drivers/irqchip/irq-gic-v3.c                  | 293 ++++++
 drivers/irqchip/irq-gic-v4.c                  |   5 +
 drivers/irqchip/irq-gic.c                     |  38 +
 drivers/net/tap.c                             |   5 +
 drivers/net/virtio_net.c                      |   5 +
 .../net/wireless/intel/iwlwifi/mvm/mac-ctxt.c |   4 +
 drivers/nvme/host/core.c                      |   6 +
 drivers/pci/hotplug/pciehp_ctrl.c             |   4 +
 drivers/scsi/virtio_scsi.c                    |  30 +
 drivers/vhost/net.c                           | 150 +++
 drivers/vhost/scsi.c                          |   5 +
 include/kvm/arm_arch_timer.h                  |  58 ++
 include/kvm/arm_vgic.h                        |  41 +
 include/linux/irqchip.h                       |  10 +
 include/linux/kvm_host.h                      |  10 +
 kernel/cpu.c                                  |  28 +
 kernel/events/core.c                          |  13 +
 kernel/irq/irqdomain.c                        |  73 ++
 kernel/irq/manage.c                           |  35 +
 kernel/smp.c                                  |  10 +
 kernel/time/clocksource.c                     |   6 +
 kernel/time/timekeeping.c                     |  39 +
 kernel/watchdog.c                             |   8 +
 .../selftests/kvm/include/x86_64/processor.h  |   9 +
 tools/testing/selftests/kvm/lib/kvm_util.c    |  13 +
 .../selftests/kvm/max_guest_memory_test.c     |  34 +
 tools/testing/selftests/kvm/steal_time.c      |   6 +
 .../selftests/kvm/x86_64/kvm_clock_test.c     |  29 +
 .../selftests/kvm/x86_64/nx_huge_pages_test.c | 102 ++
 virt/kvm/eventfd.c                            |  18 +
 virt/kvm/irqchip.c                            |   3 +
 virt/kvm/kvm_main.c                           | 114 ++
 95 files changed, 5882 insertions(+)

diff --git a/arch/arm64/include/asm/arch_gicv3.h b/arch/arm64/include/asm/arch_gicv3.h
index 01281a533..d49fae843 100644
--- a/arch/arm64/include/asm/arch_gicv3.h
+++ b/arch/arm64/include/asm/arch_gicv3.h
@@ -32,6 +32,10 @@ static __always_inline void gic_write_dir(u32 irq)
 	isb();
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|421| <<gic_read_iar>> return gic_read_iar_common();
+ */
 static inline u64 gic_read_iar_common(void)
 {
 	u64 irqstat;
@@ -96,6 +100,10 @@ static inline void gic_write_grpen1(u32 val)
 	isb();
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|1383| <<gic_send_sgi>> gic_write_sgi1r(val);
+ */
 static inline void gic_write_sgi1r(u64 val)
 {
 	write_sysreg_s(val, SYS_ICC_SGI1R_EL1);
diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 3d6725ff0..40ff9b339 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -66,6 +66,10 @@ static __always_inline bool vcpu_el1_is_32bit(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1315| <<kvm_arch_vcpu_ioctl_vcpu_init>> vcpu_reset_hcr(vcpu);
+ */
 static inline void vcpu_reset_hcr(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hcr_el2 = HCR_GUEST_FLAGS;
@@ -103,6 +107,15 @@ static inline void vcpu_reset_hcr(struct kvm_vcpu *vcpu)
 		vcpu->arch.hcr_el2 |= HCR_ATA;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|558| <<kvm_arch_vcpu_runnable>> bool irq_lines = *vcpu_hcr(v) & (HCR_VI | HCR_VF);
+ *   - arch/arm64/kvm/arm.c|1103| <<vcpu_interrupt_line>> hcr = vcpu_hcr(vcpu);
+ *   - arch/arm64/kvm/inject_fault.c|236| <<kvm_set_sei_esr>> *vcpu_hcr(vcpu) |= HCR_VSE;
+ *   - arch/arm64/kvm/mmu.c|2061| <<kvm_set_way_flush>> unsigned long hcr = *vcpu_hcr(vcpu);
+ *   - arch/arm64/kvm/mmu.c|2076| <<kvm_set_way_flush>> *vcpu_hcr(vcpu) = hcr | HCR_TVM;
+ *   - arch/arm64/kvm/mmu.c|2094| <<kvm_toggle_cache>> *vcpu_hcr(vcpu) &= ~HCR_TVM;
+ */
 static inline unsigned long *vcpu_hcr(struct kvm_vcpu *vcpu)
 {
 	return (unsigned long *)&vcpu->arch.hcr_el2;
@@ -463,6 +476,14 @@ static inline bool kvm_is_write_fault(struct kvm_vcpu *vcpu)
 	return kvm_vcpu_dabt_iswrite(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2386| <<kvm_mpidr_to_vcpu>> if (mpidr == kvm_vcpu_get_mpidr_aff(vcpu))
+ *   - arch/arm64/kvm/psci.c|150| <<kvm_psci_vcpu_affinity_info>> mpidr = kvm_vcpu_get_mpidr_aff(tmp);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|295| <<vgic_init>> irq->mpidr = kvm_vcpu_get_mpidr_aff(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|322| <<vgic_mmio_read_v3r_typer>> unsigned long mpidr = kvm_vcpu_get_mpidr_aff(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1031| <<match_mpidr>> affinity = kvm_vcpu_get_mpidr_aff(vcpu);
+ */
 static inline unsigned long kvm_vcpu_get_mpidr_aff(struct kvm_vcpu *vcpu)
 {
 	return vcpu_read_sys_reg(vcpu, MPIDR_EL1) & MPIDR_HWID_BITMASK;
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d3dd05bbf..ff455d5f1 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -43,6 +43,17 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_IRQ_PENDING:
+ *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+ *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+ */
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
@@ -933,6 +944,22 @@ void kvm_arm_resume_guest(struct kvm *kvm);
 		res.a1;							\
 	})
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|494| <<set_cntvoff>> kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
+ *   - arch/arm64/kvm/arm.c|441| <<kvm_arch_vcpu_load>> kvm_call_hyp(__kvm_flush_cpu_context, mmu);
+ *   - arch/arm64/kvm/arm.c|1086| <<kvm_arch_vcpu_ioctl_run>> kvm_call_hyp(__kvm_adjust_pc, vcpu);
+ *   - arch/arm64/kvm/hyp/pgtable.c|789| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid, mmu);
+ *   - arch/arm64/kvm/hyp/pgtable.c|791| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ctx->addr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|822| <<stage2_put_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ctx->addr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1271| <<kvm_pgtable_stage2_relax_perms>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa_nsh, pgt->mmu, addr, level);
+ *   - arch/arm64/kvm/mmu.c|172| <<kvm_flush_remote_tlbs>> kvm_call_hyp(__kvm_tlb_flush_vmid, &kvm->arch.mmu);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|544| <<kvm_vgic_init_cpu_hardware>> kvm_call_hyp(__vgic_v3_init_lrs);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|730| <<vgic_v3_load>> kvm_call_hyp(__vgic_v3_write_vmcr, cpu_if->vgic_vmcr);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|732| <<vgic_v3_load>> kvm_call_hyp(__vgic_v3_restore_aprs, cpu_if);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|756| <<vgic_v3_put>> kvm_call_hyp(__vgic_v3_save_aprs, cpu_if);
+ *   - arch/arm64/kvm/vmid.c|69| <<flush_context>> kvm_call_hyp(__kvm_flush_vm_context);
+ */
 /*
  * The couple of isb() below are there to guarantee the same behaviour
  * on VHE as on !VHE, where the eret to EL1 acts as a context
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 0e1e1ab17..90228e492 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -223,6 +223,11 @@ static inline void __clean_dcache_guest_page(void *va, size_t size)
 	kvm_flush_dcache_to_poc(va, size);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|228| <<invalidate_icache_guest_page>> __invalidate_icache_guest_page(hyp_fixmap_map(__hyp_pa(va)), size);
+ *   - arch/arm64/kvm/mmu.c|272| <<invalidate_icache_guest_page>> __invalidate_icache_guest_page(va, size);
+ */
 static inline void __invalidate_icache_guest_page(void *va, size_t size)
 {
 	if (icache_is_aliasing()) {
@@ -279,6 +284,11 @@ static inline int kvm_write_guest_lock(struct kvm *kvm, gpa_t gpa,
  * path, we rely on a previously issued DSB so that page table updates
  * and VMID reads are correctly ordered.
  */
+/*
+ * called by:
+ *   - arch/arm64/include/asm/kvm_mmu.h|307| <<__load_stage2>> write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|297| <<__pkvm_prot_finalize>> params->vttbr = kvm_get_vttbr(mmu);
+ */
 static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
 {
 	struct kvm_vmid *vmid = &mmu->vmid;
@@ -295,9 +305,28 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
  * Must be called from hyp code running at EL2 with an updated VTTBR
  * and interrupts disabled.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|89| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|310| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|306| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|65| <<__tlb_switch_to_guest>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|183| <<__kvm_vcpu_run_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|56| <<__tlb_switch_to_guest>> __load_stage2(mmu, mmu->arch);
+ *
+ * 一个例子.
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_s2_mmu *hw_mmu;
+ */
 static __always_inline void __load_stage2(struct kvm_s2_mmu *mmu,
 					  struct kvm_arch *arch)
 {
+	/*
+	 * The VTCR_EL2 controls the translation table walks required for
+	 * the stage 2 translation of memory accesses from
+	 * Non-secure EL0 and EL1.
+	 */
 	write_sysreg(arch->vtcr, vtcr_el2);
 	write_sysreg(kvm_get_vttbr(mmu), vttbr_el2);
 
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index 6dcdae4d3..03cd2fb14 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -23,12 +23,79 @@
 
 #include "trace.h"
 
+/*
+ * struct kvm:
+ * -> struct kvm_arch arch;
+ *    -> struct arch_timer_vm_data timer_data;
+ *       -> u64 voffset;
+ *       -> u64 poffset;
+ *       -> ppi[NR_KVM_TIMERS];
+ *
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_cpu_context ctxt;
+ *       -> struct user_pt_regs regs;
+ *       -> u64 sys_regs[NR_SYS_REGS];
+ *    -> struct arch_timer_cpu timer_cpu;
+ *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> struct arch_timer_offset offset;
+ *             -> u64 *vm_offset;
+ *             -> u64 *vcpu_offset;
+ *       -> struct hrtimer bg_timer;
+ *       -> bool enabled;
+ *
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
+
+/*
+ * 在以下使用timecounter:
+ *   - arch/arm64/kvm/arch_timer.c|211| <<kvm_phys_timer_read>> return timecounter->cc->read(timecounter->cc);
+ *   - arch/arm64/kvm/arch_timer.c|305| <<kvm_counter_compute_delta>> ns = cyclecounter_cyc2ns(timecounter->cc,
+ *   - arch/arm64/kvm/arch_timer.c|307| <<kvm_counter_compute_delta>> timecounter->mask,
+ *   - arch/arm64/kvm/arch_timer.c|1464| <<kvm_timer_hyp_init>> timecounter = &info->timecounter;
+ *   - arch/arm64/kvm/arch_timer.c|1466| <<kvm_timer_hyp_init>> if (!timecounter->cc) {
+ */
 static struct timecounter *timecounter;
+/*
+ * 在以下设置host_vtimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1643| <<kvm_irq_init>> host_vtimer_irq = info->virtual_irq;
+ */
 static unsigned int host_vtimer_irq;
+/*
+ * 在以下设置host_ptimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1670| <<kvm_irq_init>> host_ptimer_irq = info->physical_irq;
+ */
 static unsigned int host_ptimer_irq;
+/*
+ * 在以下啊使用host_vtimer_irq_flags:
+ *   - arch/arm64/kvm/arch_timer.c|961| <<kvm_timer_vcpu_load_nogic>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1207| <<unmask_vtimer_irq_user>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1322| <<kvm_timer_cpu_up>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1644| <<kvm_irq_init>> kvm_irq_fixup_flags(host_vtimer_irq, &host_vtimer_irq_flags);
+ */
 static u32 host_vtimer_irq_flags;
+/*
+ * 在以下使用host_ptimer_irq_flags:
+ *   - arch/arm64/kvm/arch_timer.c|1324| <<kvm_timer_cpu_up>> enable_percpu_irq(host_ptimer_irq, host_ptimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1671| <<kvm_irq_init>> kvm_irq_fixup_flags(host_ptimer_irq, &host_ptimer_irq_flags);
+ */
 static u32 host_ptimer_irq_flags;
 
+/*
+ * 在以下使用has_gic_active_state:
+ *   - arch/arm64/kvm/arch_timer.c|32| <<global>> static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
+ *   - arch/arm64/kvm/arch_timer.c|255| <<kvm_arch_timer_handler>> !static_branch_unlikely(&has_gic_active_state))
+ *   - arch/arm64/kvm/arch_timer.c|856| <<kvm_timer_vcpu_load>> if (static_branch_likely(&has_gic_active_state)) {
+ *   - arch/arm64/kvm/arch_timer.c|940| <<unmask_vtimer_irq_user>> if (static_branch_likely(&has_gic_active_state))
+ *   - arch/arm64/kvm/arch_timer.c|1424| <<kvm_timer_hyp_init>> static_branch_enable(&has_gic_active_state);
+ */
 static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
 
 static const u8 default_ppi[] = {
@@ -51,15 +118,39 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      enum kvm_arch_timer_regs treg);
 static bool kvm_arch_timer_get_input_level(int vintid);
 
+/*
+ * 在以下使用arch_timer_irq_ops:
+ *   - arch/arm64/kvm/arch_timer.c|800| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq, timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|805| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq, timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1437| <<kvm_irq_init>> arch_timer_irq_ops.flags |= VGIC_IRQ_SW_RESAMPLE;
+ *   - arch/arm64/kvm/arch_timer.c|1624| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1632| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+ */
 static struct irq_ops arch_timer_irq_ops = {
 	.get_input_level = kvm_arch_timer_get_input_level,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|545| <<set_cntpoff>> if (has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|597| <<timer_save_state>> if (!has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|692| <<timer_restore_state>> if (!has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|854| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ */
 static bool has_cntpoff(void)
 {
 	return (has_vhe() && cpus_have_final_cap(ARM64_HAS_ECV_CNTPOFF));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|354| <<kvm_timer_earliest_exp>> for (i = 0; i < nr_timers(vcpu); i++) {
+ *   - arch/arm64/kvm/arch_timer.c|1012| <<kvm_timer_vcpu_reset>> for (int i = 0; i < nr_timers(vcpu); i++)
+ *   - arch/arm64/kvm/arch_timer.c|1028| <<kvm_timer_vcpu_reset>> for (int i = 0; i < nr_timers(vcpu); i++)
+ *   - arch/arm64/kvm/arch_timer.c|1550| <<timer_irqs_are_valid>> for (int i = 0; i < nr_timers(vcpu); i++) {
+ *   - arch/arm64/kvm/arch_timer.c|1566| <<timer_irqs_are_valid>> valid = hweight32(ppis) == nr_timers(vcpu);
+ *   - arch/arm64/kvm/arch_timer.c|1583| <<kvm_arch_timer_get_input_level>> for (int i = 0; i < nr_timers(vcpu); i++) {
+ */
 static int nr_timers(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu_has_nv(vcpu))
@@ -68,6 +159,15 @@ static int nr_timers(struct kvm_vcpu *vcpu)
 	return NR_KVM_TIMERS;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|324| <<kvm_timer_irq_can_fire>> ((timer_get_ctl(timer_ctx) &
+ *   - arch/arm64/kvm/arch_timer.c|685| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTV_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|696| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTP_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1162| <<read_timer_ctl>> u32 ctl = timer_get_ctl(timer);
+ *   - arch/arm64/kvm/trace_arm.h|243| <<__field>> __entry->ctl = timer_get_ctl(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|265| <<__field>> __entry->ctl = timer_get_ctl(ctx);
+ */
 u32 timer_get_ctl(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -87,6 +187,17 @@ u32 timer_get_ctl(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|458| <<kvm_timer_should_fire>> cval = timer_get_cval(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|683| <<timer_restore_state>> write_sysreg_el0(timer_get_cval(ctx), SYS_CNTV_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|689| <<timer_restore_state>> cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1214| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1223| <<kvm_arm_timer_read>> val = timer_get_cval(timer);
+ *   - arch/arm64/kvm/trace_arm.h|244| <<__field>> __entry->cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|266| <<__field>> __entry->cval = timer_get_cval(ctx);
+ */
 u64 timer_get_cval(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -106,6 +217,18 @@ u64 timer_get_cval(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|447| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|586| <<timer_save_state>> cval -= timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|670| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|678| <<timer_restore_state>> offset = timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|842| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|1191| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1204| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1250| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 static u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
 	u64 offset = 0;
@@ -113,6 +236,12 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	if (!ctxt)
 		return 0;
 
+	/*
+	 * struct arch_timer_context *ctxt:
+	 * -> struct arch_timer_offset offset;
+	 *    -> u64 *vm_offset;
+	 *    -> u64 *vcpu_offset;
+	 */
 	if (ctxt->offset.vm_offset)
 		offset += *ctxt->offset.vm_offset;
 	if (ctxt->offset.vcpu_offset)
@@ -121,6 +250,13 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	return offset;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|568| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|594| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTP_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|1013| <<kvm_timer_vcpu_reset>> timer_set_ctl(vcpu_get_timer(vcpu, i), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1290| <<kvm_arm_timer_write>> timer_set_ctl(timer, val & ~ARCH_TIMER_CTRL_IT_STAT);
+ */
 static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -143,6 +279,13 @@ static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|569| <<timer_save_state>> timer_set_cval(ctx, read_sysreg_el0(SYS_CNTV_CVAL));
+ *   - arch/arm64/kvm/arch_timer.c|600| <<timer_save_state>> timer_set_cval(ctx, cval);
+ *   - arch/arm64/kvm/arch_timer.c|1286| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ *   - arch/arm64/kvm/arch_timer.c|1294| <<kvm_arm_timer_write>> timer_set_cval(timer, val);
+ */
 static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -165,6 +308,13 @@ static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1083| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1084| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_ptimer(vcpu), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1124| <<kvm_arm_timer_set_reg(KVM_REG_ARM_TIMER_CNT)>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1139| <<kvm_arm_timer_set_reg(KVM_REG_ARM_PTIMER_CNT)>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ */
 static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 {
 	if (!ctxt->offset.vm_offset) {
@@ -175,11 +325,40 @@ static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 	WRITE_ONCE(*ctxt->offset.vm_offset, offset);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|264| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|423| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1042| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1083| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1098| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1162| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1175| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1221| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 u64 kvm_phys_timer_read(void)
 {
 	return timecounter->cc->read(timecounter->cc);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|280| <<kvm_arch_timer_handler>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|629| <<kvm_timer_blocking>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|895| <<kvm_timer_vcpu_load>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|946| <<kvm_timer_vcpu_put>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1004| <<kvm_timer_vcpu_reset>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1253| <<kvm_arm_timer_read_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1314| <<kvm_arm_timer_write_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1619| <<kvm_timer_enable>> get_timer_map(vcpu, &map);
+ *
+ * struct timer_map {
+ *     struct arch_timer_context *direct_vtimer;
+ *     struct arch_timer_context *direct_ptimer;
+ *     struct arch_timer_context *emul_vtimer;
+ *     struct arch_timer_context *emul_ptimer;
+ * };
+ */
 static void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	if (vcpu_has_nv(vcpu)) {
@@ -195,6 +374,9 @@ static void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 			map->emul_ptimer = vcpu_hptimer(vcpu);
 		}
 	} else if (has_vhe()) {
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+		 */
 		map->direct_vtimer = vcpu_vtimer(vcpu);
 		map->direct_ptimer = vcpu_ptimer(vcpu);
 		map->emul_vtimer = NULL;
@@ -215,17 +397,37 @@ static inline bool userspace_irqchip(struct kvm *kvm)
 		unlikely(!irqchip_in_kernel(kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static void soft_timer_start(struct hrtimer *hrt, u64 ns)
 {
 	hrtimer_start(hrt, ktime_add_ns(ktime_get(), ns),
 		      HRTIMER_MODE_ABS_HARD);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|653| <<kvm_timer_unblocking>> soft_timer_cancel(&timer->bg_timer);
+ *   - arch/arm64/kvm/arch_timer.c|962| <<kvm_timer_vcpu_put>> soft_timer_cancel(&map.emul_vtimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|964| <<kvm_timer_vcpu_put>> soft_timer_cancel(&map.emul_ptimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1040| <<kvm_timer_vcpu_reset>> soft_timer_cancel(&map.emul_vtimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1042| <<kvm_timer_vcpu_reset>> soft_timer_cancel(&map.emul_ptimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1317| <<kvm_arm_timer_write_sysreg>> soft_timer_cancel(&timer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1540| <<kvm_timer_vcpu_terminate>> soft_timer_cancel(&timer->bg_timer);
+ */
 static void soft_timer_cancel(struct hrtimer *hrt)
 {
 	hrtimer_cancel(hrt);
 }
 
+/*
+ * 在以下使用kvm_arch_timer_handler():
+ *   - arch/arm64/kvm/arch_timer.c|1408| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_vtimer_irq, kvm_arch_timer_handler, "kvm guest vtimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1432| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_ptimer_irq, kvm_arch_timer_handler, "kvm guest ptimer", kvm_get_running_vcpus());
+ */
 static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 {
 	struct kvm_vcpu *vcpu = *(struct kvm_vcpu **)dev_id;
@@ -258,9 +460,36 @@ static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|342| <<wfit_delay_ns>> return kvm_counter_compute_delta(ctx, val);
+ */
 static u64 kvm_counter_compute_delta(struct arch_timer_context *timer_ctx,
 				     u64 val)
 {
+	/*
+	 * struct kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct arch_timer_vm_data timer_data;
+	 *       -> u64 voffset;
+	 *       -> u64 poffset;
+	 *       -> ppi[NR_KVM_TIMERS];
+	 *
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_cpu_context ctxt;
+	 *       -> struct user_pt_regs regs;
+	 *       -> u64 sys_regs[NR_SYS_REGS];
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *          -> struct kvm_vcpu *vcpu;
+	 *          -> struct arch_timer_offset offset;
+	 *             -> u64 *vm_offset;
+	 *             -> u64 *vcpu_offset;
+	 *       -> struct hrtimer bg_timer;
+	 *       -> bool enabled;
+	 */
 	u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
 
 	if (now < val) {
@@ -276,11 +505,33 @@ static u64 kvm_counter_compute_delta(struct arch_timer_context *timer_ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|359| <<kvm_timer_earliest_exp>> min_delta = min(min_delta, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|412| <<kvm_hrtimer_expire>> ns = kvm_timer_compute_delta(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|1164| <<read_timer_ctl>> if (!kvm_timer_compute_delta(timer))
+ */
 static u64 kvm_timer_compute_delta(struct arch_timer_context *timer_ctx)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|342| <<wfit_delay_ns>> return kvm_counter_compute_delta(ctx, val);
+	 */
 	return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|358| <<kvm_timer_earliest_exp>> if (kvm_timer_irq_can_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_should_fire>> if (!kvm_timer_irq_can_fire(timer_ctx))
+ *   - arch/arm64/kvm/arch_timer.c|522| <<timer_emulate>> if (should_fire || !kvm_timer_irq_can_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|635| <<kvm_timer_blocking>> if (!kvm_timer_irq_can_fire(map.direct_vtimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|636| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.direct_ptimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|637| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.emul_vtimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|638| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.emul_ptimer) &&
+ */
 static bool kvm_timer_irq_can_fire(struct arch_timer_context *timer_ctx)
 {
 	WARN_ON(timer_ctx && timer_ctx->loaded);
@@ -310,6 +561,11 @@ static u64 wfit_delay_ns(struct kvm_vcpu *vcpu)
  * Returns the earliest expiration time in ns among guest timers.
  * Note that it will return 0 if none of timers can fire.
  */
+/*
+ * 在以下使用kvm_timer_earliest_exp():
+ *   - arch/arm64/kvm/arch_timer.c|386| <<kvm_bg_timer_expire>> ns = kvm_timer_earliest_exp(vcpu);
+ *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static u64 kvm_timer_earliest_exp(struct kvm_vcpu *vcpu)
 {
 	u64 min_delta = ULLONG_MAX;
@@ -357,6 +613,15 @@ static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
 static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_context *ctx;
@@ -383,6 +648,19 @@ static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|287| <<kvm_arch_timer_handler>> if (kvm_timer_should_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|481| <<kvm_timer_update_run>> if (kvm_timer_should_fire(vtimer))
+ *   - arch/arm64/kvm/arch_timer.c|483| <<kvm_timer_update_run>> if (kvm_timer_should_fire(ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|508| <<timer_emulate>> bool should_fire = kvm_timer_should_fire(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|727| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|747| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|934| <<kvm_timer_should_notify_user>> return kvm_timer_should_fire(vtimer) != vlevel ||
+ *   - arch/arm64/kvm/arch_timer.c|935| <<kvm_timer_should_notify_user>> kvm_timer_should_fire(ptimer) != plevel;
+ *   - arch/arm64/kvm/arch_timer.c|979| <<unmask_vtimer_irq_user>> if (!kvm_timer_should_fire(vtimer)) {
+ *   - arch/arm64/kvm/arch_timer.c|1588| <<kvm_arch_timer_get_input_level>> return kvm_timer_should_fire(ctx);
+ */
 static bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx)
 {
 	enum kvm_arch_timers index;
@@ -419,6 +697,18 @@ static bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx)
 	if (!kvm_timer_irq_can_fire(timer_ctx))
 		return false;
 
+	/*
+	 * 在以下使用timer_get_offset():
+	 *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|447| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|586| <<timer_save_state>> cval -= timer_get_offset(ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|670| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|678| <<timer_restore_state>> offset = timer_get_offset(ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|842| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+	 *   - arch/arm64/kvm/arch_timer.c|1191| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1204| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1250| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+	 */
 	cval = timer_get_cval(timer_ctx);
 	now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
 
@@ -448,16 +738,44 @@ void kvm_timer_update_run(struct kvm_vcpu *vcpu)
 		regs->device_irq_level |= KVM_ARM_DEV_EL1_PTIMER;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_arch_timer_handler>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_hrtimer_expire>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|513| <<timer_emulate>> kvm_timer_update_irq(ctx->vcpu, should_fire, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|727| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|747| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|980| <<unmask_vtimer_irq_user>> kvm_timer_update_irq(vcpu, false, vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1029| <<kvm_timer_vcpu_reset>> kvm_timer_update_irq(vcpu, false,
+ *   - arch/arm64/kvm/trace_arm.h|181| <<__field>> TRACE_EVENT(kvm_timer_update_irq,
+ *
+ * 核心思想是调用kvm_vgic_inject_irq()
+ */
 static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				 struct arch_timer_context *timer_ctx)
 {
 	int ret;
 
+	/*
+	 * struct arch_timer_context *timer_ctx:
+	 *   // Output level of the timer IRQ
+	 *   struct {
+	 *       bool                    level;
+	 *   } irq;
+	 */
 	timer_ctx->irq.level = new_level;
 	trace_kvm_timer_update_irq(vcpu->vcpu_id, timer_irq(timer_ctx),
 				   timer_ctx->irq.level);
 
 	if (!userspace_irqchip(vcpu->kvm)) {
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/arch_timer.c|497| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/arm.c|1172| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/arm.c|1180| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+		 *   - arch/arm64/kvm/pmu-emul.c|346| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+		 */
 		ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
 					  timer_irq(timer_ctx),
 					  timer_ctx->irq.level,
@@ -466,6 +784,12 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1137| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1139| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1542| <<kvm_arm_timer_write_sysreg>> timer_emulate(timer);
+ */
 /* Only called for a fully emulated timer */
 static void timer_emulate(struct arch_timer_context *ctx)
 {
@@ -489,17 +813,43 @@ static void timer_emulate(struct arch_timer_context *ctx)
 	soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|544| <<timer_save_state>> set_cntvoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|629| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void set_cntvoff(u64 cntvoff)
 {
 	kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|565| <<timer_save_state>> set_cntpoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|650| <<timer_restore_state>> set_cntpoff(offset);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void set_cntpoff(u64 cntpoff)
 {
 	if (has_cntpoff())
 		write_sysreg_s(cntpoff, SYS_CNTPOFF_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1135| <<kvm_timer_vcpu_put>> timer_save_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1137| <<kvm_timer_vcpu_put>> timer_save_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1447| <<kvm_arm_timer_read_sysreg>> timer_save_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1509| <<kvm_arm_timer_write_sysreg>> timer_save_state(timer);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void timer_save_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -519,6 +869,10 @@ static void timer_save_state(struct arch_timer_context *ctx)
 
 	case TIMER_VTIMER:
 	case TIMER_HVTIMER:
+		/*
+		 * 因为寄存器像是x86的ibrs一样是共享的
+		 * 所以在退出guest mode的时候要保存
+		 */
 		timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
 		timer_set_cval(ctx, read_sysreg_el0(SYS_CNTV_CVAL));
 
@@ -575,8 +929,27 @@ static void timer_save_state(struct arch_timer_context *ctx)
  * thread is removed from its waitqueue and made runnable when there's a timer
  * interrupt to handle.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1190| <<kvm_timer_vcpu_put>> kvm_timer_blocking(vcpu);
+ */
 static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct arch_timer_cpu {
+	 *     struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *
+	 *     // Background timer used when the guest is not running
+	 *     struct hrtimer                  bg_timer;
+	 *
+	 *     // Is the timer enabled
+	 *     bool                    enabled;
+	 * };
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 */
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
@@ -593,6 +966,13 @@ static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 	    !vcpu_has_wfit_active(vcpu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+	 *
+	 * Background timer used when the guest is not running
+	 */
 	/*
 	 * At least one guest time will expire. Schedule a background timer.
 	 * Set the earliest expiration time among the guest timers.
@@ -600,6 +980,10 @@ static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 	soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1110| <<kvm_timer_vcpu_load>> kvm_timer_unblocking(vcpu);
+ */
 static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -607,6 +991,16 @@ static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|857| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|859| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1196| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1247| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void timer_restore_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -653,13 +1047,35 @@ static void timer_restore_state(struct arch_timer_context *ctx)
 	local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|933| <<kvm_timer_vcpu_load_gic>> set_timer_irq_phys_active(ctx, phys_active);
+ *   - arch/arm64/kvm/arch_timer.c|1205| <<unmask_vtimer_irq_user>> set_timer_irq_phys_active(vtimer, false);
+ */
 static inline void set_timer_irq_phys_active(struct arch_timer_context *ctx, bool active)
 {
 	int r;
+	/*
+	 * irq_set_irqchip_state - set the state of a forwarded interrupt.
+	 *      @irq: Interrupt line that is forwarded to a VM
+	 *      @which: State to be restored (one of IRQCHIP_STATE_*)
+	 *      @val: Value corresponding to @which
+	 *
+	 *      This call sets the internal irqchip state of an interrupt,
+	 *      depending on the value of @which.
+	 *
+	 *      This function should be called with migration disabled if the
+	 *      interrupt controller has per-cpu registers.
+	 */
 	r = irq_set_irqchip_state(ctx->host_timer_irq, IRQCHIP_STATE_ACTIVE, active);
 	WARN_ON(r);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1192| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_gic(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1194| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_gic(map.direct_ptimer);
+ */
 static void kvm_timer_vcpu_load_gic(struct arch_timer_context *ctx)
 {
 	struct kvm_vcpu *vcpu = ctx->vcpu;
@@ -681,6 +1097,10 @@ static void kvm_timer_vcpu_load_gic(struct arch_timer_context *ctx)
 	set_timer_irq_phys_active(ctx, phys_active);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1196| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_nogic(vcpu);
+ */
 static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -709,6 +1129,11 @@ static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 		enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1168| <<timer_set_traps>> assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
+ *   - arch/arm64/kvm/arch_timer.c|1169| <<timer_set_traps>> assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
+ */
 /* If _pred is true, set bit in _set, otherwise set it in _clr */
 #define assign_clear_set_bit(_pred, _bit, _clr, _set)			\
 	do {								\
@@ -718,6 +1143,10 @@ static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 			(_clr) |= (_bit);				\
 	} while (0)
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1190| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_nested_switch(vcpu, &map);
+ */
 static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 					      struct timer_map *map)
 {
@@ -769,6 +1198,10 @@ static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1230| <<kvm_timer_vcpu_load>> timer_set_traps(vcpu, &map);
+ */
 static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	bool tpt, tpc;
@@ -827,12 +1260,26 @@ static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 	assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
 	assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
 
+	/*
+	 * 注释
+	 * Controls the generation of an event stream from the physical
+	 * counter, and access from EL1 to the physical counter and the EL1
+	 * physical timer.
+	 * 控制哪些timer的counter access会trap!
+	 */
 	/* This only happens on VHE, so use the CNTHCTL_EL2 accessor. */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|448| <<kvm_arch_vcpu_load>> kvm_timer_vcpu_load(vcpu);
+ */
 void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 返回&(v)->arch.timer_cpu
+	 */
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
@@ -852,8 +1299,32 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 		kvm_timer_vcpu_load_nogic(vcpu);
 	}
 
+	/*
+	 * 只在此处调用
+	 */
 	kvm_timer_unblocking(vcpu);
 
+	/*
+	 * vcpu_vtimer(vcpu);
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *          -> struct arch_timer_offset offset;
+	 *             -> u64 *vm_offset;
+	 *             -> u64 *vcpu_offset;
+	 *
+	 *
+	 * struct timer_map {
+	 *     struct arch_timer_context *direct_vtimer;
+	 *     struct arch_timer_context *direct_ptimer;
+	 *     struct arch_timer_context *emul_vtimer;
+	 *     struct arch_timer_context *emul_ptimer;
+	 * };
+	 *
+	 * direct_vtimer: 比如(&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+	 */
 	timer_restore_state(map.direct_vtimer);
 	if (map.direct_ptimer)
 		timer_restore_state(map.direct_ptimer);
@@ -865,6 +1336,12 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 	timer_set_traps(vcpu, &map);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|850| <<kvm_vcpu_exit_request>> if (kvm_timer_should_notify_user(vcpu) ||
+ *
+ * 如果irqchip_in_kernel就返回false
+ */
 bool kvm_timer_should_notify_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -882,6 +1359,10 @@ bool kvm_timer_should_notify_user(struct kvm_vcpu *vcpu)
 	       kvm_timer_should_fire(ptimer) != plevel;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|475| <<kvm_arch_vcpu_put>> kvm_timer_vcpu_put(vcpu);
+ */
 void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -919,6 +1400,12 @@ void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
  * timer and if so, unmask the timer irq signal on the host interrupt
  * controller to ensure that we see future timer signals.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1312| <<kvm_timer_sync_user>> unmask_vtimer_irq_user(vcpu);
+ *
+ * 只在irqchip_in_kernel为false的时候调用
+ */
 static void unmask_vtimer_irq_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -932,6 +1419,11 @@ static void unmask_vtimer_irq_user(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|969| <<kvm_arch_vcpu_ioctl_run>> kvm_timer_sync_user(vcpu);
+ *   - arch/arm64/kvm/arm.c|1015| <<kvm_arch_vcpu_ioctl_run>> kvm_timer_sync_user(vcpu);
+ */
 void kvm_timer_sync_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -943,11 +1435,23 @@ void kvm_timer_sync_user(struct kvm_vcpu *vcpu)
 		unmask_vtimer_irq_user(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/reset.c|302| <<kvm_reset_vcpu>> ret = kvm_timer_vcpu_reset(vcpu);
+ */
 int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
+	/*
+	 * struct timer_map {
+	 *     struct arch_timer_context *direct_vtimer;
+	 *     struct arch_timer_context *direct_ptimer;
+	 *     struct arch_timer_context *emul_vtimer;
+	 *     struct arch_timer_context *emul_ptimer;
+	 * };
+	 */
 	get_timer_map(vcpu, &map);
 
 	/*
@@ -977,6 +1481,15 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 					     vcpu_get_timer(vcpu, i));
 
 		if (irqchip_in_kernel(vcpu->kvm)) {
+			/*
+			 * kvm_vgic_reset_mapped_irq - Reset a mapped IRQ
+			 * @vcpu: The VCPU pointer
+			 * @vintid: The INTID of the interrupt
+			 *
+			 * Reset the active and pending states of a mapped interrupt.  Kernel
+			 * subsystems injecting mapped interrupts should reset their interrupt lines
+			 * when we are doing a reset of the VM.
+			 */
 			kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_vtimer));
 			if (map.direct_ptimer)
 				kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_ptimer));
@@ -991,6 +1504,10 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1448| <<kvm_timer_vcpu_init>> timer_context_init(vcpu, i);
+ */
 static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 {
 	struct arch_timer_context *ctxt = vcpu_get_timer(vcpu, timerid);
@@ -1003,6 +1520,15 @@ static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 	else
 		ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
 
+	/*
+	 * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+	 * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+	 * 然后再次进入guest时,就有中断了.
+	 *
+	 * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+	 * timer到期的时候(还在host mode)唤醒guest并inject irq.
+	 * KVM注册了一个hrtimer来处理这种情况.
+	 */
 	hrtimer_init(&ctxt->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
 	ctxt->hrtimer.function = kvm_hrtimer_expire;
 
@@ -1018,6 +1544,10 @@ static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|380| <<kvm_arch_vcpu_create>> kvm_timer_vcpu_init(vcpu);
+ */
 void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1035,12 +1565,20 @@ void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 	timer->bg_timer.function = kvm_bg_timer_expire;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|169| <<kvm_arch_init_vm>> kvm_timer_init_vm(kvm);
+ */
 void kvm_timer_init_vm(struct kvm *kvm)
 {
 	for (int i = 0; i < NR_KVM_TIMERS; i++)
 		kvm->arch.timer_data.ppi[i] = default_ppi[i];
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1906| <<kvm_arch_hardware_enable>> kvm_timer_cpu_up();
+ */
 void kvm_timer_cpu_up(void)
 {
 	enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
@@ -1048,6 +1586,10 @@ void kvm_timer_cpu_up(void)
 		enable_percpu_irq(host_ptimer_irq, host_ptimer_irq_flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1915| <<kvm_arch_hardware_disable>> kvm_timer_cpu_down();
+ */
 void kvm_timer_cpu_down(void)
 {
 	disable_percpu_irq(host_vtimer_irq);
@@ -1055,6 +1597,10 @@ void kvm_timer_cpu_down(void)
 		disable_percpu_irq(host_ptimer_irq);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|639| <<set_timer_reg>> return kvm_arm_timer_set_reg(vcpu, reg->id, val);
+ */
 int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 {
 	struct arch_timer_context *timer;
@@ -1098,6 +1644,10 @@ int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1588| <<kvm_arm_timer_read(TIMER_REG_CTL)>> val = read_timer_ctl(timer);
+ */
 static u64 read_timer_ctl(struct arch_timer_context *timer)
 {
 	/*
@@ -1114,6 +1664,11 @@ static u64 read_timer_ctl(struct arch_timer_context *timer)
 	return ctl;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|647| <<get_timer_reg>> val = kvm_arm_timer_get_reg(vcpu, reg->id);
+ *   - arch/arm64/kvm/handle_exit.c|130| <<kvm_handle_wfx>> now = kvm_arm_timer_get_reg(vcpu, KVM_REG_ARM_TIMER_CNT);
+ */
 u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 {
 	switch (regid) {
@@ -1139,6 +1694,17 @@ u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 	return (u64)-1;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1174| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1177| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1180| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1183| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1186| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1189| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1242| <<kvm_arm_timer_read_sysreg>> return kvm_arm_timer_read(vcpu, timer, treg);
+ *   - arch/arm64/kvm/arch_timer.c|1247| <<kvm_arm_timer_read_sysreg>> val = kvm_arm_timer_read(vcpu, timer, treg);
+ */
 static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      struct arch_timer_context *timer,
 			      enum kvm_arch_timer_regs treg)
@@ -1160,6 +1726,28 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 		break;
 
 	case TIMER_REG_CNT:
+		/*
+		 * struct kvm:
+		 * -> struct kvm_arch arch;
+		 *    -> struct arch_timer_vm_data timer_data;
+		 *       -> u64 voffset;
+		 *       -> u64 poffset;
+		 *       -> ppi[NR_KVM_TIMERS];
+		 *
+		 * struct kvm_vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct kvm_cpu_context ctxt;
+		 *       -> struct user_pt_regs regs;
+		 *       -> u64 sys_regs[NR_SYS_REGS];
+		 *    -> struct arch_timer_cpu timer_cpu;
+		 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+		 *          -> struct kvm_vcpu *vcpu;
+		 *          -> struct arch_timer_offset offset;
+		 *             -> u64 *vm_offset;
+		 *             -> u64 *vcpu_offset;
+		 *       -> struct hrtimer bg_timer;
+		 *       -> bool enabled;
+		 */
 		val = kvm_phys_timer_read() - timer_get_offset(timer);
 		break;
 
@@ -1174,6 +1762,22 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1206| <<access_arch_timer>> p->regval = kvm_arm_timer_read_sysreg(vcpu, tmr, treg);
+ */
 u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 			      enum kvm_arch_timers tmr,
 			      enum kvm_arch_timer_regs treg)
@@ -1199,6 +1803,15 @@ u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1118| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1129| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1133| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1144| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1294| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ *   - arch/arm64/kvm/arch_timer.c|1299| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ */
 static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 				struct arch_timer_context *timer,
 				enum kvm_arch_timer_regs treg,
@@ -1226,6 +1839,22 @@ static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1204| <<access_arch_timer>> kvm_arm_timer_write_sysreg(vcpu, tmr, treg, p->regval);
+ */
 void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 				enum kvm_arch_timers tmr,
 				enum kvm_arch_timer_regs treg,
@@ -1249,6 +1878,9 @@ void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * struct irq_chip timer_chip.irq_set_vcpu_affinity = timer_irq_set_vcpu_affinity()
+ */
 static int timer_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu)
 {
 	if (vcpu)
@@ -1259,6 +1891,9 @@ static int timer_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu)
 	return 0;
 }
 
+/*
+ * struct irq_chip timer_chip.irq_set_irqchip_state = timer_irq_set_irqchip_state()
+ */
 static int timer_irq_set_irqchip_state(struct irq_data *d,
 				       enum irqchip_irq_state which, bool val)
 {
@@ -1273,12 +1908,18 @@ static int timer_irq_set_irqchip_state(struct irq_data *d,
 	return 0;
 }
 
+/*
+ * struct irq_chip timer_chip.irq_eoi = timer_irq_eoi()
+ */
 static void timer_irq_eoi(struct irq_data *d)
 {
 	if (!irqd_is_forwarded_to_vcpu(d))
 		irq_chip_eoi_parent(d);
 }
 
+/*
+ * struct irq_chip timer_chip.irq_ack = timer_irq_ack()
+ */
 static void timer_irq_ack(struct irq_data *d)
 {
 	d = d->parent_data;
@@ -1286,6 +1927,10 @@ static void timer_irq_ack(struct irq_data *d)
 		d->chip->irq_ack(d);
 }
 
+/*
+ * 在以下使用timer_chip:
+ *   - arch/arm64/kvm/arch_timer.c|1383| <<timer_irq_domain_alloc>> return irq_domain_set_hwirq_and_chip(domain, virq, hwirq, &timer_chip, NULL);
+ */
 static struct irq_chip timer_chip = {
 	.name			= "KVM",
 	.irq_ack		= timer_irq_ack,
@@ -1311,11 +1956,20 @@ static void timer_irq_domain_free(struct irq_domain *domain, unsigned int virq,
 {
 }
 
+/*
+ * 在以下使用timer_domain_ops:
+ *   - arch/arm64/kvm/arch_timer.c|1853| <<kvm_irq_init>> domain = irq_domain_create_hierarchy(data->domain, 0, NR_KVM_TIMERS, fwnode, &timer_domain_ops, NULL);
+ */
 static const struct irq_domain_ops timer_domain_ops = {
 	.alloc	= timer_irq_domain_alloc,
 	.free	= timer_irq_domain_free,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1839| <<kvm_irq_init>> kvm_irq_fixup_flags(host_vtimer_irq, &host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1866| <<kvm_irq_init>> kvm_irq_fixup_flags(host_ptimer_irq, &host_ptimer_irq_flags);
+ */
 static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 {
 	*flags = irq_get_trigger_type(virq);
@@ -1326,6 +1980,10 @@ static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1906| <<kvm_timer_hyp_init>> err = kvm_irq_init(info);
+ */
 static int kvm_irq_init(struct arch_timer_kvm_info *info)
 {
 	struct irq_domain *domain = NULL;
@@ -1374,11 +2032,28 @@ static int kvm_irq_init(struct arch_timer_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2044| <<init_subsystems>> err = kvm_timer_hyp_init(vgic_present);
+ */
 int __init kvm_timer_hyp_init(bool has_gic)
 {
 	struct arch_timer_kvm_info *info;
 	int err;
 
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 *
+	 * 在以下使用arch_timer_kvm_info:
+	 *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+	 *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+	 *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+	 *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+	 */
 	info = arch_timer_get_kvm_info();
 	timecounter = &info->timecounter;
 
@@ -1452,6 +2127,10 @@ int __init kvm_timer_hyp_init(bool has_gic)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|407| <<kvm_arch_vcpu_destroy>> kvm_timer_vcpu_terminate(vcpu);
+ */
 void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1459,6 +2138,10 @@ void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|2053| <<kvm_timer_enable>> if (!timer_irqs_are_valid(vcpu)) {
+ */
 static bool timer_irqs_are_valid(struct kvm_vcpu *vcpu)
 {
 	u32 ppis = 0;
@@ -1492,6 +2175,9 @@ static bool timer_irqs_are_valid(struct kvm_vcpu *vcpu)
 	return valid;
 }
 
+/*
+ * struct irq_ops arch_timer_irq_ops..get_input_level = kvm_arch_timer_get_input_level,
+ */
 static bool kvm_arch_timer_get_input_level(int vintid)
 {
 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
@@ -1513,6 +2199,10 @@ static bool kvm_arch_timer_get_input_level(int vintid)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|615| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_timer_enable(vcpu);
+ */
 int kvm_timer_enable(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1537,6 +2227,17 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 
 	get_timer_map(vcpu, &map);
 
+	/*
+	 * 在以下使用arch_timer_irq_ops:
+	 *   - arch/arm64/kvm/arch_timer.c|800| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq,
+	 *                                                                                      timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|805| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq,
+	 *                                                                                      timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|1437| <<kvm_irq_init>> arch_timer_irq_ops.flags |= VGIC_IRQ_SW_RESAMPLE;
+	 *   - arch/arm64/kvm/arch_timer.c|1624| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|1632| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+	 */
+
 	ret = kvm_vgic_map_phys_irq(vcpu,
 				    map.direct_vtimer->host_timer_irq,
 				    timer_irq(map.direct_vtimer),
@@ -1559,6 +2260,10 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1864| <<cpu_hyp_init_features>> kvm_timer_init_vhe();
+ */
 /* If we have CNTPOFF, permanently set ECV to enable it */
 void kvm_timer_init_vhe(void)
 {
@@ -1658,6 +2363,10 @@ int kvm_arm_timer_has_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1619| <<kvm_arch_vm_ioctl(KVM_ARM_SET_COUNTER_OFFSET)>> return kvm_vm_ioctl_set_counter_offset(kvm, &offset);
+ */
 int kvm_vm_ioctl_set_counter_offset(struct kvm *kvm,
 				    struct kvm_arm_counter_offset *offset)
 {
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index d1cb298a5..cbdb300a0 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -420,6 +420,11 @@ void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)
 
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5985| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_s2_mmu *mmu;
@@ -704,6 +709,12 @@ static void kvm_vcpu_sleep(struct kvm_vcpu *vcpu)
  * the vCPU is runnable.  The vCPU may or may not be scheduled out, depending
  * on when a wake event arrives, e.g. there may already be a pending wake event.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|739| <<kvm_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|147| <<kvm_handle_wfx>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/psci.c|49| <<kvm_psci_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ */
 void kvm_vcpu_wfi(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -773,6 +784,10 @@ static int kvm_vcpu_suspend(struct kvm_vcpu *vcpu)
  *	   < 0 if we should exit to userspace, where the return value indicates
  *	   an error
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|936| <<kvm_arch_vcpu_ioctl_run>> ret = check_vcpu_requests(vcpu);
+ */
 static int check_vcpu_requests(struct kvm_vcpu *vcpu)
 {
 	if (kvm_request_pending(vcpu)) {
@@ -895,6 +910,10 @@ static int noinstr kvm_arm_vcpu_enter_exit(struct kvm_vcpu *vcpu)
  * return with return value 0 and with the kvm_run structure filled in with the
  * required data for the requested emulation.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4128| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_run *run = vcpu->run;
@@ -906,6 +925,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 			return ret;
 	}
 
+	/*
+	 * 除了调度
+	 * 核心是kvm_arch_vcpu_load()
+	 */
 	vcpu_load(vcpu);
 
 	if (run->immediate_exit) {
@@ -913,6 +936,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arm.c|922| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/riscv/kvm/vcpu.c|1173| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/s390/kvm/kvm-s390.c|4992| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 *   - arch/x86/kvm/x86.c|11710| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+	 */
 	kvm_sigset_activate(vcpu);
 
 	ret = 1;
@@ -922,6 +954,14 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		/*
 		 * Check conditions before entering the guest
 		 */
+		/*
+		 * 注释.
+		 * xfer_to_guest_mode_handle_work - Check and handle pending work which needs
+		 *                                  to be handled before going to guest mode
+		 * @vcpu:       Pointer to current's VCPU data
+		 *
+		 * Returns: 0 or an error code
+		 */
 		ret = xfer_to_guest_mode_handle_work(vcpu);
 		if (!ret)
 			ret = 1;
@@ -1653,6 +1693,16 @@ void unlock_all_vcpus(struct kvm *kvm)
 	unlock_vcpus(kvm, atomic_read(&kvm->online_vcpus) - 1);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1756| <<kvm_vm_ioctl_set_counter_offset>> if (lock_all_vcpus(kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|91| <<kvm_vgic_create>> if (!lock_all_vcpus(kvm))
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2064| <<vgic_its_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2772| <<vgic_its_ctrl>> if (!lock_all_vcpus(kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|265| <<vgic_set_common_attr>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|385| <<vgic_v2_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|546| <<vgic_v3_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ */
 /* Returns true if all vcpus were locked, false otherwise */
 bool lock_all_vcpus(struct kvm *kvm)
 {
@@ -2360,6 +2410,12 @@ static int __init init_hyp_mode(void)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/psci.c|72| <<kvm_psci_vcpu_on>> vcpu = kvm_mpidr_to_vcpu(kvm, cpu_id);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|491| <<vgic_v3_parse_attr>> reg_attr->vcpu = kvm_mpidr_to_vcpu(dev->kvm, mpidr_reg);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|232| <<vgic_mmio_write_irouter>> irq->target_vcpu = kvm_mpidr_to_vcpu(vcpu->kvm, irq->mpidr);
+ */
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr)
 {
 	struct kvm_vcpu *vcpu;
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 20280a523..3eaf2d3d9 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -759,6 +759,10 @@ int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
 	return kvm_arm_copy_sys_reg_indices(vcpu, uindices);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1443| <<kvm_arch_vcpu_ioctl(KVM_GET_ONE_REG)>> r = kvm_arm_get_reg(vcpu, &reg);
+ */
 int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	/* We currently use nothing arch-specific in upper 32 bits */
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 6dcd6604b..a2c1029c1 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -111,6 +111,9 @@ static int handle_no_fpsimd(struct kvm_vcpu *vcpu)
  *
  * WF{I,E}T can immediately return if the deadline has already expired.
  */
+/*
+ * static exit_handle_fn arm_exit_handlers[ESR_ELx_EC_WFx] = kvm_handle_wfx()
+ */
 static int kvm_handle_wfx(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
@@ -226,6 +229,10 @@ static int kvm_handle_eret(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用arm_exit_handlers[]:
+ *   - arch/arm64/kvm/handle_exit.c|261| <<kvm_get_exit_handler>> return arm_exit_handlers[esr_ec];
+ */
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -253,6 +260,10 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|284| <<handle_trap_exceptions>> exit_handler = kvm_get_exit_handler(vcpu);
+ */
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
@@ -267,6 +278,10 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
  * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
  * emulation first.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|315| <<handle_exit>> return handle_trap_exceptions(vcpu);
+ */
 static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
 {
 	int handled;
@@ -292,6 +307,10 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1065| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
@@ -335,6 +354,10 @@ int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1042| <<kvm_arch_vcpu_ioctl_run>> handle_exit_early(vcpu, ret);
+ */
 /* For exit types that need handling before we can be preempted */
 void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 {
@@ -356,6 +379,10 @@ void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 		kvm_handle_guest_serror(vcpu, kvm_vcpu_get_esr(vcpu));
 }
 
+/*
+ * 在以下使用nvhe_hyp_panic_handler():
+ *   - arch/arm64/kernel/image-vars.h|62| <<global>> KVM_NVHE_ALIAS(nvhe_hyp_panic_handler);
+ */
 void __noreturn __cold nvhe_hyp_panic_handler(u64 esr, u64 spsr,
 					      u64 elr_virt, u64 elr_phys,
 					      u64 par, uintptr_t vcpu,
diff --git a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
index bb6b571ec..4306880c5 100644
--- a/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
+++ b/arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h
@@ -88,6 +88,10 @@ static inline void __sysreg_save_el2_return_state(struct kvm_cpu_context *ctxt)
 
 static inline void __sysreg_restore_common_state(struct kvm_cpu_context *ctxt)
 {
+	/*
+	 * MDSCR_EL1 is a 32-bit register, and is part of the Debug registers
+	 * functional group.
+	 */
 	write_sysreg(ctxt_sys_reg(ctxt, MDSCR_EL1),  mdscr_el1);
 }
 
@@ -97,6 +101,20 @@ static inline void __sysreg_restore_user_state(struct kvm_cpu_context *ctxt)
 	write_sysreg(ctxt_sys_reg(ctxt, TPIDRRO_EL0),	tpidrro_el0);
 }
 
+/*
+ * vcpu_load() or kvm_sched_in()
+ * -> kvm_arch_vcpu_load()
+ *    -> kvm_vcpu_load_sysregs_vhe()
+ *       -> __sysreg32_restore_state(vcpu);
+ *       -> __sysreg_restore_user_state(guest_ctxt);
+ *       -> __sysreg_restore_el1_state(guest_ctxt);
+ *          -> write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1), vmpidr_el2);
+ *          -> write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR0_EL1), SYS_TTBR0);
+ *          -> write_sysreg_el1(ctxt_sys_reg(ctxt, TTBR1_EL1), SYS_TTBR1);
+ *
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/sysreg-sr.c|96| <<kvm_vcpu_load_sysregs_vhe>> __sysreg_restore_el1_state(guest_ctxt);
+ */
 static inline void __sysreg_restore_el1_state(struct kvm_cpu_context *ctxt)
 {
 	write_sysreg(ctxt_sys_reg(ctxt, MPIDR_EL1),	vmpidr_el2);
diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 9d7034412..2f298b14d 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
@@ -407,6 +407,10 @@ static bool range_is_memory(u64 start, u64 end)
 	return is_in_mem_range(end - 1, &r);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|477| <<host_stage2_idmap_locked>> return host_stage2_try(__host_stage2_idmap, addr, addr + size, prot);
+ */
 static inline int __host_stage2_idmap(u64 start, u64 end,
 				      enum kvm_pgtable_prot prot)
 {
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index f7a93ef29..cf8e9c779 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -990,6 +990,15 @@ static int stage2_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|413| <<__host_stage2_idmap>> return kvm_pgtable_stage2_map(&host_mmu.pgt, start, end - start, start,
+ *   - arch/arm64/kvm/mmu.c|1026| <<kvm_phys_addr_ioremap>> ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
+ *   - arch/arm64/kvm/mmu.c|1530| <<user_mem_abort>> ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
+ *   - arch/arm64/kvm/mmu.c|1754| <<kvm_set_spte_gfn>> kvm_pgtable_stage2_map(kvm->arch.mmu.pgt, range->start << PAGE_SHIFT,
+ *
+ * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
+ */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
 			   void *mc, enum kvm_pgtable_walk_flags flags)
@@ -1245,6 +1254,10 @@ bool kvm_pgtable_stage2_test_clear_young(struct kvm_pgtable *pgt, u64 addr,
 	return data.young;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1538| <<user_mem_abort>> ret = kvm_pgtable_stage2_relax_perms(pgt, fault_ipa, prot);
+ */
 int kvm_pgtable_stage2_relax_perms(struct kvm_pgtable *pgt, u64 addr,
 				   enum kvm_pgtable_prot prot)
 {
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..c736dc247 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -58,6 +58,15 @@ static u64 __gic_v3_get_lr(unsigned int lr)
 	unreachable();
 }
 
+/*
+ * 注释
+ * In GICv3, the hypervisor uses the System registers to present LPIs to a
+ * virtualized system. A virtual LPI (vLPI) is generated when the hypervisor
+ * writes a vINTID corresponding to the LPI range, to a List register, in
+ * this case, the vINTID that has a value greater than 8191. As an LPI does
+ * not have an active state, it is not possible to associate a virtual LPI
+ * with a physical interrupt.
+ */
 static void __gic_v3_set_lr(u64 val, int lr)
 {
 	switch (lr & 0xf) {
@@ -220,6 +229,22 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 
 		write_gicreg(cpu_if->vgic_hcr & ~ICH_HCR_EN, ICH_HCR_EL2);
 
+		/*
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct vgic_cpu vgic_cpu;
+		 *       -> struct vgic_v3_cpu_if vgic_v3;
+		 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+		 *
+		 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+		 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+		 */
 		for (i = 0; i < used_lrs; i++) {
 			if (elrsr & (1 << i))
 				cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
@@ -231,6 +256,49 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * The virtual GIC works by holding a prioritized list of pending virtual
+ * interrupts for each PE. In GICv3 this list is compiled in software and a
+ * number of the top entries are held in List registers in hardware. For LPIs,
+ * this list can be compiled using tables for each vPE. These tables are
+ * controlled by the GICR_* registers.
+ *
+ * A hypervisor uses a System register interface that is accessible at EL2 to
+ * switch context and to control multiple VMs. The context held in the ICH_*
+ * System registers is the context for the scheduled vPE. A vPE is scheduled
+ *
+ * when:
+ * x ICH_HCR_EL2.En == 1.
+ * x HCR_EL2.FMO == 1, when virtualizing Group 0 interrupts.
+ * x HCR_EL2.IMO == 1, when virtualizing Group 1 interrupts.
+ * x The PE is executing at EL1 and either:
+ *    - SCR_EL3.NS == 1.
+ *    — SCR_EL3.EEL2 == 1.
+ *
+ * When a vPE is scheduled, the ICH_*_EL2 registers affect software executing at EL1.
+ *
+ * The ICH_*_EL2 registers control and maintain a vPE as follows:
+ * x ICH_HCR_EL2 is used for the top-level configuration and control of virtual interrupts.
+ * x Information about the implementation, such as the size of the supported
+ * virtual INTIDs and the number of levels of prioritization is read from
+ * ICH_VTR_EL2.
+ * x A hypervisor can monitor and provide context for ICV_CTLR_EL1 using ICH_VMCR_EL2.
+ * x A set of List registers, ICH_LR<n>_EL2, are used by the hypervisor to
+ * forward a queue of pending interrupts to the PE, see Usage model for the
+ * List registers on page 6-155. The status of free locations in ICH_LR<n>_EL2
+ * is held in ICH_ELRSR_EL2.
+ * x The end of interrupt status for the List registers is held in ICH_EISR_EL2.
+ * x The VM maintenance interrupt status is held in ICH_MISR_EL2.
+ * x The active priority status is held in:
+ *   - ICH_AP0R<n>_EL2, where n = 0-3.
+ *   - ICH_AP1R<n>_EL2, where n = 0-3.
+ */
+
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|129| <<__hyp_vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic.c|969| <<vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ */
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
@@ -239,6 +307,25 @@ void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	if (used_lrs || cpu_if->its_vpe.its_vm) {
 		write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
 
+		/*
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct vgic_cpu vgic_cpu;
+		 *       -> struct vgic_v3_cpu_if vgic_v3;
+		 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+		 *
+		 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+		 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+		 *
+		 * ICH_LR<n>_EL2, Interrupt Controller List Registers
+		 * Provides interrupt context information for the virtual CPU interface.
+		 */
 		for (i = 0; i < used_lrs; i++)
 			__gic_v3_set_lr(cpu_if->vgic_lr[i], i);
 	}
@@ -481,6 +568,11 @@ static int __vgic_v3_get_group(struct kvm_vcpu *vcpu)
 
 #define GICv3_IDLE_PRIORITY	0xff
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|769| <<__vgic_v3_read_iar>> lr = __vgic_v3_highest_priority_lr(vcpu, vmcr, &lr_val);
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|1027| <<__vgic_v3_read_hppir>> lr = __vgic_v3_highest_priority_lr(vcpu, vmcr, &lr_val);
+ */
 static int __vgic_v3_highest_priority_lr(struct kvm_vcpu *vcpu, u32 vmcr,
 					 u64 *lr_val)
 {
@@ -1013,6 +1105,11 @@ static void __vgic_v3_write_ctlr(struct kvm_vcpu *vcpu, u32 vmcr, int rt)
 	write_gicreg(vmcr, ICH_VMCR_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|475| <<kvm_hyp_handle_sysreg>> __vgic_v3_perform_cpuif_access(vcpu) == 1)
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|490| <<kvm_hyp_handle_cp15_32>> __vgic_v3_perform_cpuif_access(vcpu) == 1)
+ */
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu)
 {
 	int rt;
diff --git a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
index b35a178e7..1b7e93028 100644
--- a/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/sysreg-sr.c
@@ -25,6 +25,10 @@
  * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.
  */
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|170| <<__kvm_vcpu_run_vhe>> sysreg_save_host_state_vhe(host_ctxt);
+ */
 void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt)
 {
 	__sysreg_save_common_state(ctxt);
@@ -62,6 +66,10 @@ NOKPROBE_SYMBOL(sysreg_restore_guest_state_vhe);
  * and loading system register state early avoids having to load them on
  * every entry to the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|450| <<kvm_arch_vcpu_load>> kvm_vcpu_load_sysregs_vhe(vcpu);
+ */
 void kvm_vcpu_load_sysregs_vhe(struct kvm_vcpu *vcpu)
 {
 	struct kvm_cpu_context *guest_ctxt = &vcpu->arch.ctxt;
diff --git a/arch/arm64/kvm/hyp/vhe/timer-sr.c b/arch/arm64/kvm/hyp/vhe/timer-sr.c
index 4cda674a8..51aa2d3bd 100644
--- a/arch/arm64/kvm/hyp/vhe/timer-sr.c
+++ b/arch/arm64/kvm/hyp/vhe/timer-sr.c
@@ -6,6 +6,10 @@
 
 #include <asm/kvm_hyp.h>
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|826| <<set_cntvoff>> kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
+ */
 void __kvm_timer_set_cntvoff(u64 cntvoff)
 {
 	write_sysreg(cntvoff, cntvoff_el2);
diff --git a/arch/arm64/kvm/mmio.c b/arch/arm64/kvm/mmio.c
index 3dd38a151..d822016de 100644
--- a/arch/arm64/kvm/mmio.c
+++ b/arch/arm64/kvm/mmio.c
@@ -120,6 +120,10 @@ int kvm_handle_mmio_return(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1699| <<kvm_handle_guest_abort>> ret = io_mem_abort(vcpu, fault_ipa);
+ */
 int io_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
 {
 	struct kvm_run *run = vcpu->run;
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index d3b4feed4..632058cd3 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -267,6 +267,9 @@ static void clean_dcache_guest_page(void *va, size_t size)
 	__clean_dcache_guest_page(va, size);
 }
 
+/*
+ * struct kvm_pgtable_mm_ops kvm_s2_mm_ops.icache_inval_pou = invalidate_icache_guest_page()
+ */
 static void invalidate_icache_guest_page(void *va, size_t size)
 {
 	__invalidate_icache_guest_page(va, size);
@@ -603,6 +606,11 @@ int create_hyp_mappings(void *from, void *to, enum kvm_pgtable_prot prot)
  *
  * Return: 0 on success or negative error code on failure.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2302| <<init_hyp_mode>> err = hyp_alloc_private_va_range(PAGE_SIZE * 2, &hyp_addr);
+ *   - arch/arm64/kvm/mmu.c|659| <<__create_hyp_private_mapping>> ret = hyp_alloc_private_va_range(size, &addr);
+ */
 int hyp_alloc_private_va_range(size_t size, unsigned long *haddr)
 {
 	unsigned long base;
@@ -638,6 +646,11 @@ int hyp_alloc_private_va_range(size_t size, unsigned long *haddr)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|697| <<create_hyp_io_mappings>> ret = __create_hyp_private_mapping(phys_addr, size,
+ *   - arch/arm64/kvm/mmu.c|724| <<create_hyp_exec_mappings>> ret = __create_hyp_private_mapping(phys_addr, size,
+ */
 static int __create_hyp_private_mapping(phys_addr_t phys_addr, size_t size,
 					unsigned long *haddr,
 					enum kvm_pgtable_prot prot)
@@ -999,6 +1012,10 @@ int topup_hyp_memcache(struct kvm_hyp_memcache *mc, unsigned long min_pages)
  * @size:	The size of the mapping
  * @writable:   Whether or not to create a writable mapping
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-v2.c|316| <<vgic_v2_map_resources>> ret = kvm_phys_addr_ioremap(kvm, dist->vgic_cpu_base, kvm_vgic_global_state.vcpu_base, KVM_VGIC_V2_CPU_SIZE, true);
+ */
 int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,
 			  phys_addr_t pa, unsigned long size, bool writable)
 {
@@ -1329,6 +1346,10 @@ static bool kvm_vma_mte_allowed(struct vm_area_struct *vma)
 	return vma->vm_flags & VM_MTE_ALLOWED;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1712| <<kvm_handle_guest_abort>> ret = user_mem_abort(vcpu, fault_ipa, memslot, hva, fault_status);
+ */
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 			  struct kvm_memory_slot *memslot, unsigned long hva,
 			  unsigned long fault_status)
@@ -1519,6 +1540,15 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
 	else if (cpus_have_const_cap(ARM64_HAS_CACHE_DIC))
 		prot |= KVM_PGTABLE_PROT_X;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|413| <<__host_stage2_idmap>> return kvm_pgtable_stage2_map(&host_mmu.pgt, start, end - start, start,
+	 *   - arch/arm64/kvm/mmu.c|1026| <<kvm_phys_addr_ioremap>> ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
+	 *   - arch/arm64/kvm/mmu.c|1530| <<user_mem_abort>> ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
+	 *   - arch/arm64/kvm/mmu.c|1754| <<kvm_set_spte_gfn>> kvm_pgtable_stage2_map(kvm->arch.mmu.pgt, range->start << PAGE_SHIFT,
+	 *
+	 * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
+	 */
 	/*
 	 * Under the premise of getting a FSC_PERM fault, we just need to relax
 	 * permissions only if vma_pagesize equals fault_granule. Otherwise,
@@ -1574,6 +1604,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * 在以下使用kvm_handle_guest_abort():
+ *   - arch/arm64/kvm/handle_exit.c|245| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|246| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long fault_status;
@@ -1719,6 +1754,10 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	return false;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|687| <<kvm_change_spte_gfn>> return kvm_set_spte_gfn(kvm, range);
+ */
 bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	kvm_pfn_t pfn = pte_pfn(range->pte);
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 560650972..9efe94e7a 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -501,6 +501,17 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 					  ARMV8_PMUV3_PERFCTR_CHAIN);
 
 	if (kvm_pmu_overflow_status(vcpu)) {
+		/*
+		 * 在以下使用KVM_REQ_IRQ_PENDING:
+		 *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+		 *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+		 */
 		kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 
 		if (!in_nmi())
diff --git a/arch/arm64/kvm/pvtime.c b/arch/arm64/kvm/pvtime.c
index 4ceabaa4c..df44b075e 100644
--- a/arch/arm64/kvm/pvtime.c
+++ b/arch/arm64/kvm/pvtime.c
@@ -10,6 +10,14 @@
 
 #include <kvm/arm_hypercalls.h>
 
+/*
+ * 在以下使用KVM_REQ_RECORD_STEAL:
+ *   - arch/arm64/kvm/arm.c|454| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_RECORD_STEAL, vcpu);
+ *   - arch/arm64/kvm/arm.c|791| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RECORD_STEAL, vcpu))
+ *
+ * 处理KVM_REQ_RECORD_STEAL:
+ *   - arch/arm64/kvm/arm.c|792| <<check_vcpu_requests>> kvm_update_stolen_time(vcpu);
+ */
 void kvm_update_stolen_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -32,6 +40,10 @@ void kvm_update_stolen_time(struct kvm_vcpu *vcpu)
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hypercalls.c|338| <<kvm_smccc_call_handler>> val[0] = kvm_hypercall_pv_features(vcpu);
+ */
 long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu)
 {
 	u32 feature = smccc_get_arg1(vcpu);
@@ -48,6 +60,10 @@ long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hypercalls.c|341| <<kvm_smccc_call_handler>> gpa = kvm_init_stolen_time(vcpu);
+ */
 gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu)
 {
 	struct pvclock_vcpu_stolen_time init_values = {};
@@ -67,11 +83,22 @@ gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu)
 	return base;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|283| <<kvm_vm_ioctl_check_extension>> r = kvm_arm_pvtime_supported();
+ *   - arch/arm64/kvm/pvtime.c|84| <<kvm_arm_pvtime_set_attr>> if (!kvm_arm_pvtime_supported() ||
+ *   - arch/arm64/kvm/pvtime.c|113| <<kvm_arm_pvtime_get_attr>> if (!kvm_arm_pvtime_supported() ||
+ *   - arch/arm64/kvm/pvtime.c|129| <<kvm_arm_pvtime_has_attr>> if (kvm_arm_pvtime_supported())
+ */
 bool kvm_arm_pvtime_supported(void)
 {
 	return !!sched_info_on();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|973| <<kvm_arm_vcpu_arch_set_attr>> ret = kvm_arm_pvtime_set_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
@@ -104,6 +131,10 @@ int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|996| <<kvm_arm_vcpu_arch_get_attr>> ret = kvm_arm_pvtime_get_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
@@ -121,6 +152,10 @@ int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|1019| <<kvm_arm_vcpu_arch_has_attr>> ret = kvm_arm_pvtime_has_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_has_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 2ca2973ab..488bd8323 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -35,6 +35,17 @@
 
 #include "trace.h"
 
+/*
+ * 根据manual, GIC的寄存器似乎不支持trap.
+ *
+ * The GIC does not provide additional mechanisms for the virtualization of the
+ * GICD_*, GICR_*, and GITS_* registers. To virtualize VM accesses to these registers,
+ * the hypervisor must set stage 2 Data Aborts to those memory locations so
+ * that the hypervisor can emulate these effects. For more information about
+ * stage 2 Data Aborts, see Arm Architecture Reference Manual, Armv8,
+ * for Armv8-A architecture profile.
+ */
+
 /*
  * For AArch32, we only take care of what is being trapped. Anything
  * that has to do with init and userspace access has to go via the
@@ -295,6 +306,15 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
  * The cp15_64 code makes sure this automatically works
  * for both AArch64 and AArch32 accesses.
  */
+/*
+ * 在以下使用access_gic_sgi():
+ *   - arch/arm64/kvm/sys_regs.c|2156| <<global>> { SYS_DESC(SYS_ICC_SGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2157| <<global>> { SYS_DESC(SYS_ICC_ASGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2158| <<global>> { SYS_DESC(SYS_ICC_SGI0R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2749| <<global>> { Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2752| <<global>> { Op1( 1), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2753| <<global>> { Op1( 2), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ */
 static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
@@ -1164,6 +1184,19 @@ static unsigned int ptrauth_visibility(const struct kvm_vcpu *vcpu,
 	__PTRAUTH_KEY(k ## KEYLO_EL1),					\
 	__PTRAUTH_KEY(k ## KEYHI_EL1)
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ */
 static bool access_arch_timer(struct kvm_vcpu *vcpu,
 			      struct sys_reg_params *p,
 			      const struct sys_reg_desc *r)
@@ -1916,6 +1949,18 @@ static bool access_spsr(struct kvm_vcpu *vcpu,
  * This should be revisited if we ever encounter a more demanding
  * guest...
  */
+/*
+ * 在以下使用sys_reg_descs[]:
+ *   - arch/arm64/kvm/sys_regs.c|3130| <<emulate_sys_reg>> r = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|3183| <<kvm_reset_sys_regs>> for (i = 0; i < ARRAY_SIZE(sys_reg_descs); i++) {
+ *   - arch/arm64/kvm/sys_regs.c|3184| <<kvm_reset_sys_regs>> const struct sys_reg_desc *r = &sys_reg_descs[i];
+ *   - arch/arm64/kvm/sys_regs.c|3440| <<kvm_arm_sys_reg_get_reg>> sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|3484| <<kvm_arm_sys_reg_set_reg>> sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ *   - arch/arm64/kvm/sys_regs.c|3558| <<walk_sys_regs>> i2 = sys_reg_descs;
+ *   - arch/arm64/kvm/sys_regs.c|3559| <<walk_sys_regs>> end2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);
+ *   - arch/arm64/kvm/sys_regs.c|3603| <<kvm_sys_reg_table_init>> valid &= check_sysreg_table(sys_reg_descs, ARRAY_SIZE(sys_reg_descs), false);
+ *   - arch/arm64/kvm/sys_regs.c|3619| <<kvm_sys_reg_table_init>> first_idreg = find_reg(&params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));
+ */
 static const struct sys_reg_desc sys_reg_descs[] = {
 	{ SYS_DESC(SYS_DC_ISW), access_dcsw },
 	{ SYS_DESC(SYS_DC_IGSW), access_dcgsw },
@@ -3435,6 +3480,10 @@ int kvm_sys_reg_set_user(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|803| <<kvm_arm_set_reg>> return kvm_arm_sys_reg_set_reg(vcpu, reg);
+ */
 int kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	void __user *uaddr = (void __user *)(unsigned long)reg->addr;
diff --git a/arch/arm64/kvm/vgic-sys-reg-v3.c b/arch/arm64/kvm/vgic-sys-reg-v3.c
index 9e7c486b4..499bd5544 100644
--- a/arch/arm64/kvm/vgic-sys-reg-v3.c
+++ b/arch/arm64/kvm/vgic-sys-reg-v3.c
@@ -10,6 +10,10 @@
 #include "vgic/vgic.h"
 #include "sys_regs.h"
 
+/*
+ * ICC_CTLR_EL1 controls aspects of the behavior of the GIC CPU interface and
+ * provides information about the features implemented.
+ */
 static int set_gic_ctlr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 			u64 val)
 {
@@ -297,6 +301,15 @@ static int get_gic_sre(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 	return 0;
 }
 
+/*
+ * 在以下使用gic_v3_icc_reg_descs[]:
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|344| <<vgic_v3_has_cpu_sysregs_attr>> if (get_reg_by_id(attr_to_id(attr->attr), gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|345| <<vgic_v3_has_cpu_sysregs_attr>> ARRAY_SIZE(gic_v3_icc_reg_descs)))
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|361| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_set_user(vcpu, &reg, gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|362| <<vgic_v3_cpu_sysregs_uaccess>> ARRAY_SIZE(gic_v3_icc_reg_descs));
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|364| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_get_user(vcpu, &reg, gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|365| <<vgic_v3_cpu_sysregs_uaccess>> ARRAY_SIZE(gic_v3_icc_reg_descs));
+ */
 static const struct sys_reg_desc gic_v3_icc_reg_descs[] = {
 	{ SYS_DESC(SYS_ICC_PMR_EL1),
 	  .set_user = set_gic_pmr, .get_user = get_gic_pmr, },
@@ -348,6 +361,10 @@ int vgic_v3_has_cpu_sysregs_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|566| <<vgic_v3_attr_regs_access>> ret = vgic_v3_cpu_sysregs_uaccess(vcpu, attr, is_write);
+ */
 int vgic_v3_cpu_sysregs_uaccess(struct kvm_vcpu *vcpu,
 				struct kvm_device_attr *attr,
 				bool is_write)
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index c8c3cb812..91fa38905 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -515,8 +515,33 @@ static irqreturn_t vgic_maintenance_handler(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
+/*
+ * 在以下使用gic_kvm_info:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|522| <<vgic_set_kvm_info>> BUG_ON(gic_kvm_info != NULL);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|523| <<vgic_set_kvm_info>> gic_kvm_info = kmalloc(sizeof(*info), GFP_KERNEL);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|524| <<vgic_set_kvm_info>> if (gic_kvm_info)
+ *   - arch/arm64/kvm/vgic/vgic-init.c|525| <<vgic_set_kvm_info>> *gic_kvm_info = *info;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|558| <<kvm_vgic_hyp_init>> if (!gic_kvm_info)
+ *   - arch/arm64/kvm/vgic/vgic-init.c|561| <<kvm_vgic_hyp_init>> has_mask = !gic_kvm_info->no_maint_irq_mask;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|563| <<kvm_vgic_hyp_init>> if (has_mask && !gic_kvm_info->maint_irq) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|572| <<kvm_vgic_hyp_init>> if (gic_kvm_info->no_hw_deactivation) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|578| <<kvm_vgic_hyp_init>> switch (gic_kvm_info->type) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|580| <<kvm_vgic_hyp_init>> ret = vgic_v2_probe(gic_kvm_info);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|592| <<kvm_vgic_hyp_init>> ret = vgic_v3_probe(gic_kvm_info);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|602| <<kvm_vgic_hyp_init>> kvm_vgic_global_state.maint_irq = gic_kvm_info->maint_irq;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|604| <<kvm_vgic_hyp_init>> kfree(gic_kvm_info);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|605| <<kvm_vgic_hyp_init>> gic_kvm_info = NULL;
+ */
 static struct gic_kvm_info *gic_kvm_info;
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-apple-aic.c|1063| <<aic_of_ic_init>> vgic_set_kvm_info(&vgic_info);
+ *   - drivers/irqchip/irq-gic-v3.c|2245| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+ *   - drivers/irqchip/irq-gic-v3.c|2587| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+ *   - drivers/irqchip/irq-gic.c|1466| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+ *   - drivers/irqchip/irq-gic.c|1627| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+ */
 void __init vgic_set_kvm_info(const struct gic_kvm_info *info)
 {
 	BUG_ON(gic_kvm_info != NULL);
@@ -580,6 +605,15 @@ int kvm_vgic_hyp_init(void)
 		ret = vgic_v2_probe(gic_kvm_info);
 		break;
 	case GIC_V3:
+		/*
+		 * 例子.
+		 * [   14.829157] kvm [1]: IPA Size Limit: 48 bits
+		 * [   14.833671] kvm [1]: GICv3: no GICV resource entry
+		 * [   14.838458] kvm [1]: disabling GICv2 emulation
+		 * [   14.842929] kvm [1]: GIC system register CPU interface enabled
+		 * [   14.848777] kvm [1]: vgic interrupt IRQ9
+		 * [   14.852918] kvm [1]: VHE mode initialized successfully
+		 */
 		ret = vgic_v3_probe(gic_kvm_info);
 		if (!ret) {
 			static_branch_enable(&kvm_vgic_global_state.gicv3_cpuif);
diff --git a/arch/arm64/kvm/vgic/vgic-irqfd.c b/arch/arm64/kvm/vgic/vgic-irqfd.c
index 475059bac..d29aa5374 100644
--- a/arch/arm64/kvm/vgic/vgic-irqfd.c
+++ b/arch/arm64/kvm/vgic/vgic-irqfd.c
@@ -15,6 +15,11 @@
  *
  * This is the entry point for irqfd IRQ injection
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|46| <<kvm_set_routing_entry>> e->set = vgic_irqfd_set_irq;
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|142| <<kvm_arch_set_irq_inatomic>> return vgic_irqfd_set_irq(e, kvm, irq_source_id, 1, line_status);
+ */
 static int vgic_irqfd_set_irq(struct kvm_kernel_irq_routing_entry *e,
 			struct kvm *kvm, int irq_source_id,
 			int level, bool line_status)
@@ -66,6 +71,11 @@ int kvm_set_routing_entry(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+ */
 static void kvm_populate_msi(struct kvm_kernel_irq_routing_entry *e,
 			     struct kvm_msi *msi)
 {
@@ -94,13 +104,33 @@ int kvm_set_msi(struct kvm_kernel_irq_routing_entry *e,
 	if (!level)
 		return -1;
 
+	/*
+	 * 实际的执行:
+	 * 82         msi->address_lo = e->msi.address_lo;
+	 * 83         msi->address_hi = e->msi.address_hi;
+	 * 84         msi->data = e->msi.data;
+	 * 85         msi->flags = e->msi.flags;
+	 * 86         msi->devid = e->msi.devid;
+	 *
+	 *
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+	 */
 	kvm_populate_msi(e, &msi);
+	/*
+	 * 只在这里
+	 */
 	return vgic_its_inject_msi(kvm, &msi);
 }
 
 /**
  * kvm_arch_set_irq_inatomic: fast-path for irqfd injection
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|215| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -115,7 +145,17 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 		if (!vgic_has_its(kvm))
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|97| <<kvm_set_msi>> kvm_populate_msi(e, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|118| <<kvm_arch_set_irq_inatomic>> kvm_populate_msi(e, &msi);
+		 */
 		kvm_populate_msi(e, &msi);
+		/*
+		 * called by:
+		 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+		 *   - arch/arm64/kvm/vgic/vgic-its.c|816| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+		 */
 		return vgic_its_inject_cached_translation(kvm, &msi);
 	}
 
@@ -132,6 +172,10 @@ int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 	return -EWOULDBLOCK;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|325| <<vgic_init>> ret = kvm_vgic_setup_default_irq_routing(kvm);
+ */
 int kvm_vgic_setup_default_irq_routing(struct kvm *kvm)
 {
 	struct kvm_irq_routing_entry *entries;
diff --git a/arch/arm64/kvm/vgic/vgic-its.c b/arch/arm64/kvm/vgic/vgic-its.c
index 5fe2365a6..3bce178cf 100644
--- a/arch/arm64/kvm/vgic/vgic-its.c
+++ b/arch/arm64/kvm/vgic/vgic-its.c
@@ -36,6 +36,11 @@ static int update_lpi_config(struct kvm *kvm, struct vgic_irq *irq,
  * If this is a "new" LPI, we allocate and initialize a new struct vgic_irq.
  * This function returns a pointer to the _unlocked_ structure.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1273| <<vgic_its_cmd_handle_mapi>> irq = vgic_add_lpi(kvm, lpi_nr, vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2410| <<vgic_its_restore_ite>> irq = vgic_add_lpi(kvm, lpi_id, vcpu);
+ */
 static struct vgic_irq *vgic_add_lpi(struct kvm *kvm, u32 intid,
 				     struct kvm_vcpu *vcpu)
 {
@@ -223,6 +228,15 @@ static struct its_device *find_its_device(struct vgic_its *its, u32 device_id)
  * Device ID/Event ID pair on an ITS.
  * Must be called with the its_lock mutex held.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|721| <<vgic_its_resolve_lpi>> ite = find_ite(its, devid, eventid);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|946| <<vgic_its_cmd_handle_discard>> ite = find_ite(its, device_id, event_id);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|976| <<vgic_its_cmd_handle_movi>> ite = find_ite(its, device_id, event_id);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1198| <<vgic_its_cmd_handle_mapi>> if (find_ite(its, device_id, event_id))
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1393| <<vgic_its_cmd_handle_clear>> ite = find_ite(its, device_id, event_id);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1423| <<vgic_its_cmd_handle_inv>> ite = find_ite(its, device_id, event_id);
+ */
 static struct its_ite *find_ite(struct vgic_its *its, u32 device_id,
 				  u32 event_id)
 {
@@ -545,12 +559,36 @@ static unsigned long vgic_mmio_read_its_idregs(struct kvm *kvm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|587| <<vgic_its_check_cache>> irq = __vgic_its_check_cache(dist, db, devid, eventid);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|617| <<vgic_its_cache_translation>> if (__vgic_its_check_cache(dist, db, devid, eventid))
+ */
 static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 					       phys_addr_t db,
 					       u32 devid, u32 eventid)
 {
 	struct vgic_translation_cache_entry *cte;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1929| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1943| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1955| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 *
+	 * struct kvm *kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct vgic_dist vgic;
+	 *       -> struct list_head lpi_translation_cache;
+	 */
 	list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
 		/*
 		 * If we hit a NULL entry, there is nothing after this
@@ -559,6 +597,9 @@ static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 		if (!cte->irq)
 			break;
 
+		/*
+		 * db, devid, eventid都要match
+		 */
 		if (cte->db != db || cte->devid != devid ||
 		    cte->eventid != eventid)
 			continue;
@@ -576,9 +617,19 @@ static struct vgic_irq *__vgic_its_check_cache(struct vgic_dist *dist,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|818| <<vgic_its_inject_cached_translation>> irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
+ */
 static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 					     u32 devid, u32 eventid)
 {
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct vgic_dist vgic;
+	 *       -> struct list_head lpi_translation_cache;
+	 */
 	struct vgic_dist *dist = &kvm->arch.vgic;
 	struct vgic_irq *irq;
 	unsigned long flags;
@@ -590,6 +641,10 @@ static struct vgic_irq *vgic_its_check_cache(struct kvm *kvm, phys_addr_t db,
 	return irq;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|737| <<vgic_its_resolve_lpi>> vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
+ */
 static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 				       u32 devid, u32 eventid,
 				       struct vgic_irq *irq)
@@ -605,6 +660,20 @@ static void vgic_its_cache_translation(struct kvm *kvm, struct vgic_its *its,
 
 	raw_spin_lock_irqsave(&dist->lpi_list_lock, flags);
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1929| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1943| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1955| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	if (unlikely(list_empty(&dist->lpi_translation_cache)))
 		goto out;
 
@@ -666,6 +735,12 @@ void vgic_its_invalidate_cache(struct kvm *kvm)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|795| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|434| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|506| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+ */
 int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 			 u32 devid, u32 eventid, struct vgic_irq **irq)
 {
@@ -675,10 +750,24 @@ int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 	if (!its->enabled)
 		return -EBUSY;
 
+	/*
+	 * struct its_ite {
+	 *     struct list_head ite_list;
+	 *
+	 *     struct vgic_irq *irq;
+	 *     struct its_collection *collection;
+	 *     u32 event_id;
+	 * };
+	 */
 	ite = find_ite(its, devid, eventid);
 	if (!ite || !its_is_collection_mapped(ite->collection))
 		return E_ITS_INT_UNMAPPED_INTERRUPT;
 
+	/*
+	 * struct its_ite *ite;
+	 * -> struct its_collection *collection;
+	 *    -> u32 target_addr;
+	 */
 	vcpu = kvm_get_vcpu(kvm, ite->collection->target_addr);
 	if (!vcpu)
 		return E_ITS_INT_UNMAPPED_INTERRUPT;
@@ -686,12 +775,20 @@ int vgic_its_resolve_lpi(struct kvm *kvm, struct vgic_its *its,
 	if (!vgic_lpis_enabled(vcpu))
 		return -EBUSY;
 
+	/*
+	 * 只在这里调用
+	 */
 	vgic_its_cache_translation(kvm, its, devid, eventid, ite->irq);
 
 	*irq = ite->irq;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|861| <<vgic_its_inject_msi>> its = vgic_msi_to_its(kvm, msi);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|408| <<vgic_get_its>> return vgic_msi_to_its(kvm, &msi);
+ */
 struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
 {
 	u64 address;
@@ -713,6 +810,19 @@ struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
 	if (kvm_io_dev->ops != &kvm_io_gic_ops)
 		return ERR_PTR(-EINVAL);
 
+	/*
+	 * 175 struct vgic_io_device {
+	 * 176         gpa_t base_addr;
+	 * 177         union {
+	 * 178                 struct kvm_vcpu *redist_vcpu;
+	 * 179                 struct vgic_its *its;
+	 * 180         };
+	 * 181         const struct vgic_register_region *regions;
+	 * 182         enum iodev_type iodev_type;
+	 * 183         int nr_regions;
+	 * 184         struct kvm_io_device dev; <<<<=====
+	 * 185 };
+	 */
 	iodev = container_of(kvm_io_dev, struct vgic_io_device, dev);
 	if (iodev->iodev_type != IODEV_ITS)
 		return ERR_PTR(-EINVAL);
@@ -727,6 +837,22 @@ struct vgic_its *vgic_msi_to_its(struct kvm *kvm, struct kvm_msi *msi)
  * Returns 0 on success, a positive error value for any ITS mapping
  * related errors and negative error values for generic errors.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|789| <<vgic_its_inject_msi>> ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1447| <<vgic_its_cmd_handle_int>> return vgic_its_trigger_msi(kvm, its, msi_devid, msi_data);
+ *
+ * The configuration of LPIs is very different to the other interrupt types,
+ * and involves the following:
+ * - Redistributors
+ * - ITSs (Interrupt Translation Service)
+ *
+ * LPIs are always message–based interrupts, and they can be supported by an
+ * ITS. An ITS is responsible for receiving interrupts from peripherals and
+ * forwarding them to the appropriate Redistributor as LPIs. A system might
+ * include more than one ITS, in which case each ITS must be configured
+ * individually.
+ */
 static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 				u32 devid, u32 eventid)
 {
@@ -749,12 +875,27 @@ static int vgic_its_trigger_msi(struct kvm *kvm, struct vgic_its *its,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|119| <<kvm_arch_set_irq_inatomic>> return vgic_its_inject_cached_translation(kvm, &msi);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|816| <<vgic_its_inject_msi>> if (!vgic_its_inject_cached_translation(kvm, msi))
+ */
 int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_irq *irq;
 	unsigned long flags;
 	phys_addr_t db;
 
+	/*
+	 * struct kvm_msi {
+	 *     __u32 address_lo;
+	 *     __u32 address_hi;
+	 *     __u32 data;
+	 *     __u32 flags;
+	 *     __u32 devid;
+	 *     __u8  pad[12];
+	 * };
+	 */
 	db = (u64)msi->address_hi << 32 | msi->address_lo;
 	irq = vgic_its_check_cache(kvm, db, msi->devid, msi->data);
 	if (!irq)
@@ -773,6 +914,20 @@ int vgic_its_inject_cached_translation(struct kvm *kvm, struct kvm_msi *msi)
  * We then call vgic_its_trigger_msi() with the decoded data.
  * According to the KVM_SIGNAL_MSI API description returns 1 on success.
  */
+/*
+ * generic不分架构
+ * struct kvm_msi {
+ *     __u32 address_lo;
+ *     __u32 address_hi;
+ *     __u32 data;
+ *     __u32 flags;
+ *     __u32 devid;
+ *     __u8  pad[12];
+ * };
+ *
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|98| <<kvm_set_msi>> return vgic_its_inject_msi(kvm, &msi);
+ */
 int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct vgic_its *its;
@@ -781,11 +936,17 @@ int vgic_its_inject_msi(struct kvm *kvm, struct kvm_msi *msi)
 	if (!vgic_its_inject_cached_translation(kvm, msi))
 		return 1;
 
+	/*
+	 * 用address寻找设备和its
+	 */
 	its = vgic_msi_to_its(kvm, msi);
 	if (IS_ERR(its))
 		return PTR_ERR(its);
 
 	mutex_lock(&its->its_lock);
+	/*
+	 * 用data转换成中断
+	 */
 	ret = vgic_its_trigger_msi(kvm, its, msi->devid, msi->data);
 	mutex_unlock(&its->its_lock);
 
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index 188d2187e..9da770d74 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -1066,6 +1066,10 @@ static int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)
  * check for matching ones. If this bit is set, we signal all, but not the
  * calling VCPU.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|338| <<access_gic_sgi>> vgic_v3_dispatch_sgi(vcpu, p->regval, g1);
+ */
 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -1126,6 +1130,21 @@ void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 				irq->pending_latch = true;
 				vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
 			} else {
+				/*
+				 * 似乎这里是direct SGI!!!
+				 *
+				 * When GITS_CTLR.Enabled==1 and
+				 * GITS_CTLR.Quiescent==0, a write to GITS_SGIR
+				 * results in a virtual
+				 * interrupt being generated with the vPEID and
+				 * vINTID from the write. If the vPEID is not
+				 * mapped on any ITS, the write is silently
+				 * discarded. If the vPEID is not mapped on this
+				 * ITS, but is mapped on a different ITS, it is
+				 * CONSTRAINED UNPREDICTABLE whether the
+				 * interrupt is delivered or discarded. Virtual
+				 * SGIs have no priority shift.
+				 */
 				/* HW SGI? Ask the GIC to inject it */
 				int err;
 				err = irq_set_irqchip_state(irq->host_irq,
diff --git a/arch/arm64/kvm/vgic/vgic-v3.c b/arch/arm64/kvm/vgic/vgic-v3.c
index 3dfc8b84e..f8b90279a 100644
--- a/arch/arm64/kvm/vgic/vgic-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-v3.c
@@ -32,6 +32,10 @@ static bool lr_signals_eoi_mi(u64 lr_val)
 	       !(lr_val & ICH_LR_HW);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|899| <<vgic_fold_lr_state>> vgic_v3_fold_lr_state(vcpu);
+ */
 void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -103,6 +107,16 @@ void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 	cpuif->used_lrs = 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|819| <<vgic_populate_lr>> vgic_v3_populate_lr(vcpu, irq, lr);
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> kvm_vgic_flush_hwstate()
+ *    -> vgic_flush_lr_state()
+ *       -> vgic_populate_lr()
+ *          -> vgic_v3_populate_lr()
+ */
 /* Requires the irq to be locked already */
 void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 {
@@ -183,6 +197,22 @@ void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 
 	val |= (u64)irq->priority << ICH_LR_PRIORITY_SHIFT;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct vgic_cpu vgic_cpu;
+	 *       -> struct vgic_v3_cpu_if vgic_v3;
+	 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+	 *
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
 }
 
@@ -574,6 +604,15 @@ int vgic_v3_map_resources(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * 在以下使用vgic_v3_cpuif_trap:
+ *   - arch/arm64/kernel/image-vars.h|69| <<global>> KVM_NVHE_ALIAS(vgic_v3_cpuif_trap);
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|474| <<kvm_hyp_handle_sysreg>> if (static_branch_unlikely(&vgic_v3_cpuif_trap) &&
+ *   - arch/arm64/kvm/hyp/include/hyp/switch.h|489| <<kvm_hyp_handle_cp15_32>> if (static_branch_unlikely(&vgic_v3_cpuif_trap) &&
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|389| <<__vgic_v3_activate_traps>> if (static_branch_unlikely(&vgic_v3_cpuif_trap) ||
+ *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|415| <<__vgic_v3_deactivate_traps>> if (static_branch_unlikely(&vgic_v3_cpuif_trap) ||
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|749| <<vgic_v3_probe>> static_branch_enable(&vgic_v3_cpuif_trap);
+ */
 DEFINE_STATIC_KEY_FALSE(vgic_v3_cpuif_trap);
 
 static int __init early_group0_trap_cfg(char *buf)
@@ -629,6 +668,10 @@ static bool vgic_v3_broken_seis(void)
  * Returns 0 if the VGICv3 has been probed successfully, returns an error code
  * otherwise
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|592| <<kvm_vgic_hyp_init>> ret = vgic_v3_probe(gic_kvm_info);
+ */
 int vgic_v3_probe(const struct gic_kvm_info *info)
 {
 	u64 ich_vtr_el2 = kvm_call_hyp_ret(__vgic_v3_get_gic_config);
@@ -657,6 +700,11 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 
 	kvm_vgic_global_state.vcpu_base = 0;
 
+	/*
+	 * struct gic_kvm_info *info:
+	 * -> struct resource vcpu;
+	 *    -> resource_size_t start;
+	 */
 	if (!info->vcpu.start) {
 		kvm_info("GICv3: no GICV resource entry\n");
 	} else if (!has_v2) {
@@ -717,6 +765,10 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1127| <<kvm_vgic_load>> vgic_v3_load(vcpu);
+ */
 void vgic_v3_load(struct kvm_vcpu *vcpu)
 {
 	struct vgic_v3_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v3;
diff --git a/arch/arm64/kvm/vgic/vgic-v4.c b/arch/arm64/kvm/vgic/vgic-v4.c
index 339a55194..a18a864cc 100644
--- a/arch/arm64/kvm/vgic/vgic-v4.c
+++ b/arch/arm64/kvm/vgic/vgic-v4.c
@@ -408,6 +408,10 @@ static struct vgic_its *vgic_get_its(struct kvm *kvm,
 	return vgic_msi_to_its(kvm, &msi);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2448| <<kvm_arch_irq_bypass_add_producer>> return kvm_vgic_v4_set_forwarding(irqfd->kvm, prod->irq, &irqfd->irq_entry);
+ */
 int kvm_vgic_v4_set_forwarding(struct kvm *kvm, int virq,
 			       struct kvm_kernel_irq_routing_entry *irq_entry)
 {
@@ -430,6 +434,12 @@ int kvm_vgic_v4_set_forwarding(struct kvm *kvm, int virq,
 
 	mutex_lock(&its->its_lock);
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|795| <<vgic_its_trigger_msi>> err = vgic_its_resolve_lpi(kvm, its, devid, eventid, &irq);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|434| <<kvm_vgic_v4_set_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|506| <<kvm_vgic_v4_unset_forwarding>> ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
+	 */
 	/* Perform the actual DevID/EventID -> LPI translation. */
 	ret = vgic_its_resolve_lpi(kvm, its, irq_entry->msi.devid,
 				   irq_entry->msi.data, &irq);
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index 8be4c1ebd..0c1db02d9 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -17,6 +17,80 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * SPI
+ * PPI
+ * SGI
+ * LPI
+ *
+ * GIC(Generic Interrupt Controller)是ARM公司提供的中断控制器统一架构,它详细定义了ARM平台上,
+ * 中断控制器的内部分发逻辑(Distributor)和CPU接口(CPU Interface).
+ *
+ * 目前GICv3引入了对ITS(Interrupt Translation Service)的支持,而GICv4则引入了
+ * LPIs(Locality-specific Peripheral Interrupts)中断的透传功能.
+ *
+ * 从GICv2开始,支持中断虚拟化硬件扩展.vGIC为每个CPU引入了一个vGIC CPU接口和相应的hypervisor控制接口,
+ * 虚拟机可以被配置成直接使用vGIC CPU接口.当非安全物理中断发送给某个CPU CORE后,可以触发该CPU CORE
+ * 陷入到EL2模式,并允许VMM中的中断处理程序,中断处理程序会根据该物理中断对应的VMID等信息,
+ * 通过List Registers,配置并发送虚拟中断到vGIC CPU接口,并路由到相应的vCPU CORE上.
+ */
+/*
+ * https://www.zhihu.com/column/c_1520029500636696576
+ *
+ * 1. 外设触发irq中断,gic的distributor接收到该中断后,通过distributor的仲裁器确定其应该被发送的cpu.
+ *
+ * 2. 在target cpu确定以后,将中断发送给该cpu相关的redistributor
+ *
+ * 3. redistributor将中断发送到其对应的cpu interface上
+ *
+ * 4. 该中断被路由到EL2的hypervisor中断处理入口
+ *
+ * 5. hypervisor退出到host,然后重新使能中断,使中断再次触发,并由host中断处理函数执行中断处理
+ *
+ * 6. host通过写lr的方式向guest注入虚拟中断,并切入guest执行
+ *
+ * 7. 虚拟中断通过virq触发,并进入guest的中断入口函数
+ *
+ * 8. guest执行中断处理流程,并通过虚拟cpu interface执行中断应答以及EOI等操作
+ *
+ * 9. 若该虚拟中断与物理中断关联,则EOI操作将会作用到实际的物理中断上
+ *
+ * 10. 中断处理完成
+ *
+ *
+ * With GICv3, the GIC forwards a packet to the core describing the interrupt
+ * (ID, Group, Priority).  When, within the interrupt handler, software reads
+ * ICC_IARx_EL1 to acknowledge the interrupt, the core already has all the
+ * information it needs to return the interrupt ID (INTID).
+ *
+ * With GICv2, the equivalent step requires a read of an MMIO register.  That
+ * read has to get from the core to the GIC, and then back, before software knows
+ * the INTID.  How long will that take?  It depends on the specific system design
+ * and can also depend on how much contention there is on the memory system at
+ * the time.
+ *
+ * Another advantage is to software - in the interrupt handling path you don't
+ * need to know the address of the interrupt controller to perform the
+ * acknowledge.
+ *
+ * 在gicv2中,cpu interface的寄存器,是实现在gic内部的,因此当core收到一个中断时,
+ * 会通过axi总线(假设memory总线是axi总线,去访问cpu interface的寄存器.而中断在
+ * 一个soc系统中,是会频繁的产生的,这就意味着,core会频繁的去访问gic的寄存器,这样
+ * 会占用axi总线的带宽,总而会影响中断的实时响应.而且core通过axi总线去访问cpu
+ * interface寄存器,延迟,也比较大.
+ *
+ * 在gicv3中,将cpu interface从gic中抽离出来,实现在core内部,而不实现在gic中.
+ * core对cpu interface的访问,通过系统寄存器方式访问,也就是使用msr,mrs访问,那么core对
+ * cpu interface的寄存器访问,就加速了,而且还不占用axi总线带宽.这样core对中断的处理,
+ * 就加速了.
+ *
+ *
+ * The GIC does not provide additional mechanisms for the virtualization of the
+ * GICD_*, GICR_*, and GITS_* registers. To virtualize VM accesses to these
+ * registers, the hypervisor must set stage 2 Data Aborts to those memory
+ * locations so that the hypervisor can emulate these effects.
+ */
+
 struct vgic_global kvm_vgic_global_state __ro_after_init = {
 	.gicv3_cpuif = STATIC_KEY_FALSE_INIT,
 };
@@ -149,6 +223,11 @@ void vgic_put_irq(struct kvm *kvm, struct vgic_irq *irq)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|379| <<kvm_vgic_vcpu_destroy>> vgic_flush_pending_lpis(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|279| <<vgic_mmio_write_v3r_ctlr>> vgic_flush_pending_lpis(vcpu);
+ */
 void vgic_flush_pending_lpis(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -172,6 +251,18 @@ void vgic_flush_pending_lpis(struct kvm_vcpu *vcpu)
 
 void vgic_irq_set_phys_pending(struct vgic_irq *irq, bool pending)
 {
+	/*
+	 * irq_set_irqchip_state - set the state of a forwarded interrupt.
+	 * @irq: Interrupt line that is forwarded to a VM
+	 * @which: State to be restored (one of IRQCHIP_STATE_*)
+	 * @val: Value corresponding to @which
+	 *
+	 * This call sets the internal irqchip state of an interrupt,
+	 * depending on the value of @which.
+	 *
+	 * This function should be called with migration disabled if the
+	 * interrupt controller has per-cpu registers.
+	 */
 	WARN_ON(irq_set_irqchip_state(irq->host_irq,
 				      IRQCHIP_STATE_PENDING,
 				      pending));
@@ -213,6 +304,14 @@ void vgic_irq_set_phys_active(struct vgic_irq *irq, bool active)
  *
  * Requires the IRQ lock to be held.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|407| <<vgic_queue_irq_unlock>> vcpu = vgic_target_oracle(irq);
+ *   - arch/arm64/kvm/vgic/vgic.c|459| <<vgic_queue_irq_unlock>> if (unlikely(irq->vcpu || vcpu != vgic_target_oracle(irq))) {
+ *   - arch/arm64/kvm/vgic/vgic.c|742| <<vgic_prune_ap_list>> target_vcpu = vgic_target_oracle(irq);
+ *   - arch/arm64/kvm/vgic/vgic.c|801| <<vgic_prune_ap_list>> if (target_vcpu == vgic_target_oracle(irq)) {
+ *   - arch/arm64/kvm/vgic/vgic.c|928| <<vgic_flush_lr_state>> if (likely(vgic_target_oracle(irq) == vcpu)) {
+ */
 static struct kvm_vcpu *vgic_target_oracle(struct vgic_irq *irq)
 {
 	lockdep_assert_held(&irq->irq_lock);
@@ -333,6 +432,27 @@ static bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owne
  * Needs to be entered with the IRQ lock already held, but will return
  * with all locks dropped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|301| <<update_lpi_config>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|467| <<its_sync_lpi_pending_table>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|747| <<vgic_its_trigger_msi>> vgic_queue_irq_unlock(kvm, irq, flags); 
+ *   - arch/arm64/kvm/vgic/vgic-its.c|765| <<vgic_its_inject_cached_translation>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|157| <<vgic_mmio_write_sgir>> vgic_queue_irq_unlock(source_vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|264| <<vgic_mmio_write_sgipends>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|375| <<vgic_v3_uaccess_write_pending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1127| <<vgic_v3_dispatch_sgi>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|85| <<vgic_mmio_write_group>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|159| <<vgic_mmio_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|200| <<vgic_uaccess_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|341| <<vgic_mmio_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|368| <<vgic_uaccess_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|603| <<vgic_mmio_change_active>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|828| <<vgic_write_irq_line_level_info>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|358| <<vgic_v3_lpi_sync_pending_status>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|476| <<kvm_vgic_v4_set_forwarding>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *    - arch/arm64/kvm/vgic/vgic.c|552| <<kvm_vgic_inject_irq>> vgic_queue_irq_unlock(kvm, irq, flags);
+ */
 bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 			   unsigned long flags)
 {
@@ -402,6 +522,26 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 		goto retry;
 	}
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *
+	 * kvm_arch_vcpu_ioctl_run()
+	 */
+
 	/*
 	 * Grab a reference to the irq to reflect the fact that it is
 	 * now in the ap_list.
@@ -413,6 +553,17 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 	raw_spin_unlock(&irq->irq_lock);
 	raw_spin_unlock_irqrestore(&vcpu->arch.vgic_cpu.ap_list_lock, flags);
 
+	/*
+	 * 在以下使用KVM_REQ_IRQ_PENDING:
+	 *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+	 *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+	 */
 	kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 	kvm_vcpu_kick(vcpu);
 
@@ -436,6 +587,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|497| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/arm.c|1172| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1180| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|346| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 			bool level, void *owner)
 {
@@ -454,6 +613,12 @@ int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 	if (!vcpu && intid < VGIC_NR_PRIVATE_IRQS)
 		return -EINVAL;
 
+	/*
+	 * 注释
+	 * This looks up the virtual interrupt ID to get the corresponding
+	 * struct vgic_irq. It also increases the refcount, so any caller is expected
+	 * to call vgic_put_irq() once it's finished with this IRQ.
+	 */
 	irq = vgic_get_irq(kvm, vcpu, intid);
 	if (!irq)
 		return -EINVAL;
@@ -478,6 +643,10 @@ int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|692| <<kvm_vgic_map_phys_irq>> ret = kvm_vgic_map_irq(vcpu, irq, host_irq, ops);
+ */
 /* @irq->irq_lock must be held */
 static int kvm_vgic_map_irq(struct kvm_vcpu *vcpu, struct vgic_irq *irq,
 			    unsigned int host_irq,
@@ -513,6 +682,13 @@ static inline void kvm_vgic_unmap_irq(struct vgic_irq *irq)
 	irq->ops = NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|797| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq, timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|802| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq, timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1621| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1629| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+ */
 int kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, unsigned int host_irq,
 			  u32 vintid, struct irq_ops *ops)
 {
@@ -539,6 +715,11 @@ int kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, unsigned int host_irq,
  * subsystems injecting mapped interrupts should reset their interrupt lines
  * when we are doing a reset of the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1402| <<kvm_timer_vcpu_reset>> kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_vtimer));
+ *   - arch/arm64/kvm/arch_timer.c|1404| <<kvm_timer_vcpu_reset>> kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_ptimer));
+ */
 void kvm_vgic_reset_mapped_irq(struct kvm_vcpu *vcpu, u32 vintid)
 {
 	struct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, vintid);
@@ -742,6 +923,10 @@ static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 }
 
 /* Requires the irq_lock to be held. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|903| <<vgic_flush_lr_state>> vgic_populate_lr(vcpu, irq, count++);
+ */
 static inline void vgic_populate_lr(struct kvm_vcpu *vcpu,
 				    struct vgic_irq *irq, int lr)
 {
@@ -795,6 +980,10 @@ static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1004| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+ */
 /* Requires the VCPU's ap_list_lock to be held. */
 static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
@@ -855,6 +1044,11 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 		vcpu->arch.vgic_cpu.vgic_v3.used_lrs = count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|956| <<kvm_vgic_sync_hwstate>> if (can_access_vgic_from_kernel())
+ *   - arch/arm64/kvm/vgic/vgic.c|1008| <<kvm_vgic_flush_hwstate>> if (can_access_vgic_from_kernel())
+ */
 static inline bool can_access_vgic_from_kernel(void)
 {
 	/*
@@ -895,6 +1089,10 @@ void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 	vgic_prune_ap_list(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1009| <<kvm_vgic_flush_hwstate>> vgic_restore_state(vcpu);
+ */
 static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 {
 	if (!static_branch_unlikely(&kvm_vgic_global_state.gicv3_cpuif))
@@ -903,6 +1101,10 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|952| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ */
 /* Flush our emulation state into the GIC hardware before entering the guest. */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -924,6 +1126,23 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 
 	DEBUG_SPINLOCK_BUG_ON(!irqs_disabled());
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
 		raw_spin_lock(&vcpu->arch.vgic_cpu.ap_list_lock);
 		vgic_flush_lr_state(vcpu);
@@ -937,6 +1156,10 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 		vgic_v4_commit(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|447| <<kvm_arch_vcpu_load>> kvm_vgic_load(vcpu);
+ */
 void kvm_vgic_load(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!vgic_initialized(vcpu->kvm)))
@@ -1015,6 +1238,17 @@ void vgic_kick_vcpus(struct kvm *kvm)
 	 */
 	kvm_for_each_vcpu(c, vcpu, kvm) {
 		if (kvm_vgic_vcpu_pending_irq(vcpu)) {
+			/*
+			 * 在以下使用KVM_REQ_IRQ_PENDING:
+			 *   - arch/arm64/kvm/arm.c|799| <<check_vcpu_requests>> kvm_check_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/arm.c|1151| <<vcpu_interrupt_line>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/pmu-emul.c|504| <<kvm_pmu_perf_overflow>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic-v4.c|102| <<vgic_v4_doorbell_handler>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|481| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|548| <<vgic_queue_irq_unlock>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|884| <<vgic_prune_ap_list>> kvm_make_request(KVM_REQ_IRQ_PENDING, target_vcpu);
+			 *   - arch/arm64/kvm/vgic/vgic.c|1218| <<vgic_kick_vcpus>> kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
+			 */
 			kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
 			kvm_vcpu_kick(vcpu);
 		}
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 9d248703c..d1f235579 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1716,6 +1716,12 @@ int x86_pmu_handle_irq(struct pt_regs *regs)
 	return handled;
 }
 
+/*
+ * called by:
+ *   - arch/x86/events/core.c|1352| <<x86_pmu_enable>> perf_events_lapic_init();
+ *   - arch/x86/events/core.c|2103| <<init_hw_perf_events>> perf_events_lapic_init();
+ *   - arch/x86/kernel/apic/apic.c|1685| <<setup_local_APIC>> perf_events_lapic_init();
+ */
 void perf_events_lapic_init(void)
 {
 	if (!x86_pmu.apic || !x86_pmu_initialized())
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3bc146dfd..12411772e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -74,22 +74,81 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * kvm_guest_time_update()
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+ *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *
+ * 处理函数是kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+ *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理函数是kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
 #endif
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+ *   - kvm_update_masterclock()
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * kvm_gen_kvmclock_update()
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -493,8 +552,43 @@ struct kvm_pmc {
 	bool is_paused;
 	bool intr;
 	u64 counter;
+	/*
+	 * 在以下使用kvm_pmc->prev_counter:
+	 *   - arch/x86/kvm/pmu.c|471| <<reprogram_counter>> if (pmc->counter < pmc->prev_counter)
+	 *   - arch/x86/kvm/pmu.c|511| <<reprogram_counter>> pmc->prev_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|800| <<kvm_pmu_incr_counter>> pmc->prev_counter = pmc->counter;
+	 *   - arch/x86/kvm/svm/pmu.c|245| <<amd_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|614| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = pmc->eventsel = 0;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|621| <<intel_pmu_reset>> pmc->counter = pmc->prev_counter = 0;
+	 */
 	u64 prev_counter;
 	u64 eventsel;
+	/*
+	 * 在以下设置kvm_pmc->perf_event:
+	 *   - arch/x86/kvm/pmu.c|318| <<pmc_reprogram_counter>> pmc->perf_event = event;
+	 *   - arch/x86/kvm/pmu.h|81| <<pmc_release_perf_event>> pmc->perf_event = NULL;
+	 * 在以下使用kvm_pmc->perf_event:
+	 *   -  arch/x86/kvm/pmu.c|166| <<__kvm_perf_overflow>> if (pmc->perf_event && pmc->perf_event->attr.precise_ip) {
+	 *   - arch/x86/kvm/pmu.c|222| <<kvm_perf_overflow>> struct kvm_pmc *pmc = perf_event->overflow_handler_context;
+	 *   - arch/x86/kvm/pmu.c|333| <<pmc_pause_counter>> if (!pmc->perf_event || pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.c|337| <<pmc_pause_counter>> counter += perf_event_pause(pmc->perf_event, true);
+	 *   - arch/x86/kvm/pmu.c|348| <<pmc_resume_counter>> if (!pmc->perf_event)
+	 *   - arch/x86/kvm/pmu.c|352| <<pmc_resume_counter>> if (is_sampling_event(pmc->perf_event) &&
+	 *   - arch/x86/kvm/pmu.c|353| <<pmc_resume_counter>> perf_event_period(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.c|358| <<pmc_resume_counter>> (!!pmc->perf_event->attr.precise_ip))
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_resume_counter>> perf_event_enable(pmc->perf_event);
+	 *   - arch/x86/kvm/pmu.c|827| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+	 *   - arch/x86/kvm/pmu.h|70| <<pmc_read_counter>> if (pmc->perf_event && !pmc->is_paused)
+	 *   - arch/x86/kvm/pmu.h|71| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event,
+	 *   - arch/x86/kvm/pmu.h|79| <<pmc_release_perf_event>> if (pmc->perf_event) {
+	 *   - arch/x86/kvm/pmu.h|80| <<pmc_release_perf_event>> perf_event_release_kernel(pmc->perf_event);
+	 *   - arch/x86/kvm/pmu.h|89| <<pmc_stop_counter>> if (pmc->perf_event) {
+	 *   - arch/x86/kvm/pmu.h|154| <<pmc_update_sample_period>> if (!pmc->perf_event || pmc->is_paused ||
+	 *   - arch/x86/kvm/pmu.h|155| <<pmc_update_sample_period>> !is_sampling_event(pmc->perf_event))
+	 *   - arch/x86/kvm/pmu.h|158| <<pmc_update_sample_period>> perf_event_period(pmc->perf_event,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|756| <<intel_pmu_cross_mapped_check>> !pmc_is_globally_enabled(pmc) || !pmc->perf_event)
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|763| <<intel_pmu_cross_mapped_check>> hw_idx = pmc->perf_event->hw.idx;
+	 */
 	struct perf_event *perf_event;
 	struct kvm_vcpu *vcpu;
 	/*
@@ -528,6 +622,12 @@ struct kvm_pmu {
 	u64 raw_event_mask;
 	struct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];
 	struct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];
+	/*
+	 * 在以下使用kvm_pmu->irq_work:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+	 *   - arch/x86/kvm/pmu.c|737| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+	 *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+	 */
 	struct irq_work irq_work;
 
 	/*
@@ -536,7 +636,23 @@ struct kvm_pmu {
 	 * filter changes.
 	 */
 	union {
+		/*
+		 * 在以下使用kvm_pmu->reprogram_pmi:
+		 *   - arch/x86/include/asm/kvm_host.h|539| <<global>> DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+		 *   - arch/x86/kvm/pmu.c|155| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+		 *   - arch/x86/kvm/pmu.c|455| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|464| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+		 *   - arch/x86/kvm/pmu.c|468| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|909| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+		 *   - arch/x86/kvm/pmu.h|219| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.h|231| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+		 */
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+		/*
+		 * 在以下使用kvm_pmu->_reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|910| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+		 *   - arch/x86/kvm/pmu.c|913| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+		 */
 		atomic64_t __reprogram_pmi;
 	};
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
@@ -844,6 +960,17 @@ struct kvm_vcpu_arch {
 	gpa_t time;
 	struct pvclock_vcpu_time_info hv_clock;
 	unsigned int hw_tsc_khz;
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2451| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3758| <<kvm_guest_time_update>> if (vcpu->pv_time.active)
+	 *   - arch/x86/kvm/x86.c|3759| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4089| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6123| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|7336| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|12645| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm, vcpu, KVM_HOST_USES_PFN);
+	 */
 	struct gfn_to_pfn_cache pv_time;
 	/* set guest stopped flag in pvclock flags field */
 	bool pvclock_set_guest_stopped_request;
@@ -860,10 +987,32 @@ struct kvm_vcpu_arch {
 	u64 last_guest_tsc;
 	u64 last_host_tsc;
 	u64 tsc_offset_adjustment;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2628| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2936| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 this_tsc_nsec;
 	u64 this_tsc_write;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2945| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2950| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2521| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3563| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5305| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2561| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10413| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|11646| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
 	s8 virtual_tsc_shift;
 	u32 virtual_tsc_mult;
@@ -873,11 +1022,25 @@ struct kvm_vcpu_arch {
 	u64 l1_tsc_scaling_ratio;
 	u64 tsc_scaling_ratio; /* current scaling ratio */
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_queued:
+	 *   - arch/x86/kvm/x86.c|830| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5902| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10985| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12800| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
 	/* Number of NMIs pending injection, not including hardware vNMIs. */
 	unsigned int nmi_pending;
 	bool nmi_injected;    /* Trying to inject an NMI this entry */
 	bool smi_pending;    /* SMI queued after currently running handler */
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1861| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|441| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|446| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|451| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	u8 handling_intr_from_guest;
 
 	struct kvm_mtrr mtrr_state;
@@ -1246,6 +1409,13 @@ struct kvm_arch {
 	 * guest attempts to execute from the region then KVM obviously can't
 	 * create an NX huge page (without hanging the guest).
 	 */
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|898| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link, &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6321| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7228| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7238| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages, struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	struct list_head possible_nx_huge_pages;
 	struct kvm_page_track_notifier_node mmu_sp_tracker;
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1287,6 +1457,15 @@ struct kvm_arch {
 	bool cstate_in_guest;
 
 	unsigned long irq_sources_bitmap;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 
 	/*
@@ -1298,19 +1477,77 @@ struct kvm_arch {
 	u64 last_tsc_write;
 	u32 last_tsc_khz;
 	u64 last_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2931| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2946| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 cur_tsc_nsec;
 	u64 cur_tsc_write;
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2935| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2945| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2950| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下修改kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2939| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2941| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 */
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->default_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|7688| <<kvm_arch_vm_ioctl>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|7694| <<kvm_arch_vm_ioctl>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|12731| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|13185| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+	 */
 	u32 default_tsc_khz;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|3077| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3094| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3201| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3203| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|3301| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|3314| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|12741| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_t pvclock_sc;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3218| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched && !ka->backwards_tsc_observed && !ka->boot_vcpu_runs_old_kvmclock;
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3033| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3157| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3279| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|6953| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|12539| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 	struct kvm_xen_hvm_config xen_hvm_config;
@@ -1321,6 +1558,11 @@ struct kvm_arch {
 	struct kvm_hv hyperv;
 	struct kvm_xen xen;
 
+	/*
+	 * 在以下使用backwards_tsc_observed:
+	 *   - arch/x86/kvm/x86.c|3293| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched && !ka->backwards_tsc_observed && !ka->boot_vcpu_runs_old_kvmclock;
+	 *   - arch/x86/kvm/x86.c|13037| <<kvm_arch_hardware_enable>> kvm->arch.backwards_tsc_observed = true;
+	 */
 	bool backwards_tsc_observed;
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
@@ -1421,6 +1663,11 @@ struct kvm_arch {
 	 */
 	u32 max_vcpu_ids;
 
+	/*
+	 * 在以下使用kvm_arch->disable_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|202| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
+	 *   - arch/x86/kvm/x86.c|7124| <<kvm_vm_ioctl_enable_cap>> kvm->arch.disable_nx_huge_pages = true;
+	 */
 	bool disable_nx_huge_pages;
 
 	/*
@@ -1463,6 +1710,15 @@ struct kvm_vm_stat {
 		};
 		atomic64_t pages[KVM_NR_PAGE_SIZES];
 	};
+	/*
+	 * 在以下使用kvm_vm_stat->nx_lpage_splits:
+	 *   - arch/x86/include/asm/kvm_host.h|1646| <<global>> u64 nx_lpage_splits;
+	 *   - arch/x86/kvm/x86.c|266| <<global>> STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+	 *   - arch/x86/kvm/mmu/mmu.c|896| <<track_possible_nx_huge_page>> ++kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<untrack_possible_nx_huge_page>> --kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7179| <<kvm_recover_nx_huge_pages>> unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7199| <<kvm_recover_nx_huge_pages>> to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
+	 */
 	u64 nx_lpage_splits;
 	u64 max_mmu_page_hash_collisions;
 	u64 max_mmu_rmap_size;
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 5c5f1e56c..298181878 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -44,6 +44,28 @@ struct nmiaction {
 	const char		*name;
 };
 
+/*
+ * called by:
+ *   - arch/x86/events/amd/ibs.c|1240| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2110| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler, 0, "arch_bt");
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|431| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST, "hv_nmi_unknown");
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler, 0, "kgdb");
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler, 0, "kgdb");
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback, NMI_FLAG_FIRST, "nmi_selftest", __initdata)) {
+ *   - arch/x86/kernel/reboot.c|878| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback, NMI_FLAG_FIRST, "crash"))
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback, NMI_FLAG_FIRST, "smp_stop");
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1199| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0, "ipmi");
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler, 0, IGEN6_NMI_NAME);
+ *   - drivers/watchdog/hpwdt.c|249| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|252| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|255| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h
index b49778664..e78aa06d5 100644
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@ -22,10 +22,22 @@ u64 dummy_steal_clock(int cpu);
 u64 dummy_sched_clock(void);
 
 DECLARE_STATIC_CALL(pv_steal_clock, dummy_steal_clock);
+/*
+ * 在以下使用pv_sched_clock:
+ *   - arch/x86/include/asm/paravirt.h|25| <<global>> DECLARE_STATIC_CALL(pv_sched_clock, dummy_sched_clock);
+ *   - arch/x86/kernel/paravirt.c|113| <<global>> DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
+ *   - arch/x86/include/asm/paravirt.h|31| <<paravirt_sched_clock>> return static_call(pv_sched_clock)();
+ *   - arch/x86/kernel/paravirt.c|124| <<u64>> static_call_update(pv_sched_clock, func);
+ *   - arch/x86/kernel/tsc.c|283| <<using_native_sched_clock>> return static_call_query(pv_sched_clock) == native_sched_clock;
+ */
 DECLARE_STATIC_CALL(pv_sched_clock, dummy_sched_clock);
 
 void paravirt_set_sched_clock(u64 (*func)(void));
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|278| <<sched_clock_noinstr>> return paravirt_sched_clock();
+ */
 static __always_inline u64 paravirt_sched_clock(void)
 {
 	return static_call(pv_sched_clock)();
diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h
index 0c92db844..2affc26e1 100644
--- a/arch/x86/include/asm/pvclock.h
+++ b/arch/x86/include/asm/pvclock.h
@@ -79,6 +79,13 @@ static __always_inline u64 pvclock_scale_delta(u64 delta, u32 mul_frac, int shif
 	return product;
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+ *   - arch/x86/kernel/pvclock.c|82| <<__pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+ *   - arch/x86/kvm/x86.c|3456| <<__get_kvmclock>> data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
+ *   - drivers/ptp/ptp_kvm_x86.c|123| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair->tsc);
+ */
 static __always_inline
 u64 __pvclock_read_cycles(const struct pvclock_vcpu_time_info *src, u64 tsc)
 {
diff --git a/arch/x86/include/asm/x86_init.h b/arch/x86/include/asm/x86_init.h
index 5240d88db..14bd5da92 100644
--- a/arch/x86/include/asm/x86_init.h
+++ b/arch/x86/include/asm/x86_init.h
@@ -298,6 +298,15 @@ struct x86_hyper_runtime {
  */
 struct x86_platform_ops {
 	unsigned long (*calibrate_cpu)(void);
+	/*
+	 * 几个calibrate_tsc():
+	 *   - arch/x86/kernel/x86_init.c|142| <<global>> .calibrate_tsc = native_calibrate_tsc,
+	 *   - arch/x86/kernel/cpu/acrn.c|32| <<acrn_init_platform>> x86_platform.calibrate_tsc = acrn_get_tsc_khz;
+	 *   - arch/x86/kernel/cpu/mshyperv.c|390| <<ms_hyperv_init_platform>> x86_platform.calibrate_tsc = hv_get_tsc_khz;
+	 *   - arch/x86/kernel/cpu/vmware.c|411| <<vmware_platform_setup>> x86_platform.calibrate_tsc = vmware_get_tsc_khz;
+	 *   - arch/x86/kernel/jailhouse.c|212| <<jailhouse_init_platform>> x86_platform.calibrate_tsc = jailhouse_get_tsc;
+	 *   - arch/x86/xen/time.c|569| <<xen_init_time_common>> x86_platform.calibrate_tsc = xen_tsc_khz;
+	 */
 	unsigned long (*calibrate_tsc)(void);
 	void (*get_wallclock)(struct timespec64 *ts);
 	int (*set_wallclock)(const struct timespec64 *ts);
diff --git a/arch/x86/include/uapi/asm/kvm.h b/arch/x86/include/uapi/asm/kvm.h
index 1a6a1f987..58aa1920f 100644
--- a/arch/x86/include/uapi/asm/kvm.h
+++ b/arch/x86/include/uapi/asm/kvm.h
@@ -318,6 +318,21 @@ struct kvm_reinject_control {
 	__u8 reserved[31];
 };
 
+/*
+ * 在QEMU设置KVM_VCPUEVENT_VALID_SMM的地方
+ *
+ * 4426 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
+ * 4427 {
+ * ... ...
+ * 4474          * Stop SMI delivery on old machine types to avoid a reboot
+ * 4475          * on an inward migration of an old VM.
+ * 4476          *
+ * 4477         if (!cpu->kvm_no_smi_migration) {
+ * 4478             events.flags |= KVM_VCPUEVENT_VALID_SMM;
+ * 4479         }
+ * 4480     }
+ */
+
 /* When set in flags, include corresponding fields on KVM_SET_VCPU_EVENTS */
 #define KVM_VCPUEVENT_VALID_NMI_PENDING	0x00000001
 #define KVM_VCPUEVENT_VALID_SIPI_VECTOR	0x00000002
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index af49e24b4..4a7ed7dae 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -482,6 +482,10 @@ static int lapic_next_event(unsigned long delta,
 	return 0;
 }
 
+/*
+ * 在以下使用lapic_next_deadline():
+ *   - arch/x86/kernel/apic/apic.c|642| <<setup_APIC_timer>> levt->set_next_event = lapic_next_deadline;
+ */
 static int lapic_next_deadline(unsigned long delta,
 			       struct clock_event_device *evt)
 {
diff --git a/arch/x86/kernel/cpu/vmware.c b/arch/x86/kernel/cpu/vmware.c
index 11f83d079..47fe2c319 100644
--- a/arch/x86/kernel/cpu/vmware.c
+++ b/arch/x86/kernel/cpu/vmware.c
@@ -124,6 +124,11 @@ static unsigned long vmware_get_tsc_khz(void)
 
 #ifdef CONFIG_PARAVIRT
 static struct cyc2ns_data vmware_cyc2ns __ro_after_init;
+/*
+ * 在以下使用vmw_sched_clock:
+ *   - arch/x86/kernel/cpu/vmware.c|134| <<setup_vmw_sched_clock>> vmw_sched_clock = false;
+ *   - arch/x86/kernel/cpu/vmware.c|339| <<vmware_paravirt_ops_setup>> if (vmw_sched_clock)
+ */
 static bool vmw_sched_clock __initdata = true;
 static DEFINE_PER_CPU_DECRYPTED(struct vmware_steal_time, vmw_steal_time) __aligned(64);
 static bool has_steal_clock;
@@ -336,6 +341,13 @@ static void __init vmware_paravirt_ops_setup(void)
 
 	vmware_cyc2ns_setup();
 
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+	 *   - arch/x86/kernel/kvmclock.c|108| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+	 *   - arch/x86/xen/time.c|567| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+	 *   - drivers/clocksource/hyperv_timer.c|514| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+	 */
 	if (vmw_sched_clock)
 		paravirt_set_sched_clock(vmware_sched_clock);
 
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index 1cceac598..002f5d42e 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -928,6 +928,9 @@ static void kvm_sev_hc_page_enc_status(unsigned long pfn, int npages, bool enc)
 			   KVM_MAP_GPA_RANGE_ENC_STAT(enc) | KVM_MAP_GPA_RANGE_PAGE_SZ_4K);
 }
 
+/*
+ * struct hypervisor_x86 x86_hyper_kvm.arch/x86/kernel/kvm.c|1009| <<global>> .init.init_platform = kvm_init_platform,
+ */
 static void __init kvm_init_platform(void)
 {
 	if (cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT) &&
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index fb8f52149..6bcfeaa60 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -26,6 +26,13 @@ static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+/*
+ * 在以下使用kvm_sched_clock_offset:
+ *   - arch/x86/kernel/kvmclock.c|100| <<kvm_sched_clock_read>> return pvclock_clocksource_read_nowd(this_cpu_pvti()) - kvm_sched_clock_offset;
+ *   - arch/x86/kernel/kvmclock.c|107| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+ *   - arch/x86/kernel/kvmclock.c|111| <<kvm_sched_clock_init>> pr_info("kvm-clock: using sched offset of %llu cycles", kvm_sched_clock_offset);
+ *   - arch/x86/kernel/kvmclock.c|113| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) > sizeof(((struct pvclock_vcpu_time_info *)NULL)->system_time));
+ */
 static u64 kvm_sched_clock_offset __ro_after_init;
 
 static int __init parse_no_kvmclock(char *arg)
@@ -60,6 +67,15 @@ EXPORT_PER_CPU_SYMBOL_GPL(hv_clock_per_cpu);
  */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
+	/*
+	 * static struct pvclock_wall_clock wall_clock __bss_decrypted;
+	 *
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|28| <<global>> static int msr_kvm_wall_clock __ro_after_init = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|296| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|307| <<kvmclock_init>> msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
 	preempt_disable();
 	pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
@@ -86,16 +102,46 @@ static u64 kvm_clock_get_cycles(struct clocksource *cs)
 	return kvm_clock_read();
 }
 
+/*
+ * 在以下使用kvm_sched_clock_read():
+ *   - arch/x86/kernel/kvmclock.c|108| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ */
 static noinstr u64 kvm_sched_clock_read(void)
 {
+	/*
+	 * 在以下使用kvm_sched_clock_offset:
+	 *   - arch/x86/kernel/kvmclock.c|100| <<kvm_sched_clock_read>> return pvclock_clocksource_read_nowd(this_cpu_pvti()) - kvm_sched_clock_offset;
+	 *   - arch/x86/kernel/kvmclock.c|107| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+	 *   - arch/x86/kernel/kvmclock.c|111| <<kvm_sched_clock_init>> pr_info("kvm-clock: using sched offset of %llu cycles", kvm_sched_clock_offset);
+	 *   - arch/x86/kernel/kvmclock.c|113| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) > sizeof(((struct pvclock_vcpu_time_info *)NULL)->system_time));
+	 */
 	return pvclock_clocksource_read_nowd(this_cpu_pvti()) - kvm_sched_clock_offset;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|369| <<kvmclock_init>> kvm_sched_clock_init(flags & PVCLOCK_TSC_STABLE_BIT);
+ */
 static inline void kvm_sched_clock_init(bool stable)
 {
 	if (!stable)
 		clear_sched_clock_stable();
+	/*
+	 * 在以下使用kvm_sched_clock_offset:
+	 *   - arch/x86/kernel/kvmclock.c|100| <<kvm_sched_clock_read>> return pvclock_clocksource_read_nowd(this_cpu_pvti()) - kvm_sched_clock_offset;
+	 *   - arch/x86/kernel/kvmclock.c|107| <<kvm_sched_clock_init>> kvm_sched_clock_offset = kvm_clock_read();
+	 *   - arch/x86/kernel/kvmclock.c|111| <<kvm_sched_clock_init>> pr_info("kvm-clock: using sched offset of %llu cycles", kvm_sched_clock_offset);
+	 *   - arch/x86/kernel/kvmclock.c|113| <<kvm_sched_clock_init>> BUILD_BUG_ON(sizeof(kvm_sched_clock_offset) > sizeof(((struct pvclock_vcpu_time_info *)NULL)->system_time));
+	 */
 	kvm_sched_clock_offset = kvm_clock_read();
+	/*
+	 * 在以下使用pv_sched_clock:
+	 *   - arch/x86/include/asm/paravirt.h|25| <<global>> DECLARE_STATIC_CALL(pv_sched_clock, dummy_sched_clock);
+	 *   - arch/x86/kernel/paravirt.c|113| <<global>> DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
+	 *   - arch/x86/include/asm/paravirt.h|31| <<paravirt_sched_clock>> return static_call(pv_sched_clock)();
+	 *   - arch/x86/kernel/paravirt.c|124| <<u64>> static_call_update(pv_sched_clock, func);
+	 *   - arch/x86/kernel/tsc.c|283| <<using_native_sched_clock>> return static_call_query(pv_sched_clock) == native_sched_clock;
+	 */
 	paravirt_set_sched_clock(kvm_sched_clock_read);
 
 	pr_info("kvm-clock: using sched offset of %llu cycles",
@@ -164,6 +210,12 @@ struct clocksource kvm_clock = {
 };
 EXPORT_SYMBOL_GPL(kvm_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|228| <<kvm_restore_sched_clock_state>> kvm_register_clock("primary cpu clock, resume");
+ *   - arch/x86/kernel/kvmclock.c|234| <<kvm_setup_secondary_clock>> kvm_register_clock("secondary cpu clock");
+ *   - arch/x86/kernel/kvmclock.c|356| <<kvmclock_init>> kvm_register_clock("primary cpu clock");
+ */
 static void kvm_register_clock(char *txt)
 {
 	struct pvclock_vsyscall_time_info *src = this_cpu_hvclock();
@@ -284,6 +336,10 @@ static int kvmclock_setup_percpu(unsigned int cpu)
 	return p ? 0 : -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvm.c|981| <<kvm_init_platform>> kvmclock_init();
+ */
 void __init kvmclock_init(void)
 {
 	u8 flags;
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index a0c551846..4eea6c492 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -43,6 +43,13 @@ struct nmi_desc {
 	struct list_head head;
 };
 
+/*
+ * 在以下使用nmi_desc[NMI_MAX]:
+ *   - arch/x86/kernel/nmi.c|101| <<nmi_to_desc>> #define nmi_to_desc(type) (&nmi_desc[type])
+ *   - arch/x86/kernel/nmi.c|132| <<nmi_handle>> struct nmi_desc *desc = nmi_to_desc(type);
+ *   - arch/x86/kernel/nmi.c|166| <<__register_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+ *   - arch/x86/kernel/nmi.c|197| <<unregister_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+ */
 static struct nmi_desc nmi_desc[NMI_MAX] = 
 {
 	{
@@ -80,8 +87,27 @@ struct nmi_stats {
 	long idt_calls_snap;
 };
 
+/*
+ * 在以下使用percpu的nmi_stats:
+ *   - arch/x86/kernel/nmi.c|83| <<global>> static DEFINE_PER_CPU(struct nmi_stats, nmi_stats);
+ *   - arch/x86/kernel/nmi.c|299| <<unknown_nmi_error>> __this_cpu_add(nmi_stats.unknown, handled);
+ *   - arch/x86/kernel/nmi.c|303| <<unknown_nmi_error>> __this_cpu_add(nmi_stats.unknown, 1);
+ *   - arch/x86/kernel/nmi.c|347| <<default_do_nmi>> __this_cpu_add(nmi_stats.normal, handled);
+ *   - arch/x86/kernel/nmi.c|389| <<default_do_nmi>> __this_cpu_add(nmi_stats.external, 1);
+ *   - arch/x86/kernel/nmi.c|426| <<default_do_nmi>> __this_cpu_add(nmi_stats.swallow, 1);
+ *   - arch/x86/kernel/nmi.c|491| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> struct nmi_stats *nsp = this_cpu_ptr(&nmi_stats);
+ *   - arch/x86/kernel/nmi.c|599| <<nmi_backtrace_stall_snap>> nsp = per_cpu_ptr(&nmi_stats, cpu);
+ *   - arch/x86/kernel/nmi.c|619| <<nmi_backtrace_stall_check>> nsp = per_cpu_ptr(&nmi_stats, cpu);
+ */
 static DEFINE_PER_CPU(struct nmi_stats, nmi_stats);
 
+/*
+ * 在以下使用ignore_nmis:
+ *   - arch/x86/kernel/nmi.c|590| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (IS_ENABLED(CONFIG_NMI_CHECK_CPU) && ignore_nmis) {
+ *   - arch/x86/kernel/nmi.c|592| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> } else if (!ignore_nmis) {
+ *   - arch/x86/kernel/nmi.c|725| <<stop_nmi>> ignore_nmis++;
+ *   - arch/x86/kernel/nmi.c|734| <<restart_nmi>> ignore_nmis--;
+ */
 static int ignore_nmis __read_mostly;
 
 int unknown_nmi_panic;
@@ -98,6 +124,13 @@ static int __init setup_unknown_nmi_panic(char *str)
 }
 __setup("unknown_nmi_panic", setup_unknown_nmi_panic);
 
+/*
+ * 在以下使用nmi_desc[NMI_MAX]:
+ *   - arch/x86/kernel/nmi.c|101| <<nmi_to_desc>> #define nmi_to_desc(type) (&nmi_desc[type])
+ *   - arch/x86/kernel/nmi.c|132| <<nmi_handle>> struct nmi_desc *desc = nmi_to_desc(type);
+ *   - arch/x86/kernel/nmi.c|166| <<__register_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+ *   - arch/x86/kernel/nmi.c|197| <<unregister_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+ */
 #define nmi_to_desc(type) (&nmi_desc[type])
 
 static u64 nmi_longest_ns = 1 * NSEC_PER_MSEC;
@@ -110,6 +143,10 @@ static int __init nmi_warning_debugfs(void)
 }
 fs_initcall(nmi_warning_debugfs);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|187| <<nmi_handle>> nmi_check_duration(a, delta);
+ */
 static void nmi_check_duration(struct nmiaction *action, u64 duration)
 {
 	int remainder_ns, decimal_msecs;
@@ -127,8 +164,22 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 		action->handler, duration, decimal_msecs);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|269| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|292| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|337| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|407| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
+	/*
+	 * 在以下使用nmi_desc[NMI_MAX]:
+	 *   - arch/x86/kernel/nmi.c|101| <<nmi_to_desc>> #define nmi_to_desc(type) (&nmi_desc[type])
+	 *   - arch/x86/kernel/nmi.c|132| <<nmi_handle>> struct nmi_desc *desc = nmi_to_desc(type);
+	 *   - arch/x86/kernel/nmi.c|166| <<__register_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+	 *   - arch/x86/kernel/nmi.c|197| <<unregister_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+	 */
 	struct nmi_desc *desc = nmi_to_desc(type);
 	struct nmiaction *a;
 	int handled=0;
@@ -161,8 +212,41 @@ static int nmi_handle(unsigned int type, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(nmi_handle);
 
+/*
+ * 在以下调用register_nmi_handler():
+ *   - arch/x86/events/amd/ibs.c|1240| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2110| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|54| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler, 0, "arch_bt");
+ *   - arch/x86/kernel/cpu/mce/inject.c|774| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|431| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST, "hv_nmi_unknown");
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler, 0, "kgdb");
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler, 0, "kgdb");
+ *   - arch/x86/kernel/nmi_selftest.c|46| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|69| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback, NMI_FLAG_FIRST, "nmi_selftest", __initdata)) {
+ *   - arch/x86/kernel/reboot.c|878| <<nmi_shootdown_cpus>> if (register_nmi_handler(NMI_LOCAL, crash_nmi_callback, NMI_FLAG_FIRST, "crash"))
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback, NMI_FLAG_FIRST, "smp_stop");
+ *   - arch/x86/platform/uv/uv_nmi.c|1026| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1029| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1199| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1274| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0, "ipmi");
+ *   - drivers/edac/igen6_edac.c|1161| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler, 0, IGEN6_NMI_NAME);
+ *   - drivers/watchdog/hpwdt.c|249| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|252| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|255| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ *
+ *
+ * called by:
+ *   - arch/x86/include/asm/nmi.h|55| <<register_nmi_handler>> __register_nmi_handler((t), &fn##_na); \
+ */
 int __register_nmi_handler(unsigned int type, struct nmiaction *action)
 {
+	/*
+	 * 在以下使用nmi_desc[NMI_MAX]:
+	 *   - arch/x86/kernel/nmi.c|101| <<nmi_to_desc>> #define nmi_to_desc(type) (&nmi_desc[type])
+	 *   - arch/x86/kernel/nmi.c|132| <<nmi_handle>> struct nmi_desc *desc = nmi_to_desc(type);
+	 *   - arch/x86/kernel/nmi.c|166| <<__register_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+	 *   - arch/x86/kernel/nmi.c|197| <<unregister_nmi_handler>> struct nmi_desc *desc = nmi_to_desc(type);
+	 */
 	struct nmi_desc *desc = nmi_to_desc(type);
 	unsigned long flags;
 
@@ -222,6 +306,10 @@ void unregister_nmi_handler(unsigned int type, const char *name)
 }
 EXPORT_SYMBOL_GPL(unregister_nmi_handler);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|440| <<default_do_nmi>> pci_serr_error(reason, regs);
+ */
 static void
 pci_serr_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -243,6 +331,10 @@ pci_serr_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(pci_serr_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|490| <<default_do_nmi>> io_check_error(reason, regs);
+ */
 static void
 io_check_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -283,6 +375,10 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(io_check_error);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|537| <<default_do_nmi>> unknown_nmi_error(reason, regs);
+ */
 static void
 unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 {
@@ -313,8 +409,19 @@ unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 NOKPROBE_SYMBOL(unknown_nmi_error);
 
 static DEFINE_PER_CPU(bool, swallow_nmi);
+/*
+ * 在以下使用percpu的last_nmi_rip:
+ *   - arch/x86/kernel/nmi.c|316| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+ *   - arch/x86/kernel/nmi.c|337| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+ *   - arch/x86/kernel/nmi.c|342| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+ *   - arch/x86/kernel/nmi.c|659| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+ */
 static DEFINE_PER_CPU(unsigned long, last_nmi_rip);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/nmi.c|645| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> default_do_nmi(regs);
+ */
 static noinstr void default_do_nmi(struct pt_regs *regs)
 {
 	unsigned char reason = 0;
@@ -334,11 +441,25 @@ static noinstr void default_do_nmi(struct pt_regs *regs)
 	 * of the back-to-back NMI, assume we dropped things and process
 	 * more handlers.  Otherwise reset the 'swallow' NMI behaviour
 	 */
+	/*
+	 * 在以下使用percpu的last_nmi_rip:
+	 *   - arch/x86/kernel/nmi.c|316| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+	 *   - arch/x86/kernel/nmi.c|337| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+	 *   - arch/x86/kernel/nmi.c|342| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+	 *   - arch/x86/kernel/nmi.c|659| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+	 */
 	if (regs->ip == __this_cpu_read(last_nmi_rip))
 		b2b = true;
 	else
 		__this_cpu_write(swallow_nmi, false);
 
+	/*
+	 * 在以下使用percpu的last_nmi_rip:
+	 *   - arch/x86/kernel/nmi.c|316| <<global>> static DEFINE_PER_CPU(unsigned long , last_nmi_rip);
+	 *   - arch/x86/kernel/nmi.c|337| <<default_do_nmi>> if (regs->ip == __this_cpu_read(last_nmi_rip))
+	 *   - arch/x86/kernel/nmi.c|342| <<default_do_nmi>> __this_cpu_write(last_nmi_rip, regs->ip);
+	 *   - arch/x86/kernel/nmi.c|659| <<local_touch_nmi>> __this_cpu_write(last_nmi_rip, 0);
+	 */
 	__this_cpu_write(last_nmi_rip, regs->ip);
 
 	instrumentation_begin();
@@ -481,6 +602,13 @@ enum nmi_states {
 	NMI_EXECUTING,
 	NMI_LATCHED,
 };
+/*
+ * 在以下使用percpu的nmi_state:
+ *   - arch/x86/kernel/nmi.c|617| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (this_cpu_read(nmi_state) != NMI_NOT_RUNNING) {
+ *   - arch/x86/kernel/nmi.c|618| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> this_cpu_write(nmi_state, NMI_LATCHED);
+ *   - arch/x86/kernel/nmi.c|621| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> this_cpu_write(nmi_state, NMI_EXECUTING);
+ *   - arch/x86/kernel/nmi.c|664| <<DEFINE_IDTENTRY_RAW(exc_nmi)>> if (this_cpu_dec_return(nmi_state))
+ */
 static DEFINE_PER_CPU(enum nmi_states, nmi_state);
 static DEFINE_PER_CPU(unsigned long, nmi_cr2);
 static DEFINE_PER_CPU(unsigned long, nmi_dr7);
@@ -572,6 +700,10 @@ EXPORT_SYMBOL_GPL(asm_exc_nmi_kvm_vmx);
 
 #ifdef CONFIG_NMI_CHECK_CPU
 
+/*
+ * 在以下使用nmi_check_stall_msg[]:
+ *   = arch/x86/kernel/nmi.c|631| <<nmi_backtrace_stall_check>> msgp = nmi_check_stall_msg[idx];
+ */
 static char *nmi_check_stall_msg[] = {
 /*									*/
 /* +--------- nsp->idt_seq_snap & 0x1: CPU is in NMI handler.		*/
@@ -590,6 +722,10 @@ static char *nmi_check_stall_msg[] = {
 /* 1 1 1 */ "CPU is offline in exc_nmi() handler which is legitimately ignoring NMIs",
 };
 
+/*
+ * called by:
+ *   - lib/nmi_backtrace.c|67| <<nmi_trigger_cpumask_backtrace>> nmi_backtrace_stall_snap(to_cpumask(backtrace_mask));
+ */
 void nmi_backtrace_stall_snap(const struct cpumask *btp)
 {
 	int cpu;
@@ -604,6 +740,10 @@ void nmi_backtrace_stall_snap(const struct cpumask *btp)
 	}
 }
 
+/*
+ * called by:
+ *   - lib/nmi_backtrace.c|78| <<nmi_trigger_cpumask_backtrace>> nmi_backtrace_stall_check(to_cpumask(backtrace_mask));
+ */
 void nmi_backtrace_stall_check(const struct cpumask *btp)
 {
 	int cpu;
@@ -643,16 +783,28 @@ void nmi_backtrace_stall_check(const struct cpumask *btp)
 
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kernel/alternative.c|1566| <<alternative_instructions>> stop_nmi();
+ */
 void stop_nmi(void)
 {
 	ignore_nmis++;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/alternative.c|1645| <<alternative_instructions>> restart_nmi();
+ */
 void restart_nmi(void)
 {
 	ignore_nmis--;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|754| <<arch_cpu_idle_enter>> local_touch_nmi();
+ */
 /* reset the back-to-back NMI logic */
 void local_touch_nmi(void)
 {
diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c
index ac10b46c5..fd6170e85 100644
--- a/arch/x86/kernel/paravirt.c
+++ b/arch/x86/kernel/paravirt.c
@@ -109,9 +109,24 @@ static u64 native_steal_clock(int cpu)
 	return 0;
 }
 
+/*
+ * 在以下使用pv_sched_clock:
+ *   - arch/x86/include/asm/paravirt.h|25| <<global>> DECLARE_STATIC_CALL(pv_sched_clock, dummy_sched_clock);
+ *   - arch/x86/kernel/paravirt.c|113| <<global>> DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
+ *   - arch/x86/include/asm/paravirt.h|31| <<paravirt_sched_clock>> return static_call(pv_sched_clock)();
+ *   - arch/x86/kernel/paravirt.c|124| <<u64>> static_call_update(pv_sched_clock, func);
+ *   - arch/x86/kernel/tsc.c|283| <<using_native_sched_clock>> return static_call_query(pv_sched_clock) == native_sched_clock;
+ */
 DEFINE_STATIC_CALL(pv_steal_clock, native_steal_clock);
 DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+ *   - arch/x86/kernel/kvmclock.c|108| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+ *   - arch/x86/xen/time.c|567| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+ *   - drivers/clocksource/hyperv_timer.c|514| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+ */
 void paravirt_set_sched_clock(u64 (*func)(void))
 {
 	static_call_update(pv_sched_clock, func);
diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c
index b3f81379c..bbe6a9bfe 100644
--- a/arch/x86/kernel/pvclock.c
+++ b/arch/x86/kernel/pvclock.c
@@ -64,6 +64,11 @@ u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src)
 	return flags & valid_flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/pvclock.c|115| <<pvclock_clocksource_read>> return __pvclock_clocksource_read(src, true);
+ *   - arch/x86/kernel/pvclock.c|120| <<pvclock_clocksource_read_nowd>> return __pvclock_clocksource_read(src, false);
+ */
 static __always_inline
 u64 __pvclock_clocksource_read(struct pvclock_vcpu_time_info *src, bool dowd)
 {
@@ -120,6 +125,11 @@ noinstr u64 pvclock_clocksource_read_nowd(struct pvclock_vcpu_time_info *src)
 	return __pvclock_clocksource_read(src, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/kvmclock.c|65| <<kvm_get_wallclock>> pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
+ *   - arch/x86/xen/time.c|83| <<xen_read_wallclock>> pvclock_read_wallclock(wall_clock, vcpu_time, ts);
+ */
 void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,
 			    struct pvclock_vcpu_time_info *vcpu_time,
 			    struct timespec64 *ts)
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index e1aa2cd77..1298b687c 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1129,6 +1129,10 @@ int native_kick_ap(unsigned int cpu, struct task_struct *tidle)
 	return err;
 }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|785| <<cpuhp_kick_ap_alive>> return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));
+ */
 int arch_cpuhp_kick_ap_alive(unsigned int cpu, struct task_struct *tidle)
 {
 	return smp_ops.kick_ap_alive(cpu, tidle);
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 3425c6a94..6b9f7c6d9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -33,6 +33,18 @@
 unsigned int __read_mostly cpu_khz;	/* TSC clocks / usec, not used here */
 EXPORT_SYMBOL(cpu_khz);
 
+/*
+ * 在以下修改tsc_khz:
+ *   - arch/x86/hyperv/hv_init.c|148| <<hyperv_stop_tsc_emulation>> tsc_khz = div64_u64(freq, 1000);
+ *   - arch/x86/kernel/tsc.c|942| <<recalibrate_cpu_khz>> tsc_khz = x86_platform.calibrate_tsc();
+ *   - arch/x86/kernel/tsc.c|944| <<recalibrate_cpu_khz>> tsc_khz = cpu_khz;
+ *   - arch/x86/kernel/tsc.c|1039| <<time_cpufreq_notifier>> tsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);
+ *   - arch/x86/kernel/tsc.c|1441| <<tsc_refine_calibration_work>> tsc_khz = freq;
+ *   - arch/x86/kernel/tsc.c|1509| <<determine_cpu_tsc_frequencies>> tsc_khz = tsc_early_khz;
+ *   - arch/x86/kernel/tsc.c|1511| <<determine_cpu_tsc_frequencies>> tsc_khz = x86_platform.calibrate_tsc();
+ *   - arch/x86/kernel/tsc.c|1523| <<determine_cpu_tsc_frequencies>> if (tsc_khz == 0)
+ *   - arch/x86/kernel/tsc.c|1524| <<determine_cpu_tsc_frequencies>> tsc_khz = cpu_khz;
+ */
 unsigned int __read_mostly tsc_khz;
 EXPORT_SYMBOL(tsc_khz);
 
@@ -46,6 +58,16 @@ static unsigned int __initdata tsc_early_khz;
 
 static DEFINE_STATIC_KEY_FALSE(__use_tsc);
 
+/*
+ * 在以下使用tsc_clocksource_reliable:
+ *   - arch/x86/kernel/tsc.c|341| <<tsc_setup>> tsc_clocksource_reliable = 1;
+ *   - arch/x86/kernel/tsc.c|1270| <<check_system_tsc_reliable>> tsc_clocksource_reliable = 1;
+ *   - arch/x86/kernel/tsc.c|1274| <<check_system_tsc_reliable>> tsc_clocksource_reliable = 1;
+ *   - arch/x86/kernel/tsc.c|1311| <<unsynchronized_tsc>> if (tsc_clocksource_reliable)
+ *   - arch/x86/kernel/tsc.c|1649| <<tsc_init>> if (tsc_clocksource_reliable || no_tsc_watchdog)
+ *   - arch/x86/kernel/tsc_sync.c|110| <<start_sync_check_timer>> if (!cpu_feature_enabled(X86_FEATURE_TSC_ADJUST) || tsc_clocksource_reliable)
+ *   - arch/x86/kernel/tsc_sync.c|445| <<check_tsc_sync_target>> if (tsc_store_and_check_tsc_adjust(false) || tsc_clocksource_reliable)
+ */
 int tsc_clocksource_reliable;
 
 static int __read_mostly tsc_force_recalibrate;
@@ -268,6 +290,14 @@ noinstr u64 sched_clock_noinstr(void)
 
 bool using_native_sched_clock(void)
 {
+	/*
+	 * 在以下使用pv_sched_clock:
+	 *   - arch/x86/include/asm/paravirt.h|25| <<global>> DECLARE_STATIC_CALL(pv_sched_clock, dummy_sched_clock);
+	 *   - arch/x86/kernel/paravirt.c|113| <<global>> DEFINE_STATIC_CALL(pv_sched_clock, native_sched_clock);
+	 *   - arch/x86/include/asm/paravirt.h|31| <<paravirt_sched_clock>> return static_call(pv_sched_clock)();
+	 *   - arch/x86/kernel/paravirt.c|124| <<u64>> static_call_update(pv_sched_clock, func);
+	 *   - arch/x86/kernel/tsc.c|283| <<using_native_sched_clock>> return static_call_query(pv_sched_clock) == native_sched_clock;
+	 */
 	return static_call_query(pv_sched_clock) == native_sched_clock;
 }
 #else
@@ -1215,6 +1245,11 @@ void mark_tsc_unstable(char *reason)
 
 EXPORT_SYMBOL_GPL(mark_tsc_unstable);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1274| <<check_system_tsc_reliable>> tsc_disable_clocksource_watchdog();
+ *   - arch/x86/kernel/tsc.c|1633| <<tsc_init>> tsc_disable_clocksource_watchdog();
+ */
 static void __init tsc_disable_clocksource_watchdog(void)
 {
 	clocksource_tsc_early.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
@@ -1227,6 +1262,10 @@ bool tsc_clocksource_watchdog_disabled(void)
 	       tsc_as_watchdog && !no_tsc_watchdog;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1638| <<tsc_init>> check_system_tsc_reliable();
+ */
 static void __init check_system_tsc_reliable(void)
 {
 #if defined(CONFIG_MGEODEGX1) || defined(CONFIG_MGEODE_LX) || defined(CONFIG_X86_GENERIC)
@@ -1489,6 +1528,9 @@ static int __init init_tsc_clocksource(void)
 			return 0;
 	}
 
+	/*
+	 * tsc_refine_calibration_work()
+	 */
 	schedule_delayed_work(&tsc_irqwork, 0);
 	return 0;
 }
@@ -1498,6 +1540,11 @@ static int __init init_tsc_clocksource(void)
  */
 device_initcall(init_tsc_clocksource);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/tsc.c|1569| <<tsc_early_init>> if (!determine_cpu_tsc_frequencies(true))
+ *   - arch/x86/kernel/tsc.c|1590| <<tsc_init>> if (!determine_cpu_tsc_frequencies(false)) {
+ */
 static bool __init determine_cpu_tsc_frequencies(bool early)
 {
 	/* Make sure that cpu and tsc are not already calibrated */
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index b28fd0200..eb2cf8e19 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -1385,6 +1385,18 @@ static int kvm_hv_set_msr_pw(struct kvm_vcpu *vcpu, u32 msr, u64 data,
 				hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
 			else
 				hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+			/*
+			 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+			 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+			 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+			 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+			 *   - kvm_update_masterclock()
+			 */
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 		} else {
 			hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 16d076a1b..151750a37 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -44,6 +44,15 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|495| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|473| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, &ioapic->rtc_status.dest_map);
+ *   - arch/x86/kvm/ioapic.c|477| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq_comm.c|144| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1506| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|10442| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a983a1616..2d7b4dfef 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -796,6 +796,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|839| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2174| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|77| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|99| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|821| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1200| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1213| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|13978| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1264,6 +1275,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|804| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+ *   - arch/x86/kvm/lapic.c|2792| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1301,6 +1317,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		static_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,
 						       trig_mode, vector);
 		break;
@@ -1321,6 +1341,11 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 
 	case APIC_DM_NMI:
 		result = 1;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|1328| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+		 *   - arch/x86/kvm/x86.c|5611| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+		 */
 		kvm_inject_nmi(vcpu);
 		kvm_vcpu_kick(vcpu);
 		break;
@@ -1853,6 +1878,12 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1882| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|1897| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2859| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1866,6 +1897,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1935| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2047| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2108| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2157| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2770| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1966,6 +2005,10 @@ static void update_target_expiration(struct kvm_lapic *apic, uint32_t old_diviso
 	apic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2198| <<__start_apic_timer>> && !set_target_expiration(apic, count_reg))
+ */
 static bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)
 {
 	ktime_t now;
@@ -2086,6 +2129,9 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (!ktimer->tscdeadline)
 		return false;
 
+	/*
+	 * vmx_set_hv_timer()
+	 */
 	if (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
@@ -2201,6 +2247,11 @@ static void __start_apic_timer(struct kvm_lapic *apic, u32 count_reg)
 	restart_apic_timer(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2358| <<kvm_lapic_reg_write(APIC_TMICT)>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2515| <<kvm_set_lapic_tscdeadline_msr>> start_apic_timer(apic);
+ */
 static void start_apic_timer(struct kvm_lapic *apic)
 {
 	__start_apic_timer(apic, APIC_TMICT);
@@ -2468,6 +2519,10 @@ void kvm_free_lapic(struct kvm_vcpu *vcpu)
  * LAPIC interface
  *----------------------------------------------------------------------
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4167| <<kvm_get_msr_common(MSR_IA32_TSC_DEADLINE)>> msr_info->data = kvm_get_lapic_tscdeadline_msr(vcpu);
+ */
 u64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2478,6 +2533,11 @@ u64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)
 	return apic->lapic_timer.tscdeadline;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2177| <<handle_fastpath_set_tscdeadline>> kvm_set_lapic_tscdeadline_msr(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3737| <<kvm_set_msr_common>> kvm_set_lapic_tscdeadline_msr(vcpu, data);
+ */
 void kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2734,6 +2794,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1870| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2796| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|767| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5686| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2981,6 +3048,9 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	kvm_lapic_set_reg(apic, APIC_TMCCT, 0);
 	kvm_apic_update_apicv(vcpu);
 	if (apic->apicv_active) {
+		/*
+		 * vmx_apicv_post_state_restore()
+		 */
 		static_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);
 		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
 		static_call_cond(kvm_x86_hwapic_isr_update)(apic_find_highest_isr(apic));
@@ -3228,6 +3298,13 @@ int kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11483| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+ *   - arch/x86/kvm/x86.c|11759| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+ *   - arch/x86/kvm/x86.c|11955| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+ *   - arch/x86/kvm/x86.c|12203| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+ */
 int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -3259,6 +3336,20 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 		return 0;
 	}
 
+	/*
+	 * #define KVM_MP_STATE_RUNNABLE          0
+	 * #define KVM_MP_STATE_UNINITIALIZED     1
+	 * #define KVM_MP_STATE_INIT_RECEIVED     2
+	 * #define KVM_MP_STATE_HALTED            3
+	 * #define KVM_MP_STATE_SIPI_RECEIVED     4
+	 * #define KVM_MP_STATE_STOPPED           5
+	 * #define KVM_MP_STATE_CHECK_STOP        6
+	 * #define KVM_MP_STATE_OPERATING         7
+	 * #define KVM_MP_STATE_LOAD              8
+	 * #define KVM_MP_STATE_AP_RESET_HOLD     9
+	 * #define KVM_MP_STATE_SUSPENDED         10
+	 */
+
 	if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
 		kvm_vcpu_reset(vcpu, true);
 		if (kvm_vcpu_is_bsp(apic->vcpu))
@@ -3271,6 +3362,9 @@ int kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 			/* evaluate pending_events before reading the vector */
 			smp_rmb();
 			sipi_vector = apic->sipi_vector;
+			/*
+			 * kvm_vcpu_deliver_sipi_vector()
+			 */
 			static_call(kvm_x86_vcpu_deliver_sipi_vector)(vcpu, sipi_vector);
 			vcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;
 		}
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 0a0ea4b5d..a89961b81 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -78,6 +78,26 @@ struct kvm_lapic {
 	void *regs;
 	gpa_t vapic_addr;
 	struct gfn_to_hva_cache vapic_cache;
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1357| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1368| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3328| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3332| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.c|3339| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.h|233| <<kvm_apic_has_pending_init_or_sipi>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|250| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1442| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4074| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4078| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|4088| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4092| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6003| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6005| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|12259| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 *
+	 * 似乎真的只有KVM_APIC_SIPI和KVM_APIC_INIT使用这里
+	 */
 	unsigned long pending_events;
 	unsigned int sipi_vector;
 	int nr_lvt_entries;
@@ -230,11 +250,40 @@ static inline bool kvm_vcpu_apicv_active(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_apic_has_pending_init_or_sipi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_lapic->pending_events:
+	 *   - arch/x86/kvm/lapic.c|1357| <<__apic_accept_irq>> apic->pending_events = (1UL << KVM_APIC_INIT);
+	 *   - arch/x86/kvm/lapic.c|1368| <<__apic_accept_irq>> set_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3328| <<kvm_apic_accept_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/lapic.c|3332| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.c|3339| <<kvm_apic_accept_events>> if (test_and_clear_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/lapic.h|233| <<kvm_apic_has_pending_init_or_sipi>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
+	 *   - arch/x86/kvm/lapic.h|250| <<kvm_lapic_latched_init>> return lapic_in_kernel(vcpu) && test_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/svm/nested.c|1442| <<svm_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4074| <<vmx_check_nested_events>> test_bit(KVM_APIC_INIT, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4078| <<vmx_check_nested_events>> clear_bit(KVM_APIC_INIT, &apic->pending_events);
+	 *   - arch/x86/kvm/vmx/nested.c|4088| <<vmx_check_nested_events>> test_bit(KVM_APIC_SIPI, &apic->pending_events)) {
+	 *   - arch/x86/kvm/vmx/nested.c|4092| <<vmx_check_nested_events>> clear_bit(KVM_APIC_SIPI, &apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6003| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> set_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|6005| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> clear_bit(KVM_APIC_INIT, &vcpu->arch.apic->pending_events);
+	 *   - arch/x86/kvm/x86.c|12259| <<kvm_arch_vcpu_ioctl_set_mpstate>> set_bit(KVM_APIC_SIPI, &vcpu->arch.apic->pending_events);
+	 *
+	 * 似乎真的只有KVM_APIC_SIPI和KVM_APIC_INIT使用这里
+	 */
 	return lapic_in_kernel(vcpu) && vcpu->arch.apic->pending_events;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3326| <<kvm_apic_accept_events>> if (!kvm_apic_init_sipi_allowed(vcpu)) {
+ *   - arch/x86/kvm/x86.c|12252| <<kvm_arch_vcpu_ioctl_set_mpstate>> if ((!kvm_apic_init_sipi_allowed(vcpu) || vcpu->arch.smi_pending) &&
+ *   - arch/x86/kvm/x86.c|13713| <<kvm_vcpu_has_events>> kvm_apic_init_sipi_allowed(vcpu))
+ */
 static inline bool kvm_apic_init_sipi_allowed(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * vmx_apic_init_signal_blocked()
+	 */
 	return !is_smm(vcpu) &&
 	       !static_call(kvm_x86_apic_init_signal_blocked)(vcpu);
 }
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index ec169f5c7..2d23ebaed 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -570,13 +570,28 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
  * state bits, it is used to clear the last level sptep.
  * Returns the old PTE.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1047| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1062| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+ *   - arch/x86/kvm/mmu/mmu.c|1070| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+ *   - arch/x86/kvm/mmu/mmu.c|1204| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1495| <<kvm_set_pte_rmap>> mmu_spte_clear_track_bits(kvm, sptep);
+ */
 static u64 mmu_spte_clear_track_bits(struct kvm *kvm, u64 *sptep)
 {
 	kvm_pfn_t pfn;
 	u64 old_spte = *sptep;
+	/*
+	 * spte是在一个page里的, 获得那个page的kvm_mmu_page
+	 */
 	int level = sptep_to_sp(sptep)->role.level;
 	struct page *page;
 
+	/*
+	 * 用不同的方式把pte (也就是sptep指向的内存)
+	 * 设置成0, clear
+	 */
 	if (!is_shadow_present_pte(old_spte) ||
 	    !spte_has_volatile_bits(old_spte))
 		__update_clear_spte_fast(sptep, 0ull);
@@ -726,6 +741,19 @@ static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
 
 static bool sp_has_gptes(struct kvm_mmu_page *sp);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|299| <<kvm_flush_remote_tlbs_sptep>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|798| <<kvm_mmu_page_set_translation>> WARN_ONCE(gfn != kvm_mmu_page_get_gfn(sp, index),
+ *   - arch/x86/kvm/mmu/mmu.c|801| <<kvm_mmu_page_set_translation>> sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|807| <<kvm_mmu_page_set_access>> gfn_t gfn = kvm_mmu_page_get_gfn(sp, index);
+ *   - arch/x86/kvm/mmu/mmu.c|1148| <<rmap_remove>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
+ *   - arch/x86/kvm/mmu/mmu.c|3060| <<direct_pte_prefetch_many>> gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
+ *   - arch/x86/kvm/mmu/mmu.c|6456| <<shadow_mmu_get_sp_for_split>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/mmu.c|6492| <<shadow_mmu_split_huge_page>> gfn = kvm_mmu_page_get_gfn(sp, index);
+ *   - arch/x86/kvm/mmu/mmu.c|6531| <<shadow_mmu_try_split_huge_page>> gfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|924| <<FNAME>> gfn != kvm_mmu_page_get_gfn(sp, i)) {
+ */
 static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
 {
 	if (sp->role.passthrough)
@@ -734,6 +762,10 @@ static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
 	if (!sp->role.direct)
 		return sp->shadowed_translation[index] >> PAGE_SHIFT;
 
+	/*
+	 * struct kvm_mmu_page *sp:
+	 * -> gfn_t gfn;
+	 */
 	return sp->gfn + (index << ((sp->role.level - 1) * SPTE_LEVEL_BITS));
 }
 
@@ -848,6 +880,11 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 		kvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|914| <<account_nx_huge_page>> track_possible_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1134| <<kvm_tdp_mmu_map>> track_possible_nx_huge_page(kvm, sp);
+ */
 void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	/*
@@ -861,11 +898,32 @@ void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 	if (!list_empty(&sp->possible_nx_huge_page_link))
 		return;
 
+	/*
+	 * 在以下使用kvm_vm_stat->nx_lpage_splits:
+	 *   - arch/x86/include/asm/kvm_host.h|1646| <<global>> u64 nx_lpage_splits;
+	 *   - arch/x86/kvm/x86.c|266| <<global>> STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+	 *   - arch/x86/kvm/mmu/mmu.c|896| <<track_possible_nx_huge_page>> ++kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<untrack_possible_nx_huge_page>> --kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7179| <<kvm_recover_nx_huge_pages>> unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7199| <<kvm_recover_nx_huge_pages>> to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
+	 */
 	++kvm->stat.nx_lpage_splits;
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|898| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link, &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|6321| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7228| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7238| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages, struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	list_add_tail(&sp->possible_nx_huge_page_link,
 		      &kvm->arch.possible_nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3430| <<direct_map>> account_nx_huge_page(vcpu->kvm, sp, fault->req_level >= it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|723| <<FNAME(fetch)>> account_nx_huge_page(vcpu->kvm, sp, fault->req_level >= it.level);
+ */
 static void account_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				 bool nx_huge_page_possible)
 {
@@ -901,6 +959,11 @@ void untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 	list_del_init(&sp->possible_nx_huge_page_link);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2802| <<__kvm_mmu_prepare_zap_page>> unaccount_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/mmu.c|7411| <<kvm_recover_nx_huge_pages>> unaccount_nx_huge_page(kvm, sp);
+ */
 static void unaccount_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	sp->nx_huge_page_disallowed = false;
@@ -934,6 +997,11 @@ static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu
 /*
  * Returns the number of pointers in the rmap chain, not counting the new one.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1717| <<__rmap_add>> rmap_count = pte_list_add(cache, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1823| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+ */
 static int pte_list_add(struct kvm_mmu_memory_cache *cache, u64 *spte,
 			struct kvm_rmap_head *rmap_head)
 {
@@ -1009,6 +1077,12 @@ static void pte_list_desc_remove_entry(struct kvm_rmap_head *rmap_head,
 	mmu_free_pte_list_desc(head_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1048| <<kvm_zap_one_rmap_spte>> pte_list_remove(sptep, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1123| <<rmap_remove>> pte_list_remove(spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1785| <<mmu_page_remove_parent_pte>> pte_list_remove(parent_pte, &sp->parent_ptes);
+ */
 static void pte_list_remove(u64 *spte, struct kvm_rmap_head *rmap_head)
 {
 	struct pte_list_desc *desc;
@@ -1090,23 +1164,45 @@ unsigned int pte_list_count(struct kvm_rmap_head *rmap_head)
 	return desc->tail_count + desc->spte_count;
 }
 
+/*
+ *  所有map到这个page的sptep??? (最后一个p, 是指针!!!)
+ */
 static struct kvm_rmap_head *gfn_to_rmap(gfn_t gfn, int level,
 					 const struct kvm_memory_slot *slot)
 {
 	unsigned long idx;
 
 	idx = gfn_to_index(gfn, slot->base_gfn, level);
+	/*
+	 * struct kvm_memor_slot *slot:
+	 * -> struct kvm_arch_memory_slot arch;
+	 *    -> struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+	 *    -> struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	 *    -> unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
+	 */
 	return &slot->arch.rmap[level - PG_LEVEL_4K][idx];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1251| <<drop_spte>> rmap_remove(kvm, sptep);
+ */
 static void rmap_remove(struct kvm *kvm, u64 *spte)
 {
 	struct kvm_memslots *slots;
 	struct kvm_memory_slot *slot;
 	struct kvm_mmu_page *sp;
 	gfn_t gfn;
+	/*
+	 * struct kvm_rmap_head {
+	 *     unsigned long val;
+	 * };
+	 */
 	struct kvm_rmap_head *rmap_head;
 
+	/*
+	 * spte是在一个page里的, 获得那个page的kvm_mmu_page
+	 */
 	sp = sptep_to_sp(spte);
 	gfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));
 
@@ -1118,8 +1214,23 @@ static void rmap_remove(struct kvm *kvm, u64 *spte)
 	slots = kvm_memslots_for_spte_role(kvm, sp->role);
 
 	slot = __gfn_to_memslot(slots, gfn);
+	/*
+	 * struct kvm_memor_slot *slot:
+	 * -> struct kvm_arch_memory_slot arch;
+	 *    -> struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
+	 *    -> struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
+	 *    -> unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
+	 *
+	 * 所有map到这个page的sptep??? (最后一个p, 是指针!!!)
+	 */
 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1048| <<kvm_zap_one_rmap_spte>> pte_list_remove(sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1123| <<rmap_remove>> pte_list_remove(spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1785| <<mmu_page_remove_parent_pte>> pte_list_remove(parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(spte, rmap_head);
 }
 
@@ -1199,6 +1310,14 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
 	     _spte_; _spte_ = rmap_get_next(_iter_))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1261| <<drop_large_spte>> drop_spte(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|2565| <<mmu_page_zap_pte>> drop_spte(kvm, spte);
+ *   - arch/x86/kvm/mmu/mmu.c|3013| <<mmu_set_spte>> drop_spte(vcpu->kvm, sptep);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|169| <<FNAME(prefetch_invalid_gpte)>> drop_spte(vcpu->kvm, spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|925| <<FNAME(sync_spte)>> drop_spte(vcpu->kvm, &sp->spt[i]);
+ */
 static void drop_spte(struct kvm *kvm, u64 *sptep)
 {
 	u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
@@ -1207,6 +1326,10 @@ static void drop_spte(struct kvm *kvm, u64 *sptep)
 		rmap_remove(kvm, sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2589| <<__link_shadow_page>> drop_large_spte(kvm, sptep, flush);
+ */
 static void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)
 {
 	struct kvm_mmu_page *sp;
@@ -1458,6 +1581,10 @@ static bool __kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 	return kvm_zap_all_rmap_sptes(kvm, rmap_head);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1645| <<kvm_unmap_gfn_range>> flush = kvm_handle_gfn_range(kvm, range, kvm_zap_rmap);
+ */
 static bool kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			 struct kvm_memory_slot *slot, gfn_t gfn, int level,
 			 pte_t unused)
@@ -1610,6 +1737,10 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	return flush;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|687| <<kvm_change_spte_gfn>> return kvm_set_spte_gfn(kvm, range);
+ */
 bool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool flush = false;
@@ -1652,6 +1783,11 @@ static bool kvm_test_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 
 #define RMAP_RECYCLE_THRESHOLD 1000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1732| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|6515| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+ */
 static void __rmap_add(struct kvm *kvm,
 		       struct kvm_mmu_memory_cache *cache,
 		       const struct kvm_memory_slot *slot,
@@ -1676,6 +1812,10 @@ static void __rmap_add(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3041| <<mmu_set_spte>> rmap_add(vcpu, slot, sptep, gfn, pte_access);
+ */
 static void rmap_add(struct kvm_vcpu *vcpu, const struct kvm_memory_slot *slot,
 		     u64 *spte, gfn_t gfn, unsigned int access)
 {
@@ -2504,6 +2644,12 @@ static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2658| <<kvm_mmu_page_unlink_children>> zapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5851| <<kvm_mmu_pte_write>> mmu_page_zap_pte(vcpu->kvm, sp, spte, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|5950| <<__kvm_mmu_invalidate_addr>> mmu_page_zap_pte(vcpu->kvm, sp, iterator.sptep, NULL);
+ */
 /* Returns the number of zapped non-leaf child shadow pages. */
 static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 			    u64 *spte, struct list_head *invalid_list)
@@ -2514,6 +2660,14 @@ static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 	pte = *spte;
 	if (is_shadow_present_pte(pte)) {
 		if (is_last_spte(pte, sp->role.level)) {
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/mmu/mmu.c|1261| <<drop_large_spte>> drop_spte(kvm, sptep);
+			 *   - arch/x86/kvm/mmu/mmu.c|2565| <<mmu_page_zap_pte>> drop_spte(kvm, spte);
+			 *   - arch/x86/kvm/mmu/mmu.c|3013| <<mmu_set_spte>> drop_spte(vcpu->kvm, sptep);
+			 *   - arch/x86/kvm/mmu/paging_tmpl.h|169| <<FNAME(prefetch_invalid_gpte)>> drop_spte(vcpu->kvm, spte);
+			 *   - arch/x86/kvm/mmu/paging_tmpl.h|925| <<FNAME(sync_spte)>> drop_spte(vcpu->kvm, &sp->spt[i]);
+			 */
 			drop_spte(kvm, spte);
 		} else {
 			child = spte_to_child_sp(pte);
@@ -2535,6 +2689,10 @@ static int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2707| <<__kvm_mmu_prepare_zap_page>> *nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);
+ */
 static int kvm_mmu_page_unlink_children(struct kvm *kvm,
 					struct kvm_mmu_page *sp,
 					struct list_head *invalid_list)
@@ -2542,6 +2700,12 @@ static int kvm_mmu_page_unlink_children(struct kvm *kvm,
 	int zapped = 0;
 	unsigned i;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2658| <<kvm_mmu_page_unlink_children>> zapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);
+	 *   - arch/x86/kvm/mmu/mmu.c|5851| <<kvm_mmu_pte_write>> mmu_page_zap_pte(vcpu->kvm, sp, spte, NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|5950| <<__kvm_mmu_invalidate_addr>> mmu_page_zap_pte(vcpu->kvm, sp, iterator.sptep, NULL);
+	 */
 	for (i = 0; i < SPTE_ENT_PER_PAGE; ++i)
 		zapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);
 
@@ -2581,6 +2745,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2778| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2828| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|6241| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &kvm->arch.zapped_obsolete_pages, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|6870| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2646,6 +2817,17 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 	return list_unstable;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2113| <<kvm_sync_page>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2299| <<kvm_mmu_find_shadow_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2634| <<mmu_page_zap_pte>> return kvm_mmu_prepare_zap_page(kvm, child, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2680| <<mmu_zap_unsync_children>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|2890| <<kvm_mmu_unprotect_page>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|3693| <<mmu_free_root_page>> kvm_mmu_prepare_zap_page(kvm, sp, invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|5821| <<kvm_mmu_pte_write>> kvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);
+ *   - arch/x86/kvm/mmu/mmu.c|7278| <<kvm_recover_nx_huge_pages>> kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+ */
 static bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				     struct list_head *invalid_list)
 {
@@ -2921,10 +3103,20 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3199| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn,
+ *   - arch/x86/kvm/mmu/mmu.c|3479| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|730| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
 {
+	/*
+	 * spte是在一个page里的, 获得那个page的kvm_mmu_page
+	 */
 	struct kvm_mmu_page *sp = sptep_to_sp(sptep);
 	int level = sp->role.level;
 	int was_rmapped = 0;
@@ -3194,6 +3386,15 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3351| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3368| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3429| <<direct_map>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1114| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1130| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -3224,8 +3425,21 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->pfn &= ~mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3446| <<direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|708| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1090| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+ */
 void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|911| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|945| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1114| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 */
 	if (cur_level > PG_LEVEL_4K &&
 	    cur_level == fault->goal_level &&
 	    is_shadow_present_pte(spte) &&
@@ -3245,6 +3459,10 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4617| <<direct_page_fault>> r = direct_map(vcpu, fault);
+ */
 static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_shadow_walk_iterator it;
@@ -3255,6 +3473,10 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
 	trace_kvm_mmu_spte_requested(fault);
+	/*
+	 * 本来是hugepage, 要变成普通pte
+	 * 本来是普通pte, 发现可以huge page (merge了???)
+	 */
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
 		 * We cannot overwrite existing page tables with an NX
@@ -3280,6 +3502,13 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3199| <<direct_pte_prefetch_many>> mmu_set_spte(vcpu, slot, start, access, gfn,
+	 *   - arch/x86/kvm/mmu/mmu.c|3479| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> mmu_set_spte(vcpu, slot, spte, pte_access, gfn, pfn, NULL);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|730| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -4403,8 +4632,16 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_invalidate_retry_hva(vcpu->kvm, fault->mmu_seq, fault->hva);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4632| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4734| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
+	/*
+	 * 在kvm_mmu_do_page_fault()配置fault (struct kvm_page_fault)
+	 */
 	int r;
 
 	if (page_fault_handle_page_track(vcpu, fault))
@@ -4519,6 +4756,12 @@ static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 }
 #endif
 
+/*
+ * 在以下使用kvm_tdp_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|5433| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *   - arch/x86/kvm/mmu/mmu_internal.h|325| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *   - arch/x86/kvm/mmu/mmu_internal.h|349| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	/*
@@ -6143,6 +6386,13 @@ static void kvm_zap_obsolete_pages(struct kvm *kvm)
  * not use any resource of the being-deleted slot or all slots
  * after calling the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6449| <<kvm_mmu_invalidate_zap_pages_in_memslot>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|7024| <<kvm_mmu_invalidate_mmio_sptes>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmu.c|7162| <<set_nx_huge_pages>> kvm_mmu_zap_all_fast(kvm);
+ *   - arch/x86/kvm/mmu/mmutrace.h|290| <<__field>> kvm_mmu_zap_all_fast,
+ */
 static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 {
 	lockdep_assert_held(&kvm->slots_lock);
@@ -7016,6 +7266,12 @@ void kvm_mmu_vendor_module_exit(void)
  * Calculate the effective recovery period, accounting for '0' meaning "let KVM
  * select a halving time of 1 hour".  Returns true if recovery is enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7154| <<set_nx_huge_pages_recovery_param>> was_recovery_enabled = calc_nx_huge_pages_recovery_period(&old_period);
+ *   - arch/x86/kvm/mmu/mmu.c|7160| <<set_nx_huge_pages_recovery_param>> is_recovery_enabled = calc_nx_huge_pages_recovery_period(&new_period);
+ *   - arch/x86/kvm/mmu/mmu.c|7277| <<get_nx_huge_page_recovery_timeout>> enabled = calc_nx_huge_pages_recovery_period(&period);
+ */
 static bool calc_nx_huge_pages_recovery_period(uint *period)
 {
 	/*
@@ -7071,6 +7327,15 @@ static int set_nx_huge_pages_recovery_param(const char *val, const struct kernel
 
 static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_vm_stat->nx_lpage_splits:
+	 *   - arch/x86/include/asm/kvm_host.h|1646| <<global>> u64 nx_lpage_splits;
+	 *   - arch/x86/kvm/x86.c|266| <<global>> STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+	 *   - arch/x86/kvm/mmu/mmu.c|896| <<track_possible_nx_huge_page>> ++kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<untrack_possible_nx_huge_page>> --kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7179| <<kvm_recover_nx_huge_pages>> unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7199| <<kvm_recover_nx_huge_pages>> to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
+	 */
 	unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
 	struct kvm_memory_slot *slot;
 	int rcu_idx;
@@ -7093,6 +7358,24 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * 注释
+		 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+		 * replaced by an NX huge page.  A shadow page is on this list if its
+		 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+		 * and there are no other conditions that prevent a huge page, e.g.
+		 * the backing host page is huge, dirtly logging is not enabled for its
+		 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+		 * guarantee an NX huge page will be created in its stead, e.g. if the
+		 * guest attempts to execute from the region then KVM obviously can't
+		 * create an NX huge page (without hanging the guest).
+		 *
+		 * 在以下使用kvm_arch->possible_nx_huge_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|898| <<track_possible_nx_huge_page>> &kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|6321| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|7228| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|7238| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages, struct kvm_mmu_page, possible_nx_huge_page_link);
+		 */
 		if (list_empty(&kvm->arch.possible_nx_huge_pages))
 			break;
 
@@ -7103,9 +7386,21 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 		 * the total number of shadow pages.  And because the TDP MMU
 		 * doesn't use active_mmu_pages.
 		 */
+		/*
+		 * struct kvm_mmu_page *sp;
+		 */
 		sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
 				      struct kvm_mmu_page,
 				      possible_nx_huge_page_link);
+		/*
+		 * 只要是在链表上的必须是sp->nx_huge_page_disallowed=true
+		 * 和sp->role.direct=true
+		 *
+		 * 注释:
+		 * The shadow page can't be replaced by an equivalent huge page
+		 * because it is being used to map an executable page in the guest
+		 * and the NX huge page mitigation is enabled.
+		 */
 		WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
 		WARN_ON_ONCE(!sp->role.direct);
 
@@ -7144,6 +7439,13 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 			flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
 		else
 			kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);
+		/*
+		 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|911| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+		 *   - arch/x86/kvm/mmu/mmu.c|945| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1114| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 */
 		WARN_ON_ONCE(sp->nx_huge_page_disallowed);
 
 		if (need_resched() || rwlock_needbreak(&kvm->mmu_lock)) {
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index d39af5639..cb1f24499 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -58,6 +58,20 @@ struct kvm_mmu_page {
 	bool unsync;
 	u8 mmu_valid_gen;
 
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|911| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|945| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|345| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1114| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2747| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3357| <<disallowed_hugepage_adjust>> spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7278| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7316| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|337| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1133| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 */
 	 /*
 	  * The shadow page can't be replaced by an equivalent huge page
 	  * because it is being used to map an executable page in the guest
@@ -203,8 +217,25 @@ struct kvm_page_fault {
 
 	/* Derived from mmu and global state.  */
 	const bool is_tdp;
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu/mmu.c|3351| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3417| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|309| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|707| <<FNAME>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1089| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	const bool nx_huge_page_workaround_enabled;
 
+	/*
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3351| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3368| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3429| <<direct_map>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|722| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1114| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1130| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 */
 	/*
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
@@ -279,6 +310,11 @@ enum {
 	RET_PF_SPURIOUS,
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4522| <<kvm_arch_async_page_ready>> kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|5999| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u32 err, bool prefetch, int *emulation_type)
 {
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index 1279db2ea..5d73b41dc 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -12,6 +12,15 @@
  * better code than for a high bit, e.g. 56+.  MMU present checks are pervasive
  * enough that the improved code generation is noticeable in KVM's footprint.
  */
+/*
+ * 在以下使用SPTE_MMU_PRESENT_MASK:
+ *   - arch/x86/kvm/mmu/spte.h|123| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+ *   - arch/x86/kvm/mmu/spte.h|138| <<global>> (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+ *   - arch/x86/kvm/mmu/spte.h|199| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/spte.c|144| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.c|314| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+ *   - arch/x86/kvm/mmu/spte.h|254| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+ */
 #define SPTE_MMU_PRESENT_MASK		BIT_ULL(11)
 
 /*
@@ -206,6 +215,10 @@ static inline bool is_removed_spte(u64 spte)
 /* Get an SPTE's index into its parent's page table (and the spt array). */
 static inline int spte_index(u64 *sptep)
 {
+	/*
+	 * SPTE_ENT_PER_PAGE应该是512
+	 * 512个entry一个page
+	 */
 	return ((unsigned long)sptep / sizeof(*sptep)) & (SPTE_ENT_PER_PAGE - 1);
 }
 
@@ -231,6 +244,9 @@ static inline struct kvm_mmu_page *spte_to_child_sp(u64 spte)
 	return to_shadow_page(spte & SPTE_BASE_ADDR_MASK);
 }
 
+/*
+ * spte是在一个page里的, 获得那个page的kvm_mmu_page
+ */
 static inline struct kvm_mmu_page *sptep_to_sp(u64 *sptep)
 {
 	return to_shadow_page(__pa(sptep));
@@ -244,6 +260,15 @@ static inline bool is_mmio_spte(u64 spte)
 
 static inline bool is_shadow_present_pte(u64 pte)
 {
+	/*
+	 * 在以下使用SPTE_MMU_PRESENT_MASK:
+	 *   - arch/x86/kvm/mmu/spte.h|123| <<global>> static_assert(!(SPTE_MMU_PRESENT_MASK &
+	 *   - arch/x86/kvm/mmu/spte.h|138| <<global>> (SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));
+	 *   - arch/x86/kvm/mmu/spte.h|199| <<global>> static_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/spte.c|144| <<make_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.c|314| <<make_nonleaf_spte>> u64 spte = SPTE_MMU_PRESENT_MASK;
+	 *   - arch/x86/kvm/mmu/spte.h|254| <<is_shadow_present_pte>> return !!(pte & SPTE_MMU_PRESENT_MASK);
+	 */
 	return !!(pte & SPTE_MMU_PRESENT_MASK);
 }
 
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 512163d52..6ea87ed3e 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -809,6 +809,10 @@ static void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7276| <<kvm_recover_nx_huge_pages>> flush |= kvm_tdp_mmu_zap_sp(kvm, sp);
+ */
 bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	u64 old_spte;
@@ -915,6 +919,10 @@ void kvm_tdp_mmu_zap_all(struct kvm *kvm)
  * Zap all invalidated roots to ensure all SPTEs are dropped before the "fast
  * zap" completes.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6194| <<kvm_mmu_zap_all_fast>> kvm_tdp_mmu_zap_invalidated_roots(kvm);
+ */
 void kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)
 {
 	flush_workqueue(kvm->arch.tdp_mmu_zap_wq);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index bf653df86..77f5de5c7 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,12 +23,55 @@
 #include "lapic.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - arch/x86/kvm/pmu.c|994| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ */
 /* This is enough to filter the vast majority of currently defined events. */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/pmu.c|29| <<global>> struct x86_pmu_capability __read_mostly kvm_pmu_cap;
+ *   - arch/x86/kvm/cpuid.c|958| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|959| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|960| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|961| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len;
+ *   - arch/x86/kvm/cpuid.c|962| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|963| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|965| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|971| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/cpuid.c|1251| <<__do_cpuid_func>> ebx.split.num_core_pmc = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/pmu.h|190| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|198| <<kvm_init_pmu_capability>> if (!kvm_pmu_cap.num_counters_gp || WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|199| <<kvm_init_pmu_capability>> WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|201| <<kvm_init_pmu_capability>> else if (is_intel && !kvm_pmu_cap.version)
+ *   - arch/x86/kvm/pmu.h|206| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|210| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|211| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_gp = min(kvm_pmu_cap.num_counters_gp, pmu_ops->MAX_NR_GP_COUNTERS);
+ *   - arch/x86/kvm/pmu.h|213| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed, KVM_PMC_MAX_FIXED);
+ *   - arch/x86/kvm/svm/pmu.c|204| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int, pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5015| <<svm_set_cpu_caps>> if (kvm_pmu_cap.num_counters_gp < AMD64_NUM_COUNTERS_CORE)
+ *   - arch/x86/kvm/svm/svm.c|5016| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5017| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5021| <<svm_set_cpu_caps>> if (kvm_pmu_cap.version != 2 || !kvm_cpu_cap_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/vmx/capabilities.h|395| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|499| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|501| <<intel_pmu_refresh>> eax.split.bit_width = min_t(int, eax.split.bit_width, kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> eax.split.mask_length = min_t(int, eax.split.mask_length, kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|514| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min3(ARRAY_SIZE(fixed_pmc_events), (size_t) edx.split.num_counters_fixed, (size_t)kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|516| <<intel_pmu_refresh>> edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed, kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|7194| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_PERFCTR0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7199| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_EVENTSEL0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7204| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_FIXED_CTR0 >= kvm_pmu_cap.num_counters_fixed)
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
+/*
+ * 在以下使用vmx_pebs_pdir_cpu[]:
+ *   - arch/x86/kvm/pmu.c|216| <<pmc_get_pebs_precise_level>> (pmc->idx == 32 && x86_match_cpu(vmx_pebs_pdir_cpu)))
+ */
 /* Precise Distribution of Instructions Retired (PDIR) */
 static const struct x86_cpu_id vmx_pebs_pdir_cpu[] = {
 	X86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D, NULL),
@@ -38,6 +81,10 @@ static const struct x86_cpu_id vmx_pebs_pdir_cpu[] = {
 	{}
 };
 
+/*
+ * 在以下使用vmx_pebs_pdist_cpu[]:
+ *   - arch/x86/kvm/pmu.c|215| <<pmc_get_pebs_precise_level>> if ((pmc->idx == 0 && x86_match_cpu(vmx_pebs_pdist_cpu)) ||
+ */
 /* Precise Distribution (PDist) */
 static const struct x86_cpu_id vmx_pebs_pdist_cpu[] = {
 	X86_MATCH_INTEL_FAM6_MODEL(SAPPHIRERAPIDS_X, NULL),
@@ -80,6 +127,10 @@ static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
 #include <asm/kvm-x86-pmu-ops.h>
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9450| <<kvm_ops_update>> kvm_pmu_ops_update(ops->pmu_ops);
+ */
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 {
 	memcpy(&kvm_pmu_ops, pmu_ops, sizeof(kvm_pmu_ops));
@@ -93,14 +144,33 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * 在以下使用kvm_pmu->irq_work:
+ *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+ *   - arch/x86/kvm/pmu.c|737| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+ *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ *
+ * 在以下使用kvm_pmi_trigger_fn():
+ *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+ */
 static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 {
 	struct kvm_pmu *pmu = container_of(irq_work, struct kvm_pmu, irq_work);
 	struct kvm_vcpu *vcpu = pmu_to_vcpu(pmu);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|101| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+	 *   - arch/x86/kvm/x86.c|10594| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+	 */
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|158| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|417| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -127,6 +197,26 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 	if (!pmc->intr || skip_pmi)
 		return;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理函数是kvm_pmu_deliver_pmi()
+	 *
+	 * 在以下使用kvm_pmu->irq_work:
+	 *   - arch/x86/kvm/pmu.c|144| <<__kvm_perf_overflow>> irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
+	 *   - arch/x86/kvm/pmu.c|737| <<kvm_pmu_reset>> irq_work_sync(&pmu->irq_work);
+	 *   - arch/x86/kvm/pmu.c|747| <<kvm_pmu_init>> init_irq_work(&pmu->irq_work, kvm_pmi_trigger_fn);
+	 *
+	 * irq_work是kvm_pmi_trigger_fn()
+	 *
+	 * 这里就两种选择:
+	 * (1) 要么通过通过work_queue调用kvm_pmi_trigger_fn()
+	 * (2) 要么通过KVM_REQ_PMI调用kvm_pmu_deliver_pmi()
+	 */
 	/*
 	 * Inject PMI. If vcpu was in a guest mode during NMI PMI
 	 * can be ejected on a guest mode re-entry. Otherwise we can't
@@ -139,8 +229,19 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
 	else
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|225| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -155,8 +256,25 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 	if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.c|158| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+	 *   - arch/x86/kvm/pmu.c|417| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+	 */
 	__kvm_perf_overflow(pmc, true);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -182,6 +300,16 @@ static u64 pmc_get_pebs_precise_level(struct kvm_pmc *pmc)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|545| <<reprogram_counter>> if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ *
+ * 575         if (pmc_reprogram_counter(pmc, PERF_TYPE_RAW,
+ * 576                                   (eventsel & pmu->raw_event_mask),
+ * 577                                   !(eventsel & ARCH_PERFMON_EVENTSEL_USR),
+ * 578                                   !(eventsel & ARCH_PERFMON_EVENTSEL_OS),
+ * 579                                   eventsel & ARCH_PERFMON_EVENTSEL_INT))
+ */
 static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 				 bool exclude_user, bool exclude_kernel,
 				 bool intr)
@@ -236,6 +364,10 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|420| <<reprogram_counter>> pmc_pause_counter(pmc);
+ */
 static void pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -249,6 +381,10 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|443| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -271,6 +407,11 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|415| <<filter_sort_cmp>> return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT |
+ *   - arch/x86/kvm/pmu.c|426| <<filter_event_cmp>> return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT));
+ */
 static int filter_cmp(const void *pa, const void *pb, u64 mask)
 {
 	u64 a = *(u64 *)pa & mask;
@@ -280,6 +421,10 @@ static int filter_cmp(const void *pa, const void *pb, u64 mask)
 }
 
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|995| <<prepare_filter_lists>> sort(&filter->events, filter->nevents, sizeof(filter->events[0]), filter_sort_cmp, NULL);
+ */
 static int filter_sort_cmp(const void *pa, const void *pb)
 {
 	return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT |
@@ -291,11 +436,21 @@ static int filter_sort_cmp(const void *pa, const void *pb)
  * 'excludes' list separately rather than on the 'events' list (which
  * has both).  As a result the exclude bit can be ignored.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|432| <<find_filter_index>> u64 *fe = bsearch(&key, events, nevents, sizeof(events[0]), filter_event_cmp);
+ *   - arch/x86/kvm/pmu.c|467| <<filter_contains_match>> if (filter_event_cmp(&events[i], &event_select))
+ *   - arch/x86/kvm/pmu.c|475| <<filter_contains_match>> if (filter_event_cmp(&events[i], &event_select))
+ */
 static int filter_event_cmp(const void *pa, const void *pb)
 {
 	return filter_cmp(pa, pb, (KVM_PMU_MASKED_ENTRY_EVENT_SELECT));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|458| <<filter_contains_match>> index = find_filter_index(events, nevents, event_select);
+ */
 static int find_filter_index(u64 *events, u64 nevents, u64 key)
 {
 	u64 *fe = bsearch(&key, events, nevents, sizeof(events[0]),
@@ -307,6 +462,11 @@ static int find_filter_index(u64 *events, u64 nevents, u64 key)
 	return fe - events;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|470| <<filter_contains_match>> if (is_filter_entry_match(events[i], umask))
+ *   - arch/x86/kvm/pmu.c|478| <<filter_contains_match>> if (is_filter_entry_match(events[i], umask))
+ */
 static bool is_filter_entry_match(u64 filter_event, u64 umask)
 {
 	u64 mask = filter_event >> (KVM_PMU_MASKED_ENTRY_UMASK_MASK_SHIFT - 8);
@@ -319,6 +479,11 @@ static bool is_filter_entry_match(u64 filter_event, u64 umask)
 	return (umask & mask) == match;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|488| <<is_gp_event_allowed>> if (filter_contains_match(f->includes, f->nr_includes, eventsel) &&
+ *   - arch/x86/kvm/pmu.c|489| <<is_gp_event_allowed>> !filter_contains_match(f->excludes, f->nr_excludes, eventsel))
+ */
 static bool filter_contains_match(u64 *events, u64 nevents, u64 eventsel)
 {
 	u64 event_select = eventsel & kvm_pmu_ops.EVENTSEL_EVENT;
@@ -352,6 +517,10 @@ static bool filter_contains_match(u64 *events, u64 nevents, u64 eventsel)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|552| <<check_pmu_event_filter>> return is_gp_event_allowed(filter, pmc->eventsel);
+ */
 static bool is_gp_event_allowed(struct kvm_x86_pmu_event_filter *f,
 				u64 eventsel)
 {
@@ -362,6 +531,10 @@ static bool is_gp_event_allowed(struct kvm_x86_pmu_event_filter *f,
 	return f->action == KVM_PMU_EVENT_DENY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|554| <<check_pmu_event_filter>> return is_fixed_event_allowed(filter, pmc->idx);
+ */
 static bool is_fixed_event_allowed(struct kvm_x86_pmu_event_filter *filter,
 				   int idx)
 {
@@ -377,6 +550,10 @@ static bool is_fixed_event_allowed(struct kvm_x86_pmu_event_filter *filter,
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|560| <<pmc_event_is_allowed>> check_pmu_event_filter(pmc);
+ */
 static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 {
 	struct kvm_x86_pmu_event_filter *filter;
@@ -395,12 +572,21 @@ static bool check_pmu_event_filter(struct kvm_pmc *pmc)
 	return is_fixed_event_allowed(filter, pmc->idx);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|576| <<reprogram_counter>> if (!pmc_event_is_allowed(pmc))
+ *   - arch/x86/kvm/pmu.c|952| <<kvm_pmu_trigger_event>> if (!pmc || !pmc_event_is_allowed(pmc))
+ */
 static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 {
 	return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
 	       check_pmu_event_filter(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|481| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ */
 static void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -456,6 +642,10 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	pmc->prev_counter = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10592| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -469,6 +659,9 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
+		/*
+		 * 只在这里调用
+		 */
 		reprogram_counter(pmc);
 	}
 
@@ -481,12 +674,21 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 		kvm_pmu_cleanup(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8248| <<emulator_check_pmc>> if (kvm_pmu_is_valid_rdpmc_ecx(emul_to_vcpu(ctxt), pmc))
+ */
 /* check if idx is a valid index to access PMU */
 bool kvm_pmu_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)
 {
 	return static_call(kvm_x86_pmu_is_valid_rdpmc_ecx)(vcpu, idx);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/emulate.c|3955| <<check_rdpmc>> if (enable_vmware_backdoor && is_vmware_backdoor_pmc(rcx))
+ *   - arch/x86/kvm/pmu.c|721| <<kvm_pmu_rdpmc>> if (is_vmware_backdoor_pmc(idx))
+ */
 bool is_vmware_backdoor_pmc(u32 pmc_idx)
 {
 	switch (pmc_idx) {
@@ -498,6 +700,10 @@ bool is_vmware_backdoor_pmc(u32 pmc_idx)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|722| <<kvm_pmu_rdpmc>> return kvm_pmu_rdpmc_vmware(vcpu, idx, data);
+ */
 static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 {
 	u64 ctr_val;
@@ -521,6 +727,11 @@ static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1423| <<kvm_emulate_rdpmc>> if (kvm_pmu_rdpmc(vcpu, ecx, &data)) {
+ *   - arch/x86/kvm/x86.c|8256| <<emulator_read_pmc>> return kvm_pmu_rdpmc(emul_to_vcpu(ctxt), pmc, pdata);
+ */
 int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 {
 	bool fast_mode = idx & (1u << 31);
@@ -547,14 +758,29 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|101| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10594| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * intel_pmu_deliver_pmi()
+		 */
 		static_call_cond(kvm_x86_pmu_deliver_pmi)(vcpu);
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3929| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|4010| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|4114| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ *   - arch/x86/kvm/x86.c|4366| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ */
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	switch (msr) {
@@ -569,6 +795,10 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 		static_call(kvm_x86_pmu_is_valid_msr)(vcpu, msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|853| <<kvm_pmu_set_msr(MSR_CORE_PERF_GLOBAL_OVF_CTRL)>> kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+ */
 static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -578,6 +808,11 @@ static void kvm_pmu_mark_pmc_in_use(struct kvm_vcpu *vcpu, u32 msr)
 		__set_bit(pmc->idx, pmu->pmc_in_use);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4115| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4367| <<kvm_get_msr_common>> return kvm_pmu_get_msr(vcpu, msr_info);
+ */
 int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -603,6 +838,11 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3930| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|4011| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -666,6 +906,12 @@ int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
  * settings are changed (such as changes of PMU CPUID by guest VMs), which
  * should rarely happen.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|342| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|890| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3699| <<kvm_set_msr_common>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	if (KVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm))
@@ -675,6 +921,11 @@ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_refresh)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|924| <<kvm_pmu_destroy>> kvm_pmu_reset(vcpu);
+ *   - arch/x86/kvm/x86.c|12129| <<kvm_vcpu_reset>> kvm_pmu_reset(vcpu);
+ */
 void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -683,6 +934,10 @@ void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 	static_call(kvm_x86_pmu_reset)(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11958| <<kvm_arch_vcpu_create>> kvm_pmu_init(vcpu);
+ */
 void kvm_pmu_init(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -695,6 +950,10 @@ void kvm_pmu_init(struct kvm_vcpu *vcpu)
 	kvm_pmu_refresh(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|586| <<kvm_pmu_handle_event>> kvm_pmu_cleanup(vcpu);
+ */
 /* Release perf_events for vPMCs that have been unused for a full time slice.  */
 void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 {
@@ -720,11 +979,19 @@ void kvm_pmu_cleanup(struct kvm_vcpu *vcpu)
 	bitmap_zero(pmu->pmc_in_use, X86_PMC_IDX_MAX);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12041| <<kvm_arch_vcpu_destroy>> kvm_pmu_destroy(vcpu);
+ */
 void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|985| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
 	pmc->prev_counter = pmc->counter;
@@ -732,6 +999,10 @@ static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 	kvm_pmu_request_counter_reprogram(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|984| <<kvm_pmu_trigger_event>> if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))
+ */
 static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
 	unsigned int perf_hw_id)
 {
@@ -739,6 +1010,10 @@ static inline bool eventsel_match_perf_hw_id(struct kvm_pmc *pmc,
 		AMD64_RAW_EVENT_MASK_NB);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|973| <<kvm_pmu_trigger_event>> if (eventsel_match_perf_hw_id(pmc, perf_hw_id) && cpl_is_matched(pmc))
+ */
 static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 {
 	bool select_os, select_user;
@@ -758,6 +1033,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 	return (static_call(kvm_x86_get_cpl)(pmc->vcpu) == 0) ? select_os : select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3567| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|8758| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9065| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/x86.c|9067| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -777,6 +1059,10 @@ void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 perf_hw_id)
 }
 EXPORT_SYMBOL_GPL(kvm_pmu_trigger_event);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|1030| <<prepare_filter_lists>> else if (!is_masked_filter_valid(filter))
+ */
 static bool is_masked_filter_valid(const struct kvm_x86_pmu_event_filter *filter)
 {
 	u64 mask = kvm_pmu_ops.EVENTSEL_EVENT |
@@ -793,6 +1079,10 @@ static bool is_masked_filter_valid(const struct kvm_x86_pmu_event_filter *filter
 	return true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|1029| <<prepare_filter_lists>> convert_to_masked_filter(filter);
+ */
 static void convert_to_masked_filter(struct kvm_x86_pmu_event_filter *filter)
 {
 	int i, j;
@@ -822,6 +1112,10 @@ static void convert_to_masked_filter(struct kvm_x86_pmu_event_filter *filter)
 	filter->nevents = j;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|1102| <<kvm_vm_ioctl_set_pmu_event_filter>> r = prepare_filter_lists(filter);
+ */
 static int prepare_filter_lists(struct kvm_x86_pmu_event_filter *filter)
 {
 	int i;
@@ -858,6 +1152,10 @@ static int prepare_filter_lists(struct kvm_x86_pmu_event_filter *filter)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7113| <<kvm_arch_vm_ioctl(KVM_SET_PMU_EVENT_FILTER)>> r = kvm_vm_ioctl_set_pmu_event_filter(kvm, argp);
+ */
 int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_pmu_event_filter __user *user_filter = argp;
@@ -912,6 +1210,18 @@ int kvm_vm_ioctl_set_pmu_event_filter(struct kvm *kvm, void __user *argp)
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
 
 	r = 0;
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 7d9ba301c..c02cd470f 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -41,6 +41,13 @@ struct kvm_pmu_ops {
 
 void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.h|279| <<pmc_is_globally_enabled>> if (!kvm_pmu_has_perf_global_ctrl(pmu))
+ *   - arch/x86/kvm/vmx/nested.c|2651| <<prepare_vmcs02>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)) &&
+ *   - arch/x86/kvm/vmx/nested.c|4526| <<load_vmcs12_host_state>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|178| <<intel_is_valid_msr>> return kvm_pmu_has_perf_global_ctrl(pmu);
+ */
 static inline bool kvm_pmu_has_perf_global_ctrl(struct kvm_pmu *pmu)
 {
 	/*
@@ -55,6 +62,14 @@ static inline bool kvm_pmu_has_perf_global_ctrl(struct kvm_pmu *pmu)
 	return pmu->version > 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|374| <<pmc_pause_counter>> pmc->counter = counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.c|992| <<kvm_pmu_incr_counter>> pmc->counter = (pmc->counter + 1) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|81| <<pmc_read_counter>> return counter & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|164| <<get_sample_period>> u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
+ *   - arch/x86/kvm/pmu.h|167| <<get_sample_period>> sample_period = pmc_bitmask(pmc) + 1;
+ */
 static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -62,6 +77,17 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|751| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/pmu.h|109| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|140| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|163| <<amd_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|340| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|345| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|409| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|413| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
@@ -74,6 +100,11 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 	return counter & pmc_bitmask(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|535| <<reprogram_counter>> pmc_release_perf_event(pmc);
+ *   - arch/x86/kvm/pmu.h|91| <<pmc_stop_counter>> pmc_release_perf_event(pmc);
+ */
 static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -84,6 +115,13 @@ static inline void pmc_release_perf_event(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|828| <<kvm_pmu_cleanup>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|244| <<amd_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|613| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|620| <<intel_pmu_reset>> pmc_stop_counter(pmc);
+ */
 static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 {
 	if (pmc->perf_event) {
@@ -92,22 +130,53 @@ static inline void pmc_stop_counter(struct kvm_pmc *pmc)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|563| <<check_pmu_event_filter>> if (pmc_is_gp(pmc))
+ *   - arch/x86/kvm/pmu.c|1016| <<cpl_is_matched>> if (pmc_is_gp(pmc)) {
+ */
 static inline bool pmc_is_gp(struct kvm_pmc *pmc)
 {
 	return pmc->type == KVM_PMC_GP;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|602| <<reprogram_counter>> if (pmc_is_fixed(pmc)) {
+ *   - arch/x86/kvm/pmu.h|185| <<pmc_speculative_in_use>> if (pmc_is_fixed(pmc))
+ */
 static inline bool pmc_is_fixed(struct kvm_pmc *pmc)
 {
 	return pmc->type == KVM_PMC_FIXED;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|870| <<kvm_pmu_set_msr>> if (!kvm_valid_perf_global_ctrl(pmu, data))
+ *   - arch/x86/kvm/vmx/nested.c|2927| <<nested_vmx_check_host_state>> CC(!kvm_valid_perf_global_ctrl(vcpu_to_pmu(vcpu),
+ *   - arch/x86/kvm/vmx/nested.c|3046| <<nested_vmx_check_guest_state>> CC(!kvm_valid_perf_global_ctrl(vcpu_to_pmu(vcpu),
+ */
 static inline bool kvm_valid_perf_global_ctrl(struct kvm_pmu *pmu,
 						 u64 data)
 {
 	return !(pmu->global_ctrl_mask & data);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|67| <<intel_pmc_idx_to_pmc>> return get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|149| <<get_fw_gp_pmc>> return get_gp_pmc(pmu, msr, MSR_IA32_PMC0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|191| <<intel_is_valid_msr>> ret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|192| <<intel_is_valid_msr>> get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|207| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|208| <<intel_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|338| <<intel_pmu_get_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|339| <<intel_pmu_get_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|349| <<intel_pmu_get_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|400| <<intel_pmu_set_msr>> if ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|401| <<intel_pmu_set_msr>> (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|416| <<intel_pmu_set_msr>> } else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {
+ */
 /* returns general purpose PMC with the specified MSR. Note that it can be
  * used for both PERFCTRn and EVNTSELn; that is why it accepts base as a
  * parameter to tell them apart.
@@ -125,6 +194,15 @@ static inline struct kvm_pmc *get_gp_pmc(struct kvm_pmu *pmu, u32 msr,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|57| <<reprogram_fixed_counters>> pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|72| <<intel_pmc_idx_to_pmc>> return get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|193| <<intel_is_valid_msr>> get_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|206| <<intel_msr_idx_to_pmc>> pmc = get_fixed_pmc(pmu, msr);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|344| <<intel_pmu_get_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|412| <<intel_pmu_set_msr>> } else if ((pmc = get_fixed_pmc(pmu, msr))) {
+ */
 /* returns fixed PMC with the specified MSR */
 static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 {
@@ -140,6 +218,12 @@ static inline struct kvm_pmc *get_fixed_pmc(struct kvm_pmu *pmu, u32 msr)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|325| <<pmc_reprogram_counter>> attr.sample_period = get_sample_period(pmc, pmc->counter);
+ *   - arch/x86/kvm/pmu.c|390| <<pmc_resume_counter>> get_sample_period(pmc, pmc->counter)))
+ *   - arch/x86/kvm/pmu.h|178| <<pmc_update_sample_period>> get_sample_period(pmc, pmc->counter));
+ */
 static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 {
 	u64 sample_period = (-counter_value) & pmc_bitmask(pmc);
@@ -149,6 +233,12 @@ static inline u64 get_sample_period(struct kvm_pmc *pmc, u64 counter_value)
 	return sample_period;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/pmu.c|164| <<amd_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|410| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|414| <<intel_pmu_set_msr>> pmc_update_sample_period(pmc);
+ */
 static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event || pmc->is_paused ||
@@ -159,6 +249,12 @@ static inline void pmc_update_sample_period(struct kvm_pmc *pmc)
 			  get_sample_period(pmc, pmc->counter));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|576| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/pmu.c|967| <<kvm_pmu_cleanup>> if (pmc && pmc->perf_event && !pmc_speculative_in_use(pmc))
+ *   - arch/x86/kvm/vmx/pmu_intel.c|755| <<intel_pmu_cross_mapped_check>> if (!pmc || !pmc_speculative_in_use(pmc) ||
+ */
 static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -172,6 +268,10 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 
 extern struct x86_pmu_capability kvm_pmu_cap;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9544| <<__kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -214,12 +314,35 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 					     KVM_PMC_MAX_FIXED);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|746| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - rch/x86/kvm/svm/pmu.c|173| <<amd_pmu_set_msr(PMU_TYPE_EVNTSEL)>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|426| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ */
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * called b:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|384| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
@@ -237,10 +360,22 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
  *
  * If the vPMU doesn't have global_ctrl MSR, all vPMCs are enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|576| <<pmc_event_is_allowed>> return pmc_is_globally_enabled(pmc) && pmc_speculative_in_use(pmc) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|756| <<intel_pmu_cross_mapped_check>> !pmc_is_globally_enabled(pmc) || !pmc->perf_event)
+ */
 static inline bool pmc_is_globally_enabled(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/pmu.h|279| <<pmc_is_globally_enabled>> if (!kvm_pmu_has_perf_global_ctrl(pmu))
+	 *   - arch/x86/kvm/vmx/nested.c|2651| <<prepare_vmcs02>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)) &&
+	 *   - arch/x86/kvm/vmx/nested.c|4526| <<load_vmcs12_host_state>> kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|178| <<intel_is_valid_msr>> return kvm_pmu_has_perf_global_ctrl(pmu);
+	 */
 	if (!kvm_pmu_has_perf_global_ctrl(pmu))
 		return true;
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 80c769c58..edb80565e 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -20,8 +20,22 @@
 #include "nested.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用MSR_PMC_FULL_WIDTH_BIT:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|402| <<intel_pmu_set_msr>> if ((msr & MSR_PMC_FULL_WIDTH_BIT) &&
+ *   - arch/x86/kvm/vmx/pmu_intel.c|407| <<intel_pmu_set_msr>> !(msr & MSR_PMC_FULL_WIDTH_BIT))
+ */
 #define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)
 
+/*
+ * 在以下使用intel_arch_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|28| <<global>> } const intel_arch_events[] = {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|83| <<intel_hw_event_available>> for (i = 0; i < ARRAY_SIZE(intel_arch_events); i++) {
+ *   - arch/x86/kvm/vmx/pmu_intel.c|84| <<intel_hw_event_available>> if (intel_arch_events[i].eventsel != event_select ||
+ *   - arch/x86/kvm/vmx/pmu_intel.c|85| <<intel_hw_event_available>> intel_arch_events[i].unit_mask != unit_mask)
+ *   - arch/x86/kvm/vmx/pmu_intel.c|449| <<setup_fixed_pmc_eventsel>> pmc->eventsel = (intel_arch_events[event].unit_mask << 8) |
+ *   - arch/x86/kvm/vmx/pmu_intel.c|450| <<setup_fixed_pmc_eventsel>> intel_arch_events[event].eventsel;
+ */
 static struct {
 	u8 eventsel;
 	u8 unit_mask;
@@ -37,6 +51,13 @@ static struct {
 	[7] = { 0x00, 0x03 },
 };
 
+/*
+ * 在以下使用fixed_pmc_events[]:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|41| <<global>> static int fixed_pmc_events[] = {1, 0, 7};
+ *   - arch/x86/kvm/vmx/pmu_intel.c|441| <<setup_fixed_pmc_eventsel>> size_t size = ARRAY_SIZE(fixed_pmc_events);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|448| <<setup_fixed_pmc_eventsel>> event = fixed_pmc_events[array_index_nospec(i, size)];
+ *   - arch/x86/kvm/vmx/pmu_intel.c|512| <<intel_pmu_refresh>> min3(ARRAY_SIZE(fixed_pmc_events),
+ */
 /* mapping between fixed pmc index and intel_arch_events array */
 static int fixed_pmc_events[] = {1, 0, 7};
 
@@ -57,6 +78,13 @@ static void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)
 		pmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);
 
 		__set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|746| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+		 *   - rch/x86/kvm/svm/pmu.c|173| <<amd_pmu_set_msr(PMU_TYPE_EVNTSEL)>> kvm_pmu_request_counter_reprogram(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|426| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+		 */
 		kvm_pmu_request_counter_reprogram(pmc);
 	}
 }
@@ -634,6 +662,10 @@ static void intel_pmu_reset(struct kvm_vcpu *vcpu)
  *
  * Guest needs to re-enable LBR to resume branches recording.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|655| <<intel_pmu_deliver_pmi>> intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);
+ */
 static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 {
 	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);
@@ -644,6 +676,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.deliver_pmi = intel_pmu_deliver_pmi()
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index df461f387..dc8ae4fd1 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -766,6 +766,11 @@ static void crash_vmclear_local_loaded_vmcss(void)
 }
 #endif /* CONFIG_KEXEC_CORE */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|804| <<loaded_vmcs_clear>> __loaded_vmcs_clear, loaded_vmcs, 1);
+ *   - arch/x86/kvm/vmx/vmx.c|2829| <<vmclear_local_loaded_vmcss>> __loaded_vmcs_clear(v);
+ */
 static void __loaded_vmcs_clear(void *arg)
 {
 	struct loaded_vmcs *loaded_vmcs = arg;
@@ -8349,6 +8354,13 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c381770bc..7ea096a9a 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -150,9 +150,23 @@ bool __read_mostly report_ignored_msrs = true;
 module_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);
 EXPORT_SYMBOL_GPL(report_ignored_msrs);
 
+/*
+ * 在以下使用min_timer_period_us:
+ *   - arch/x86/kvm/x86.c|153| <<global>> unsigned int min_timer_period_us = 200;
+ *   - arch/x86/kvm/x86.c|154| <<global>> module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/i8254.c|350| <<create_pit_timer>> s64 min_period = min_timer_period_us * 1000LL;
+ *   - arch/x86/kvm/lapic.c|1710| <<limit_periodic_timer_frequency>> s64 min_period = min_timer_period_us * 1000LL;
+ */
 unsigned int min_timer_period_us = 200;
 module_param(min_timer_period_us, uint, S_IRUGO | S_IWUSR);
 
+/*
+ * 在以下使用kvmclock_periodic_sync:
+ *   - arch/x86/kvm/x86.c|156| <<global>> static bool __read_mostly kvmclock_periodic_sync = true;
+ *   - arch/x86/kvm/x86.c|157| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+ *   - arch/x86/kvm/x86.c|3742| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+ *   - arch/x86/kvm/x86.c|12578| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+ */
 static bool __read_mostly kvmclock_periodic_sync = true;
 module_param(kvmclock_periodic_sync, bool, S_IRUGO);
 
@@ -811,8 +825,20 @@ void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1328| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5611| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->nmi_queued:
+	 *   - arch/x86/kvm/x86.c|830| <<kvm_inject_nmi>> atomic_inc(&vcpu->arch.nmi_queued);
+	 *   - arch/x86/kvm/x86.c|5902| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+	 *   - arch/x86/kvm/x86.c|10985| <<process_nmi>> vcpu->arch.nmi_pending += atomic_xchg(&vcpu->arch.nmi_queued, 0);
+	 *   - arch/x86/kvm/x86.c|12800| <<kvm_vcpu_reset>> atomic_set(&vcpu->arch.nmi_queued, 0);
+	 */
 	atomic_inc(&vcpu->arch.nmi_queued);
 	kvm_make_request(KVM_REQ_NMI, vcpu);
 }
@@ -2250,6 +2276,13 @@ struct pvclock_clock {
 	u64 mask;
 	u32 mult;
 	u32 shift;
+	/*
+	 * 在以下使用pvclock_clock->base_cycles:
+	 *   - arch/x86/kvm/x86.c|2299| <<update_pvclock_gtod>> vdata->clock.base_cycles = tk->tkr_mono.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|2307| <<update_pvclock_gtod>> vdata->raw_clock.base_cycles = tk->tkr_raw.xtime_nsec;
+	 *   - arch/x86/kvm/x86.c|3123| <<do_monotonic_raw>> ns = gtod->raw_clock.base_cycles;
+	 *   - arch/x86/kvm/x86.c|3147| <<do_realtime>> ns = gtod->clock.base_cycles;
+	 */
 	u64 base_cycles;
 	u64 offset;
 };
@@ -2266,6 +2299,10 @@ struct pvclock_gtod_data {
 
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9925| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
@@ -2309,6 +2346,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4230| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK_NEW)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|4237| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2337,6 +2379,12 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	 * system time (updated by kvm_guest_time_update below) to the
 	 * wall clock specified here.  We do the reverse here.
 	 */
+	/*
+	 * 写完msr后wall_clock就是更新后的墙上时间,即guest启动的日期.
+	 *
+	 * get_kvmclock_ns(kvm)应该是虚拟机启动后到现在的时间
+	 * 所以wall_nsec是虚拟机启动时候的时间
+	 */
 	wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
 
 	wc.nsec = do_div(wall_nsec, 1000000000);
@@ -2355,12 +2403,29 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|4023| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|4029| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+		 *   - kvm_update_masterclock()
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
@@ -2368,6 +2433,14 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	}
 
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * kvm_gen_kvmclock_update()
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
@@ -2380,12 +2453,22 @@ static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 	return;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2439| <<kvm_get_time_scale>> *pmultiplier = div_frac(scaled64, tps32);
+ */
 static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 {
 	do_shl32_div32(dividend, divisor);
 	return dividend;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2514| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+ *   - arch/x86/kvm/x86.c|3532| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2415,12 +2498,39 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2430| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+ *   - arch/x86/kvm/x86.c|3027| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|9443| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|9487| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
+/*
+ * 在以下使用percpu的cpu_tsc_khz:
+ *   - arch/x86/kvm/x86.c|2453| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3303| <<get_cpu_tsc_khz>> return __this_cpu_read(cpu_tsc_khz);
+ *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {
+ *   - arch/x86/kvm/x86.c|9590| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+ *   - arch/x86/kvm/x86.c|9607| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+ *   - arch/x86/kvm/x86.c|9626| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+ */
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+/*
+ * 在以下使用max_tsc_khz:
+ *   - arch/x86/kvm/x86.c|9765| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|9775| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+ *   - arch/x86/kvm/x86.c|12922| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+ */
 static unsigned long max_tsc_khz;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2525| <<kvm_set_tsc_khz>> thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
+ *   - arch/x86/kvm/x86.c|2526| <<kvm_set_tsc_khz>> thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
+ */
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 {
 	u64 v = (u64)khz * (1000000 + ppm);
@@ -2430,6 +2540,10 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2532| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
@@ -2466,6 +2580,11 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6353| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|12484| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2500,8 +2619,17 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2628| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2936| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
@@ -2510,12 +2638,25 @@ static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2586| <<kvm_track_tsc_matching>> (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
+ *   - arch/x86/kvm/x86.c|3000| <<kvm_get_time_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|3003| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns,
+ *   - arch/x86/kvm/x86.c|3017| <<kvm_get_walltime_and_clockread>> if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
+ *   - arch/x86/kvm/x86.c|3020| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ *   - arch/x86/kvm/x86.c|9815| <<pvclock_gtod_notify>> if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
+ */
 static inline int gtod_is_based_on_tsc(int mode)
 {
 	return mode == VDSO_CLOCKMODE_TSC || mode == VDSO_CLOCKMODE_HVCLOCK;
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2725| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 {
 #ifdef CONFIG_X86_64
@@ -2523,6 +2664,11 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
 
+	/*
+	 * 在以下修改kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2939| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2941| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			 atomic_read(&vcpu->kvm->online_vcpus));
 
@@ -2534,6 +2680,18 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
 	 * and the vcpus need to have matched TSCs.  When that happens,
 	 * perform request to enable masterclock.
 	 */
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+	 *   - kvm_update_masterclock()
+	 */
 	if (ka->use_master_clock ||
 	    (gtod_is_based_on_tsc(gtod->clock.vclock_mode) && vcpus_matched))
 		kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
@@ -2554,11 +2712,25 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu)
  *
  * N equals to kvm_caps.tsc_scaling_ratio_frac_bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2615| <<kvm_scale_tsc>> _tsc = __scale_tsc(ratio, tsc);
+ *   - arch/x86/kvm/x86.c|9982| <<__kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+ */
 static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 {
 	return mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2633| <<kvm_compute_l1_tsc_offset>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2641| <<kvm_read_l1_tsc>> kvm_scale_tsc(host_tsc, vcpu->arch.l1_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2873| <<adjust_tsc_offset_host>> adjustment = kvm_scale_tsc((u64) adjustment,
+ *   - arch/x86/kvm/x86.c|3449| <<kvm_guest_time_update>> tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,
+ *   - arch/x86/kvm/x86.c|4468| <<kvm_get_msr_common>> msr_info->data = kvm_scale_tsc(rdtsc(), ratio) + offset;
+ *   - arch/x86/kvm/x86.c|5882| <<kvm_arch_tsc_set_attr>> tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
+ */
 u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 {
 	u64 _tsc = tsc;
@@ -2569,6 +2741,15 @@ u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 	return _tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|2767| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3788| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|4841| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+ *
+ * 基于当前的rdtsc()计算
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2578,6 +2759,21 @@ static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 	return target_tsc - tsc;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|582| <<get_time_ref_counter>> tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1828| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1839| <<__kvm_wait_lapic_expire>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|1938| <<start_sw_tscdeadline>> guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ *   - arch/x86/kvm/lapic.c|2030| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/lapic.c|2054| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+ *   - arch/x86/kvm/vmx/nested.c|961| <<nested_vmx_get_vmexit_msr_value>> *data = kvm_read_l1_tsc(vcpu, val);
+ *   - arch/x86/kvm/vmx/nested.c|2116| <<vmx_calc_preemption_timer_value>> u64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>
+ *   - arch/x86/kvm/vmx/vmx.c|8026| <<vmx_set_hv_timer>> guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
+ *   - arch/x86/kvm/x86.c|3424| <<kvm_guest_time_update>> tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+ *   - arch/x86/kvm/x86.c|10116| <<kvm_pv_clock_pairing>> clock_pairing.tsc = kvm_read_l1_tsc(vcpu, cycle);
+ *   - arch/x86/kvm/x86.c|11288| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+ */
 u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 {
 	return vcpu->arch.l1_tsc_offset +
@@ -2585,6 +2781,12 @@ u64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc)
 }
 EXPORT_SYMBOL_GPL(kvm_read_l1_tsc);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|691| <<nested_vmcb02_prepare_control>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+ *   - arch/x86/kvm/vmx/nested.c|2578| <<prepare_vmcs02>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+ *   - arch/x86/kvm/x86.c|2684| <<kvm_vcpu_write_tsc_offset>> vcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(
+ */
 u64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)
 {
 	u64 nested_offset;
@@ -2600,6 +2802,12 @@ u64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_offset);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/nested.c|1538| <<nested_svm_update_tsc_ratio_msr>> kvm_calc_nested_tsc_multiplier(vcpu->arch.l1_tsc_scaling_ratio
+ *   - arch/x86/kvm/vmx/nested.c|2583| <<prepare_vmcs02>> vcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(
+ *   - arch/x86/kvm/x86.c|2700| <<kvm_vcpu_write_tsc_multiplier>> vcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(
+ */
 u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 {
 	if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
@@ -2610,6 +2818,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_GPL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2747| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2862| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|5177| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	trace_kvm_write_tsc_offset(vcpu->vcpu_id,
@@ -2634,6 +2848,12 @@ static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 	static_call(kvm_x86_write_tsc_offset)(vcpu, vcpu->arch.tsc_offset);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2471| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ *   - arch/x86/kvm/x86.c|2497| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, ratio);
+ *   - arch/x86/kvm/x86.c|2509| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+ */
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier)
 {
 	vcpu->arch.l1_tsc_scaling_ratio = l1_multiplier;
@@ -2651,6 +2871,14 @@ static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multipli
 			vcpu, vcpu->arch.tsc_scaling_ratio);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2837| <<kvm_synchronize_tsc>> if (!kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5168| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|5174| <<kvm_arch_vcpu_load>> if (kvm_check_tsc_unstable()) {
+ *   - arch/x86/kvm/x86.c|12297| <<kvm_arch_vcpu_precreate>> if (kvm_check_tsc_unstable() && kvm->created_vcpus)
+ *   - arch/x86/kvm/x86.c|12667| <<kvm_arch_hardware_enable>> stable = !kvm_check_tsc_unstable();
+ */
 static inline bool kvm_check_tsc_unstable(void)
 {
 #ifdef CONFIG_X86_64
@@ -2669,6 +2897,11 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2847| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched);
+ *   - arch/x86/kvm/x86.c|5885| <<kvm_arch_tsc_set_attr>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched)
 {
@@ -2699,23 +2932,93 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2935| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2945| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		 *   - arch/x86/kvm/x86.c|2950| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 */
 		kvm->arch.cur_tsc_generation++;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_nsec:
+		 *   - arch/x86/kvm/x86.c|2931| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+		 *   - arch/x86/kvm/x86.c|2946| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+		 */
 		kvm->arch.cur_tsc_nsec = ns;
 		kvm->arch.cur_tsc_write = tsc;
 		kvm->arch.cur_tsc_offset = offset;
+		/*
+		 * 在以下修改kvm_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2939| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2941| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 */
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2935| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2945| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		 *   - arch/x86/kvm/x86.c|2950| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 *
+		 * 在以下使用kvm_vcpu_arch->this_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2945| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		 *   - arch/x86/kvm/x86.c|2950| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 *
+		 * 在以下修改kvm_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2939| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2941| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 */
 		kvm->arch.nr_vcpus_matched_tsc++;
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2945| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2950| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	/* Keep track of which generation this VCPU has synchronized to */
 	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2628| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2936| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
 	kvm_track_tsc_matching(vcpu);
 }
 
+/*
+ * 创建一个2-CPU的虚拟机:
+ *
+ * 先是post create, 后来是被QEMU的MSR_IA32_TSC调用(两次每个vcpu)
+ *
+ * 因为master clock是在run的时候check request调用, 所以每个cpu只调用一次
+ *
+ * [  197.631757] kvm: orabug: kvm_synchronize_tsc() post create()
+ * [  197.633896] kvm: orabug: kvm_synchronize_tsc() post create()
+ * [  197.654083] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=0
+ * [  197.654924] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=1
+ * [  197.663893] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=0
+ * [  197.664745] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=1
+ * [  197.667353] kvm: orabug: kvm_update_masterclock()
+ * [  197.711664] kvm: orabug: kvm_update_masterclock()
+ *
+ *
+ * 如果此时hot-add cpu:
+ *
+ * [  717.672590] kvm: orabug: kvm_synchronize_tsc() post create()
+ * [  717.673603] kvm: orabug: kvm_synchronize_tsc() MSR_IA32_TSC() id=2
+ *
+ * 只有online cpu的时候才有下面的.
+ *
+ * [  774.337876] kvm: orabug: kvm_update_masterclock()
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|4025| <<kvm_set_msr_common(MSR_IA32_TSC,只能QEMU)>> kvm_synchronize_tsc(vcpu, data);
+ *   - arch/x86/kvm/x86.c|12348| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, 0);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -2725,6 +3028,15 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	bool synchronizing = false;
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|2728| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+	 *   - arch/x86/kvm/x86.c|2767| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+	 *   - arch/x86/kvm/x86.c|3788| <<kvm_set_msr_common>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+	 *   - arch/x86/kvm/x86.c|4841| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+	 *
+	 * 基于当前的rdtsc()计算
+	 */
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
 	ns = get_kvmclock_base_ns();
 	elapsed = ns - kvm->arch.last_tsc_nsec;
@@ -2746,6 +3058,10 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 			 * of virtual cycle time against real time is
 			 * interpreted as an attempt to synchronize the CPU.
 			 */
+			/*
+			 * data < tsc_exp + tsc_hz
+			 * data > tsc_exp - tsc_hz
+			 */
 			synchronizing = data < tsc_exp + tsc_hz &&
 					data + tsc_hz > tsc_exp;
 		}
@@ -2773,6 +3089,13 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 data)
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2859| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|3344| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|3982| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|4028| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
@@ -2780,6 +3103,10 @@ static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|5068| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+ */
 static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 {
 	if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
@@ -2791,6 +3118,10 @@ static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 
 #ifdef CONFIG_X86_64
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2905| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2811,6 +3142,11 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2929| <<do_monotonic_raw>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|2949| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2846,6 +3182,10 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 	return v * clock->mult;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2966| <<kvm_get_time_and_clockread>> return gtod_is_based_on_tsc(do_monotonic_raw(kernel_ns, tsc_timestamp));
+ */
 static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2865,6 +3205,10 @@ static int do_monotonic_raw(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2978| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2886,6 +3230,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3045| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+ */
 /* returns true if host is using TSC based clocksource */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
@@ -2897,6 +3245,11 @@ static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 						      tsc_timestamp));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3174| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+ *   - arch/x86/kvm/x86.c|10016| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+ */
 /* returns true if host is using TSC based clocksource */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
@@ -2950,6 +3303,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3099| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|6919| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9398| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|12636| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -2958,6 +3318,11 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	bool host_tsc_clocksource, vcpus_matched;
 
 	lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下修改kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2939| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2941| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
@@ -2969,10 +3334,24 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 4个条件:
+	 * 1. host_tsc_clocksource
+	 * 2. vcpus_matched
+	 * 3. !ka->backwards_tsc_observed
+	 * 4. !ka->boot_vcpu_runs_old_kvmclock
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2430| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3027| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9443| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9487| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -2982,17 +3361,32 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3082| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+ *   - arch/x86/kvm/x86.c|9444| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 static void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3085| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9457| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+ */
 static void __kvm_start_pvclock_update(struct kvm *kvm)
 {
 	raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
 	write_seqcount_begin(&kvm->arch.pvclock_sc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3138| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|6978| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+ */
 static void kvm_start_pvclock_update(struct kvm *kvm)
 {
 	kvm_make_mclock_inprogress_request(kvm);
@@ -3001,6 +3395,12 @@ static void kvm_start_pvclock_update(struct kvm *kvm)
 	__kvm_start_pvclock_update(kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3140| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7003| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9459| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+ */
 static void kvm_end_pvclock_update(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -3009,6 +3409,24 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 
 	write_seqcount_end(&ka->pvclock_sc);
 	raw_spin_unlock_irq(&ka->tsc_write_lock);
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *
+	 * kvm_guest_time_update()
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -3017,6 +3435,19 @@ static void kvm_end_pvclock_update(struct kvm *kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|10779| <<vcpu_enter_guest(KVM_REQ_MASTERCLOCK_UPDATE)>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
 	kvm_hv_request_tsc_page_update(kvm);
@@ -3033,14 +3464,32 @@ static void kvm_update_masterclock(struct kvm *kvm)
  * notification when calibration completes, but practically speaking calibration
  * will complete before userspace is alive enough to create VMs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3184| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+ *   - arch/x86/kvm/x86.c|3318| <<kvm_guest_time_update>> tgt_tsc_khz = get_cpu_tsc_khz();
+ */
 static unsigned long get_cpu_tsc_khz(void)
 {
+	/*
+	 * 在以下使用percpu的cpu_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2453| <<global>> static DEFINE_PER_CPU(unsigned long , cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|3303| <<get_cpu_tsc_khz>> return __this_cpu_read(cpu_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> (static_cpu_has(X86_FEATURE_CONSTANT_TSC) || __this_cpu_read(cpu_tsc_khz))) {
+	 *   - arch/x86/kvm/x86.c|9590| <<kvmclock_cpu_down_prep>> __this_cpu_write(cpu_tsc_khz, 0);
+	 *   - arch/x86/kvm/x86.c|9607| <<tsc_khz_changed>> __this_cpu_write(cpu_tsc_khz, khz);
+	 *   - arch/x86/kvm/x86.c|9626| <<kvm_hyperv_tsc_notifier>> per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
+	 */
 	if (static_cpu_has(X86_FEATURE_CONSTANT_TSC))
 		return tsc_khz;
 	else
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3202| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3056,6 +3505,10 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 #ifdef CONFIG_X86_64
 		struct timespec64 ts;
 
+		/*
+		 * 返回a pair of ns and tsc
+		 * 都是realtime的
+		 */
 		if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
 			data->realtime = ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec;
 			data->flags |= KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC;
@@ -3064,30 +3517,89 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 		data->host_tsc = rdtsc();
 
 		data->flags |= KVM_CLOCK_TSC_STABLE;
+		/*
+		 * 下面的两个:
+		 *   - ka->master_cycle_now对应收集master的时候的host tsc
+		 *   - "ka->master_kernel_ns + ka->kvmclock_offset"对应收集master的时候VM启动了经历了的时间
+		 */
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|2514| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+		 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+		 *   - arch/x86/kvm/x86.c|3532| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
+		/*
+		 * called by:
+		 *   - arch/x86/include/asm/vdso/gettimeofday.h|231| <<vread_pvclock>> ret = __pvclock_read_cycles(pvti, rdtsc_ordered());
+		 *   - arch/x86/kernel/pvclock.c|82| <<__pvclock_clocksource_read>> ret = __pvclock_read_cycles(src, rdtsc_ordered());
+		 *   - arch/x86/kvm/x86.c|3456| <<__get_kvmclock>> data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
+		 *   - drivers/ptp/ptp_kvm_x86.c|123| <<kvm_arch_ptp_get_crosststamp>> *cycle = __pvclock_read_cycles(src, clock_pair->tsc);
+		 */
 		data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
 	} else {
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+		 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 *
+		 * get_kvmclock_base_ns()是host启动后一共花的时间
+		 */
 		data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
 	}
 
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3210| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|6954| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
 	unsigned seq;
 
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|3077| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3094| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3201| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3203| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|3301| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|3314| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|12741| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 只在这里调用
+		 */
 		__get_kvmclock(kvm, data);
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|579| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|2340| <<kvm_write_wall_clock>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|62| <<kvm_xen_shared_info_init>> wall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|458| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|846| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|887| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|939| <<kvm_xen_vcpu_set_attr>> get_kvmclock_ns(vcpu->kvm));
+ *   - arch/x86/kvm/xen.c|1389| <<kvm_xen_hcall_vcpu_op>> delta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|1419| <<kvm_xen_hcall_set_timer_op>> uint64_t guest_now = get_kvmclock_ns(vcpu->kvm);
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
@@ -3096,6 +3608,12 @@ u64 get_kvmclock_ns(struct kvm *kvm)
 	return data.clock;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3381| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3383| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|3386| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 				    struct gfn_to_pfn_cache *gpc,
 				    unsigned int offset)
@@ -3145,6 +3663,25 @@ static void kvm_setup_guest_pvclock(struct kvm_vcpu *v,
 	trace_kvm_pvclock_update(v->vcpu_id, &vcpu->hv_clock);
 }
 
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *
+ * 处理KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10581| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+ */
 static int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	unsigned long flags, tgt_tsc_khz;
@@ -3165,6 +3702,17 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 */
 	do {
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|3218| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 *                                                              && !ka->backwards_tsc_observed && !ka->boot_vcpu_runs_old_kvmclock;
+		 *
+		 * 在以下使用kvm_arch->master_kernel_ns:
+		 *   - arch/x86/kvm/x86.c|3033| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(&ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3157| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3279| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+		 *   - arch/x86/kvm/x86.c|6953| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+		 */
 		use_master_clock = ka->use_master_clock;
 		if (use_master_clock) {
 			host_tsc = ka->master_cycle_now;
@@ -3197,6 +3745,12 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2521| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3563| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5305| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	if (vcpu->tsc_catchup) {
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
 		if (tsc > tsc_timestamp) {
@@ -3221,6 +3775,10 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		kvm_xen_update_tsc_info(v);
 	}
 
+	/*
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
 	vcpu->last_guest_tsc = tsc_timestamp;
@@ -3232,6 +3790,17 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 
 	vcpu->hv_clock.flags = pvclock_flags;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->pv_time:
+	 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_gpc_activate(&vcpu->arch.pv_time, system_time & ~1ULL,
+	 *   - arch/x86/kvm/x86.c|2451| <<kvm_write_system_time>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|3758| <<kvm_guest_time_update>> if (vcpu->pv_time.active)
+	 *   - arch/x86/kvm/x86.c|3759| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|4089| <<kvmclock_reset>> kvm_gpc_deactivate(&vcpu->arch.pv_time);
+	 *   - arch/x86/kvm/x86.c|6123| <<kvm_set_guest_paused>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|7336| <<kvm_arch_suspend_notifier>> if (!vcpu->arch.pv_time.active)
+	 *   - arch/x86/kvm/x86.c|12645| <<kvm_arch_vcpu_create>> kvm_gpc_init(&vcpu->arch.pv_time, vcpu->kvm, vcpu, KVM_HOST_USES_PFN);
+	 */
 	if (vcpu->pv_time.active)
 		kvm_setup_guest_pvclock(v, &vcpu->pv_time, 0);
 	if (vcpu->xen.vcpu_info_cache.active)
@@ -3257,8 +3826,19 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
  * by the delay we use to rate-limit the updates.
  */
 
+/*
+ * 在以下使用KVMCLOCK_UPDATE_DELAY:
+ *   - arch/x86/kvm/x86.c|3449| <<kvm_gen_kvmclock_update>> KVMCLOCK_UPDATE_DELAY);
+ */
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_update_work:
+ *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+ *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+ *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	unsigned long i;
@@ -3269,22 +3849,91 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *
+		 * kvm_guest_time_update()
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|10635| <<vcpu_enter_guest(KVM_REQ_GLOBAL_CLOCK_UPDATE)>> kvm_gen_kvmclock_update(vcpu);
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *
+	 * kvm_guest_time_update()
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 *
+	 * kvmclock_update_fn()
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
+/*
+ * 在以下使用KVMCLOCK_SYNC_PERIOD:
+ *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ */
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_sync_work:
+ *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ *   - arch/x86/kvm/x86.c|12539| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+ *
+ * 在以下使用kvmclock_sync_fn():
+ *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ */
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -3292,10 +3941,35 @@ static void kvmclock_sync_fn(struct work_struct *work)
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|156| <<global>> static bool __read_mostly kvmclock_periodic_sync = true;
+	 *   - arch/x86/kvm/x86.c|157| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|3742| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|12578| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (!kvmclock_periodic_sync)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3340| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3356| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|12497| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|12540| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 *
+	 * 调用kvmclock_update_fn()
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3357| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12097| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|12498| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|12539| <<kvm_arch_sync_events>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 *
+	 * 一共在两处schedue, kvm_arch_vcpu_postcreate()和这里.
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -4804,6 +5478,11 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|216| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|5985| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	/* Address WBINVD may be executed by guest */
@@ -4936,6 +5615,11 @@ static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 	return kvm_apic_get_state(vcpu, s);
 }
 
+/*
+ * struct kvm_lapic_state {
+ *     char regs[KVM_APIC_REG_SIZE];
+ * };
+ */
 static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
@@ -5007,6 +5691,10 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6308| <<kvm_arch_vcpu_ioctl(KVM_NMI)>> r = kvm_vcpu_ioctl_nmi(vcpu);
+ */
 static int kvm_vcpu_ioctl_nmi(struct kvm_vcpu *vcpu)
 {
 	kvm_inject_nmi(vcpu);
@@ -5309,6 +5997,20 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	    lapic_in_kernel(vcpu))
 		vcpu->arch.apic->sipi_vector = events->sipi_vector;
 
+	/*
+	 * 在QEMU设置KVM_VCPUEVENT_VALID_SMM的地方
+	 *
+	 * 4426 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
+	 * 4427 {
+	 * ... ...
+	 * 4474          * Stop SMI delivery on old machine types to avoid a reboot
+	 * 4475          * on an inward migration of an old VM.
+	 * 4476          *
+	 * 4477         if (!cpu->kvm_no_smi_migration) {
+	 * 4478             events.flags |= KVM_VCPUEVENT_VALID_SMM;
+	 * 4479         }
+	 * 4480     }
+	 */
 	if (events->flags & KVM_VCPUEVENT_VALID_SMM) {
 #ifdef CONFIG_KVM_SMM
 		if (!!(vcpu->arch.hflags & HF_SMM_MASK) != events->smi.smm) {
@@ -6492,6 +7194,11 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 
 		mutex_lock(&kvm->lock);
 		if (!kvm->created_vcpus) {
+			/*
+			 * 在以下使用kvm_arch->disable_nx_huge_pages:
+			 *   - arch/x86/kvm/mmu/mmu_internal.h|202| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
+			 *   - arch/x86/kvm/x86.c|7124| <<kvm_vm_ioctl_enable_cap>> kvm->arch.disable_nx_huge_pages = true;
+			 */
 			kvm->arch.disable_nx_huge_pages = true;
 			r = 0;
 		}
@@ -6751,7 +7458,22 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 		now_raw_ns = ka->master_kernel_ns;
 	else
 		now_raw_ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	ka->kvmclock_offset = data.clock - now_raw_ns;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|3140| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7003| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9459| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+	 */
 	kvm_end_pvclock_update(kvm);
 	return 0;
 }
@@ -9268,6 +9990,24 @@ static void __kvmclock_cpufreq_notifier(struct cpufreq_freqs *freq, int cpu)
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			if (vcpu->cpu != cpu)
 				continue;
+			/*
+			 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+			 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+			 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+			 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			 *
+			 * kvm_guest_time_update()
+			 */
 			kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 			if (vcpu->cpu != raw_smp_processor_id())
 				send_ipi = 1;
@@ -9346,20 +10086,57 @@ static void kvm_timer_init(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9403| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9640| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
 	struct kvm_vcpu *vcpu;
 	unsigned long i;
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+	 *   - kvm_update_masterclock()
+	 */
 	mutex_lock(&kvm_lock);
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2430| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3027| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|9443| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|9487| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
 
+/*
+ * 在以下使用pvclock_gtod_update_fn():
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|9394| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|9403| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|9640| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
 
 /*
@@ -9367,16 +10144,34 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  * region to prevent possible deadlocks against time accessors which
  * are invoked with work related locks held.
  */
+/*
+ * 在以下使用pvclock_irq_work_fn():
+ *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9634| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9847| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
 	queue_work(system_long_wq, &pvclock_gtod_work);
 }
 
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|9634| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|9847| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
 
 /*
  * Notification about pvclock gtod data update.
  */
+/*
+ * struct notifier_block pvclock_gtod_notifier.notifier_call = pvclock_gtod_notify()
+ */
 static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 			       void *priv)
 {
@@ -9390,6 +10185,12 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用pvclock_irq_work:
+	 *   - arch/x86/kvm/x86.c|9614| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+	 *   - arch/x86/kvm/x86.c|9634| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+	 *   - arch/x86/kvm/x86.c|9847| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
@@ -9668,6 +10469,10 @@ int kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu)
 EXPORT_SYMBOL_GPL(kvm_emulate_ap_reset_hold);
 
 #ifdef CONFIG_X86_64
+/*
+ * 处理KVM_HC_CLOCK_PAIRING:
+ *   - arch/x86/kvm/x86.c|10471| <<kvm_emulate_hypercall(KVM_HC_CLOCK_PAIRING)>> ret = kvm_pv_clock_pairing(vcpu, a0, a1);
+ */
 static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 			        unsigned long clock_type)
 {
@@ -10524,10 +11329,48 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_mmu_free_obsolete_roots(vcpu);
 		if (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))
 			__kvm_migrate_timers(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+		 *   - kvm_update_masterclock()
+		 */
 		if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
 			kvm_update_masterclock(vcpu->kvm);
+		/*
+		 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2383| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4909| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10634| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+		 *
+		 * kvm_gen_kvmclock_update()
+		 */
 		if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
 			kvm_gen_kvmclock_update(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|3063| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3234| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3330| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3339| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3801| <<kvm_set_msr_common>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|4885| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5533| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9334| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10636| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|10987| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|12316| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|750| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|765| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *
+		 * kvm_guest_time_update()
+		 */
 		if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
 			r = kvm_guest_time_update(vcpu);
 			if (unlikely(r))
@@ -10588,8 +11431,27 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 #endif
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+		 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 处理函数是kvm_pmu_handle_event()
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+		 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -10879,6 +11741,10 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 }
 
 /* Called within kvm->srcu read side.  */
+/*
+ * 对于没有hot-add的(曾经在但是remove了的)
+ * mp_state是3 (KVM_MP_STATE_HALTED)
+ */
 static inline int vcpu_block(struct kvm_vcpu *vcpu)
 {
 	bool hv_timer;
@@ -10896,6 +11762,12 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 			kvm_lapic_switch_to_sw_timer(vcpu);
 
 		kvm_vcpu_srcu_read_unlock(vcpu);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|11508| <<vcpu_block>> kvm_vcpu_block(vcpu);
+		 *   - arch/x86/kvm/x86.c|11727| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+		 *   - virt/kvm/kvm_main.c|3555| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+		 */
 		if (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)
 			kvm_vcpu_halt(vcpu);
 		else
@@ -10952,6 +11824,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12023| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 /* Called within kvm->srcu read side.  */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
@@ -11094,6 +11970,10 @@ static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	trace_kvm_fpu(0);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4184| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_ioctl_run(vcpu);
+ */
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
 	struct kvm_queued_exception *ex = &vcpu->arch.exception;
@@ -11118,9 +11998,22 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		WARN_ON_ONCE(kvm_lapic_hv_timer_in_use(vcpu));
 
 		kvm_vcpu_srcu_read_unlock(vcpu);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|11508| <<vcpu_block>> kvm_vcpu_block(vcpu);
+		 *   - arch/x86/kvm/x86.c|11727| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+		 *   - virt/kvm/kvm_main.c|3555| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+		 */
 		kvm_vcpu_block(vcpu);
 		kvm_vcpu_srcu_read_lock(vcpu);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|11483| <<vcpu_enter_guest>> r = kvm_apic_accept_events(vcpu);
+		 *   - arch/x86/kvm/x86.c|11759| <<vcpu_block>> if (kvm_apic_accept_events(vcpu) < 0)
+		 *   - arch/x86/kvm/x86.c|11955| <<kvm_arch_vcpu_ioctl_run>> if (kvm_apic_accept_events(vcpu) < 0) {
+		 *   - arch/x86/kvm/x86.c|12203| <<kvm_arch_vcpu_ioctl_get_mpstate>> r = kvm_apic_accept_events(vcpu);
+		 */
 		if (kvm_apic_accept_events(vcpu) < 0) {
 			r = 0;
 			goto out;
@@ -11185,6 +12078,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
+	/*
+	 * vmx_vcpu_pre_run()
+	 */
 	r = static_call(kvm_x86_vcpu_pre_run)(vcpu);
 	if (r <= 0)
 		goto out;
@@ -11923,6 +12819,13 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	kvm_xen_init_vcpu(vcpu);
 	kvm_vcpu_mtrr_init(vcpu);
 	vcpu_load(vcpu);
+	/*
+	 * 在以下使用kvm_arch->default_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|7688| <<kvm_arch_vm_ioctl>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|7694| <<kvm_arch_vm_ioctl>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|12731| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|13185| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+	 */
 	kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
 	kvm_vcpu_reset(vcpu, false);
 	kvm_init_mmu(vcpu);
@@ -11946,6 +12849,14 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3994| <<kvm_vm_ioctl_create_vcpu>> kvm_arch_vcpu_postcreate(vcpu);
+ *
+ * kvm_vm_ioctl(KVM_CREATE_VCPU)
+ * -> kvm_vm_ioctl_create_vcpu()
+ *    -> kvm_arch_vcpu_postcreate()
+ */
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -11961,6 +12872,13 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 	mutex_unlock(&vcpu->mutex);
 
+	/*
+	 * 在以下使用kvmclock_periodic_sync:
+	 *   - arch/x86/kvm/x86.c|156| <<global>> static bool __read_mostly kvmclock_periodic_sync = true;
+	 *   - arch/x86/kvm/x86.c|157| <<global>> module_param(kvmclock_periodic_sync, bool, S_IRUGO);
+	 *   - arch/x86/kvm/x86.c|3742| <<kvmclock_sync_fn>> if (!kvmclock_periodic_sync)
+	 *   - arch/x86/kvm/x86.c|12578| <<kvm_arch_vcpu_postcreate>> if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
+	 */
 	if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
 		schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 						KVMCLOCK_SYNC_PERIOD);
@@ -11993,6 +12911,12 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 		static_branch_dec(&kvm_has_noapic_vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3333| <<kvm_apic_accept_events>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/svm/svm.c|2148| <<shutdown_interception>> kvm_vcpu_reset(vcpu, true);
+ *   - arch/x86/kvm/x86.c|12765| <<kvm_arch_vcpu_create>> kvm_vcpu_reset(vcpu, false);
+ */
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_cpuid_entry2 *cpuid_0x1;
@@ -12235,6 +13159,18 @@ int kvm_arch_hardware_enable(void)
 			kvm_for_each_vcpu(i, vcpu, kvm) {
 				vcpu->arch.tsc_offset_adjustment += delta_cyc;
 				vcpu->arch.last_host_tsc = local_tsc;
+				/*
+				 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+				 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+				 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+				 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+				 *   - kvm_update_masterclock()
+				 */
 				kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 			}
 
@@ -12278,6 +13214,18 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+		 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 处理函数是kvm_pmu_handle_event()
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 	static_call(kvm_x86_sched_in)(vcpu, cpu);
@@ -12323,6 +13271,15 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
 	mutex_init(&kvm->arch.apic_map_lock);
 	seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|714| <<kvm_pmu_rdpmc_vmware>> vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3421| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3427| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3641| <<kvm_guest_time_update>> vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7285| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13032| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
@@ -12378,6 +13335,10 @@ static void kvm_unload_vcpu_mmus(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1283| <<kvm_destroy_vm>> kvm_arch_sync_events(kvm);
+ */
 void kvm_arch_sync_events(struct kvm *kvm)
 {
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
@@ -12407,6 +13368,16 @@ void kvm_arch_sync_events(struct kvm *kvm)
  * address, i.e. its accessibility is not guaranteed, and must be
  * accessed via __copy_{to,from}_user().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2646| <<kvm_alloc_apic_access_page>> hva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/lapic.c|2683| <<kvm_inhibit_apic_access_page>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|3872| <<init_rmode_identity_map>> uaddr = __x86_set_memory_region(kvm,
+ *   - arch/x86/kvm/vmx/vmx.c|5081| <<vmx_set_tss_addr>> ret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,
+ *   - arch/x86/kvm/x86.c|13403| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,
+ *   - arch/x86/kvm/x86.c|13407| <<kvm_arch_destroy_vm>> __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
+ */
 void __user * __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,
 				      u32 size)
 {
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index 82e3dafc5..560a78761 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -448,6 +448,13 @@ static __always_inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)
 
 static inline bool kvm_handling_nmi_from_guest(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->handling_intr_from_guest:
+	 *   - arch/x86/include/asm/kvm_host.h|1861| <<kvm_arch_pmi_in_guest>> ((vcpu) && (vcpu)->arch.handling_intr_from_guest)
+	 *   - arch/x86/kvm/x86.h|441| <<kvm_before_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);
+	 *   - arch/x86/kvm/x86.h|446| <<kvm_after_interrupt>> WRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);
+	 *   - arch/x86/kvm/x86.h|451| <<kvm_handling_nmi_from_guest>> return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
+	 */
 	return vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;
 }
 
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index 40edf4d19..495b354f5 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -106,6 +106,18 @@ static int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)
 	wc->version = wc_version + 1;
 	read_unlock_irq(&gpc->lock);
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1388| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2365| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2539| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9398| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10576| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|12314| <<kvm_arch_hardware_enable>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|109| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 * 处理KVM_REQ_MASTERCLOCK_UPDATE的函数:
+	 *   - kvm_update_masterclock()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
 
 out:
diff --git a/arch/x86/xen/time.c b/arch/x86/xen/time.c
index 52fa5609b..5ab481483 100644
--- a/arch/x86/xen/time.c
+++ b/arch/x86/xen/time.c
@@ -32,6 +32,13 @@
 /* Minimum amount of time until next clock event fires */
 #define TIMER_SLOP	100000
 
+/*
+ * 在以下使用xen_sched_clock_offset:
+ *   - arch/x86/xen/time.c|71| <<xen_sched_clock>> ret -= xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+ *   - arch/x86/xen/time.c|565| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+ */
 static u64 xen_sched_clock_offset __read_mostly;
 
 /* Get the TSC speed from Xen */
@@ -68,6 +75,13 @@ static noinstr u64 xen_sched_clock(void)
 
 	src = &__this_cpu_read(xen_vcpu)->time;
 	ret = pvclock_clocksource_read_nowd(src);
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|71| <<xen_sched_clock>> ret -= xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|565| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 */
 	ret -= xen_sched_clock_offset;
 
 	return ret;
@@ -388,6 +402,11 @@ void xen_timer_resume(void)
 }
 
 static struct pvclock_vsyscall_time_info *xen_clock __read_mostly;
+/*
+ * 在以下使用xen_clock_value_saved:
+ *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+ *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+ */
 static u64 xen_clock_value_saved;
 
 void xen_save_time_memory_area(void)
@@ -395,6 +414,17 @@ void xen_save_time_memory_area(void)
 	struct vcpu_register_time_memory_area t;
 	int ret;
 
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|71| <<xen_sched_clock>> ret -= xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|565| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 *
+	 * 在以下使用xen_clock_value_saved:
+	 *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 */
 	xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
 
 	if (!xen_clock)
@@ -436,8 +466,16 @@ void xen_restore_time_memory_area(void)
 			  ret);
 
 out:
+	/*
+	 * 只在这里调用
+	 */
 	/* Need pvclock_resume() before using xen_clocksource_read(). */
 	pvclock_resume();
+	/*
+	 * 在以下使用xen_clock_value_saved:
+	 *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 */
 	xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
 }
 
@@ -562,8 +600,22 @@ static void __init xen_time_init(void)
 
 static void __init xen_init_time_common(void)
 {
+	/*
+	 * 在以下使用xen_sched_clock_offset:
+	 *   - arch/x86/xen/time.c|71| <<xen_sched_clock>> ret -= xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|398| <<xen_save_time_memory_area>> xen_clock_value_saved = xen_clocksource_read() - xen_sched_clock_offset;
+	 *   - arch/x86/xen/time.c|441| <<xen_restore_time_memory_area>> xen_sched_clock_offset = xen_clocksource_read() - xen_clock_value_saved;
+	 *   - arch/x86/xen/time.c|565| <<xen_init_time_common>> xen_sched_clock_offset = xen_clocksource_read();
+	 */
 	xen_sched_clock_offset = xen_clocksource_read();
 	static_call_update(pv_steal_clock, xen_steal_clock);
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+	 *   - arch/x86/kernel/kvmclock.c|108| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+	 *   - arch/x86/xen/time.c|567| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+	 *   - drivers/clocksource/hyperv_timer.c|514| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+	 */
 	paravirt_set_sched_clock(xen_sched_clock);
 
 	x86_platform.calibrate_tsc = xen_tsc_khz;
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index e733a2a19..ff034f863 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -71,6 +71,23 @@ static struct arch_timer *arch_timer_mem __ro_after_init;
 #define to_arch_timer(e) container_of(e, struct arch_timer, evt)
 
 static u32 arch_timer_rate __ro_after_init;
+/*
+ * VM的数据
+ * crash> arch_timer_ppi
+ * arch_timer_ppi = $1 =
+ * {0, 9, 10, 11, 0}
+ *
+ *   irq_data = {
+ *       mask = 0,
+ *       irq = 10,
+ *       hwirq = 27,
+ *       common = 0xffff0000c0021800,
+ *       chip = 0xffff80000ad592f0 <gic_chip>,
+ *       domain = 0xffff0000c0145e00,
+ *       parent_data = 0x0,
+ *       chip_data = 0xffff80000a8e3b88 <gic_data>
+ *   },
+ */
 static int arch_timer_ppi[ARCH_TIMER_MAX_TIMER_PPI] __ro_after_init;
 
 static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
@@ -83,8 +100,25 @@ static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
 
 static struct clock_event_device __percpu *arch_timer_evt;
 
+/*
+ * 在以下设置arch_timer_uses_ppi:
+ *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ */
 static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
 static bool arch_timer_c3stop __ro_after_init;
+/*
+ * 在以下使用arch_timer_mem_use_virtual:
+ *   - drivers/clocksource/arm_arch_timer.c|88| <<global>> static bool arch_timer_mem_use_virtual __ro_after_init;
+ *   - drivers/clocksource/arm_arch_timer.c|887| <<__arch_timer_setup>> if (arch_timer_mem_use_virtual) {
+ *   - drivers/clocksource/arm_arch_timer.c|1062| <<arch_timer_banner>> arch_timer_mem_use_virtual ? "virt" : "phys" :
+ *   - drivers/clocksource/arm_arch_timer.c|1325| <<arch_timer_mem_register>> if (arch_timer_mem_use_virtual)
+ *   - drivers/clocksource/arm_arch_timer.c|1544| <<arch_timer_mem_find_best_frame>> arch_timer_mem_use_virtual = true;
+ *   - drivers/clocksource/arm_arch_timer.c|1565| <<arch_timer_mem_frame_register>> if (arch_timer_mem_use_virtual)
+ *   - drivers/clocksource/arm_arch_timer.c|1572| <<arch_timer_mem_frame_register>> arch_timer_mem_use_virtual ? "virt" : "phys");
+ */
 static bool arch_timer_mem_use_virtual __ro_after_init;
 static bool arch_counter_suspend_stop __ro_after_init;
 #ifdef CONFIG_GENERIC_GETTIMEOFDAY
@@ -1083,13 +1117,43 @@ static noinstr u64 arch_counter_get_cntvct_mem(void)
 	return arch_counter_get_cnt_mem(arch_timer_mem, CNTVCT_LO);
 }
 
+/*
+ * struct arch_timer_kvm_info {
+ *     struct timecounter timecounter;
+ *     int virtual_irq;         
+ *     int physical_irq;
+ * };
+ *
+ * 在以下使用arch_timer_kvm_info:
+ *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+ *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+ *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+ *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+ */
 static struct arch_timer_kvm_info arch_timer_kvm_info;
 
 struct arch_timer_kvm_info *arch_timer_get_kvm_info(void)
 {
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 *
+	 * 在以下使用arch_timer_kvm_info:
+	 *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+	 *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+	 *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+	 *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+	 */
 	return &arch_timer_kvm_info;
 }
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1348| <<arch_timer_common_init>> arch_counter_register(arch_timers_present);
+ */
 static void __init arch_counter_register(unsigned type)
 {
 	u64 (*scr)(void);
@@ -1100,12 +1164,22 @@ static void __init arch_counter_register(unsigned type)
 	if (type & ARCH_TIMER_TYPE_CP15) {
 		u64 (*rd)(void);
 
+		/*
+		 * 在以下设置arch_timer_uses_ppi:
+		 *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+		 *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+		 *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+		 *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+		 */
 		if ((IS_ENABLED(CONFIG_ARM64) && !is_hyp_mode_available()) ||
 		    arch_timer_uses_ppi == ARCH_TIMER_VIRT_PPI) {
 			if (arch_timer_counter_has_wa()) {
 				rd = arch_counter_get_cntvct_stable;
 				scr = raw_counter_get_cntvct_stable;
 			} else {
+				/*
+				 * kvm似乎是这里
+				 */
 				rd = arch_counter_get_cntvct;
 				scr = arch_counter_get_cntvct;
 			}
@@ -1206,6 +1280,11 @@ static void __init arch_timer_cpu_pm_deinit(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1477| <<arch_timer_of_init>> ret = arch_timer_register();
+ *   - drivers/clocksource/arm_arch_timer.c|1800| <<arch_timer_acpi_init>> ret = arch_timer_register();
+ */
 static int __init arch_timer_register(void)
 {
 	int err;
@@ -1217,6 +1296,29 @@ static int __init arch_timer_register(void)
 		goto out;
 	}
 
+	/*
+	 * 在以下设置arch_timer_uses_ppi:
+	 *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+	 *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+	 *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+	 *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+	 *
+	 * VM的数据
+	 * crash> arch_timer_ppi
+	 * arch_timer_ppi = $1 = 
+	 * {0, 9, 10, 11, 0}
+	 *
+	 *   irq_data = {
+	 *       mask = 0,
+	 *       irq = 10,
+	 *       hwirq = 27,
+	 *       common = 0xffff0000c0021800,
+	 *       chip = 0xffff80000ad592f0 <gic_chip>,
+	 *       domain = 0xffff0000c0145e00,
+	 *       parent_data = 0x0,
+	 *       chip_data = 0xffff80000a8e3b88 <gic_data>
+	 *   },
+	 */
 	ppi = arch_timer_ppi[arch_timer_uses_ppi];
 	switch (arch_timer_uses_ppi) {
 	case ARCH_TIMER_VIRT_PPI:
diff --git a/drivers/clocksource/hyperv_timer.c b/drivers/clocksource/hyperv_timer.c
index e56307a81..1fbe37226 100644
--- a/drivers/clocksource/hyperv_timer.c
+++ b/drivers/clocksource/hyperv_timer.c
@@ -27,6 +27,11 @@
 #include <asm/mshyperv.h>
 
 static struct clock_event_device __percpu *hv_clock_event;
+/*
+ * 在以下使用hv_sched_clock_offset:
+ *   - drivers/clocksource/hyperv_timer.c|433| <<read_hv_sched_clock_tsc>> return (read_hv_clock_tsc() - hv_sched_clock_offset) * (NSEC_PER_SEC / HV_CLOCK_HZ);
+ *   - drivers/clocksource/hyperv_timer.c|591| <<hv_init_tsc_clocksource>> hv_sched_clock_offset = hv_read_reference_counter();
+ */
 static u64 hv_sched_clock_offset __ro_after_init;
 
 /*
@@ -508,8 +513,19 @@ static __always_inline void hv_setup_sched_clock(void *sched_clock)
 	sched_clock_register(sched_clock, 64, NSEC_PER_SEC);
 }
 #elif defined CONFIG_PARAVIRT
+/*
+ * called by:
+ *   - drivers/clocksource/hyperv_timer.c|577| <<hv_init_tsc_clocksource>> hv_setup_sched_clock(read_hv_sched_clock_tsc);
+ */
 static __always_inline void hv_setup_sched_clock(void *sched_clock)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/cpu/vmware.c|340| <<vmware_paravirt_ops_setup>> paravirt_set_sched_clock(vmware_sched_clock);
+	 *   - arch/x86/kernel/kvmclock.c|108| <<kvm_sched_clock_init>> paravirt_set_sched_clock(kvm_sched_clock_read);
+	 *   - arch/x86/xen/time.c|567| <<xen_init_time_common>> paravirt_set_sched_clock(xen_sched_clock);
+	 *   - drivers/clocksource/hyperv_timer.c|514| <<hv_setup_sched_clock>> paravirt_set_sched_clock(sched_clock);
+	 */
 	/* We're on x86/x64 *and* using PV ops */
 	paravirt_set_sched_clock(sched_clock);
 }
@@ -517,6 +533,10 @@ static __always_inline void hv_setup_sched_clock(void *sched_clock)
 static __always_inline void hv_setup_sched_clock(void *sched_clock) {}
 #endif /* CONFIG_GENERIC_SCHED_CLOCK */
 
+/*
+ * called by:
+ *   - drivers/clocksource/hyperv_timer.c|593| <<hv_init_clocksource>> hv_init_tsc_clocksource();
+ */
 static void __init hv_init_tsc_clocksource(void)
 {
 	union hv_reference_tsc_msr tsc_msr;
@@ -573,6 +593,11 @@ static void __init hv_init_tsc_clocksource(void)
 	 * frequencies is handled correctly.
 	 */
 	if (!(ms_hyperv.features & HV_ACCESS_TSC_INVARIANT)) {
+		/*
+		 * 在以下使用hv_sched_clock_offset:
+		 *   - drivers/clocksource/hyperv_timer.c|433| <<read_hv_sched_clock_tsc>> return (read_hv_clock_tsc() - hv_sched_clock_offset) * (NSEC_PER_SEC / HV_CLOCK_HZ);
+		 *   - drivers/clocksource/hyperv_timer.c|591| <<hv_init_tsc_clocksource>> hv_sched_clock_offset = hv_read_reference_counter();
+		 */
 		hv_sched_clock_offset = hv_read_reference_counter();
 		hv_setup_sched_clock(read_hv_sched_clock_tsc);
 	}
diff --git a/drivers/irqchip/irq-gic-v3-its-pci-msi.c b/drivers/irqchip/irq-gic-v3-its-pci-msi.c
index 93f77a819..3efae51ca 100644
--- a/drivers/irqchip/irq-gic-v3-its-pci-msi.c
+++ b/drivers/irqchip/irq-gic-v3-its-pci-msi.c
@@ -170,6 +170,10 @@ its_pci_msi_parse_madt(union acpi_subtable_headers *header,
 		goto out;
 	}
 
+	/*
+	 * [    0.010681] Platform MSI: ITS@0x8080000 domain created
+	 * [    0.010825] PCI/MSI: ITS@0x8080000 domain created
+	 */
 	err = its_pci_msi_init_one(dom_handle, node_name);
 	if (!err)
 		pr_info("PCI/MSI: %s domain created\n", node_name);
diff --git a/drivers/irqchip/irq-gic-v3-its.c b/drivers/irqchip/irq-gic-v3-its.c
index e0c2b10d1..8969a8d0f 100644
--- a/drivers/irqchip/irq-gic-v3-its.c
+++ b/drivers/irqchip/irq-gic-v3-its.c
@@ -109,6 +109,13 @@ struct its_node {
 	struct its_baser	tables[GITS_BASER_NR_REGS];
 	struct its_collection	*collections;
 	struct fwnode_handle	*fwnode_handle;
+	/*
+	 * 在以下使用its_node->get_msi_base:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1726| <<its_irq_compose_msi_msg>> addr = its->get_msi_base(its_dev);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3599| <<its_irq_domain_alloc>> err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
+	 *   - drivers/irqchip/irq-gic-v3-its.c|4716| <<its_enable_quirk_socionext_synquacer>> its->get_msi_base = its_irq_get_msi_base_pre_its;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|5129| <<its_probe_one>> its->get_msi_base = its_irq_get_msi_base;
+	 */
 	u64			(*get_msi_base)(struct its_device *its_dev);
 	u64			typer;
 	u64			cbaser_save;
@@ -227,12 +234,54 @@ static u16 get_its_list(struct its_vm *vm)
 	return (u16)its_list;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|264| <<get_vlpi_map>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|303| <<irq_to_cpuid_lock>> cpu = its_dev->event_map.col_map[its_get_event_id(d)];
+ *   - drivers/irqchip/irq-gic-v3-its.c|1508| <<lpi_update_config>> its_send_inv(its_dev, its_get_event_id(d));
+ *   - drivers/irqchip/irq-gic-v3-its.c|1510| <<lpi_update_config>> its_send_vinv(its_dev, its_get_event_id(d));
+ *   - drivers/irqchip/irq-gic-v3-its.c|1516| <<its_vlpi_set_doorbell>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1684| <<its_set_affinity>> u32 id = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1744| <<its_irq_compose_msi_msg>> msg->data = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1754| <<its_irq_set_irqchip_state>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1855| <<its_vlpi_map>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1940| <<its_vlpi_unmap>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|3647| <<its_irq_domain_activate>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|3667| <<its_irq_domain_deactivate>> u32 event = its_get_event_id(d);
+ *   - drivers/irqchip/irq-gic-v3-its.c|3683| <<its_irq_domain_free>> its_get_event_id(irq_domain_get_irq_data(domain, virq)),
+ */
 static inline u32 its_get_event_id(struct irq_data *d)
 {
 	struct its_device *its_dev = irq_data_get_irq_chip_data(d);
+	/*
+	 * @irq:                interrupt number
+	 * @hwirq:              hardware interrupt number, local to the interrupt domain
+	 *
+	 * struct irq_data *d:
+	 * -> unsigned int irq;
+	 * -> unsigned long hwirq;
+	 *
+	 *
+	 * struct its_device *its_dev:
+	 * -> struct event_lpi_map event_map;
+	 *    -> irq_hw_number_t lpi_base;
+	 *    -> struct its_vlpi_map *vlpi_maps;
+	 *    -> int nr_vlpis;
+	 * -> void *itt;
+	 * -> u32 nr_ites;
+	 */
 	return d->hwirq - its_dev->event_map.lpi_base;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|662| <<its_build_mapti_cmd>> col = dev_event_to_col(desc->its_mapti_cmd.dev, desc->its_mapti_cmd.event_id);
+ *   - drivers/irqchip/irq-gic-v3-its.c|682| <<its_build_movi_cmd>> col = dev_event_to_col(desc->its_movi_cmd.dev, desc->its_movi_cmd.event_id);
+ *   - drivers/irqchip/irq-gic-v3-its.c|701| <<its_build_discard_cmd>> col = dev_event_to_col(desc->its_discard_cmd.dev, desc->its_discard_cmd.event_id);
+ *   - drivers/irqchip/irq-gic-v3-its.c|719| <<its_build_inv_cmd>> col = dev_event_to_col(desc->its_inv_cmd.dev, desc->its_inv_cmd.event_id);
+ *   - drivers/irqchip/irq-gic-v3-its.c|737| <<its_build_int_cmd>> col = dev_event_to_col(desc->its_int_cmd.dev, desc->its_int_cmd.event_id);
+ *   - drivers/irqchip/irq-gic-v3-its.c|755| <<its_build_clear_cmd>> col = dev_event_to_col(desc->its_clear_cmd.dev, desc->its_clear_cmd.event_id);
+ */
 static struct its_collection *dev_event_to_col(struct its_device *its_dev,
 					       u32 event)
 {
@@ -721,6 +770,10 @@ static struct its_collection *its_build_inv_cmd(struct its_node *its,
 	return valid_col(col);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|1233| <<its_send_int>> its_send_single_command(dev->its, its_build_int_cmd, &desc);
+ */
 static struct its_collection *its_build_int_cmd(struct its_node *its,
 						struct its_cmd_block *cmd,
 						struct its_cmd_desc *desc)
@@ -1158,6 +1211,18 @@ static void its_build_sync_cmd(struct its_node *its,
 	its_fixup_cmd(sync_cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|1237| <<its_send_int>> its_send_single_command(dev->its, its_build_int_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1247| <<its_send_clear>> its_send_single_command(dev->its, its_build_clear_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1257| <<its_send_inv>> its_send_single_command(dev->its, its_build_inv_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1267| <<its_send_mapd>> its_send_single_command(dev->its, its_build_mapd_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1278| <<its_send_mapc>> its_send_single_command(its, its_build_mapc_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1289| <<its_send_mapti>> its_send_single_command(dev->its, its_build_mapti_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1301| <<its_send_movi>> its_send_single_command(dev->its, its_build_movi_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1311| <<its_send_discard>> its_send_single_command(dev->its, its_build_discard_cmd, &desc);
+ *   - drivers/irqchip/irq-gic-v3-its.c|1320| <<its_send_invall>> its_send_single_command(its, its_build_invall_cmd, &desc);
+ */
 static BUILD_SINGLE_CMD_FUNC(its_send_single_command, its_cmd_builder_t,
 			     struct its_collection, its_build_sync_cmd)
 
@@ -1236,6 +1301,11 @@ static void its_send_mapti(struct its_device *dev, u32 irq_id, u32 id)
 	its_send_single_command(dev->its, its_build_mapti_cmd, &desc);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|1751| <<its_set_affinity>> its_send_movi(its_dev, target_col, id);
+ *   - drivers/irqchip/irq-gic-v3-its.c|3880| <<its_vpe_db_proxy_move>> its_send_movi(vpe_proxy.dev, target_col, vpe->vpe_proxy_event);
+ */
 static void its_send_movi(struct its_device *dev,
 			  struct its_collection *col, u32 id)
 {
@@ -1723,6 +1793,13 @@ static void its_irq_compose_msi_msg(struct irq_data *d, struct msi_msg *msg)
 	u64 addr;
 
 	its = its_dev->its;
+	/*
+	 * 在以下使用its_node->get_msi_base:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1726| <<its_irq_compose_msi_msg>> addr = its->get_msi_base(its_dev);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3599| <<its_irq_domain_alloc>> err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
+	 *   - drivers/irqchip/irq-gic-v3-its.c|4716| <<its_enable_quirk_socionext_synquacer>> its->get_msi_base = its_irq_get_msi_base_pre_its;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|5129| <<its_probe_one>> its->get_msi_base = its_irq_get_msi_base;
+	 */
 	addr = its->get_msi_base(its_dev);
 
 	msg->address_lo		= lower_32_bits(addr);
@@ -2603,6 +2680,10 @@ static int its_probe_baser_psz(struct its_node *its, struct its_baser *baser)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|5201| <<its_probe_one>> err = its_alloc_tables(its);
+ */
 static int its_alloc_tables(struct its_node *its)
 {
 	u64 shr = GITS_BASER_InnerShareable;
@@ -2952,6 +3033,10 @@ static int allocate_vpe_l1_table(void)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|5218| <<its_probe_one>> err = its_alloc_collections(its);
+ */
 static int its_alloc_collections(struct its_node *its)
 {
 	int i;
@@ -3596,6 +3681,13 @@ static int its_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,
 	if (err)
 		return err;
 
+	/*
+	 * 在以下使用its_node->get_msi_base:
+	 *   - drivers/irqchip/irq-gic-v3-its.c|1726| <<its_irq_compose_msi_msg>> addr = its->get_msi_base(its_dev);
+	 *   - drivers/irqchip/irq-gic-v3-its.c|3599| <<its_irq_domain_alloc>> err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
+	 *   - drivers/irqchip/irq-gic-v3-its.c|4716| <<its_enable_quirk_socionext_synquacer>> its->get_msi_base = its_irq_get_msi_base_pre_its;
+	 *   - drivers/irqchip/irq-gic-v3-its.c|5129| <<its_probe_one>> its->get_msi_base = its_irq_get_msi_base;
+	 */
 	err = iommu_dma_prepare_msi(info->desc, its->get_msi_base(its_dev));
 	if (err)
 		return err;
@@ -3871,6 +3963,10 @@ static void its_wait_vpt_parse_complete(void)
 						       1, 500));
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|4036| <<its_vpe_set_vcpu_affinity>> its_vpe_schedule(vpe);
+ */
 static void its_vpe_schedule(struct its_vpe *vpe)
 {
 	void __iomem *vlpi_base = gic_data_rdist_vlpi_base();
@@ -4250,6 +4346,15 @@ static int its_sgi_set_affinity(struct irq_data *d,
 	return IRQ_SET_MASK_OK;
 }
 
+/*
+ * 注释.
+ * When GITS_CTLR.Enabled==1 and GITS_CTLR.Quiescent==0, a write to GITS_SGIR
+ * results in a virtual interrupt being generated with the vPEID and vINTID
+ * from the write. If the vPEID is not mapped on any ITS, the write is silently
+ * discarded. If the vPEID is not mapped on this ITS, but is mapped on a
+ * different ITS, it is CONSTRAINED UNPREDICTABLE whether the interrupt is
+ * delivered or discarded. Virtual SGIs have no priority shift.
+ */
 static int its_sgi_set_irqchip_state(struct irq_data *d,
 				     enum irqchip_irq_state which,
 				     bool state)
@@ -4952,6 +5057,10 @@ static void __init __iomem *its_map_one(struct resource *res, int *err)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|5198| <<its_probe_one>> err = its_init_domain(handle, its);
+ */
 static int its_init_domain(struct fwnode_handle *handle, struct its_node *its)
 {
 	struct irq_domain *inner_domain;
@@ -5055,6 +5164,11 @@ static int __init its_compute_its_list_map(struct resource *res,
 	return its_number;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3-its.c|5466| <<its_of_probe>> its_probe_one(&res, &np->fwnode, of_node_to_nid(np));
+ *   - drivers/irqchip/irq-gic-v3-its.c|5602| <<gic_acpi_parse_madt_its>> err = its_probe_one(&res, dom_handle,
+ */
 static int __init its_probe_one(struct resource *res,
 				struct fwnode_handle *handle, int numa_node)
 {
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index eedfa8e9f..a0fc0a08d 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -35,6 +35,121 @@
 
 #include "irq-gic-common.h"
 
+/*
+ * intid是针对gic的中断号, 也就是irq_data->hwirq
+ * 应该不是proc/interrupts的irq
+ *
+ *---------------------------
+ *
+ * virtio的中断
+ *
+ * # cat /sys/kernel/debug/irq/irqs/85
+ * handler:  handle_fasteoi_irq
+ * device:   0000:00:06.0
+ * status:   0x00000000
+ * istate:   0x00004000
+ * ddepth:   0
+ * wdepth:   0
+ * dstate:   0x31401200
+ *             IRQD_ACTIVATED
+ *             IRQD_IRQ_STARTED
+ *             IRQD_SINGLE_TARGET
+ *             IRQD_AFFINITY_SET
+ *             IRQD_AFFINITY_ON_ACTIVATE
+ *             IRQD_HANDLE_ENFORCE_IRQCTX
+ * node:     -1
+ * affinity: 0
+ * effectiv: 0
+ * domain:  irqchip@0x0000000008080000-3
+ *  hwirq:   0x18001
+ *  chip:    ITS-MSI
+ *   flags:   0x20
+ *              IRQCHIP_ONESHOT_SAFE
+ *  parent:
+ *     domain:  irqchip@0x0000000008080000-5
+ *      hwirq:   0x2025
+ *      chip:    ITS
+ *       flags:   0x0
+ *      parent:
+ *         domain:  irqchip@0x0000000008000000-1
+ *          hwirq:   0x2025
+ *          chip:    GICv3
+ *           flags:   0x15
+ *                      IRQCHIP_SET_TYPE_MASKED
+ *                      IRQCHIP_MASK_ON_SUSPEND
+ *                      IRQCHIP_SKIP_SET_WAKE
+ *
+ *---------------------------
+ *
+ * arch_timer的中断
+ *
+ * # cat /sys/kernel/debug/irq/irqs/10
+ * handler:  handle_percpu_devid_irq
+ * device:   (null)
+ * status:   0x00031704
+ *             _IRQ_NOPROBE
+ *             _IRQ_NOTHREAD
+ *             _IRQ_NOAUTOEN
+ *             _IRQ_PER_CPU_DEVID
+ * istate:   0x00004000
+ * ddepth:   1
+ * wdepth:   0
+ * dstate:   0x12032a04
+ *             IRQ_TYPE_LEVEL_HIGH
+ *             IRQD_LEVEL
+ *             IRQD_ACTIVATED
+ *             IRQD_IRQ_DISABLED
+ *             IRQD_IRQ_MASKED
+ *             IRQD_PER_CPU
+ *             IRQD_DEFAULT_TRIGGER_SET
+ *             IRQD_HANDLE_ENFORCE_IRQCTX
+ * node:     -1
+ * affinity: 0-3
+ * effectiv:
+ * domain:  irqchip@0x0000000008000000-1
+ *  hwirq:   0x1b
+ *  chip:    GICv3
+ *   flags:   0x15
+ *              IRQCHIP_SET_TYPE_MASKED
+ *              IRQCHIP_MASK_ON_SUSPEND
+ *              IRQCHIP_SKIP_SET_WAKE
+ *
+ *---------------------------
+ *
+ * pmu的中断
+ *
+ * # cat /sys/kernel/debug/irq/irqs/50
+ * handler:  handle_percpu_devid_irq
+ * device:   (null)
+ * status:   0x00031704
+ *             _IRQ_NOPROBE
+ *             _IRQ_NOTHREAD
+ *             _IRQ_NOAUTOEN
+ *             _IRQ_PER_CPU_DEVID
+ * istate:   0x00004000
+ * ddepth:   1
+ * wdepth:   0
+ * dstate:   0x12032a04
+ *             IRQ_TYPE_LEVEL_HIGH
+ *             IRQD_LEVEL
+ *             IRQD_ACTIVATED
+ *             IRQD_IRQ_DISABLED
+ *             IRQD_IRQ_MASKED
+ *             IRQD_PER_CPU
+ *             IRQD_DEFAULT_TRIGGER_SET
+ *             IRQD_HANDLE_ENFORCE_IRQCTX
+ * node:     -1
+ * affinity: 0-3
+ * effectiv: 
+ * domain:  irqchip@0x0000000008000000-1
+ *  hwirq:   0x17
+ *  chip:    GICv3
+ *   flags:   0x15
+ *              IRQCHIP_SET_TYPE_MASKED
+ *              IRQCHIP_MASK_ON_SUSPEND
+ *              IRQCHIP_SKIP_SET_WAKE
+ */
+
 #define GICD_INT_NMI_PRI	(GICD_INT_DEF_PRI & ~0x80)
 
 #define FLAGS_WORKAROUND_GICR_WAKER_MSM8996	(1ULL << 0)
@@ -131,6 +246,10 @@ static refcount_t *ppi_nmi_refs;
 static struct gic_kvm_info gic_v3_kvm_info __initdata;
 static DEFINE_PER_CPU(bool, has_rss);
 
+/*
+ * 每个核都有一个MPIDR寄存器,并且每个核的MPIDR寄存器的值都不相同,这可以作为每个核的唯一标志.
+ * 比如我希望某个中断由核1处理,那么GIC中的路由寄存器需要根据核1的MPIDR值去配置.
+ */
 #define MPIDR_RS(mpidr)			(((mpidr) & 0xF0UL) >> 4)
 #define gic_data_rdist()		(this_cpu_ptr(gic_data.rdists.rdist))
 #define gic_data_rdist_rd_base()	(gic_data_rdist()->rd_base)
@@ -149,6 +268,14 @@ enum gic_intid_range {
 	__INVALID_RANGE__
 };
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|302| <<get_intid_range>> return __get_intid_range(d->hwirq);
+ *   - drivers/irqchip/irq-gic-v3.c|339| <<gic_dist_base_alias>> switch (__get_intid_range(hwirq)) {
+ *   - drivers/irqchip/irq-gic-v3.c|650| <<__gic_get_ppi_index>> switch (__get_intid_range(hwirq)) {
+ *   - drivers/irqchip/irq-gic-v3.c|1676| <<gic_irq_domain_map>> switch (__get_intid_range(hw)) {
+ *   - drivers/irqchip/irq-gic-v3.c|1839| <<fwspec_is_partitioned_ppi>> range = __get_intid_range(hwirq);
+ */
 static enum gic_intid_range __get_intid_range(irq_hw_number_t hwirq)
 {
 	switch (hwirq) {
@@ -169,6 +296,15 @@ static enum gic_intid_range __get_intid_range(irq_hw_number_t hwirq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|184| <<gic_irq_in_rdist>> switch (get_intid_range(d)) {
+ *   - drivers/irqchip/irq-gic-v3.c|229| <<gic_dist_base>> switch (get_intid_range(d)) {
+ *   - drivers/irqchip/irq-gic-v3.c|326| <<convert_offset_index>> switch (get_intid_range(d)) {
+ *   - drivers/irqchip/irq-gic-v3.c|604| <<gic_arm64_erratum_2941627_needed>> range = get_intid_range(d);
+ *   - drivers/irqchip/irq-gic-v3.c|655| <<gic_set_type>> range = get_intid_range(d);
+ *   - drivers/irqchip/irq-gic-v3.c|685| <<gic_irq_set_vcpu_affinity>> if (get_intid_range(d) == SGI_RANGE)
+ */
 static enum gic_intid_range get_intid_range(struct irq_data *d)
 {
 	return __get_intid_range(d->hwirq);
@@ -272,6 +408,11 @@ static void gic_redist_wait_for_rwp(void)
 
 #ifdef CONFIG_ARM64
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|952| <<__gic_handle_irq_from_irqson>> irqnr = gic_read_iar();
+ *   - drivers/irqchip/irq-gic-v3.c|1001| <<__gic_handle_irq_from_irqsoff>> irqnr = gic_read_iar();
+ */
 static u64 __maybe_unused gic_read_iar(void)
 {
 	if (cpus_have_const_cap(ARM64_WORKAROUND_CAVIUM_23154))
@@ -387,6 +528,15 @@ static int gic_peek_irq(struct irq_data *d, u32 offset)
 	return !!(readl_relaxed(base + offset + (index / 32) * 4) & mask);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|554| <<gic_mask_irq>> gic_poke_irq(d, GICD_ICENABLER);
+ *   - drivers/irqchip/irq-gic-v3.c|573| <<gic_eoimode1_mask_irq>> gic_poke_irq(d, GICD_ICACTIVER);
+ *   - drivers/irqchip/irq-gic-v3.c|578| <<gic_unmask_irq>> gic_poke_irq(d, GICD_ISENABLER);
+ *   - drivers/irqchip/irq-gic-v3.c|616| <<gic_irq_set_irqchip_state>> gic_poke_irq(d, reg);
+ *   - drivers/irqchip/irq-gic-v3.c|792| <<gic_eoi_irq>> gic_poke_irq(d, GICD_ICACTIVER);
+ *   - drivers/irqchip/irq-gic-v3.c|808| <<gic_eoimode1_eoi_irq>> gic_poke_irq(d, GICD_ICACTIVER);
+ */
 static void gic_poke_irq(struct irq_data *d, u32 offset)
 {
 	void __iomem *base;
@@ -405,6 +555,11 @@ static void gic_poke_irq(struct irq_data *d, u32 offset)
 
 static void gic_mask_irq(struct irq_data *d)
 {
+	/*
+	 * GICD_ICENABLER寄存器为GIC支持的每个中断提供一个clear-enable位,
+	 * 写1到clear-enable位将禁止相应的中断从分发器转发到CPU接口端.
+	 * 即禁止了所有中断转发.
+	 */
 	gic_poke_irq(d, GICD_ICENABLER);
 	if (gic_irq_in_rdist(d))
 		gic_redist_wait_for_rwp();
@@ -438,6 +593,12 @@ static inline bool gic_supports_nmi(void)
 	       static_branch_likely(&supports_pseudo_nmis);
 }
 
+/*
+ * 在以下使用gic_irq_set_irqchip_state():
+ *   - drivers/irqchip/irq-gic-v3.c|1669| <<global>> struct irq_chip gic_chip.irq_set_irqchip_state = gic_irq_set_irqchip_state,
+ *   - drivers/irqchip/irq-gic-v3.c|1687| <<global>> struct irq_chip gic_eoimode1_chip.irq_set_irqchip_state = gic_irq_set_irqchip_state,
+ *   - drivers/irqchip/irq-gic-v3.c|1629| <<gic_retrigger>> return !gic_irq_set_irqchip_state(data, IRQCHIP_STATE_PENDING, true);
+ */
 static int gic_irq_set_irqchip_state(struct irq_data *d,
 				     enum irqchip_irq_state which, bool val)
 {
@@ -448,10 +609,34 @@ static int gic_irq_set_irqchip_state(struct irq_data *d,
 
 	switch (which) {
 	case IRQCHIP_STATE_PENDING:
+		/*
+		 * The GICD_ISPENDRs provide a Set-pending bit for each
+		 * interrupt supported by the GIC. Writing 1 to a Set-pending
+		 * bit sets the status of the corresponding peripheral
+		 * interrupt to pending. Reading a bit identifies whether the
+		 * interrupt is pending.
+		 *
+		 * The GICD_ICPENDRs provide a Clear-pending bit for each
+		 * interrupt supported by the GIC. Writing 1 to a Clear-pending
+		 * bit clears the pending state of the corresponding peripheral
+		 * interrupt. Reading a bit identifies whether the interrupt is
+		 * pending.
+		 */
 		reg = val ? GICD_ISPENDR : GICD_ICPENDR;
 		break;
 
 	case IRQCHIP_STATE_ACTIVE:
+		/*
+		 * The GICD_ISACTIVERs provide a Set-active bit for each
+		 * interrupt that the GIC supports. Writing to a Set-active bit
+		 * Activates the corresponding interrupt. These registers are
+		 * used when preserving and restoring GIC state.
+		 *
+		 * The GICD_ICACTIVERs provide a Clear-active bit for each
+		 * interrupt that the GIC supports. Writing to a Clear-active
+		 * bit Deactivates the corresponding interrupt. These registers
+		 * are used when preserving and restoring GIC state.
+		 */
 		reg = val ? GICD_ISACTIVER : GICD_ICACTIVER;
 		break;
 
@@ -460,6 +645,13 @@ static int gic_irq_set_irqchip_state(struct irq_data *d,
 			gic_mask_irq(d);
 			return 0;
 		}
+		/*
+		 * The GICD_ISENABLERs provide a Set-enable bit for each
+		 * interrupt supported by the GIC. Writing 1 to a Set-enable
+		 * bit enables forwarding of the corresponding interrupt from
+		 * the Distributor to the CPU interfaces. Reading a bit
+		 * identifies whether the interrupt is enabled.
+		 */
 		reg = GICD_ISENABLER;
 		break;
 
@@ -497,6 +689,11 @@ static int gic_irq_get_irqchip_state(struct irq_data *d,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|577| <<gic_irq_nmi_setup>> gic_irq_set_prio(d, GICD_INT_NMI_PRI);
+ *   - drivers/irqchip/irq-gic-v3.c|612| <<gic_irq_nmi_teardown>> gic_irq_set_prio(d, GICD_INT_DEF_PRI);
+ */
 static void gic_irq_set_prio(struct irq_data *d, u8 prio)
 {
 	void __iomem *base = gic_dist_base(d);
@@ -615,6 +812,19 @@ static bool gic_arm64_erratum_2941627_needed(struct irq_data *d)
 
 static void gic_eoi_irq(struct irq_data *d)
 {
+	/*
+	 * 注释:
+	 *
+	 * After an interrupt has been acknowledged, a valid write to
+	 * ICC_EOIR0_EL1 for Group 0 interrupts, or a valid write to
+	 * ICC_EOIR1_EL1 for Group 1 interrupts, results in a priority
+	 * drop.
+	 *
+	 * A valid write to ICC_EOIR0_EL1 or ICC_EOIR1_EL1 to perform a
+	 * priority drop is required for each acknowledged interrupt, even
+	 * for LPIs which do not have an active state. A priority drop must
+	 * be performed by the same PE that activated the interrupt.
+	 */
 	write_gicreg(gic_irq(d), ICC_EOIR1_EL1);
 	isb();
 
@@ -803,6 +1013,10 @@ static void __gic_handle_irq_from_irqson(struct pt_regs *regs)
 	bool is_nmi;
 	u32 irqnr;
 
+	/*
+	 * 通过gic_read_iar()读取IAR寄存器做出ACK应答,
+	 * 而后判断中断源的类型.
+	 */
 	irqnr = gic_read_iar();
 
 	is_nmi = gic_rpr_is_nmi_prio();
@@ -852,6 +1066,10 @@ static void __gic_handle_irq_from_irqsoff(struct pt_regs *regs)
 	pmr = gic_read_pmr();
 	gic_pmr_mask_irqs();
 	isb();
+	/*
+	 * 通过gic_read_iar()读取IAR寄存器做出ACK应答,
+	 * 而后判断中断源的类型.
+	 */
 	irqnr = gic_read_iar();
 	gic_write_pmr(pmr);
 
@@ -1336,6 +1554,10 @@ static u16 gic_compute_target_list(int *base_cpu, const struct cpumask *mask,
 	(MPIDR_AFFINITY_LEVEL(cluster_id, level) \
 		<< ICC_SGI1R_AFFINITY_## level ##_SHIFT)
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|1390| <<gic_ipi_send_mask>> gic_send_sgi(cluster_id, tlist, d->hwirq);
+ */
 static void gic_send_sgi(u64 cluster_id, u16 tlist, unsigned int irq)
 {
 	u64 val;
@@ -1351,6 +1573,10 @@ static void gic_send_sgi(u64 cluster_id, u16 tlist, unsigned int irq)
 	gic_write_sgi1r(val);
 }
 
+/*
+ * struct irq_chip gic_chip.ipi_send_mask = gic_ipi_send_mask()
+ * struct irq_chip gic_eoimode1_chip.ipi_send_mask = gic_ipi_send_mask()
+ */
 static void gic_ipi_send_mask(struct irq_data *d, const struct cpumask *mask)
 {
 	int cpu;
@@ -1514,6 +1740,10 @@ static struct irq_chip gic_eoimode1_chip = {
 				  IRQCHIP_MASK_ON_SUSPEND,
 };
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|1681| <<gic_irq_domain_alloc>> ret = gic_irq_domain_map(domain, virq + i, hwirq + i);
+ */
 static int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,
 			      irq_hw_number_t hw)
 {
@@ -1556,6 +1786,13 @@ static int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - struct irq_domain_ops gic_irq_domain_ops.translate = gic_irq_domain_translate()
+ *   - drivers/irqchip/irq-gic-v3.c|1676| <<gic_irq_domain_alloc>> ret = gic_irq_domain_translate(domain, fwspec, &hwirq, &type);
+ *   - drivers/irqchip/irq-gic-v3.c|1737| <<gic_irq_domain_select>> ret = gic_irq_domain_translate(d, fwspec, &hwirq, &type);
+ *   - drivers/irqchip/irq-gic-v3.c|1776| <<partition_domain_translate>> ret = gic_irq_domain_translate(d, fwspec, &ppi_intid, type);
+ */
 static int gic_irq_domain_translate(struct irq_domain *d,
 				    struct irq_fwspec *fwspec,
 				    unsigned long *hwirq,
@@ -1629,6 +1866,9 @@ static int gic_irq_domain_translate(struct irq_domain *d,
 	return -EINVAL;
 }
 
+/*
+ * struct irq_domain_ops gic_irq_domain_ops.alloc = gic_irq_domain_alloc()
+ */
 static int gic_irq_domain_alloc(struct irq_domain *domain, unsigned int virq,
 				unsigned int nr_irqs, void *arg)
 {
@@ -1987,6 +2227,11 @@ static void gic_enable_nmi_support(void)
 		gic_chip.flags |= IRQCHIP_SUPPORTS_NMI;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2516| <<gic_of_init>> err = gic_init_bases(dist_phys_base, dist_base, rdist_regs,
+ *   - drivers/irqchip/irq-gic-v3.c|2841| <<gic_acpi_init>> err = gic_init_bases(dist->base_address, acpi_data.dist_base,
+ */
 static int __init gic_init_bases(phys_addr_t dist_phys_base,
 				 void __iomem *dist_base,
 				 struct redist_region *rdist_regs,
@@ -2094,6 +2339,10 @@ static int __init gic_validate_dist_version(void __iomem *dist_base)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2355| <<gic_of_init>> gic_populate_ppi_partitions(node);
+ */
 /* Create all possible partitions at boot time */
 static void __init gic_populate_ppi_partitions(struct device_node *gic_node)
 {
@@ -2198,6 +2447,26 @@ static void __init gic_of_setup_kvm_info(struct device_node *node)
 	struct resource r;
 	u32 gicv_idx;
 
+	/*
+	 * struct gic_kvm_info {
+	 *     // GIC type
+	 *     enum gic_type   type;
+	 *     // Virtual CPU interface
+	 *     struct resource vcpu;
+	 *     // Interrupt number
+	 *     unsigned int    maint_irq;
+	 *     // No interrupt mask, no need to use the above field
+	 *     bool            no_maint_irq_mask;
+	 *     // Virtual control interface
+	 *     struct resource vctrl;
+	 *     // vlpi support
+	 *     bool            has_v4;
+	 *     // rvpeid support
+	 *     bool            has_v4_1;
+	 *     // Deactivation impared, subpar stuff
+	 *     bool            no_hw_deactivation;
+	 * };
+	 */
 	gic_v3_kvm_info.type = GIC_V3;
 
 	gic_v3_kvm_info.maint_irq = irq_of_parse_and_map(node, 0);
@@ -2242,6 +2511,10 @@ static void __iomem *gic_of_iomap(struct device_node *node, int idx,
 	return base ?: IOMEM_ERR_PTR(-ENOMEM);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2371| <<global>> IRQCHIP_DECLARE(gic_v3, "arm,gic-v3", gic_of_init);
+ */
 static int __init gic_of_init(struct device_node *node, struct device_node *parent)
 {
 	phys_addr_t dist_phys_base;
@@ -2557,6 +2830,14 @@ static void __init gic_acpi_setup_kvm_info(void)
 
 	gic_v3_kvm_info.has_v4 = gic_data.rdists.has_vlpis;
 	gic_v3_kvm_info.has_v4_1 = gic_data.rdists.has_rvpeid;
+	/*
+	 * called by:
+	 *   - drivers/irqchip/irq-apple-aic.c|1063| <<aic_of_ic_init>> vgic_set_kvm_info(&vgic_info);
+	 *   - drivers/irqchip/irq-gic-v3.c|2245| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+	 *   - drivers/irqchip/irq-gic-v3.c|2587| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v3_kvm_info);
+	 *   - drivers/irqchip/irq-gic.c|1466| <<gic_of_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+	 *   - drivers/irqchip/irq-gic.c|1627| <<gic_acpi_setup_kvm_info>> vgic_set_kvm_info(&gic_v2_kvm_info);
+	 */
 	vgic_set_kvm_info(&gic_v3_kvm_info);
 }
 
@@ -2632,6 +2913,18 @@ gic_acpi_init(union acpi_subtable_headers *header, const unsigned long end)
 	iounmap(acpi_data.dist_base);
 	return err;
 }
+
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2687| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V3, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2690| <<global>> IRQCHIP_ACPI_DECLARE(gic_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V4, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2693| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3_or_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_NONE, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1703| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_V2, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1706| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2_maybe, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_NONE, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-loongarch-cpu.c|171| <<global>> IRQCHIP_ACPI_DECLARE(cpuintc_v1, ACPI_MADT_TYPE_CORE_PIC, NULL, ACPI_MADT_CORE_PIC_VERSION_V1, cpuintc_acpi_init);
+ *   - drivers/irqchip/irq-riscv-intc.c|194| <<global>> IRQCHIP_ACPI_DECLARE(riscv_intc, ACPI_MADT_TYPE_RINTC, NULL, ACPI_MADT_RINTC_VERSION_V1, riscv_intc_acpi_init);
+ */
+
 IRQCHIP_ACPI_DECLARE(gic_v3, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR,
 		     acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V3,
 		     gic_acpi_init);
diff --git a/drivers/irqchip/irq-gic-v4.c b/drivers/irqchip/irq-gic-v4.c
index 94d56a03b..b11ee6733 100644
--- a/drivers/irqchip/irq-gic-v4.c
+++ b/drivers/irqchip/irq-gic-v4.c
@@ -306,6 +306,11 @@ int its_invall_vpe(struct its_vpe *vpe)
 	return its_send_vpe_cmd(vpe, &info);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|389| <<update_affinity>> ret = its_map_vlpi(irq->host_irq, &map);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|455| <<kvm_vgic_v4_set_forwarding>> ret = its_map_vlpi(virq, &map);
+ */
 int its_map_vlpi(int irq, struct its_vlpi_map *map)
 {
 	struct its_cmd_info info = {
diff --git a/drivers/irqchip/irq-gic.c b/drivers/irqchip/irq-gic.c
index 412196a7d..84a36d66e 100644
--- a/drivers/irqchip/irq-gic.c
+++ b/drivers/irqchip/irq-gic.c
@@ -181,6 +181,13 @@ static inline bool cascading_gic_irq(struct irq_data *d)
 /*
  * Routines to acknowledge, disable and enable interrupts
  */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic.c|198| <<gic_mask_irq>> gic_poke_irq(d, GIC_DIST_ENABLE_CLEAR);
+ *   - drivers/irqchip/irq-gic.c|213| <<gic_eoimode1_mask_irq>> gic_poke_irq(d, GIC_DIST_ACTIVE_CLEAR);
+ *   - drivers/irqchip/irq-gic.c|218| <<gic_unmask_irq>> gic_poke_irq(d, GIC_DIST_ENABLE_SET);
+ *   - drivers/irqchip/irq-gic.c|267| <<gic_irq_set_irqchip_state>> gic_poke_irq(d, reg);
+ */
 static void gic_poke_irq(struct irq_data *d, u32 offset)
 {
 	u32 mask = 1 << (gic_irq(d) % 32);
@@ -242,6 +249,12 @@ static void gic_eoimode1_eoi_irq(struct irq_data *d)
 	writel_relaxed(hwirq, gic_cpu_base(d) + GIC_CPU_DEACTIVATE);
 }
 
+/*
+ * 在以下使用gic_irq_set_irqchip_state():
+ *   - drivers/irqchip/irq-gic.c|896| <<global>> struct irq_chip gic_chip.irq_set_irqchip_state = gic_irq_set_irqchip_state,
+ *   - drivers/irqchip/irq-gic.c|913| <<global>> struct irq_chip gic_chip_mode1.irq_set_irqchip_state = gic_irq_set_irqchip_state,
+ *   - drivers/irqchip/irq-gic.c|334| <<gic_retrigger>> return !gic_irq_set_irqchip_state(data, IRQCHIP_STATE_PENDING, true);
+ */
 static int gic_irq_set_irqchip_state(struct irq_data *d,
 				     enum irqchip_irq_state which, bool val)
 {
@@ -820,6 +833,10 @@ static int gic_set_affinity(struct irq_data *d, const struct cpumask *mask_val,
 	return IRQ_SET_MASK_OK_DONE;
 }
 
+/*
+ * struct irq_chip gic_chip.ipi_send_mask = gic_ipi_send_mask()
+ * struct irq_chip gic_chip_mode1.ipi_send_mask = gic_ipi_send_mask()
+ */
 static void gic_ipi_send_mask(struct irq_data *d, const struct cpumask *mask)
 {
 	int cpu;
@@ -920,6 +937,10 @@ static const struct irq_chip gic_chip_mode1 = {
  * cpu_id: the ID for the destination CPU interface
  * irq: the IPI number to send a SGI for
  */
+/*
+ * called by:
+ *   - arch/arm/common/bL_switcher.c|198| <<bL_switch_to>> gic_send_sgi(bL_gic_id[ib_cpu][ib_cluster], 0);
+ */
 void gic_send_sgi(unsigned int cpu_id, unsigned int irq)
 {
 	BUG_ON(cpu_id >= NR_GIC_CPU_IF);
@@ -1163,6 +1184,11 @@ static const struct irq_domain_ops gic_irq_domain_hierarchy_ops = {
 	.free = irq_domain_free_irqs_top,
 };
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic.c|1271| <<__gic_init_bases>> ret = gic_init_bases(gic, handle);
+ *   - drivers/irqchip/irq-gic.c|1441| <<gic_of_init_child>> ret = gic_init_bases(*gic, &dev->of_node->fwnode);
+ */
 static int gic_init_bases(struct gic_chip_data *gic,
 			  struct fwnode_handle *handle)
 {
@@ -1466,6 +1492,18 @@ static void __init gic_of_setup_kvm_info(struct device_node *node)
 		vgic_set_kvm_info(&gic_v2_kvm_info);
 }
 
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic.c|1524| <<global>> IRQCHIP_DECLARE(gic_400, "arm,gic-400", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1525| <<global>> IRQCHIP_DECLARE(arm11mp_gic, "arm,arm11mp-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1526| <<global>> IRQCHIP_DECLARE(arm1176jzf_dc_gic, "arm,arm1176jzf-devchip-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1527| <<global>> IRQCHIP_DECLARE(cortex_a15_gic, "arm,cortex-a15-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1528| <<global>> IRQCHIP_DECLARE(cortex_a9_gic, "arm,cortex-a9-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1529| <<global>> IRQCHIP_DECLARE(cortex_a7_gic, "arm,cortex-a7-gic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1530| <<global>> IRQCHIP_DECLARE(msm_8660_qgic, "qcom,msm-8660-qgic", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1531| <<global>> IRQCHIP_DECLARE(msm_qgic2, "qcom,msm-qgic2", gic_of_init);
+ *   - drivers/irqchip/irq-gic.c|1532| <<global>> IRQCHIP_DECLARE(pl390, "arm,pl390", gic_of_init);
+ */
 int __init
 gic_of_init(struct device_node *node, struct device_node *parent)
 {
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 49d1d6acf..66bff6e9a 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -842,6 +842,11 @@ static ssize_t tap_put_user(struct tap_queue *q,
 	return ret ? ret : total;
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|904| <<tap_read_iter>> ret = tap_do_read(q, to, noblock, NULL);
+ *   - drivers/net/tap.c|1259| <<tap_recvmsg>> ret = tap_do_read(q, &m->msg_iter, flags & MSG_DONTWAIT, skb);
+ */
 static ssize_t tap_do_read(struct tap_queue *q,
 			   struct iov_iter *to,
 			   int noblock, struct sk_buff *skb)
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 8e9f4cfe9..9ad37e99c 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -1570,6 +1570,11 @@ static void virtio_skb_set_hash(const struct virtio_net_hdr_v1_hash *hdr_hash,
 	skb_set_hash(skb, __le32_to_cpu(hdr_hash->hash_value), rss_hash_type);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1888| <<virtnet_receive>> receive_buf(vi, rq, buf, len, ctx, xdp_xmit, &stats);
+ *   - drivers/net/virtio_net.c|1894| <<virtnet_receive>> receive_buf(vi, rq, buf, len, NULL, xdp_xmit, &stats);
+ */
 static void receive_buf(struct virtnet_info *vi, struct receive_queue *rq,
 			void *buf, unsigned int len, void **ctx,
 			unsigned int *xdp_xmit,
diff --git a/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c b/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
index 7369a45f7..4d0abe89c 100644
--- a/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
+++ b/drivers/net/wireless/intel/iwlwifi/mvm/mac-ctxt.c
@@ -1554,6 +1554,10 @@ void iwl_mvm_rx_beacon_notif(struct iwl_mvm *mvm,
 	}
 }
 
+/*
+ * 在以下使用iwl_mvm_rx_missed_beacons_notif():
+ *   - drivers/net/wireless/intel/iwlwifi/mvm/ops.c|355| <<global>> RX_HANDLER(MISSED_BEACONS_NOTIFICATION, iwl_mvm_rx_missed_beacons_notif, RX_HANDLER_SYNC, struct iwl_missed_beacons_notif),
+ */
 void iwl_mvm_rx_missed_beacons_notif(struct iwl_mvm *mvm,
 				     struct iwl_rx_cmd_buffer *rxb)
 {
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index f3a01b791..f65bd44be 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -2167,6 +2167,12 @@ const struct block_device_operations nvme_bdev_ops = {
 	.pr_ops		= &nvme_pr_ops,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|2212| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, NVME_CSTS_SHST_MASK,
+ *   - drivers/nvme/host/core.c|2218| <<nvme_disable_ctrl>> return nvme_wait_ready(ctrl, NVME_CSTS_RDY, 0,
+ *   - drivers/nvme/host/core.c|2284| <<nvme_enable_ctrl>> return nvme_wait_ready(ctrl, NVME_CSTS_RDY, NVME_CSTS_RDY,
+ */
 static int nvme_wait_ready(struct nvme_ctrl *ctrl, u32 mask, u32 val,
 		u32 timeout, const char *op)
 {
diff --git a/drivers/pci/hotplug/pciehp_ctrl.c b/drivers/pci/hotplug/pciehp_ctrl.c
index dcdbfcf40..cf6c26546 100644
--- a/drivers/pci/hotplug/pciehp_ctrl.c
+++ b/drivers/pci/hotplug/pciehp_ctrl.c
@@ -223,6 +223,10 @@ void pciehp_handle_disable_request(struct controller *ctrl)
 	ctrl->request_result = pciehp_disable_slot(ctrl, SAFE_REMOVAL);
 }
 
+/*
+ * called by:
+ *   - drivers/pci/hotplug/pciehp_hpc.c|753| <<pciehp_ist>> pciehp_handle_presence_or_link_change(ctrl, events);
+ */
 void pciehp_handle_presence_or_link_change(struct controller *ctrl, u32 events)
 {
 	int present, link_active;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index bd5633667..87c7cf40b 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -422,6 +422,14 @@ static void virtscsi_event_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
 };
 
+/*
+ * req_size和resp_size的例子:
+ * struct virtio_scsi_cmd_req
+ * struct virtio_scsi_cmd_resp
+ *
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|511| <<virtscsi_add_cmd>> err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int __virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -488,6 +496,16 @@ static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
  * @resp_size	: size of the response buffer
  * @kick	: whether to kick the virtqueue immediately
  */
+/*
+ * req_size和resp_size的例子:
+ * struct virtio_scsi_cmd_req
+ * struct virtio_scsi_cmd_resp
+ */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|617| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|635| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ */
 static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size,
@@ -569,6 +587,9 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	int req_size;
 	int ret;
 
+	/*
+	 * 返回cmd->sdb.table.nents
+	 */
 	BUG_ON(scsi_sg_count(sc) > shost->sg_tablesize);
 
 	/* TODO: check feature bit and fail if unsupported?  */
@@ -589,6 +610,15 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	} else
 #endif
 	{
+		/*
+		 * struct virtio_scsi_cmd *cmd:
+		 *     union {
+		 *         struct virtio_scsi_cmd_req       cmd;
+		 *         struct virtio_scsi_cmd_req_pi    cmd_pi;
+		 *         struct virtio_scsi_ctrl_tmf_req  tmf;
+		 *         struct virtio_scsi_ctrl_an_req   an;
+		 *     } req;
+		 */
 		virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
 		memcpy(cmd->req.cmd.cdb, sc->cmnd, sc->cmd_len);
 		req_size = sizeof(cmd->req.cmd);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index f2ed7167c..c13245d1a 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -35,6 +35,14 @@
 
 #include "vhost.h"
 
+/*
+ * 在以下使用experimental_zcopytx:
+ *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+ *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+ *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+ *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+ *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+ */
 static int experimental_zcopytx = 0;
 module_param(experimental_zcopytx, int, 0444);
 MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
@@ -326,11 +334,19 @@ static void vhost_net_tx_packet(struct vhost_net *net)
 	net->tx_zcopy_err = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|369| <<vhost_zerocopy_signal_used>> vhost_net_tx_err(net);
+ */
 static void vhost_net_tx_err(struct vhost_net *net)
 {
 	++net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|922| <<handle_tx_zerocopy>> && vhost_net_tx_select_zcopy(net);
+ */
 static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 {
 	/* TX flush waits for outstanding DMAs to be done.
@@ -340,12 +356,32 @@ static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 		net->tx_packets / 64 >= net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|1002| <<handle_tx>> if (vhost_sock_zcopy(sock))
+ *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+ *
+ * experimental_zcopytx似乎一直是false
+ */
 static bool vhost_sock_zcopy(struct socket *sock)
 {
+	/*
+	 * 在以下使用experimental_zcopytx:
+	 *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+	 *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+	 *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+	 *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+	 *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+	 */
 	return unlikely(experimental_zcopytx) &&
 		sock_flag(sock->sk, SOCK_ZEROCOPY);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|724| <<vhost_net_build_xdp>> int headroom = vhost_sock_xdp(sock) ? XDP_PACKET_HEADROOM : 0;
+ */
 static bool vhost_sock_xdp(struct socket *sock)
 {
 	return sock_flag(sock->sk, SOCK_XDP);
@@ -459,6 +495,22 @@ static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 	nvq->done_idx = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|587| <<vhost_net_tx_get_vq_desc>> vhost_tx_batch(net, tnvq, vhost_vq_get_backend(tvq), msghdr);
+ *   - drivers/vhost/net.c|791| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|820| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|830| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|856| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *
+ * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+ *
+ * 2. 否则, 是下面的path
+ * handle_tx_copy() or handle_tx_zerocopy()
+ * -> get_tx_bufs()
+ *    -> vhost_net_tx_get_vq_desc()
+ *       -> vhost_tx_batch()
+ */
 static void vhost_tx_batch(struct vhost_net *net,
 			   struct vhost_net_virtqueue *nvq,
 			   struct socket *sock,
@@ -476,6 +528,9 @@ static void vhost_tx_batch(struct vhost_net *net,
 
 	msghdr->msg_control = &ctl;
 	msghdr->msg_controllen = sizeof(ctl);
+	/*
+	 * tap_sendmsg()
+	 */
 	err = sock->ops->sendmsg(sock, msghdr, 0);
 	if (unlikely(err < 0)) {
 		vq_err(&nvq->vq, "Fail to batch sending packets\n");
@@ -569,6 +624,10 @@ static void vhost_net_busy_poll(struct vhost_net *net,
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|630| <<get_tx_bufs>> ret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);
+ */
 static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				    struct vhost_net_virtqueue *tnvq,
 				    unsigned int *out_num, unsigned int *in_num,
@@ -582,6 +641,15 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				  out_num, in_num, NULL, NULL);
 
 	if (r == tvq->num && tvq->busyloop_timeout) {
+		/*
+		 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+		 *
+		 * 2. 否则, 是下面的path
+		 * handle_tx_copy() or handle_tx_zerocopy()
+		 * -> get_tx_bufs()
+		 *    -> vhost_net_tx_get_vq_desc()
+		 *       -> vhost_tx_batch()
+		 */
 		/* Flush batched packets first */
 		if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
 			vhost_tx_batch(net, tnvq,
@@ -618,6 +686,11 @@ static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
 	return iov_iter_count(iter);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|793| <<handle_tx_copy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ *   - drivers/vhost/net.c|887| <<handle_tx_zerocopy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ */
 static int get_tx_bufs(struct vhost_net *net,
 		       struct vhost_net_virtqueue *nvq,
 		       struct msghdr *msg,
@@ -766,6 +839,10 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1005| <<handle_tx>> handle_tx_copy(net, sock);
+ */
 static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -787,6 +864,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 	do {
 		bool busyloop_intr = false;
 
+		/*
+		 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+		 *
+		 * 2. 否则, 是下面的path
+		 * handle_tx_copy() or handle_tx_zerocopy()
+		 * -> get_tx_bufs()
+		 *    -> vhost_net_tx_get_vq_desc()
+		 *       -> vhost_tx_batch()
+		 */
 		if (nvq->done_idx == VHOST_NET_BATCH)
 			vhost_tx_batch(net, nvq, sock, &msg);
 
@@ -817,12 +903,30 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 			if (!err) {
 				goto done;
 			} else if (unlikely(err != -ENOSPC)) {
+				/*
+				 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+				 *
+				 * 2. 否则, 是下面的path
+				 * handle_tx_copy() or handle_tx_zerocopy()
+				 * -> get_tx_bufs()
+				 *    -> vhost_net_tx_get_vq_desc()
+				 *       -> vhost_tx_batch()
+				 */
 				vhost_tx_batch(net, nvq, sock, &msg);
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
 			}
 
+			/*
+			 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+			 *
+			 * 2. 否则, 是下面的path
+			 * handle_tx_copy() or handle_tx_zerocopy()
+			 * -> get_tx_bufs()
+			 *    -> vhost_net_tx_get_vq_desc()
+			 *       -> vhost_tx_batch()
+			 */
 			/* We can't build XDP buff, go for single
 			 * packet path but let's flush batched
 			 * packets.
@@ -836,6 +940,9 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 				msg.msg_flags &= ~MSG_MORE;
 		}
 
+		/*
+		 * tap_sendmsg()
+		 */
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			if (err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS) {
@@ -853,6 +960,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		++nvq->done_idx;
 	} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
 
+	/*
+	 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+	 *
+	 * 2. 否则, 是下面的path
+	 * handle_tx_copy() or handle_tx_zerocopy()
+	 * -> get_tx_bufs()
+	 *    -> vhost_net_tx_get_vq_desc()
+	 *       -> vhost_tx_batch()
+	 */
 	vhost_tx_batch(net, nvq, sock, &msg);
 }
 
@@ -933,6 +1049,9 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 			msg.msg_flags &= ~MSG_MORE;
 		}
 
+		/*
+		 * tap_sendmsg()
+		 */
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			bool retry = err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS;
@@ -965,6 +1084,11 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1284| <<handle_tx_kick>> handle_tx(net);
+ *   - drivers/vhost/net.c|1300| <<handle_tx_net>> handle_tx(net);
+ */
 static void handle_tx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -982,6 +1106,14 @@ static void handle_tx(struct vhost_net *net)
 	vhost_disable_notify(&net->dev, vq);
 	vhost_net_disable_vq(net, vq);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+	 *   - drivers/vhost/net.c|1002| <<handle_tx>> if (vhost_sock_zcopy(sock))
+	 *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+	 *
+	 * experimental_zcopytx似乎一直是false
+	 */
 	if (vhost_sock_zcopy(sock))
 		handle_tx_zerocopy(net, sock);
 	else
@@ -1191,6 +1323,9 @@ static void handle_rx(struct vhost_net *net)
 		/* On overrun, truncate and discard */
 		if (unlikely(headcount > UIO_MAXIOV)) {
 			iov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, 1, 1);
+			/*
+			 * tap_recvmsg()
+			 */
 			err = sock->ops->recvmsg(sock, &msg,
 						 1, MSG_DONTWAIT | MSG_TRUNC);
 			pr_debug("Discarded rx packet: len %zd\n", sock_len);
@@ -1205,6 +1340,9 @@ static void handle_rx(struct vhost_net *net)
 			 */
 			iov_iter_advance(&msg.msg_iter, vhost_hlen);
 		}
+		/*
+		 * tap_recvmsg()
+		 */
 		err = sock->ops->recvmsg(sock, &msg,
 					 sock_len, MSG_DONTWAIT | MSG_TRUNC);
 		/* Userspace might have consumed the packet meanwhile:
@@ -1258,6 +1396,10 @@ static void handle_rx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用handle_tx_kick():
+ *   - drivers/vhost/net.c|1349| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;
+ */
 static void handle_tx_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1795,6 +1937,14 @@ static struct miscdevice vhost_net_misc = {
 
 static int __init vhost_net_init(void)
 {
+	/*
+	 * 在以下使用experimental_zcopytx:
+	 *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+	 *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+	 *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+	 *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+	 *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+	 */
 	if (experimental_zcopytx)
 		vhost_net_enable_zcopy(VHOST_NET_VQ_TX);
 	return misc_register(&vhost_net_misc);
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index abef0619c..4bd4df5d4 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -999,6 +999,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1098| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1453| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
diff --git a/include/kvm/arm_arch_timer.h b/include/kvm/arm_arch_timer.h
index bb3cb0058..815154c47 100644
--- a/include/kvm/arm_arch_timer.h
+++ b/include/kvm/arm_arch_timer.h
@@ -32,17 +32,55 @@ struct arch_timer_offset {
 	 * If set, pointer to one of the offsets in the kvm's offset
 	 * structure. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vm_offset:
+	 *   - arch/arm64/kvm/arch_timer.c|136| <<timer_get_offset>> if (ctxt->offset.vm_offset)
+	 *   - arch/arm64/kvm/arch_timer.c|137| <<timer_get_offset>> offset += *ctxt->offset.vm_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|190| <<timer_set_offset>> if (!ctxt->offset.vm_offset) {
+	 *   - arch/arm64/kvm/arch_timer.c|195| <<timer_set_offset>> WRITE_ONCE(*ctxt->offset.vm_offset, offset);
+	 *   - arch/arm64/kvm/arch_timer.c|1024| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1055| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1057| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|430| <<kvm_hyp_handle_cntpct>> if (ctxt->offset.vm_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|431| <<kvm_hyp_handle_cntpct>> val -= *kern_hyp_va(ctxt->offset.vm_offset);
+	 */
 	u64	*vm_offset;
 	/*
 	 * If set, pointer to one of the offsets in the vcpu's sysreg
 	 * array. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vcpu_offset:
+	 *   - arch/arm64/kvm/arch_timer.c|138| <<timer_get_offset>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/arch_timer.c|139| <<timer_get_offset>> offset += *ctxt->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|818| <<kvm_timer_vcpu_load_nested_switch>> offs->vcpu_offset = NULL;
+	 *   - arch/arm64/kvm/arch_timer.c|820| <<kvm_timer_vcpu_load_nested_switch>> offs->vcpu_offset = &__vcpu_sys_reg(vcpu, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_reset>> offs->vcpu_offset = &__vcpu_sys_reg(vcpu, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1220| <<kvm_arm_timer_read>> val = *timer->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|1274| <<kvm_arm_timer_write>> *timer->offset.vcpu_offset = val;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|432| <<kvm_hyp_handle_cntpct>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|433| <<kvm_hyp_handle_cntpct>> val -= *kern_hyp_va(ctxt->offset.vcpu_offset);
+	 */
 	u64	*vcpu_offset;
 };
 
 struct arch_timer_vm_data {
+	/*
+	 * 在以下使用arch_timer_vm_data->voffset:
+	 *   - arch/arm64/kvm/arch_timer.c|1014| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1692| <<kvm_vm_ioctl_set_counter_offset>> kvm->arch.timer_data.voffset = offset->counter_offset;
+	 *   - arch/arm64/kvm/hypercalls.c|47| <<kvm_ptp_get_time>> cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.voffset;
+	 */
 	/* Offset applied to the virtual timer/counter */
 	u64	voffset;
+	/*
+	 * 在以下使用arch_timer_vm_data->poffset:
+	 *   - arch/arm64/kvm/arch_timer.c|983| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1016| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1693| <<kvm_vm_ioctl_set_counter_offset>> kvm->arch.timer_data.poffset = offset->counter_offset;
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|51| <<__timer_enable_traps>> !kern_hyp_va(vcpu->kvm)->arch.timer_data.poffset)
+	 *   - arch/arm64/kvm/hypercalls.c|50| <<kvm_ptp_get_time>> cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.poffset;
+	 */
 	/* Offset applied to the physical timer/counter */
 	u64	poffset;
 
@@ -124,6 +162,26 @@ void kvm_timer_init_vhe(void);
 #define vcpu_hvtimer(v)	(&(v)->arch.timer_cpu.timers[TIMER_HVTIMER])
 #define vcpu_hptimer(v)	(&(v)->arch.timer_cpu.timers[TIMER_HPTIMER])
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|83| <<timer_get_ctl>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|102| <<timer_get_cval>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|148| <<timer_set_ctl>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|170| <<timer_set_cval>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|191| <<timer_set_offset>> WARN(offset, "timer %ld\n", arch_timer_ctx_index(ctxt));
+ *   - arch/arm64/kvm/arch_timer.c|430| <<kvm_timer_should_fire>> index = arch_timer_ctx_index(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|552| <<timer_save_state>> enum kvm_arch_timers index = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|666| <<timer_restore_state>> enum kvm_arch_timers index = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|215| <<__field>> __entry->direct_vtimer = arch_timer_ctx_index(map->direct_vtimer);
+ *   - arch/arm64/kvm/trace_arm.h|217| <<__field>> (map->direct_ptimer) ? arch_timer_ctx_index(map->direct_ptimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|219| <<__field>> (map->emul_vtimer) ? arch_timer_ctx_index(map->emul_vtimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|221| <<__field>> (map->emul_ptimer) ? arch_timer_ctx_index(map->emul_ptimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|245| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|267| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|285| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|301| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - include/kvm/arm_arch_timer.h|156| <<timer_irq>> #define timer_irq(ctx) (timer_vm_data(ctx)->ppi[arch_timer_ctx_index(ctx)])
+ */
 #define arch_timer_ctx_index(ctx)	((ctx) - vcpu_timer((ctx)->vcpu)->timers)
 
 #define timer_vm_data(ctx)		(&(ctx)->vcpu->kvm->arch.timer_data)
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 5b27f94d4..7add08dd0 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -278,6 +278,20 @@ struct vgic_dist {
 	struct list_head	lpi_list_head;
 	int			lpi_list_count;
 
+	/*
+	 * 在以下使用vgic_dist->lpi_translation_cache:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|57| <<kvm_vgic_early_init>> INIT_LIST_HEAD(&dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|554| <<__vgic_its_check_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|570| <<__vgic_its_check_cache>> if (!list_is_first(&cte->entry, &dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|571| <<__vgic_its_check_cache>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|608| <<vgic_its_cache_translation>> if (unlikely(list_empty(&dist->lpi_translation_cache)))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|621| <<vgic_its_cache_translation>> cte = list_last_entry(&dist->lpi_translation_cache,
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|640| <<vgic_its_cache_translation>> list_move(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|654| <<vgic_its_invalidate_cache>> list_for_each_entry(cte, &dist->lpi_translation_cache, entry) {
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1929| <<vgic_lpi_translation_cache_init>> if (!list_empty(&dist->lpi_translation_cache))
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1943| <<vgic_lpi_translation_cache_init>> list_add(&cte->entry, &dist->lpi_translation_cache);
+	 *   - arch/arm64/kvm/vgic/vgic-its.c|1955| <<vgic_lpi_translation_cache_destroy>> &dist->lpi_translation_cache, entry) {
+	 */
 	/* LPI translation cache */
 	struct list_head	lpi_translation_cache;
 
@@ -309,6 +323,16 @@ struct vgic_v3_cpu_if {
 	u32		vgic_sre;	/* Restored only, change ignored */
 	u32		vgic_ap0r[4];
 	u32		vgic_ap1r[4];
+	/*
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	u64		vgic_lr[VGIC_V3_MAX_LRS];
 
 	/*
@@ -333,6 +357,23 @@ struct vgic_cpu {
 
 	raw_spinlock_t ap_list_lock;	/* Protects the ap_list */
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	/*
 	 * List of IRQs that this VCPU should consider because they are either
 	 * Active or Pending (hence the name; AP list), or because they recently
diff --git a/include/linux/irqchip.h b/include/linux/irqchip.h
index d5e6024cb..5acd01df2 100644
--- a/include/linux/irqchip.h
+++ b/include/linux/irqchip.h
@@ -74,6 +74,16 @@ builtin_platform_driver(drv_name##_driver)
  * @data: data to be checked by the validate function.
  * @fn: initialization function
  */
+/*
+ * called by:
+ *   - drivers/irqchip/irq-gic-v3.c|2687| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V3, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2690| <<global>> IRQCHIP_ACPI_DECLARE(gic_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_V4, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic-v3.c|2693| <<global>> IRQCHIP_ACPI_DECLARE(gic_v3_or_v4, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, acpi_validate_gic_table, ACPI_MADT_GIC_VERSION_NONE, gic_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1703| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_V2, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-gic.c|1706| <<global>> IRQCHIP_ACPI_DECLARE(gic_v2_maybe, ACPI_MADT_TYPE_GENERIC_DISTRIBUTOR, gic_validate_dist, ACPI_MADT_GIC_VERSION_NONE, gic_v2_acpi_init);
+ *   - drivers/irqchip/irq-loongarch-cpu.c|171| <<global>> IRQCHIP_ACPI_DECLARE(cpuintc_v1, ACPI_MADT_TYPE_CORE_PIC, NULL, ACPI_MADT_CORE_PIC_VERSION_V1, cpuintc_acpi_init);
+ *   - drivers/irqchip/irq-riscv-intc.c|194| <<global>> IRQCHIP_ACPI_DECLARE(riscv_intc, ACPI_MADT_TYPE_RINTC, NULL, ACPI_MADT_RINTC_VERSION_V1, riscv_intc_acpi_init);
+ */
 #define IRQCHIP_ACPI_DECLARE(name, subtable, validate, data, fn)	\
 	ACPI_DECLARE_SUBTABLE_PROBE_ENTRY(irqchip, name,		\
 					  ACPI_SIG_MADT, subtable,	\
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 9d3ac7720..4a0d4a24e 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -302,6 +302,10 @@ static inline bool kvm_vcpu_mapped(struct kvm_host_map *map)
 	return !!map->hva;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3552| <<kvm_vcpu_halt>> } while (kvm_vcpu_can_poll(cur, stop));
+ */
 static inline bool kvm_vcpu_can_poll(ktime_t cur, ktime_t stop)
 {
 	return single_task_running() && !need_resched() && ktime_before(cur, stop);
@@ -979,6 +983,12 @@ static inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)
 	return __kvm_memslots(kvm, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|332| <<check_mmio_spte>> gen = kvm_vcpu_memslots(vcpu)->generation;
+ *   - arch/x86/kvm/mmu/spte.c|73| <<make_mmio_spte>> u64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;
+ *   - virt/kvm/kvm_main.c|2327| <<kvm_vcpu_gfn_to_memslot>> struct kvm_memslots *slots = kvm_vcpu_memslots(vcpu);
+ */
 static inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)
 {
 	int as_id = kvm_arch_vcpu_memslots_id(vcpu);
diff --git a/kernel/cpu.c b/kernel/cpu.c
index 88a7ede32..0efa83c20 100644
--- a/kernel/cpu.c
+++ b/kernel/cpu.c
@@ -306,6 +306,11 @@ static inline void cpuhp_ap_update_sync_state(enum cpuhp_sync_state state)
 
 void __weak arch_cpuhp_sync_state_poll(void) { cpu_relax(); }
 
+/*
+ * called by:
+ *   - kernel/cpu.c|372| <<cpuhp_bp_sync_dead>> if (cpuhp_wait_for_sync_state(cpu, SYNC_STATE_DEAD, SYNC_STATE_DEAD)) {
+ *   - kernel/cpu.c|444| <<cpuhp_bp_sync_alive>> if (!cpuhp_wait_for_sync_state(cpu, SYNC_STATE_ALIVE, SYNC_STATE_SHOULD_ONLINE)) {
+ */
 static bool cpuhp_wait_for_sync_state(unsigned int cpu, enum cpuhp_sync_state state,
 				      enum cpuhp_sync_state next_state)
 {
@@ -389,6 +394,11 @@ static inline void cpuhp_bp_sync_dead(unsigned int cpu) { }
  * Updates the AP synchronization state to SYNC_STATE_ALIVE and waits
  * for the BP to release it.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/smpboot.c|287| <<start_secondary>> cpuhp_ap_sync_alive();
+ *   - arch/x86/xen/smp_pv.c|64| <<cpu_bringup>> cpuhp_ap_sync_alive();
+ */
 void cpuhp_ap_sync_alive(void)
 {
 	atomic_t *st = this_cpu_ptr(&cpuhp_state.ap_sync_state);
@@ -434,6 +444,11 @@ void __weak arch_cpuhp_cleanup_kick_cpu(unsigned int cpu) { }
  * Early CPU bringup synchronization point. Cannot use cpuhp_state::done_up
  * because the AP cannot issue complete() so early in the bringup.
  */
+/*
+ * called by:
+ *   - kernel/cpu.c|800| <<cpuhp_bringup_ap>> ret = cpuhp_bp_sync_alive(cpu);
+ *   - kernel/cpu.c|844| <<bringup_cpu>> ret = cpuhp_bp_sync_alive(cpu);
+ */
 static int cpuhp_bp_sync_alive(unsigned int cpu)
 {
 	int ret = 0;
@@ -777,6 +792,11 @@ static int bringup_wait_for_ap_online(unsigned int cpu)
 }
 
 #ifdef CONFIG_HOTPLUG_SPLIT_STARTUP
+/*
+ * 2105         [CPUHP_BP_KICK_AP] = {
+ * 2106                 .name                   = "cpu:kick_ap",2107                 .startup.single         = cpuhp_kick_ap_alive,
+ * 2108         },
+ */
 static int cpuhp_kick_ap_alive(unsigned int cpu)
 {
 	if (!cpuhp_can_boot_ap(cpu))
@@ -785,6 +805,14 @@ static int cpuhp_kick_ap_alive(unsigned int cpu)
 	return arch_cpuhp_kick_ap_alive(cpu, idle_thread_get(cpu));
 }
 
+/*
+ * 2106         [CPUHP_BRINGUP_CPU] = {
+ * 2107                 .name                   = "cpu:bringup",
+ * 2108                 .startup.single         = cpuhp_bringup_ap,
+ * 2109                 .teardown.single        = finish_cpu,
+ * 2110                 .cant_stop              = true,
+ * 2111         },
+ */
 static int cpuhp_bringup_ap(unsigned int cpu)
 {
 	struct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);
diff --git a/kernel/events/core.c b/kernel/events/core.c
index 78ae7b6f9..c224e3ac2 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -12736,6 +12736,19 @@ SYSCALL_DEFINE5(perf_event_open,
  * @overflow_handler: callback to trigger when we hit the event
  * @context: context data could be used in overflow_handler callback
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|629| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/riscv/kvm/vcpu_pmu.c|250| <<kvm_pmu_create_perf_event>> event = perf_event_create_kernel_counter(attr, -1, current, NULL, pmc);
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|954| <<measure_residency_fn>> miss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,
+ *   - arch/x86/kernel/cpu/resctrl/pseudo_lock.c|959| <<measure_residency_fn>> hit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,
+ *   - arch/x86/kvm/pmu.c|346| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current,
+ *   - arch/x86/kvm/vmx/pmu_intel.c|263| <<intel_pmu_create_guest_lbr_event>> event = perf_event_create_kernel_counter(&attr, -1,
+ *   - kernel/events/hw_breakpoint.c|774| <<register_user_hw_breakpoint>> return perf_event_create_kernel_counter(attr, -1, tsk, triggered,
+ *   - kernel/events/hw_breakpoint.c|884| <<register_wide_hw_breakpoint>> bp = perf_event_create_kernel_counter(attr, cpu, NULL,
+ *   - kernel/events/hw_breakpoint_test.c|42| <<register_test_bp>> return perf_event_create_kernel_counter(&attr, cpu, tsk, NULL, NULL);
+ *   - kernel/watchdog_perf.c|123| <<hardlockup_detector_event_create>> evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+ */
 struct perf_event *
 perf_event_create_kernel_counter(struct perf_event_attr *attr, int cpu,
 				 struct task_struct *task,
diff --git a/kernel/irq/irqdomain.c b/kernel/irq/irqdomain.c
index 0bdef4fe9..91c726206 100644
--- a/kernel/irq/irqdomain.c
+++ b/kernel/irq/irqdomain.c
@@ -20,6 +20,12 @@
 #include <linux/smp.h>
 #include <linux/fs.h>
 
+/*
+ * 在以下使用irq_domain_list:
+ *   - kernel/irq/irqdomain.c|236| <<__irq_domain_publish>> list_add(&domain->link, &irq_domain_list);
+ *   - kernel/irq/irqdomain.c|450| <<irq_find_matching_fwspec>> list_for_each_entry(h, &irq_domain_list, link) {
+ *   - kernel/irq/irqdomain.c|2017| <<irq_domain_debugfs_init>> list_for_each_entry(d, &irq_domain_list, link)
+ */
 static LIST_HEAD(irq_domain_list);
 static DEFINE_MUTEX(irq_domain_mutex);
 
@@ -229,6 +235,11 @@ static struct irq_domain *__irq_domain_create(struct fwnode_handle *fwnode,
 	return domain;
 }
 
+/*
+ * called by:
+ *   - kernel/irq/irqdomain.c|265| <<__irq_domain_add>> __irq_domain_publish(domain);
+ *   - kernel/irq/irqdomain.c|1181| <<irq_domain_create_hierarchy>> __irq_domain_publish(domain);
+ */
 static void __irq_domain_publish(struct irq_domain *domain)
 {
 	mutex_lock(&irq_domain_mutex);
@@ -252,6 +263,17 @@ static void __irq_domain_publish(struct irq_domain *domain)
  * Allocates and initializes an irq_domain structure.
  * Returns pointer to IRQ domain, or NULL on failure.
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virt-pci.c|1017| <<um_pci_init>> um_pci_inner_domain = __irq_domain_add(um_pci_fwnode, MAX_MSI_VECTORS,
+ *   - include/linux/irqdomain.h|353| <<irq_domain_add_linear>> return __irq_domain_add(of_node_to_fwnode(of_node), size, size, 0, ops, host_data);
+ *   - include/linux/irqdomain.h|362| <<irq_domain_add_nomap>> return __irq_domain_add(of_node_to_fwnode(of_node), 0, max_irq, max_irq, ops, host_data);
+ *   - include/linux/irqdomain.h|372| <<irq_domain_add_tree>> return __irq_domain_add(of_node_to_fwnode(of_node), 0, ~0, 0, ops, host_data);
+ *   - include/linux/irqdomain.h|380| <<irq_domain_create_linear>> return __irq_domain_add(fwnode, size, size, 0, ops, host_data);
+ *   - include/linux/irqdomain.h|387| <<irq_domain_create_tree>> return __irq_domain_add(fwnode, 0, ~0, 0, ops, host_data);
+ *   - kernel/irq/irqdomain.c|364| <<irq_domain_create_simple>> domain = __irq_domain_add(fwnode, size, size, 0, ops, host_data);
+ *   - kernel/irq/irqdomain.c|420| <<irq_domain_create_legacy>> domain = __irq_domain_add(fwnode, first_hwirq + size, first_hwirq + size, 0, ops, host_data);
+ */
 struct irq_domain *__irq_domain_add(struct fwnode_handle *fwnode, unsigned int size,
 				    irq_hw_number_t hwirq_max, int direct_max,
 				    const struct irq_domain_ops *ops,
@@ -788,6 +810,23 @@ void of_phandle_args_to_fwspec(struct device_node *np, const u32 *args,
 }
 EXPORT_SYMBOL_GPL(of_phandle_args_to_fwspec);
 
+/*
+ * called by:
+ *   - drivers/acpi/irq.c|71| <<acpi_register_gsi>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/acpi/irq.c|286| <<acpi_irq_get>> rc = irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/gpio/gpio-uniphier.c|177| <<uniphier_gpio_to_irq>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/gpio/gpio-xgene-sb.c|126| <<xgene_gpio_sb_to_irq>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/gpio/gpiolib.c|1518| <<gpiochip_to_irq>> return irq_create_fwspec_mapping(&spec);
+ *   - drivers/irqchip/irq-apple-aic.c|1059| <<aic_of_ic_init>> vgic_info.maint_irq = irq_create_fwspec_mapping(&mi);
+ *   - drivers/irqchip/irq-bcm2836.c|258| <<bcm2836_arm_irqchip_smp_init>> mux_irq = irq_create_fwspec_mapping(&ipi_fwspec);
+ *   - drivers/irqchip/irq-gic-v3.c|2216| <<gic_populate_ppi_partitions>> irq = irq_create_fwspec_mapping(&ppi_fwspec);
+ *   - drivers/irqchip/irq-loongson-pch-lpc.c|213| <<pch_lpc_acpi_init>> parent_irq = irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/irqchip/irq-ti-sci-inta.c|250| <<ti_sci_inta_alloc_parent_irq>> parent_virq = irq_create_fwspec_mapping(&parent_fwspec);
+ *   - drivers/pinctrl/stm32/pinctrl-stm32.c|270| <<stm32_gpio_to_irq>> return irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/remoteproc/pru_rproc.c|553| <<pru_handle_intrmap>> pru->mapped_irq[i] = irq_create_fwspec_mapping(&fwspec);
+ *   - drivers/soc/xilinx/xlnx_event_manager.c|593| <<xlnx_event_init_sgi>> virq_sgi = irq_create_fwspec_mapping(&sgi_fwspec);
+ *   - kernel/irq/irqdomain.c|896| <<irq_create_of_mapping>> return irq_create_fwspec_mapping(&fwspec);
+ */
 unsigned int irq_create_fwspec_mapping(struct irq_fwspec *fwspec)
 {
 	struct irq_domain *domain;
@@ -886,6 +925,17 @@ unsigned int irq_create_fwspec_mapping(struct irq_fwspec *fwspec)
 }
 EXPORT_SYMBOL_GPL(irq_create_fwspec_mapping);
 
+/*
+ * called by:
+ *   - arch/powerpc/platforms/fsl_uli1575.c|339| <<hpcd_final_uli5288>> dev->irq = irq_create_of_mapping(&oirq);
+ *   - drivers/bcma/main.c|197| <<bcma_of_get_irq>> return irq_create_of_mapping(&out_irq);
+ *   - drivers/irqchip/irq-realtek-rtl.c|153| <<realtek_rtl_of_init>> parent_irq = irq_create_of_mapping(&oirq);
+ *   - drivers/of/irq.c|43| <<irq_of_parse_and_map>> return irq_create_of_mapping(&oirq);
+ *   - drivers/of/irq.c|446| <<of_irq_get>> rc = irq_create_of_mapping(&oirq);
+ *   - drivers/pci/of.c|545| <<of_irq_parse_and_map_pci>> return irq_create_of_mapping(&oirq);
+ *   - drivers/soc/ti/knav_qmss_queue.c|1242| <<knav_setup_queue_range>> range->irqs[i].irq = irq_create_of_mapping(&oirq);
+ *   - drivers/staging/board/board.c|105| <<gic_fixup_resource>> virq = irq_create_of_mapping(&irq_data);
+ */
 unsigned int irq_create_of_mapping(struct of_phandle_args *irq_data)
 {
 	struct irq_fwspec fwspec;
@@ -1453,6 +1503,13 @@ static void irq_domain_free_irqs_hierarchy(struct irq_domain *domain,
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/irq/irqdomain.c|1492| <<irq_domain_alloc_irqs_locked>> ret = irq_domain_alloc_irqs_hierarchy(domain, virq, nr_irqs, arg);
+ *   - kernel/irq/irqdomain.c|1645| <<irq_domain_push_irq>> rv = irq_domain_alloc_irqs_hierarchy(domain, virq, 1, arg);
+ *   - kernel/irq/irqdomain.c|1772| <<irq_domain_alloc_irqs_parent>> return irq_domain_alloc_irqs_hierarchy(domain->parent, irq_base,
+ *   - kernel/irq/msi.c|1102| <<msi_domain_populate_irqs>> ret = irq_domain_alloc_irqs_hierarchy(domain, virq, 1, arg);
+ */
 int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain,
 				    unsigned int irq_base,
 				    unsigned int nr_irqs, void *arg)
@@ -1462,9 +1519,17 @@ int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain,
 		return -ENOSYS;
 	}
 
+	/*
+	 * 比如gic_irq_domain_alloc()
+	 */
 	return domain->ops->alloc(domain, irq_base, nr_irqs, arg);
 }
 
+/*
+ * called by:
+ *   - kernel/irq/irqdomain.c|861| <<irq_create_fwspec_mapping>> virq = irq_domain_alloc_irqs_locked(domain, -1, 1, NUMA_NO_NODE, fwspec, false, NULL);
+ *   - kernel/irq/irqdomain.c|1549| <<__irq_domain_alloc_irqs>> ret = irq_domain_alloc_irqs_locked(domain, irq_base, nr_irqs, node, arg, realloc, affinity);
+ */
 static int irq_domain_alloc_irqs_locked(struct irq_domain *domain, int irq_base,
 					unsigned int nr_irqs, int node, void *arg,
 					bool realloc, const struct irq_affinity_desc *affinity)
@@ -1533,6 +1598,14 @@ static int irq_domain_alloc_irqs_locked(struct irq_domain *domain, int irq_base,
  * resources. In this way, it's easier to rollback when failing to
  * allocate resources.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apic/io_apic.c|974| <<alloc_irq_from_domain>> return __irq_domain_alloc_irqs(domain, irq, 1,
+ *   - arch/x86/kernel/apic/io_apic.c|1010| <<alloc_isa_irq_from_domain>> irq = __irq_domain_alloc_irqs(domain, irq, 1, node, info, true,
+ *   - include/linux/irqdomain.h|507| <<irq_domain_alloc_irqs>> return __irq_domain_alloc_irqs(domain, -1, nr_irqs, node, arg, false,
+ *   - kernel/irq/ipi.c|84| <<irq_reserve_ipi>> virq = __irq_domain_alloc_irqs(domain, virq, nr_irqs, NUMA_NO_NODE,
+ *   - kernel/irq/msi.c|1301| <<__msi_domain_alloc_irqs>> virq = __irq_domain_alloc_irqs(domain, -1, desc->nvec_used,
+ */
 int __irq_domain_alloc_irqs(struct irq_domain *domain, int irq_base,
 			    unsigned int nr_irqs, int node, void *arg,
 			    bool realloc, const struct irq_affinity_desc *affinity)
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index d2742af0f..bf03a1564 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -2862,6 +2862,28 @@ EXPORT_SYMBOL_GPL(irq_get_irqchip_state);
  *	This function should be called with migration disabled if the
  *	interrupt controller has per-cpu registers.
  */
+/*
+ * called by:
+ *   - arch/arm64/kernel/machine_kexec.c|241| <<machine_kexec_mask_interrupts>> ret = irq_set_irqchip_state(i, IRQCHIP_STATE_ACTIVE, false);
+ *   - arch/arm64/kvm/arch_timer.c|1070| <<set_timer_irq_phys_active>> r = irq_set_irqchip_state(ctx->host_timer_irq, IRQCHIP_STATE_ACTIVE, active);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|868| <<vgic_its_trigger_msi>> return irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, true);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1463| <<vgic_its_cmd_handle_clear>> return irq_set_irqchip_state(ite->irq->host_irq, IRQCHIP_STATE_PENDING, false);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1135| <<vgic_v3_dispatch_sgi>> err = irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, true);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|326| <<vgic_mmio_write_spending>> err = irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, true);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|419| <<vgic_mmio_write_cpending>> err = irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, false);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|146| <<vgic_v4_enable_vsgis>> ret = irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, irq->pending_latch);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|380| <<vgic_v4_load>> err = irq_set_irqchip_state(vpe->irq, IRQCHIP_STATE_PENDING, false);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|466| <<kvm_vgic_v4_set_forwarding>> ret = irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, irq->pending_latch);
+ *   - arch/arm64/kvm/vgic/vgic.c|266| <<vgic_irq_set_phys_pending>> WARN_ON(irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_PENDING, pending));
+ *   - arch/arm64/kvm/vgic/vgic.c|291| <<vgic_irq_set_phys_active>> WARN_ON(irq_set_irqchip_state(irq->host_irq, IRQCHIP_STATE_ACTIVE, active));
+ *   - arch/riscv/kernel/machine_kexec.c|160| <<machine_kexec_mask_interrupts>> ret = irq_set_irqchip_state(i, IRQCHIP_STATE_ACTIVE, false);
+ *   - drivers/gpio/gpio-mockup.c|174| <<gpio_mockup_apply_pull>> ret = irq_set_irqchip_state(irq, IRQCHIP_STATE_PENDING, true);
+ *   - drivers/gpio/gpio-sim.c|92| <<gpio_sim_apply_pull>> ret = irq_set_irqchip_state(irq, IRQCHIP_STATE_PENDING, true);
+ *   - drivers/iio/dummy/iio_dummy_evgen.c|152| <<iio_evgen_poke>> ret = irq_set_irqchip_state(irq, IRQCHIP_STATE_PENDING, true);
+ *   - drivers/irqchip/irq-mvebu-icu.c|231| <<mvebu_icu_irq_domain_alloc>> err = irq_set_irqchip_state(virq, IRQCHIP_STATE_PENDING, false);
+ *   - drivers/irqchip/irq-qcom-mpm.c|282| <<qcom_mpm_handler>> irq_set_irqchip_state(d->irq, IRQCHIP_STATE_PENDING, true);
+ *   - kernel/irq/resend.c|180| <<irq_inject_interrupt>> if (!irq_set_irqchip_state(irq, IRQCHIP_STATE_PENDING, true))
+ */
 int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
 			  bool val)
 {
@@ -2892,6 +2914,19 @@ int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,
 #endif
 	} while (data);
 
+	/*
+	 * 对于vSGI, 大概是这个.
+	 *
+	 * 4441 static struct irq_chip its_sgi_irq_chip = {
+	 * 4442         .name                   = "GICv4.1-sgi",
+	 * 4443         .irq_mask               = its_sgi_mask_irq,
+	 * 4444         .irq_unmask             = its_sgi_unmask_irq,
+	 * 4445         .irq_set_affinity       = its_sgi_set_affinity,
+	 * 4446         .irq_set_irqchip_state  = its_sgi_set_irqchip_state,
+	 * 4447         .irq_get_irqchip_state  = its_sgi_get_irqchip_state,
+	 * 4448         .irq_set_vcpu_affinity  = its_sgi_set_vcpu_affinity,
+	 * 4449 };
+	 */
 	if (data)
 		err = chip->irq_set_irqchip_state(data, which, val);
 
diff --git a/kernel/smp.c b/kernel/smp.c
index 385179dae..955b9ee1a 100644
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -272,6 +272,10 @@ static bool csd_lock_wait_toolong(struct __call_single_data *csd, u64 ts0, u64 *
  * previous function call. For multi-cpu calls its even more interesting
  * as we'll have to ensure no other cpu is observing our csd.
  */
+/*
+ * called by:
+ *   - kernel/smp.c|292| <<csd_lock_wait>> __csd_lock_wait(csd);
+ */
 static void __csd_lock_wait(struct __call_single_data *csd)
 {
 	int bug_id = 0;
@@ -286,6 +290,12 @@ static void __csd_lock_wait(struct __call_single_data *csd)
 	smp_acquire__after_ctrl_dep();
 }
 
+/*
+ * called by:
+ *   - kernel/smp.c|311| <<csd_lock>> csd_lock_wait(csd);
+ *   - kernel/smp.c|630| <<smp_call_function_single>> csd_lock_wait(csd);
+ *   - kernel/smp.c|835| <<smp_call_function_many_cond>> csd_lock_wait(csd);
+ */
 static __always_inline void csd_lock_wait(struct __call_single_data *csd)
 {
 	if (static_branch_unlikely(&csdlock_debug_enabled)) {
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 88cbc1181..7308133be 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -1096,6 +1096,12 @@ static void clocksource_enqueue(struct clocksource *cs)
  * __clocksource_update_freq_hz() or __clocksource_update_freq_khz() helper
  * functions.
  */
+/*
+ * called by:
+ *   - include/linux/clocksource.h|256| <<__clocksource_update_freq_hz>> __clocksource_update_freq_scale(cs, 1, hz);
+ *   - include/linux/clocksource.h|261| <<__clocksource_update_freq_khz>> __clocksource_update_freq_scale(cs, 1000, khz);
+ *   - kernel/time/clocksource.c|1202| <<__clocksource_register_scale>> __clocksource_update_freq_scale(cs, scale, freq);
+ */
 void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq)
 {
 	u64 sec;
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 266d02809..5db9424b3 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -937,6 +937,45 @@ ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
 }
 EXPORT_SYMBOL_GPL(ktime_mono_to_any);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2302| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|545| <<intel_dp_hdcp2_read_msg>> msg_end = ktime_add_ms(ktime_get_raw(),
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|562| <<intel_dp_hdcp2_read_msg>> msg_expired = ktime_after(ktime_get_raw(), msg_end);
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|78| <<live_nop_switch>> times[0] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|103| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|114| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|158| <<live_nop_switch>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/gt/intel_rps.c|1755| <<vlv_c0_read>> ei->ktime = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3749| <<nop_virtual_engine>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3804| <<nop_virtual_engine>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/i915_pmu.c|191| <<ktime_since_raw>> return ktime_to_ns(ktime_sub(ktime_get_raw(), kt));
+ *   - drivers/gpu/drm/i915/i915_pmu.c|267| <<init_rc6>> pmu->sleep_last[i] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_pmu.c|277| <<park_rc6>> pmu->sleep_last[gt->info.id] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_utils.h|262| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/i915/i915_utils.h|267| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|593| <<live_nop_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|621| <<live_nop_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1077| <<live_empty_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1090| <<live_empty_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|312| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|317| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|851| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|856| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/ptp/ptp_clockmatrix.c|438| <<_idtcm_gettime>> idtcm->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|750| <<_idtcm_set_dpll_hw_tod>> ktime_t diff = ktime_sub(ktime_get_raw(),
+ *   - drivers/ptp/ptp_clockmatrix.c|1006| <<set_tod_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|1013| <<set_tod_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|393| <<_idt82p33_gettime>> idt82p33->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|433| <<_idt82p33_settime>> dynamic_overhead_ns = ktime_to_ns(ktime_get_raw())
+ *   - drivers/ptp/ptp_idt82p33.c|653| <<idt82p33_measure_one_byte_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|658| <<idt82p33_measure_one_byte_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|686| <<idt82p33_measure_one_byte_read_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|691| <<idt82p33_measure_one_byte_read_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|719| <<idt82p33_measure_tod_write_9_byte_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|730| <<idt82p33_measure_tod_write_9_byte_overhead>> stop = ktime_get_raw();
+ *   - include/linux/timekeeping.h|174| <<ktime_get_raw_ns>> return ktime_to_ns(ktime_get_raw());
+ */
 /**
  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
  */
diff --git a/kernel/watchdog.c b/kernel/watchdog.c
index be38276a3..ee926c9f8 100644
--- a/kernel/watchdog.c
+++ b/kernel/watchdog.c
@@ -407,6 +407,10 @@ void touch_softlockup_watchdog_sync(void)
 	__this_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);
 }
 
+/*
+ * called by:
+ *   - kernel/watchdog.c|501| <<watchdog_timer_fn>> duration = is_softlockup(touch_ts, period_ts, now);
+ */
 static int is_softlockup(unsigned long touch_ts,
 			 unsigned long period_ts,
 			 unsigned long now)
@@ -439,6 +443,10 @@ static int softlockup_fn(void *data)
 	return 0;
 }
 
+/*
+ * 在以下使用watchdog_timer_fn():
+ *   - kernel/watchdog.c|553| <<watchdog_enable>> hrtimer->function = watchdog_timer_fn;
+ */
 /* watchdog kicker functions */
 static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)
 {
diff --git a/tools/testing/selftests/kvm/include/x86_64/processor.h b/tools/testing/selftests/kvm/include/x86_64/processor.h
index aa434c8f1..54a397550 100644
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@ -957,6 +957,15 @@ static inline int __vcpu_set_cpuid(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|752| <<vcpu_init_cpuid>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|760| <<vcpu_set_cpuid_maxphyaddr>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|771| <<vcpu_clear_cpuid_entry>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|789| <<vcpu_set_or_clear_cpuid_feature>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_features.c|486| <<guest_test_msrs_access>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_features.c|657| <<guest_test_hcalls_access>> vcpu_set_cpuid(vcpu);
+ */
 static inline void vcpu_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	TEST_ASSERT(vcpu->cpuid, "Must do vcpu_init_cpuid() first");
diff --git a/tools/testing/selftests/kvm/lib/kvm_util.c b/tools/testing/selftests/kvm/lib/kvm_util.c
index 9741a7ff6..41658b795 100644
--- a/tools/testing/selftests/kvm/lib/kvm_util.c
+++ b/tools/testing/selftests/kvm/lib/kvm_util.c
@@ -1933,6 +1933,19 @@ const char *exit_reason_str(unsigned int exit_reason)
  * and their base address is returned. A TEST_ASSERT failure occurs if
  * not enough pages are available at or above paddr_min.
  */
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/aarch64/processor.c|102| <<virt_arch_pgd_alloc>> vm->pgd = vm_phy_pages_alloc(vm, nr_pages,
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1334| <<__vm_vaddr_alloc>> vm_paddr_t paddr = vm_phy_pages_alloc(vm, pages,
+ *   - tools/testing/selftests/kvm/lib/kvm_util.c|1979| <<vm_phy_page_alloc>> return vm_phy_pages_alloc(vm, 1, paddr_min, memslot);
+ *   - tools/testing/selftests/kvm/lib/riscv/processor.c|63| <<virt_arch_pgd_alloc>> vm->pgd = vm_phy_pages_alloc(vm, nr_pages,
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|23| <<virt_arch_pgd_alloc>> paddr = vm_phy_pages_alloc(vm, PAGES_PER_REGION,
+ *   - tools/testing/selftests/kvm/lib/s390x/processor.c|41| <<virt_alloc_region>> taddr = vm_phy_pages_alloc(vm, ri < 4 ? PAGES_PER_REGION : 1,
+ *   - tools/testing/selftests/kvm/memslot_perf_test.c|337| <<prepare_vm>> gpa = vm_phy_pages_alloc(data->vm, npages, guest_addr, slot);
+ *   - tools/testing/selftests/kvm/set_memory_region_test.c|128| <<spawn_vm>> gpa = vm_phy_pages_alloc(vm, 2, MEM_REGION_GPA, MEM_REGION_SLOT);
+ *   - tools/testing/selftests/kvm/x86_64/smaller_maxphyaddr_emulation_test.c|75| <<main>> gpa = vm_phy_pages_alloc(vm, MEM_REGION_SIZE / PAGE_SIZE,
+ *   - tools/testing/selftests/kvm/x86_64/smm_test.c|146| <<main>> TEST_ASSERT(vm_phy_pages_alloc(vm, SMRAM_PAGES, SMRAM_GPA, SMRAM_MEMSLOT)
+ */
 vm_paddr_t vm_phy_pages_alloc(struct kvm_vm *vm, size_t num,
 			      vm_paddr_t paddr_min, uint32_t memslot)
 {
diff --git a/tools/testing/selftests/kvm/max_guest_memory_test.c b/tools/testing/selftests/kvm/max_guest_memory_test.c
index feaf2be20..267c33099 100644
--- a/tools/testing/selftests/kvm/max_guest_memory_test.c
+++ b/tools/testing/selftests/kvm/max_guest_memory_test.c
@@ -35,8 +35,28 @@ struct vcpu_info {
 };
 
 static int nr_vcpus;
+/*
+ * 在以下使用rendezvous:
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|42| <<rendezvous_with_boss>> int orig = atomic_read(&rendezvous);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|45| <<rendezvous_with_boss>> atomic_dec_and_test(&rendezvous);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|46| <<rendezvous_with_boss>> while (atomic_read(&rendezvous) > 0)
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|49| <<rendezvous_with_boss>> atomic_inc(&rendezvous);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|50| <<rendezvous_with_boss>> while (atomic_read(&rendezvous) < 0)
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|131| <<rendezvous_with_vcpus>> rendezvoused = atomic_read(&rendezvous);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|137| <<rendezvous_with_vcpus>> rendezvoused = atomic_read(&rendezvous);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|145| <<rendezvous_with_vcpus>> atomic_set(&rendezvous, -nr_vcpus - 1);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|147| <<rendezvous_with_vcpus>> atomic_set(&rendezvous, nr_vcpus + 1);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|259| <<main>> atomic_set(&rendezvous, nr_vcpus + 1);
+ */
 static atomic_t rendezvous;
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|73| <<vcpu_worker>> rendezvous_with_boss();
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|76| <<vcpu_worker>> rendezvous_with_boss();
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|84| <<vcpu_worker>> rendezvous_with_boss();
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|87| <<vcpu_worker>> rendezvous_with_boss();
+ */
 static void rendezvous_with_boss(void)
 {
 	int orig = atomic_read(&rendezvous);
@@ -52,6 +72,11 @@ static void rendezvous_with_boss(void)
 	}
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|75| <<vcpu_worker>> run_vcpu(vcpu);
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|86| <<vcpu_worker>> run_vcpu(vcpu);
+ */
 static void run_vcpu(struct kvm_vcpu *vcpu)
 {
 	vcpu_run(vcpu);
@@ -89,6 +114,12 @@ static void *vcpu_worker(void *data)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/max_guest_memory_test.c|254| <<main>> threads = spawn_workers(vm, vcpus, start_gpa, gpa);
+ *
+ * 在main调用一次, 没有循环
+ */
 static pthread_t *spawn_workers(struct kvm_vm *vm, struct kvm_vcpu **vcpus,
 				uint64_t start_gpa, uint64_t end_gpa)
 {
@@ -230,6 +261,9 @@ int main(int argc, char *argv[])
 		((uint8_t *)mem)[i] = 0xaa;
 
 	gpa = 0;
+	/*
+	 * first_slot从1开始
+	 */
 	for (slot = first_slot; slot < max_slots; slot++) {
 		gpa = start_gpa + ((slot - first_slot) * slot_size);
 		if (gpa + slot_size > max_gpa)
diff --git a/tools/testing/selftests/kvm/steal_time.c b/tools/testing/selftests/kvm/steal_time.c
index c87f38712..3801dd83c 100644
--- a/tools/testing/selftests/kvm/steal_time.c
+++ b/tools/testing/selftests/kvm/steal_time.c
@@ -221,6 +221,12 @@ static void *do_steal_time(void *arg)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/steal_time.c|279| <<main>> run_vcpu(vcpus[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|282| <<main>> run_vcpu(vcpus[i]);
+ *   - tools/testing/selftests/kvm/steal_time.c|303| <<main>> run_vcpu(vcpus[i]);
+ */
 static void run_vcpu(struct kvm_vcpu *vcpu)
 {
 	struct ucall uc;
diff --git a/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c b/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c
index 177870436..68f5dca92 100644
--- a/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c
+++ b/tools/testing/selftests/kvm/x86_64/kvm_clock_test.c
@@ -28,6 +28,12 @@ static struct test_case test_cases[] = {
 	{ .kvmclock_base = 0, .realtime_offset = 180 * NSEC_PER_SEC },
 };
 
+/*
+ * '6'个参数!!!
+ *
+ * #define GUEST_SYNC_ARGS(stage, arg1, arg2, arg3, arg4)  \
+ *				ucall(UCALL_SYNC, 6, "hello", stage, arg1, arg2, arg3, arg4)
+ */
 #define GUEST_SYNC_CLOCK(__stage, __val)			\
 		GUEST_SYNC_ARGS(__stage, __val, 0, 0, 0)
 
@@ -61,6 +67,9 @@ static void handle_sync(struct ucall *uc, struct kvm_clock_data *start,
 	assert_flags(start);
 	assert_flags(end);
 
+	/*
+	 * __pvclock_read_cycles(pvti, rdtsc())必须是开始和结束之间!
+	 */
 	TEST_ASSERT(exp_lo <= obs && obs <= exp_hi,
 		    "unexpected kvm-clock value: %"PRIu64" expected range: [%"PRIu64", %"PRIu64"]",
 		    obs, exp_lo, exp_hi);
@@ -74,6 +83,14 @@ static void handle_abort(struct ucall *uc)
 	REPORT_GUEST_ASSERT(*uc);
 }
 
+/*
+ * struct test_case {
+ *     uint64_t kvmclock_base;
+ *     int64_t realtime_offset;
+ * };
+ *
+ * 通过ioctl(KVM_SET_CLOCK)设置VM的时间
+ */
 static void setup_clock(struct kvm_vm *vm, struct test_case *test_case)
 {
 	struct kvm_clock_data data;
@@ -110,6 +127,9 @@ static void enter_guest(struct kvm_vcpu *vcpu)
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(test_cases); i++) {
+		/*
+		 * 通过ioctl(KVM_SET_CLOCK)设置VM的时间
+		 */
 		setup_clock(vm, &test_cases[i]);
 
 		vm_ioctl(vm, KVM_GET_CLOCK, &start);
@@ -119,6 +139,15 @@ static void enter_guest(struct kvm_vcpu *vcpu)
 
 		TEST_ASSERT_KVM_EXIT_REASON(vcpu, KVM_EXIT_IO);
 
+		/*
+		 * struct ucall {
+		 *     uint64_t cmd;
+		 *     uint64_t args[UCALL_MAX_ARGS];
+		 *
+		 *     // Host virtual address of this struct.
+		 *     struct ucall *hva;
+		 * };
+		 */
 		switch (get_ucall(vcpu, &uc)) {
 		case UCALL_SYNC:
 			handle_sync(&uc, &start, &end);
diff --git a/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c b/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c
index 7f36c32fa..b825ff5d5 100644
--- a/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c
+++ b/tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c
@@ -19,9 +19,15 @@
 #include "processor.h"
 
 #define HPAGE_SLOT		10
+/*
+ * 这个数是4G
+ */
 #define HPAGE_GPA		(4UL << 30) /* 4G prevents collision w/ slot 0 */
 #define HPAGE_GVA		HPAGE_GPA /* GVA is arbitrary, so use GPA. */
 #define PAGES_PER_2MB_HUGE_PAGE 512
+/*
+ * 3 * 512
+ */
 #define HPAGE_SLOT_NPAGES	(3 * PAGES_PER_2MB_HUGE_PAGE)
 
 /*
@@ -52,6 +58,10 @@ static void guest_do_CALL(uint64_t target)
 void guest_code(void)
 {
 	uint64_t hpage_1 = HPAGE_GVA;
+	/*
+	 * hpage_2 = hpage_1 + 2M;
+	 * hpage_3 = hpage_2 + 2M;
+	 */
 	uint64_t hpage_2 = hpage_1 + (PAGE_SIZE * 512);
 	uint64_t hpage_3 = hpage_2 + (PAGE_SIZE * 512);
 
@@ -74,10 +84,27 @@ void guest_code(void)
 	GUEST_SYNC(6);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|157| <<run_test>> check_2m_page_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|165| <<run_test>> check_2m_page_count(vm, 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|173| <<run_test>> check_2m_page_count(vm, 2);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|183| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 2 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|193| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|198| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|210| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|218| <<run_test>> check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 2);
+ *
+ * 检查VM stat的"pages_2m"是不是和参数对的上
+ */
 static void check_2m_page_count(struct kvm_vm *vm, int expected_pages_2m)
 {
 	int actual_pages_2m;
 
+	/*
+	 * 似乎这是唯一一处修改的地方:
+	 *   - arch/x86/kvm/mmu.h|290| <<kvm_update_page_stats>> atomic64_add(count, &kvm->stat.pages[level - 1]);
+	 */
 	actual_pages_2m = vm_get_stat(vm, "pages_2m");
 
 	TEST_ASSERT(actual_pages_2m == expected_pages_2m,
@@ -85,10 +112,32 @@ static void check_2m_page_count(struct kvm_vm *vm, int expected_pages_2m)
 		    expected_pages_2m, actual_pages_2m);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|158| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|166| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|174| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|184| <<run_test>> check_split_count(vm, disable_nx_huge_pages ? 0 : 1);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|194| <<run_test>> check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|199| <<run_test>> check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|211| <<run_test>> check_split_count(vm, 0);
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|219| <<run_test>> check_split_count(vm, 0);
+ *
+ * 检查VM Stat的"nx_lpage_splits"是否和参数一样
+ */
 static void check_split_count(struct kvm_vm *vm, int expected_splits)
 {
 	int actual_splits;
 
+	/*
+	 * 在以下使用kvm_vm_stat->nx_lpage_splits:
+	 *   - arch/x86/include/asm/kvm_host.h|1646| <<global>> u64 nx_lpage_splits;
+	 *   - arch/x86/kvm/x86.c|266| <<global>> STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+	 *   - arch/x86/kvm/mmu/mmu.c|896| <<track_possible_nx_huge_page>> ++kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|932| <<untrack_possible_nx_huge_page>> --kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7179| <<kvm_recover_nx_huge_pages>> unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7199| <<kvm_recover_nx_huge_pages>> to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
+	 */
 	actual_splits = vm_get_stat(vm, "nx_lpage_splits");
 
 	TEST_ASSERT(actual_splits == expected_splits,
@@ -96,6 +145,12 @@ static void check_split_count(struct kvm_vm *vm, int expected_splits)
 		    expected_splits, actual_splits);
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|202| <<run_test>> wait_for_reclaim(reclaim_period_ms);
+ *
+ * 核心思想是nanosleep() reclaim_period_ms
+ */
 static void wait_for_reclaim(int reclaim_period_ms)
 {
 	long reclaim_wait_ms;
@@ -107,6 +162,14 @@ static void wait_for_reclaim(int reclaim_period_ms)
 	nanosleep(&ts, NULL);
 }
 
+/*
+ * 读 1
+ * 读 2
+ * call 1
+ * call 3
+ * 读 1
+ * 读 3
+ */
 void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	      bool reboot_permissions)
 {
@@ -116,6 +179,9 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	void *hva;
 	int r;
 
+	/*
+	 * 就1个vcpu???
+	 */
 	vm = vm_create(1);
 
 	if (disable_nx_huge_pages) {
@@ -135,6 +201,11 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 				    HPAGE_GPA, HPAGE_SLOT,
 				    HPAGE_SLOT_NPAGES, 0);
 
+	/*
+	 * HPAGE_SLOT_NPAGES是3 * 512
+	 *
+	 * 3个2M的hugepage????
+	 */
 	nr_bytes = HPAGE_SLOT_NPAGES * vm->page_size;
 
 	/*
@@ -161,6 +232,8 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	 * The guest code will first read from the first hugepage, resulting
 	 * in a huge page mapping being created.
 	 */
+	/* 读 1
+	 */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, 1);
 	check_split_count(vm, 0);
@@ -169,6 +242,9 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	 * Then the guest code will read from the second hugepage, resulting
 	 * in another huge page mapping being created.
 	 */
+	/*
+	 * 读 2
+	 */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, 2);
 	check_split_count(vm, 0);
@@ -179,6 +255,9 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	 *
 	 * If NX huge pages are disabled, this should have no effect.
 	 */
+	/*
+	 * call 1
+	 */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, disable_nx_huge_pages ? 2 : 1);
 	check_split_count(vm, disable_nx_huge_pages ? 0 : 1);
@@ -189,15 +268,26 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	 *
 	 * If NX huge pages are disabled, it should be mapped at 2M.
 	 */
+	/*
+	 * call 3
+	 */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
 	check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
 
 	/* Reading from the first huge page again should have no effect. */
+	/*
+	 * 读 1
+	 */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 1);
 	check_split_count(vm, disable_nx_huge_pages ? 0 : 2);
 
+	/*
+	 * 核心思想是nanosleep() reclaim_period_ms
+	 *
+	 * 等待的这些时间, KVM把不是hugepage的都zap了!
+	 */
 	/* Give recovery thread time to run. */
 	wait_for_reclaim(reclaim_period_ms);
 
@@ -214,6 +304,9 @@ void run_test(int reclaim_period_ms, bool disable_nx_huge_pages,
 	 * The 4k mapping on hpage 3 should have been removed, so check that
 	 * reading from it causes a huge page mapping to be installed.
 	 */
+	/*
+	 * 读 3
+	 */
 	vcpu_run(vcpu);
 	check_2m_page_count(vm, disable_nx_huge_pages ? 3 : 2);
 	check_split_count(vm, 0);
@@ -256,6 +349,15 @@ int main(int argc, char **argv)
 		}
 	}
 
+	/*
+	 * 在以下使用KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:
+	 *   - include/uapi/linux/kvm.h|1185| <<global>> #define KVM_CAP_VM_DISABLE_NX_HUGE_PAGES 220
+	 *   - tools/include/uapi/linux/kvm.h|1185| <<global>> #define KVM_CAP_VM_DISABLE_NX_HUGE_PAGES 220
+	 *   - arch/x86/kvm/x86.c|5122| <<kvm_vm_ioctl_check_extension>> case KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:
+	 *   - arch/x86/kvm/x86.c|7100| <<kvm_vm_ioctl_enable_cap>> case KVM_CAP_VM_DISABLE_NX_HUGE_PAGES:
+	 *   - tools/testing/selftests/kvm/include/kvm_util_base.h|902| <<__vm_disable_nx_huge_pages>> return __vm_enable_cap(vm, KVM_CAP_VM_DISABLE_NX_HUGE_PAGES, 0);
+	 *   - tools/testing/selftests/kvm/x86_64/nx_huge_pages_test.c|259| <<main>> TEST_REQUIRE(kvm_has_cap(KVM_CAP_VM_DISABLE_NX_HUGE_PAGES));
+	 */
 	TEST_REQUIRE(kvm_has_cap(KVM_CAP_VM_DISABLE_NX_HUGE_PAGES));
 
 	__TEST_REQUIRE(token == MAGIC_TOKEN,
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 89912a17f..4c7d3c4bd 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -776,6 +776,24 @@ ioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)
 	return _val == p->datamatch;
 }
 
+/*
+ * arm旧的例子
+ *
+ *  => ioeventfd_write
+ *  => __kvm_io_bus_write
+ *  => kvm_io_bus_write
+ *  => io_mem_abort
+ *  => kvm_handle_guest_abort
+ *  => handle_exit
+ *  => kvm_arch_vcpu_ioctl_run
+ *  => kvm_vcpu_ioctl
+ *  => do_vfs_ioctl
+ *  => ksys_ioctl
+ *  => __arm64_sys_ioctl
+ *  => el0_svc_common
+ *  => el0_svc_handler
+ *  => el0_svc
+ */
 /* MMIO/PIO writes trigger an event if the addr/val match */
 static int
 ioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 1e567d1f6..3b24e782f 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -85,6 +85,9 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 
 	while (i--) {
 		int r;
+		/*
+		 * kvm_set_msi()
+		 */
 		r = irq_set[i].set(&irq_set[i], kvm, irq_source_id, level,
 				   line_status);
 		if (r < 0)
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 5bbb5612b..4661500fc 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -1920,6 +1920,15 @@ static bool kvm_check_memslot_overlap(struct kvm_memslots *slots, int id,
  *
  * Must be called holding kvm->slots_lock for write.
  */
+/*
+ * struct kvm_userspace_memory_region {
+ *     __u32 slot;
+ *     __u32 flags;
+ *     __u64 guest_phys_addr;
+ *     __u64 memory_size; / bytes
+ *     __u64 userspace_addr; // start of the userspace allocated memory
+ * };
+ */
 int __kvm_set_memory_region(struct kvm *kvm,
 			    const struct kvm_userspace_memory_region *mem)
 {
@@ -1957,6 +1966,13 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	if ((mem->memory_size >> PAGE_SHIFT) > KVM_MEM_MAX_NR_PAGES)
 		return -EINVAL;
 
+	/*
+	 * struct kvm *kvm:
+	 * -> struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
+	 *    -> DECLARE_HASHTABLE(id_hash, 7);
+	 *
+	 * struct kvm_memslots *slots;
+	 */
 	slots = __kvm_memslots(kvm, as_id);
 
 	/*
@@ -1965,6 +1981,9 @@ int __kvm_set_memory_region(struct kvm *kvm,
 	 */
 	old = id_to_memslot(slots, id);
 
+	/*
+	 * 这里是要删除?
+	 */
 	if (!mem->memory_size) {
 		if (!old || !old->npages)
 			return -EINVAL;
@@ -3310,6 +3329,24 @@ int kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len)
 }
 EXPORT_SYMBOL_GPL(kvm_clear_guest);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmu.c|1569| <<user_mem_abort>> mark_page_dirty_in_slot(kvm, memslot, gfn); 
+ *   - arch/s390/kvm/gaccess.c|1041| <<access_guest_page_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/s390/kvm/gaccess.c|1263| <<cmpxchg_guest_abs_with_key>> mark_page_dirty_in_slot(kvm, slot, gfn);
+ *   - arch/x86/kvm/mmu/mmu.c|3416| <<fast_pf_fix_direct_spte>> mark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);
+ *   - arch/x86/kvm/mmu/spte.c|246| <<make_spte>> mark_page_dirty_in_slot(vcpu->kvm, slot, gfn);
+ *   - arch/x86/kvm/x86.c|3560| <<kvm_setup_guest_pvclock>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/x86.c|4179| <<record_steal_time>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/x86.c|5472| <<kvm_steal_time_set_preempted>> mark_page_dirty_in_slot(vcpu->kvm, ghc->memslot, gpa_to_gfn(ghc->gpa));
+ *   - arch/x86/kvm/xen.c|450| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc1->memslot, gpc1->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|452| <<kvm_xen_update_runstate_guest>> mark_page_dirty_in_slot(v->kvm, gpc2->memslot, gpc2->gpa >> PAGE_SHIFT);
+ *   - arch/x86/kvm/xen.c|564| <<kvm_xen_inject_pending_events>> mark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3105| <<__kvm_write_guest_page>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3243| <<kvm_write_guest_offset_cached>> mark_page_dirty_in_slot(kvm, ghc->memslot, gpa >> PAGE_SHIFT);
+ *   - virt/kvm/kvm_main.c|3343| <<mark_page_dirty>> mark_page_dirty_in_slot(kvm, memslot, gfn);
+ *   - virt/kvm/kvm_main.c|3352| <<kvm_vcpu_mark_page_dirty>> mark_page_dirty_in_slot(vcpu->kvm, memslot, gfn);
+ */
 void mark_page_dirty_in_slot(struct kvm *kvm,
 			     const struct kvm_memory_slot *memslot,
 		 	     gfn_t gfn)
@@ -3353,6 +3390,15 @@ void kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_mark_page_dirty);
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|922| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/mips/kvm/mips.c|431| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1859| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|1173| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4992| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ *   - arch/x86/kvm/x86.c|11710| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_activate(vcpu);
+ */
 void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3367,6 +3413,15 @@ void kvm_sigset_activate(struct kvm_vcpu *vcpu)
 	sigprocmask(SIG_SETMASK, &vcpu->sigset, &current->real_blocked);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1114| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/mips/kvm/mips.c|476| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|1866| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/riscv/kvm/vcpu.c|1283| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|5031| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ *   - arch/x86/kvm/x86.c|12032| <<kvm_arch_vcpu_ioctl_run>> kvm_sigset_deactivate(vcpu);
+ */
 void kvm_sigset_deactivate(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->sigset_active)
@@ -3414,6 +3469,11 @@ static void shrink_halt_poll_ns(struct kvm_vcpu *vcpu)
 	trace_kvm_halt_poll_ns_shrink(vcpu->vcpu_id, val, old);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|3466| <<kvm_vcpu_block>> if (kvm_vcpu_check_block(vcpu) < 0)
+ *   - virt/kvm/kvm_main.c|3548| <<kvm_vcpu_halt>> if (kvm_vcpu_check_block(vcpu) < 0)
+ */
 static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
 {
 	int ret = -EINTR;
@@ -3439,6 +3499,12 @@ static int kvm_vcpu_check_block(struct kvm_vcpu *vcpu)
  * pending.  This is mostly used when halting a vCPU, but may also be used
  * directly for other vCPU non-runnable states, e.g. x86's Wait-For-SIPI.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11508| <<vcpu_block>> kvm_vcpu_block(vcpu);
+ *   - arch/x86/kvm/x86.c|11727| <<kvm_arch_vcpu_ioctl_run>> kvm_vcpu_block(vcpu);
+ *   - virt/kvm/kvm_main.c|3555| <<kvm_vcpu_halt>> waited = kvm_vcpu_block(vcpu);
+ */
 bool kvm_vcpu_block(struct kvm_vcpu *vcpu)
 {
 	struct rcuwait *wait = kvm_arch_vcpu_get_wait(vcpu);
@@ -3517,6 +3583,19 @@ static unsigned int kvm_vcpu_max_halt_poll_ns(struct kvm_vcpu *vcpu)
  * expensive block+unblock sequence if a wake event arrives soon after the vCPU
  * is halted.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|731| <<kvm_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/mips/kvm/emulate.c|955| <<kvm_mips_emul_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr.c|501| <<kvmppc_set_msr_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/book3s_pr_papr.c|395| <<kvmppc_h_pr>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/booke.c|724| <<kvmppc_core_prepare_to_enter>> kvm_vcpu_halt(vcpu);
+ *   - arch/powerpc/kvm/powerpc.c|240| <<kvmppc_kvm_pv>> kvm_vcpu_halt(vcpu);
+ *   - arch/riscv/kvm/vcpu_insn.c|192| <<kvm_riscv_vcpu_wfi>> kvm_vcpu_halt(vcpu);
+ *   - arch/s390/kvm/interrupt.c|1339| <<kvm_s390_handle_wait>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/x86.c|11506| <<vcpu_block>> kvm_vcpu_halt(vcpu);
+ *   - arch/x86/kvm/xen.c|1299| <<kvm_xen_schedop_poll>> kvm_vcpu_halt(vcpu);
+ */
 void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 {
 	unsigned int max_halt_poll_ns = kvm_vcpu_max_halt_poll_ns(vcpu);
@@ -3536,6 +3615,11 @@ void kvm_vcpu_halt(struct kvm_vcpu *vcpu)
 		ktime_t stop = ktime_add_ns(start, vcpu->halt_poll_ns);
 
 		do {
+			/*
+			 * called by:
+			 *   - virt/kvm/kvm_main.c|3466| <<kvm_vcpu_block>> if (kvm_vcpu_check_block(vcpu) < 0)
+			 *   - virt/kvm/kvm_main.c|3548| <<kvm_vcpu_halt>> if (kvm_vcpu_check_block(vcpu) < 0)
+			 */
 			if (kvm_vcpu_check_block(vcpu) < 0)
 				goto out;
 			cpu_relax();
@@ -3903,6 +3987,10 @@ static void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
 /*
  * Creates some virtual cpus.  Good luck creating more than one.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4765| <<kvm_vm_ioctl(KVM_CREATE_VCPU)>> r = kvm_vm_ioctl_create_vcpu(kvm, arg);
+ */
 static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)
 {
 	int r;
@@ -4774,6 +4862,15 @@ static long kvm_vm_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_SET_USER_MEMORY_REGION: {
+		/*
+		 * struct kvm_userspace_memory_region {
+		 *     __u32 slot;
+		 *     __u32 flags;
+		 *     __u64 guest_phys_addr;
+		 *     __u64 memory_size; / bytes
+		 *     __u64 userspace_addr; // start of the userspace allocated memory
+		 * };
+		 */
 		struct kvm_userspace_memory_region kvm_userspace_mem;
 
 		r = -EFAULT;
@@ -5397,6 +5494,11 @@ static int kvm_io_bus_get_first_dev(struct kvm_io_bus *bus,
 	return off;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|5490| <<kvm_io_bus_write>> r = __kvm_io_bus_write(vcpu, bus, &range, val);
+ *   - virt/kvm/kvm_main.c|5522| <<kvm_io_bus_write_cookie>> return __kvm_io_bus_write(vcpu, bus, &range, val);
+ */
 static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 			      struct kvm_io_range *range, const void *val)
 {
@@ -5417,6 +5519,18 @@ static int __kvm_io_bus_write(struct kvm_vcpu *vcpu, struct kvm_io_bus *bus,
 	return -EOPNOTSUPP;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/mmio.c|166| <<io_mem_abort>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,
+ *   - arch/mips/kvm/emulate.c|1252| <<kvm_mips_emulate_store>> r = kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/powerpc/kvm/book3s.c|1015| <<kvmppc_h_logical_ci_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|447| <<kvmppc_hv_emulate_mmio>> ret = kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, (gpa_t) gpa, 0,
+ *   - arch/powerpc/kvm/powerpc.c|1388| <<kvmppc_handle_store>> ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
+ *   - arch/riscv/kvm/vcpu_insn.c|681| <<kvm_riscv_vcpu_mmio_store>> if (!kvm_io_bus_write(vcpu, KVM_MMIO_BUS,
+ *   - arch/x86/kvm/vmx/vmx.c|5805| <<handle_ept_misconfig>> !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
+ *   - arch/x86/kvm/x86.c|7711| <<vcpu_mmio_write>> && kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, n, v))
+ *   - arch/x86/kvm/x86.c|8324| <<emulator_pio_in_out>> r = kvm_io_bus_write(vcpu, KVM_PIO_BUS, port, size, data);
+ */
 /* kvm_io_bus_write - called under kvm->slots_lock */
 int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,
 		     int len, const void *val)
-- 
2.34.1

