From e72441a97a70a9d7c05eac3c6e1b58963a6562ba Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 13 Sep 2023 00:26:12 -0700
Subject: [PATCH 1/1] linux-v6.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/kvm_emulate.h          |  21 +
 arch/arm64/include/asm/kvm_host.h             |  16 +
 arch/arm64/kvm/arch_timer.c                   | 701 ++++++++++++++++++
 arch/arm64/kvm/arm.c                          |  22 +
 arch/arm64/kvm/guest.c                        |   4 +
 arch/arm64/kvm/handle_exit.c                  |  24 +
 arch/arm64/kvm/hyp/pgtable.c                  |   9 +
 arch/arm64/kvm/hyp/vgic-v3-sr.c               |  40 +
 arch/arm64/kvm/mmu.c                          |   5 +
 arch/arm64/kvm/pvtime.c                       |  35 +
 arch/arm64/kvm/sys_regs.c                     |  22 +
 arch/arm64/kvm/vgic-sys-reg-v3.c              |  17 +
 arch/arm64/kvm/vgic/vgic-init.c               |   9 +
 arch/arm64/kvm/vgic/vgic-mmio-v3.c            |   4 +
 arch/arm64/kvm/vgic/vgic-v3.c                 |  39 +
 arch/arm64/kvm/vgic/vgic.c                    | 183 +++++
 arch/x86/include/asm/kvm_host.h               |  37 +
 arch/x86/kvm/lapic.c                          |  24 +
 arch/x86/kvm/pmu.c                            |  52 ++
 arch/x86/kvm/pmu.h                            |  23 +
 arch/x86/kvm/vmx/pmu_intel.c                  |   7 +
 arch/x86/kvm/vmx/vmx.c                        |   7 +
 arch/x86/kvm/x86.c                            |  40 +
 drivers/clocksource/arm_arch_timer.c          |  69 ++
 drivers/net/tap.c                             |   5 +
 drivers/vhost/net.c                           | 135 ++++
 include/kvm/arm_arch_timer.h                  |  58 ++
 include/kvm/arm_vgic.h                        |  27 +
 kernel/time/clocksource.c                     |   6 +
 kernel/time/timekeeping.c                     |  39 +
 .../selftests/kvm/include/x86_64/processor.h  |   9 +
 31 files changed, 1689 insertions(+)

diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 3d6725ff0..40ff9b339 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -66,6 +66,10 @@ static __always_inline bool vcpu_el1_is_32bit(struct kvm_vcpu *vcpu)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1315| <<kvm_arch_vcpu_ioctl_vcpu_init>> vcpu_reset_hcr(vcpu);
+ */
 static inline void vcpu_reset_hcr(struct kvm_vcpu *vcpu)
 {
 	vcpu->arch.hcr_el2 = HCR_GUEST_FLAGS;
@@ -103,6 +107,15 @@ static inline void vcpu_reset_hcr(struct kvm_vcpu *vcpu)
 		vcpu->arch.hcr_el2 |= HCR_ATA;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|558| <<kvm_arch_vcpu_runnable>> bool irq_lines = *vcpu_hcr(v) & (HCR_VI | HCR_VF);
+ *   - arch/arm64/kvm/arm.c|1103| <<vcpu_interrupt_line>> hcr = vcpu_hcr(vcpu);
+ *   - arch/arm64/kvm/inject_fault.c|236| <<kvm_set_sei_esr>> *vcpu_hcr(vcpu) |= HCR_VSE;
+ *   - arch/arm64/kvm/mmu.c|2061| <<kvm_set_way_flush>> unsigned long hcr = *vcpu_hcr(vcpu);
+ *   - arch/arm64/kvm/mmu.c|2076| <<kvm_set_way_flush>> *vcpu_hcr(vcpu) = hcr | HCR_TVM;
+ *   - arch/arm64/kvm/mmu.c|2094| <<kvm_toggle_cache>> *vcpu_hcr(vcpu) &= ~HCR_TVM;
+ */
 static inline unsigned long *vcpu_hcr(struct kvm_vcpu *vcpu)
 {
 	return (unsigned long *)&vcpu->arch.hcr_el2;
@@ -463,6 +476,14 @@ static inline bool kvm_is_write_fault(struct kvm_vcpu *vcpu)
 	return kvm_vcpu_dabt_iswrite(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2386| <<kvm_mpidr_to_vcpu>> if (mpidr == kvm_vcpu_get_mpidr_aff(vcpu))
+ *   - arch/arm64/kvm/psci.c|150| <<kvm_psci_vcpu_affinity_info>> mpidr = kvm_vcpu_get_mpidr_aff(tmp);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|295| <<vgic_init>> irq->mpidr = kvm_vcpu_get_mpidr_aff(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|322| <<vgic_mmio_read_v3r_typer>> unsigned long mpidr = kvm_vcpu_get_mpidr_aff(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1031| <<match_mpidr>> affinity = kvm_vcpu_get_mpidr_aff(vcpu);
+ */
 static inline unsigned long kvm_vcpu_get_mpidr_aff(struct kvm_vcpu *vcpu)
 {
 	return vcpu_read_sys_reg(vcpu, MPIDR_EL1) & MPIDR_HWID_BITMASK;
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d3dd05bbf..7c0a0ac92 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -933,6 +933,22 @@ void kvm_arm_resume_guest(struct kvm *kvm);
 		res.a1;							\
 	})
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|494| <<set_cntvoff>> kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
+ *   - arch/arm64/kvm/arm.c|441| <<kvm_arch_vcpu_load>> kvm_call_hyp(__kvm_flush_cpu_context, mmu);
+ *   - arch/arm64/kvm/arm.c|1086| <<kvm_arch_vcpu_ioctl_run>> kvm_call_hyp(__kvm_adjust_pc, vcpu);
+ *   - arch/arm64/kvm/hyp/pgtable.c|789| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid, mmu);
+ *   - arch/arm64/kvm/hyp/pgtable.c|791| <<stage2_try_break_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ctx->addr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|822| <<stage2_put_pte>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, ctx->addr, ctx->level);
+ *   - arch/arm64/kvm/hyp/pgtable.c|1271| <<kvm_pgtable_stage2_relax_perms>> kvm_call_hyp(__kvm_tlb_flush_vmid_ipa_nsh, pgt->mmu, addr, level);
+ *   - arch/arm64/kvm/mmu.c|172| <<kvm_flush_remote_tlbs>> kvm_call_hyp(__kvm_tlb_flush_vmid, &kvm->arch.mmu);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|544| <<kvm_vgic_init_cpu_hardware>> kvm_call_hyp(__vgic_v3_init_lrs);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|730| <<vgic_v3_load>> kvm_call_hyp(__vgic_v3_write_vmcr, cpu_if->vgic_vmcr);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|732| <<vgic_v3_load>> kvm_call_hyp(__vgic_v3_restore_aprs, cpu_if);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|756| <<vgic_v3_put>> kvm_call_hyp(__vgic_v3_save_aprs, cpu_if);
+ *   - arch/arm64/kvm/vmid.c|69| <<flush_context>> kvm_call_hyp(__kvm_flush_vm_context);
+ */
 /*
  * The couple of isb() below are there to guarantee the same behaviour
  * on VHE as on !VHE, where the eret to EL1 acts as a context
diff --git a/arch/arm64/kvm/arch_timer.c b/arch/arm64/kvm/arch_timer.c
index 6dcdae4d3..db9e43bc7 100644
--- a/arch/arm64/kvm/arch_timer.c
+++ b/arch/arm64/kvm/arch_timer.c
@@ -23,12 +23,79 @@
 
 #include "trace.h"
 
+/*
+ * struct kvm:
+ * -> struct kvm_arch arch;
+ *    -> struct arch_timer_vm_data timer_data;
+ *       -> u64 voffset;
+ *       -> u64 poffset;
+ *       -> ppi[NR_KVM_TIMERS];
+ *
+ * struct kvm_vcpu:
+ * -> struct kvm_vcpu_arch arch;
+ *    -> struct kvm_cpu_context ctxt;
+ *       -> struct user_pt_regs regs;
+ *       -> u64 sys_regs[NR_SYS_REGS];
+ *    -> struct arch_timer_cpu timer_cpu;
+ *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+ *          -> struct kvm_vcpu *vcpu;
+ *          -> struct arch_timer_offset offset;
+ *             -> u64 *vm_offset;
+ *             -> u64 *vcpu_offset;
+ *       -> struct hrtimer bg_timer;
+ *       -> bool enabled;
+ *
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
+
+/*
+ * 在以下使用timecounter:
+ *   - arch/arm64/kvm/arch_timer.c|211| <<kvm_phys_timer_read>> return timecounter->cc->read(timecounter->cc);
+ *   - arch/arm64/kvm/arch_timer.c|305| <<kvm_counter_compute_delta>> ns = cyclecounter_cyc2ns(timecounter->cc,
+ *   - arch/arm64/kvm/arch_timer.c|307| <<kvm_counter_compute_delta>> timecounter->mask,
+ *   - arch/arm64/kvm/arch_timer.c|1464| <<kvm_timer_hyp_init>> timecounter = &info->timecounter;
+ *   - arch/arm64/kvm/arch_timer.c|1466| <<kvm_timer_hyp_init>> if (!timecounter->cc) {
+ */
 static struct timecounter *timecounter;
+/*
+ * 在以下设置host_vtimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1643| <<kvm_irq_init>> host_vtimer_irq = info->virtual_irq;
+ */
 static unsigned int host_vtimer_irq;
+/*
+ * 在以下设置host_ptimer_irq:
+ *   - arch/arm64/kvm/arch_timer.c|1670| <<kvm_irq_init>> host_ptimer_irq = info->physical_irq;
+ */
 static unsigned int host_ptimer_irq;
+/*
+ * 在以下啊使用host_vtimer_irq_flags:
+ *   - arch/arm64/kvm/arch_timer.c|961| <<kvm_timer_vcpu_load_nogic>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1207| <<unmask_vtimer_irq_user>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1322| <<kvm_timer_cpu_up>> enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1644| <<kvm_irq_init>> kvm_irq_fixup_flags(host_vtimer_irq, &host_vtimer_irq_flags);
+ */
 static u32 host_vtimer_irq_flags;
+/*
+ * 在以下使用host_ptimer_irq_flags:
+ *   - arch/arm64/kvm/arch_timer.c|1324| <<kvm_timer_cpu_up>> enable_percpu_irq(host_ptimer_irq, host_ptimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1671| <<kvm_irq_init>> kvm_irq_fixup_flags(host_ptimer_irq, &host_ptimer_irq_flags);
+ */
 static u32 host_ptimer_irq_flags;
 
+/*
+ * 在以下使用has_gic_active_state:
+ *   - arch/arm64/kvm/arch_timer.c|32| <<global>> static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
+ *   - arch/arm64/kvm/arch_timer.c|255| <<kvm_arch_timer_handler>> !static_branch_unlikely(&has_gic_active_state))
+ *   - arch/arm64/kvm/arch_timer.c|856| <<kvm_timer_vcpu_load>> if (static_branch_likely(&has_gic_active_state)) {
+ *   - arch/arm64/kvm/arch_timer.c|940| <<unmask_vtimer_irq_user>> if (static_branch_likely(&has_gic_active_state))
+ *   - arch/arm64/kvm/arch_timer.c|1424| <<kvm_timer_hyp_init>> static_branch_enable(&has_gic_active_state);
+ */
 static DEFINE_STATIC_KEY_FALSE(has_gic_active_state);
 
 static const u8 default_ppi[] = {
@@ -51,15 +118,39 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      enum kvm_arch_timer_regs treg);
 static bool kvm_arch_timer_get_input_level(int vintid);
 
+/*
+ * 在以下使用arch_timer_irq_ops:
+ *   - arch/arm64/kvm/arch_timer.c|800| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq, timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|805| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq, timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1437| <<kvm_irq_init>> arch_timer_irq_ops.flags |= VGIC_IRQ_SW_RESAMPLE;
+ *   - arch/arm64/kvm/arch_timer.c|1624| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1632| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+ */
 static struct irq_ops arch_timer_irq_ops = {
 	.get_input_level = kvm_arch_timer_get_input_level,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|545| <<set_cntpoff>> if (has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|597| <<timer_save_state>> if (!has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|692| <<timer_restore_state>> if (!has_cntpoff())
+ *   - arch/arm64/kvm/arch_timer.c|854| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ */
 static bool has_cntpoff(void)
 {
 	return (has_vhe() && cpus_have_final_cap(ARM64_HAS_ECV_CNTPOFF));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|354| <<kvm_timer_earliest_exp>> for (i = 0; i < nr_timers(vcpu); i++) {
+ *   - arch/arm64/kvm/arch_timer.c|1012| <<kvm_timer_vcpu_reset>> for (int i = 0; i < nr_timers(vcpu); i++)
+ *   - arch/arm64/kvm/arch_timer.c|1028| <<kvm_timer_vcpu_reset>> for (int i = 0; i < nr_timers(vcpu); i++)
+ *   - arch/arm64/kvm/arch_timer.c|1550| <<timer_irqs_are_valid>> for (int i = 0; i < nr_timers(vcpu); i++) {
+ *   - arch/arm64/kvm/arch_timer.c|1566| <<timer_irqs_are_valid>> valid = hweight32(ppis) == nr_timers(vcpu);
+ *   - arch/arm64/kvm/arch_timer.c|1583| <<kvm_arch_timer_get_input_level>> for (int i = 0; i < nr_timers(vcpu); i++) {
+ */
 static int nr_timers(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu_has_nv(vcpu))
@@ -68,6 +159,15 @@ static int nr_timers(struct kvm_vcpu *vcpu)
 	return NR_KVM_TIMERS;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|324| <<kvm_timer_irq_can_fire>> ((timer_get_ctl(timer_ctx) &
+ *   - arch/arm64/kvm/arch_timer.c|685| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTV_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|696| <<timer_restore_state>> write_sysreg_el0(timer_get_ctl(ctx), SYS_CNTP_CTL);
+ *   - arch/arm64/kvm/arch_timer.c|1162| <<read_timer_ctl>> u32 ctl = timer_get_ctl(timer);
+ *   - arch/arm64/kvm/trace_arm.h|243| <<__field>> __entry->ctl = timer_get_ctl(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|265| <<__field>> __entry->ctl = timer_get_ctl(ctx);
+ */
 u32 timer_get_ctl(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -87,6 +187,17 @@ u32 timer_get_ctl(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|458| <<kvm_timer_should_fire>> cval = timer_get_cval(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|683| <<timer_restore_state>> write_sysreg_el0(timer_get_cval(ctx), SYS_CNTV_CVAL);
+ *   - arch/arm64/kvm/arch_timer.c|689| <<timer_restore_state>> cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1214| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1223| <<kvm_arm_timer_read>> val = timer_get_cval(timer);
+ *   - arch/arm64/kvm/trace_arm.h|244| <<__field>> __entry->cval = timer_get_cval(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|266| <<__field>> __entry->cval = timer_get_cval(ctx);
+ */
 u64 timer_get_cval(struct arch_timer_context *ctxt)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -106,6 +217,18 @@ u64 timer_get_cval(struct arch_timer_context *ctxt)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|447| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|586| <<timer_save_state>> cval -= timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|670| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|678| <<timer_restore_state>> offset = timer_get_offset(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|842| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|1191| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1204| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1250| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 static u64 timer_get_offset(struct arch_timer_context *ctxt)
 {
 	u64 offset = 0;
@@ -113,6 +236,12 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	if (!ctxt)
 		return 0;
 
+	/*
+	 * struct arch_timer_context *ctxt:
+	 * -> struct arch_timer_offset offset;
+	 *    -> u64 *vm_offset;
+	 *    -> u64 *vcpu_offset;
+	 */
 	if (ctxt->offset.vm_offset)
 		offset += *ctxt->offset.vm_offset;
 	if (ctxt->offset.vcpu_offset)
@@ -121,6 +250,13 @@ static u64 timer_get_offset(struct arch_timer_context *ctxt)
 	return offset;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|568| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|594| <<timer_save_state>> timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTP_CTL));
+ *   - arch/arm64/kvm/arch_timer.c|1013| <<kvm_timer_vcpu_reset>> timer_set_ctl(vcpu_get_timer(vcpu, i), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1290| <<kvm_arm_timer_write>> timer_set_ctl(timer, val & ~ARCH_TIMER_CTRL_IT_STAT);
+ */
 static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -143,6 +279,13 @@ static void timer_set_ctl(struct arch_timer_context *ctxt, u32 ctl)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|569| <<timer_save_state>> timer_set_cval(ctx, read_sysreg_el0(SYS_CNTV_CVAL));
+ *   - arch/arm64/kvm/arch_timer.c|600| <<timer_save_state>> timer_set_cval(ctx, cval);
+ *   - arch/arm64/kvm/arch_timer.c|1286| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ *   - arch/arm64/kvm/arch_timer.c|1294| <<kvm_arm_timer_write>> timer_set_cval(timer, val);
+ */
 static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 {
 	struct kvm_vcpu *vcpu = ctxt->vcpu;
@@ -165,6 +308,13 @@ static void timer_set_cval(struct arch_timer_context *ctxt, u64 cval)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1083| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1084| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_ptimer(vcpu), 0);
+ *   - arch/arm64/kvm/arch_timer.c|1124| <<kvm_arm_timer_set_reg(KVM_REG_ARM_TIMER_CNT)>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1139| <<kvm_arm_timer_set_reg(KVM_REG_ARM_PTIMER_CNT)>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ */
 static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 {
 	if (!ctxt->offset.vm_offset) {
@@ -175,11 +325,40 @@ static void timer_set_offset(struct arch_timer_context *ctxt, u64 offset)
 	WRITE_ONCE(*ctxt->offset.vm_offset, offset);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|264| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|423| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|1042| <<kvm_timer_vcpu_init>> timer_set_offset(vcpu_vtimer(vcpu), kvm_phys_timer_read());
+ *   - arch/arm64/kvm/arch_timer.c|1083| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1098| <<kvm_arm_timer_set_reg>> timer_set_offset(timer, kvm_phys_timer_read() - value);
+ *   - arch/arm64/kvm/arch_timer.c|1162| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1175| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1221| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+ */
 u64 kvm_phys_timer_read(void)
 {
 	return timecounter->cc->read(timecounter->cc);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|280| <<kvm_arch_timer_handler>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|629| <<kvm_timer_blocking>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|895| <<kvm_timer_vcpu_load>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|946| <<kvm_timer_vcpu_put>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1004| <<kvm_timer_vcpu_reset>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1253| <<kvm_arm_timer_read_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1314| <<kvm_arm_timer_write_sysreg>> get_timer_map(vcpu, &map);
+ *   - arch/arm64/kvm/arch_timer.c|1619| <<kvm_timer_enable>> get_timer_map(vcpu, &map);
+ *
+ * struct timer_map {
+ *     struct arch_timer_context *direct_vtimer;
+ *     struct arch_timer_context *direct_ptimer;
+ *     struct arch_timer_context *emul_vtimer;
+ *     struct arch_timer_context *emul_ptimer;
+ * };
+ */
 static void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	if (vcpu_has_nv(vcpu)) {
@@ -195,6 +374,9 @@ static void get_timer_map(struct kvm_vcpu *vcpu, struct timer_map *map)
 			map->emul_ptimer = vcpu_hptimer(vcpu);
 		}
 	} else if (has_vhe()) {
+		/*
+		 * (&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+		 */
 		map->direct_vtimer = vcpu_vtimer(vcpu);
 		map->direct_ptimer = vcpu_ptimer(vcpu);
 		map->emul_vtimer = NULL;
@@ -215,17 +397,37 @@ static inline bool userspace_irqchip(struct kvm *kvm)
 		unlikely(!irqchip_in_kernel(kvm));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static void soft_timer_start(struct hrtimer *hrt, u64 ns)
 {
 	hrtimer_start(hrt, ktime_add_ns(ktime_get(), ns),
 		      HRTIMER_MODE_ABS_HARD);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|653| <<kvm_timer_unblocking>> soft_timer_cancel(&timer->bg_timer);
+ *   - arch/arm64/kvm/arch_timer.c|962| <<kvm_timer_vcpu_put>> soft_timer_cancel(&map.emul_vtimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|964| <<kvm_timer_vcpu_put>> soft_timer_cancel(&map.emul_ptimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1040| <<kvm_timer_vcpu_reset>> soft_timer_cancel(&map.emul_vtimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1042| <<kvm_timer_vcpu_reset>> soft_timer_cancel(&map.emul_ptimer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1317| <<kvm_arm_timer_write_sysreg>> soft_timer_cancel(&timer->hrtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1540| <<kvm_timer_vcpu_terminate>> soft_timer_cancel(&timer->bg_timer);
+ */
 static void soft_timer_cancel(struct hrtimer *hrt)
 {
 	hrtimer_cancel(hrt);
 }
 
+/*
+ * 在以下使用kvm_arch_timer_handler():
+ *   - arch/arm64/kvm/arch_timer.c|1408| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_vtimer_irq, kvm_arch_timer_handler, "kvm guest vtimer", kvm_get_running_vcpus());
+ *   - arch/arm64/kvm/arch_timer.c|1432| <<kvm_timer_hyp_init>> err = request_percpu_irq(host_ptimer_irq, kvm_arch_timer_handler, "kvm guest ptimer", kvm_get_running_vcpus());
+ */
 static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 {
 	struct kvm_vcpu *vcpu = *(struct kvm_vcpu **)dev_id;
@@ -258,9 +460,36 @@ static irqreturn_t kvm_arch_timer_handler(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+ *   - arch/arm64/kvm/arch_timer.c|342| <<wfit_delay_ns>> return kvm_counter_compute_delta(ctx, val);
+ */
 static u64 kvm_counter_compute_delta(struct arch_timer_context *timer_ctx,
 				     u64 val)
 {
+	/*
+	 * struct kvm:
+	 * -> struct kvm_arch arch;
+	 *    -> struct arch_timer_vm_data timer_data;
+	 *       -> u64 voffset;
+	 *       -> u64 poffset;
+	 *       -> ppi[NR_KVM_TIMERS];
+	 *
+	 * struct kvm_vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct kvm_cpu_context ctxt;
+	 *       -> struct user_pt_regs regs;
+	 *       -> u64 sys_regs[NR_SYS_REGS];
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *          -> struct kvm_vcpu *vcpu;
+	 *          -> struct arch_timer_offset offset;
+	 *             -> u64 *vm_offset;
+	 *             -> u64 *vcpu_offset;
+	 *       -> struct hrtimer bg_timer;
+	 *       -> bool enabled;
+	 */
 	u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
 
 	if (now < val) {
@@ -276,11 +505,33 @@ static u64 kvm_counter_compute_delta(struct arch_timer_context *timer_ctx,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|359| <<kvm_timer_earliest_exp>> min_delta = min(min_delta, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|412| <<kvm_hrtimer_expire>> ns = kvm_timer_compute_delta(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+ *   - arch/arm64/kvm/arch_timer.c|1164| <<read_timer_ctl>> if (!kvm_timer_compute_delta(timer))
+ */
 static u64 kvm_timer_compute_delta(struct arch_timer_context *timer_ctx)
 {
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arch_timer.c|317| <<kvm_timer_compute_delta>> return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|342| <<wfit_delay_ns>> return kvm_counter_compute_delta(ctx, val);
+	 */
 	return kvm_counter_compute_delta(timer_ctx, timer_get_cval(timer_ctx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|358| <<kvm_timer_earliest_exp>> if (kvm_timer_irq_can_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|455| <<kvm_timer_should_fire>> if (!kvm_timer_irq_can_fire(timer_ctx))
+ *   - arch/arm64/kvm/arch_timer.c|522| <<timer_emulate>> if (should_fire || !kvm_timer_irq_can_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|635| <<kvm_timer_blocking>> if (!kvm_timer_irq_can_fire(map.direct_vtimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|636| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.direct_ptimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|637| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.emul_vtimer) &&
+ *   - arch/arm64/kvm/arch_timer.c|638| <<kvm_timer_blocking>> !kvm_timer_irq_can_fire(map.emul_ptimer) &&
+ */
 static bool kvm_timer_irq_can_fire(struct arch_timer_context *timer_ctx)
 {
 	WARN_ON(timer_ctx && timer_ctx->loaded);
@@ -310,6 +561,11 @@ static u64 wfit_delay_ns(struct kvm_vcpu *vcpu)
  * Returns the earliest expiration time in ns among guest timers.
  * Note that it will return 0 if none of timers can fire.
  */
+/*
+ * 在以下使用kvm_timer_earliest_exp():
+ *   - arch/arm64/kvm/arch_timer.c|386| <<kvm_bg_timer_expire>> ns = kvm_timer_earliest_exp(vcpu);
+ *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+ */
 static u64 kvm_timer_earliest_exp(struct kvm_vcpu *vcpu)
 {
 	u64 min_delta = ULLONG_MAX;
@@ -357,6 +613,15 @@ static enum hrtimer_restart kvm_bg_timer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+ * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+ * 然后再次进入guest时,就有中断了.
+ *
+ * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+ * timer到期的时候(还在host mode)唤醒guest并inject irq.
+ * KVM注册了一个hrtimer来处理这种情况.
+ */
 static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 {
 	struct arch_timer_context *ctx;
@@ -383,6 +648,19 @@ static enum hrtimer_restart kvm_hrtimer_expire(struct hrtimer *hrt)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|287| <<kvm_arch_timer_handler>> if (kvm_timer_should_fire(ctx))
+ *   - arch/arm64/kvm/arch_timer.c|481| <<kvm_timer_update_run>> if (kvm_timer_should_fire(vtimer))
+ *   - arch/arm64/kvm/arch_timer.c|483| <<kvm_timer_update_run>> if (kvm_timer_should_fire(ptimer))
+ *   - arch/arm64/kvm/arch_timer.c|508| <<timer_emulate>> bool should_fire = kvm_timer_should_fire(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|727| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|747| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|934| <<kvm_timer_should_notify_user>> return kvm_timer_should_fire(vtimer) != vlevel ||
+ *   - arch/arm64/kvm/arch_timer.c|935| <<kvm_timer_should_notify_user>> kvm_timer_should_fire(ptimer) != plevel;
+ *   - arch/arm64/kvm/arch_timer.c|979| <<unmask_vtimer_irq_user>> if (!kvm_timer_should_fire(vtimer)) {
+ *   - arch/arm64/kvm/arch_timer.c|1588| <<kvm_arch_timer_get_input_level>> return kvm_timer_should_fire(ctx);
+ */
 static bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx)
 {
 	enum kvm_arch_timers index;
@@ -419,6 +697,18 @@ static bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx)
 	if (!kvm_timer_irq_can_fire(timer_ctx))
 		return false;
 
+	/*
+	 * 在以下使用timer_get_offset():
+	 *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_counter_compute_delta>> u64 now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|447| <<kvm_timer_should_fire>> now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|586| <<timer_save_state>> cval -= timer_get_offset(ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|670| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|678| <<timer_restore_state>> offset = timer_get_offset(ctx);
+	 *   - arch/arm64/kvm/arch_timer.c|842| <<timer_set_traps>> if (!has_cntpoff() && timer_get_offset(map->direct_ptimer))
+	 *   - arch/arm64/kvm/arch_timer.c|1191| <<kvm_arm_timer_read>> val = timer_get_cval(timer) - kvm_phys_timer_read() + timer_get_offset(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1204| <<kvm_arm_timer_read>> val = kvm_phys_timer_read() - timer_get_offset(timer);
+	 *   - arch/arm64/kvm/arch_timer.c|1250| <<kvm_arm_timer_write>> timer_set_cval(timer, kvm_phys_timer_read() - timer_get_offset(timer) + (s32)val);
+	 */
 	cval = timer_get_cval(timer_ctx);
 	now = kvm_phys_timer_read() - timer_get_offset(timer_ctx);
 
@@ -448,11 +738,31 @@ void kvm_timer_update_run(struct kvm_vcpu *vcpu)
 		regs->device_irq_level |= KVM_ARM_DEV_EL1_PTIMER;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|288| <<kvm_arch_timer_handler>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|418| <<kvm_hrtimer_expire>> kvm_timer_update_irq(vcpu, true, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|513| <<timer_emulate>> kvm_timer_update_irq(ctx->vcpu, should_fire, ctx);
+ *   - arch/arm64/kvm/arch_timer.c|727| <<kvm_timer_vcpu_load_gic>> kvm_timer_update_irq(ctx->vcpu, kvm_timer_should_fire(ctx), ctx);
+ *   - arch/arm64/kvm/arch_timer.c|747| <<kvm_timer_vcpu_load_nogic>> kvm_timer_update_irq(vcpu, kvm_timer_should_fire(vtimer), vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|980| <<unmask_vtimer_irq_user>> kvm_timer_update_irq(vcpu, false, vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1029| <<kvm_timer_vcpu_reset>> kvm_timer_update_irq(vcpu, false,
+ *   - arch/arm64/kvm/trace_arm.h|181| <<__field>> TRACE_EVENT(kvm_timer_update_irq,
+ *
+ * 核心思想是调用kvm_vgic_inject_irq()
+ */
 static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 				 struct arch_timer_context *timer_ctx)
 {
 	int ret;
 
+	/*
+	 * struct arch_timer_context *timer_ctx:
+	 *   // Output level of the timer IRQ
+	 *   struct {
+	 *       bool                    level;
+	 *   } irq;
+	 */
 	timer_ctx->irq.level = new_level;
 	trace_kvm_timer_update_irq(vcpu->vcpu_id, timer_irq(timer_ctx),
 				   timer_ctx->irq.level);
@@ -466,6 +776,12 @@ static void kvm_timer_update_irq(struct kvm_vcpu *vcpu, bool new_level,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1137| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1139| <<kvm_timer_vcpu_load>> timer_emulate(map.emul_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1542| <<kvm_arm_timer_write_sysreg>> timer_emulate(timer);
+ */
 /* Only called for a fully emulated timer */
 static void timer_emulate(struct arch_timer_context *ctx)
 {
@@ -489,17 +805,43 @@ static void timer_emulate(struct arch_timer_context *ctx)
 	soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|544| <<timer_save_state>> set_cntvoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|629| <<timer_restore_state>> set_cntvoff(timer_get_offset(ctx));
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void set_cntvoff(u64 cntvoff)
 {
 	kvm_call_hyp(__kvm_timer_set_cntvoff, cntvoff);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|565| <<timer_save_state>> set_cntpoff(0);
+ *   - arch/arm64/kvm/arch_timer.c|650| <<timer_restore_state>> set_cntpoff(offset);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void set_cntpoff(u64 cntpoff)
 {
 	if (has_cntpoff())
 		write_sysreg_s(cntpoff, SYS_CNTPOFF_EL2);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1135| <<kvm_timer_vcpu_put>> timer_save_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1137| <<kvm_timer_vcpu_put>> timer_save_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1447| <<kvm_arm_timer_read_sysreg>> timer_save_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1509| <<kvm_arm_timer_write_sysreg>> timer_save_state(timer);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void timer_save_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -519,6 +861,10 @@ static void timer_save_state(struct arch_timer_context *ctx)
 
 	case TIMER_VTIMER:
 	case TIMER_HVTIMER:
+		/*
+		 * 因为寄存器像是x86的ibrs一样是共享的
+		 * 所以在退出guest mode的时候要保存
+		 */
 		timer_set_ctl(ctx, read_sysreg_el0(SYS_CNTV_CTL));
 		timer_set_cval(ctx, read_sysreg_el0(SYS_CNTV_CVAL));
 
@@ -575,8 +921,27 @@ static void timer_save_state(struct arch_timer_context *ctx)
  * thread is removed from its waitqueue and made runnable when there's a timer
  * interrupt to handle.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1190| <<kvm_timer_vcpu_put>> kvm_timer_blocking(vcpu);
+ */
 static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * struct arch_timer_cpu {
+	 *     struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *
+	 *     // Background timer used when the guest is not running
+	 *     struct hrtimer                  bg_timer;
+	 *
+	 *     // Is the timer enabled
+	 *     bool                    enabled;
+	 * };
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 */
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
@@ -593,6 +958,13 @@ static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 	    !vcpu_has_wfit_active(vcpu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/arch_timer.c|525| <<timer_emulate>> soft_timer_start(&ctx->hrtimer, kvm_timer_compute_delta(ctx));
+	 *   - arch/arm64/kvm/arch_timer.c|646| <<kvm_timer_blocking>> soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
+	 *
+	 * Background timer used when the guest is not running
+	 */
 	/*
 	 * At least one guest time will expire. Schedule a background timer.
 	 * Set the earliest expiration time among the guest timers.
@@ -600,6 +972,10 @@ static void kvm_timer_blocking(struct kvm_vcpu *vcpu)
 	soft_timer_start(&timer->bg_timer, kvm_timer_earliest_exp(vcpu));
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1110| <<kvm_timer_vcpu_load>> kvm_timer_unblocking(vcpu);
+ */
 static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -607,6 +983,16 @@ static void kvm_timer_unblocking(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|857| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|859| <<kvm_timer_vcpu_load>> timer_restore_state(map.direct_ptimer);
+ *   - arch/arm64/kvm/arch_timer.c|1196| <<kvm_arm_timer_read_sysreg>> timer_restore_state(timer);
+ *   - arch/arm64/kvm/arch_timer.c|1247| <<kvm_arm_timer_write_sysreg>> timer_restore_state(timer);
+ *
+ * kvm_timer_vcpu_put()  --> timer_save_state()
+ * kvm_timer_vcpu_load() --> timer_restore_state()
+ */
 static void timer_restore_state(struct arch_timer_context *ctx)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(ctx->vcpu);
@@ -653,13 +1039,35 @@ static void timer_restore_state(struct arch_timer_context *ctx)
 	local_irq_restore(flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|933| <<kvm_timer_vcpu_load_gic>> set_timer_irq_phys_active(ctx, phys_active);
+ *   - arch/arm64/kvm/arch_timer.c|1205| <<unmask_vtimer_irq_user>> set_timer_irq_phys_active(vtimer, false);
+ */
 static inline void set_timer_irq_phys_active(struct arch_timer_context *ctx, bool active)
 {
 	int r;
+	/*
+	 * irq_set_irqchip_state - set the state of a forwarded interrupt.
+	 *      @irq: Interrupt line that is forwarded to a VM
+	 *      @which: State to be restored (one of IRQCHIP_STATE_*)
+	 *      @val: Value corresponding to @which
+	 *
+	 *      This call sets the internal irqchip state of an interrupt,
+	 *      depending on the value of @which.
+	 *
+	 *      This function should be called with migration disabled if the
+	 *      interrupt controller has per-cpu registers.
+	 */
 	r = irq_set_irqchip_state(ctx->host_timer_irq, IRQCHIP_STATE_ACTIVE, active);
 	WARN_ON(r);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1192| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_gic(map.direct_vtimer);
+ *   - arch/arm64/kvm/arch_timer.c|1194| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_gic(map.direct_ptimer);
+ */
 static void kvm_timer_vcpu_load_gic(struct arch_timer_context *ctx)
 {
 	struct kvm_vcpu *vcpu = ctx->vcpu;
@@ -681,6 +1089,10 @@ static void kvm_timer_vcpu_load_gic(struct arch_timer_context *ctx)
 	set_timer_irq_phys_active(ctx, phys_active);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1196| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_nogic(vcpu);
+ */
 static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -709,6 +1121,11 @@ static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 		enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1168| <<timer_set_traps>> assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
+ *   - arch/arm64/kvm/arch_timer.c|1169| <<timer_set_traps>> assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
+ */
 /* If _pred is true, set bit in _set, otherwise set it in _clr */
 #define assign_clear_set_bit(_pred, _bit, _clr, _set)			\
 	do {								\
@@ -718,6 +1135,10 @@ static void kvm_timer_vcpu_load_nogic(struct kvm_vcpu *vcpu)
 			(_clr) |= (_bit);				\
 	} while (0)
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1190| <<kvm_timer_vcpu_load>> kvm_timer_vcpu_load_nested_switch(vcpu, &map);
+ */
 static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 					      struct timer_map *map)
 {
@@ -769,6 +1190,10 @@ static void kvm_timer_vcpu_load_nested_switch(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1230| <<kvm_timer_vcpu_load>> timer_set_traps(vcpu, &map);
+ */
 static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 {
 	bool tpt, tpc;
@@ -827,12 +1252,26 @@ static void timer_set_traps(struct kvm_vcpu *vcpu, struct timer_map *map)
 	assign_clear_set_bit(tpt, CNTHCTL_EL1PCEN << 10, set, clr);
 	assign_clear_set_bit(tpc, CNTHCTL_EL1PCTEN << 10, set, clr);
 
+	/*
+	 * 注释
+	 * Controls the generation of an event stream from the physical
+	 * counter, and access from EL1 to the physical counter and the EL1
+	 * physical timer.
+	 * 控制哪些timer的counter access会trap!
+	 */
 	/* This only happens on VHE, so use the CNTHCTL_EL2 accessor. */
 	sysreg_clear_set(cnthctl_el2, clr, set);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|448| <<kvm_arch_vcpu_load>> kvm_timer_vcpu_load(vcpu);
+ */
 void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 返回&(v)->arch.timer_cpu
+	 */
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
@@ -852,8 +1291,32 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 		kvm_timer_vcpu_load_nogic(vcpu);
 	}
 
+	/*
+	 * 只在此处调用
+	 */
 	kvm_timer_unblocking(vcpu);
 
+	/*
+	 * vcpu_vtimer(vcpu);
+	 *
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct arch_timer_cpu timer_cpu;
+	 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+	 *          -> struct arch_timer_offset offset;
+	 *             -> u64 *vm_offset;
+	 *             -> u64 *vcpu_offset;
+	 *
+	 *
+	 * struct timer_map {
+	 *     struct arch_timer_context *direct_vtimer;
+	 *     struct arch_timer_context *direct_ptimer;
+	 *     struct arch_timer_context *emul_vtimer;
+	 *     struct arch_timer_context *emul_ptimer;
+	 * };
+	 *
+	 * direct_vtimer: 比如(&(v)->arch.timer_cpu.timers[TIMER_VTIMER])
+	 */
 	timer_restore_state(map.direct_vtimer);
 	if (map.direct_ptimer)
 		timer_restore_state(map.direct_ptimer);
@@ -865,6 +1328,12 @@ void kvm_timer_vcpu_load(struct kvm_vcpu *vcpu)
 	timer_set_traps(vcpu, &map);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|850| <<kvm_vcpu_exit_request>> if (kvm_timer_should_notify_user(vcpu) ||
+ *
+ * 如果irqchip_in_kernel就返回false
+ */
 bool kvm_timer_should_notify_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -882,6 +1351,10 @@ bool kvm_timer_should_notify_user(struct kvm_vcpu *vcpu)
 	       kvm_timer_should_fire(ptimer) != plevel;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|475| <<kvm_arch_vcpu_put>> kvm_timer_vcpu_put(vcpu);
+ */
 void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -919,6 +1392,12 @@ void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
  * timer and if so, unmask the timer irq signal on the host interrupt
  * controller to ensure that we see future timer signals.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1312| <<kvm_timer_sync_user>> unmask_vtimer_irq_user(vcpu);
+ *
+ * 只在irqchip_in_kernel为false的时候调用
+ */
 static void unmask_vtimer_irq_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_context *vtimer = vcpu_vtimer(vcpu);
@@ -932,6 +1411,11 @@ static void unmask_vtimer_irq_user(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|969| <<kvm_arch_vcpu_ioctl_run>> kvm_timer_sync_user(vcpu);
+ *   - arch/arm64/kvm/arm.c|1015| <<kvm_arch_vcpu_ioctl_run>> kvm_timer_sync_user(vcpu);
+ */
 void kvm_timer_sync_user(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -943,11 +1427,23 @@ void kvm_timer_sync_user(struct kvm_vcpu *vcpu)
 		unmask_vtimer_irq_user(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/reset.c|302| <<kvm_reset_vcpu>> ret = kvm_timer_vcpu_reset(vcpu);
+ */
 int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
 	struct timer_map map;
 
+	/*
+	 * struct timer_map {
+	 *     struct arch_timer_context *direct_vtimer;
+	 *     struct arch_timer_context *direct_ptimer;
+	 *     struct arch_timer_context *emul_vtimer;
+	 *     struct arch_timer_context *emul_ptimer;
+	 * };
+	 */
 	get_timer_map(vcpu, &map);
 
 	/*
@@ -977,6 +1473,15 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 					     vcpu_get_timer(vcpu, i));
 
 		if (irqchip_in_kernel(vcpu->kvm)) {
+			/*
+			 * kvm_vgic_reset_mapped_irq - Reset a mapped IRQ
+			 * @vcpu: The VCPU pointer
+			 * @vintid: The INTID of the interrupt
+			 *
+			 * Reset the active and pending states of a mapped interrupt.  Kernel
+			 * subsystems injecting mapped interrupts should reset their interrupt lines
+			 * when we are doing a reset of the VM.
+			 */
 			kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_vtimer));
 			if (map.direct_ptimer)
 				kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_ptimer));
@@ -991,6 +1496,10 @@ int kvm_timer_vcpu_reset(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1448| <<kvm_timer_vcpu_init>> timer_context_init(vcpu, i);
+ */
 static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 {
 	struct arch_timer_context *ctxt = vcpu_get_timer(vcpu, timerid);
@@ -1003,6 +1512,15 @@ static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 	else
 		ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
 
+	/*
+	 * 虚拟counter的中断被路由到el2层. 当VM执行时,时钟到期,虚拟counter的中断产生.
+	 * 这时VM exit,由KVM处理该中断,执行中断的inject(把中断路由给vGIC).
+	 * 然后再次进入guest时,就有中断了.
+	 *
+	 * 还有一种情况: guest side退出前设置一个timer,这时KVM需要维护这个timer,以在
+	 * timer到期的时候(还在host mode)唤醒guest并inject irq.
+	 * KVM注册了一个hrtimer来处理这种情况.
+	 */
 	hrtimer_init(&ctxt->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_HARD);
 	ctxt->hrtimer.function = kvm_hrtimer_expire;
 
@@ -1018,6 +1536,10 @@ static void timer_context_init(struct kvm_vcpu *vcpu, int timerid)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|380| <<kvm_arch_vcpu_create>> kvm_timer_vcpu_init(vcpu);
+ */
 void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1035,12 +1557,20 @@ void kvm_timer_vcpu_init(struct kvm_vcpu *vcpu)
 	timer->bg_timer.function = kvm_bg_timer_expire;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|169| <<kvm_arch_init_vm>> kvm_timer_init_vm(kvm);
+ */
 void kvm_timer_init_vm(struct kvm *kvm)
 {
 	for (int i = 0; i < NR_KVM_TIMERS; i++)
 		kvm->arch.timer_data.ppi[i] = default_ppi[i];
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1906| <<kvm_arch_hardware_enable>> kvm_timer_cpu_up();
+ */
 void kvm_timer_cpu_up(void)
 {
 	enable_percpu_irq(host_vtimer_irq, host_vtimer_irq_flags);
@@ -1048,6 +1578,10 @@ void kvm_timer_cpu_up(void)
 		enable_percpu_irq(host_ptimer_irq, host_ptimer_irq_flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1915| <<kvm_arch_hardware_disable>> kvm_timer_cpu_down();
+ */
 void kvm_timer_cpu_down(void)
 {
 	disable_percpu_irq(host_vtimer_irq);
@@ -1055,6 +1589,10 @@ void kvm_timer_cpu_down(void)
 		disable_percpu_irq(host_ptimer_irq);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|639| <<set_timer_reg>> return kvm_arm_timer_set_reg(vcpu, reg->id, val);
+ */
 int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 {
 	struct arch_timer_context *timer;
@@ -1098,6 +1636,10 @@ int kvm_arm_timer_set_reg(struct kvm_vcpu *vcpu, u64 regid, u64 value)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1588| <<kvm_arm_timer_read(TIMER_REG_CTL)>> val = read_timer_ctl(timer);
+ */
 static u64 read_timer_ctl(struct arch_timer_context *timer)
 {
 	/*
@@ -1114,6 +1656,11 @@ static u64 read_timer_ctl(struct arch_timer_context *timer)
 	return ctl;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|647| <<get_timer_reg>> val = kvm_arm_timer_get_reg(vcpu, reg->id);
+ *   - arch/arm64/kvm/handle_exit.c|130| <<kvm_handle_wfx>> now = kvm_arm_timer_get_reg(vcpu, KVM_REG_ARM_TIMER_CNT);
+ */
 u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 {
 	switch (regid) {
@@ -1139,6 +1686,17 @@ u64 kvm_arm_timer_get_reg(struct kvm_vcpu *vcpu, u64 regid)
 	return (u64)-1;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1174| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1177| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1180| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1183| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1186| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1189| <<kvm_arm_timer_get_reg>> return kvm_arm_timer_read(vcpu,
+ *   - arch/arm64/kvm/arch_timer.c|1242| <<kvm_arm_timer_read_sysreg>> return kvm_arm_timer_read(vcpu, timer, treg);
+ *   - arch/arm64/kvm/arch_timer.c|1247| <<kvm_arm_timer_read_sysreg>> val = kvm_arm_timer_read(vcpu, timer, treg);
+ */
 static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 			      struct arch_timer_context *timer,
 			      enum kvm_arch_timer_regs treg)
@@ -1160,6 +1718,28 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 		break;
 
 	case TIMER_REG_CNT:
+		/*
+		 * struct kvm:
+		 * -> struct kvm_arch arch;
+		 *    -> struct arch_timer_vm_data timer_data;
+		 *       -> u64 voffset;
+		 *       -> u64 poffset;
+		 *       -> ppi[NR_KVM_TIMERS];
+		 *
+		 * struct kvm_vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct kvm_cpu_context ctxt;
+		 *       -> struct user_pt_regs regs;
+		 *       -> u64 sys_regs[NR_SYS_REGS];
+		 *    -> struct arch_timer_cpu timer_cpu;
+		 *       -> struct arch_timer_context timers[NR_KVM_TIMERS];
+		 *          -> struct kvm_vcpu *vcpu;
+		 *          -> struct arch_timer_offset offset;
+		 *             -> u64 *vm_offset;
+		 *             -> u64 *vcpu_offset;
+		 *       -> struct hrtimer bg_timer;
+		 *       -> bool enabled;
+		 */
 		val = kvm_phys_timer_read() - timer_get_offset(timer);
 		break;
 
@@ -1174,6 +1754,22 @@ static u64 kvm_arm_timer_read(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1206| <<access_arch_timer>> p->regval = kvm_arm_timer_read_sysreg(vcpu, tmr, treg);
+ */
 u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 			      enum kvm_arch_timers tmr,
 			      enum kvm_arch_timer_regs treg)
@@ -1199,6 +1795,15 @@ u64 kvm_arm_timer_read_sysreg(struct kvm_vcpu *vcpu,
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1118| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1129| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1133| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CTL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1144| <<kvm_arm_timer_set_reg>> kvm_arm_timer_write(vcpu, timer, TIMER_REG_CVAL, value);
+ *   - arch/arm64/kvm/arch_timer.c|1294| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ *   - arch/arm64/kvm/arch_timer.c|1299| <<kvm_arm_timer_write_sysreg>> kvm_arm_timer_write(vcpu, timer, treg, val);
+ */
 static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 				struct arch_timer_context *timer,
 				enum kvm_arch_timer_regs treg,
@@ -1226,6 +1831,22 @@ static void kvm_arm_timer_write(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ *
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|1204| <<access_arch_timer>> kvm_arm_timer_write_sysreg(vcpu, tmr, treg, p->regval);
+ */
 void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 				enum kvm_arch_timers tmr,
 				enum kvm_arch_timer_regs treg,
@@ -1249,6 +1870,9 @@ void kvm_arm_timer_write_sysreg(struct kvm_vcpu *vcpu,
 	}
 }
 
+/*
+ * struct irq_chip timer_chip.irq_set_vcpu_affinity = timer_irq_set_vcpu_affinity()
+ */
 static int timer_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu)
 {
 	if (vcpu)
@@ -1259,6 +1883,9 @@ static int timer_irq_set_vcpu_affinity(struct irq_data *d, void *vcpu)
 	return 0;
 }
 
+/*
+ * struct irq_chip timer_chip.irq_set_irqchip_state = timer_irq_set_irqchip_state()
+ */
 static int timer_irq_set_irqchip_state(struct irq_data *d,
 				       enum irqchip_irq_state which, bool val)
 {
@@ -1273,12 +1900,18 @@ static int timer_irq_set_irqchip_state(struct irq_data *d,
 	return 0;
 }
 
+/*
+ * struct irq_chip timer_chip.irq_eoi = timer_irq_eoi()
+ */
 static void timer_irq_eoi(struct irq_data *d)
 {
 	if (!irqd_is_forwarded_to_vcpu(d))
 		irq_chip_eoi_parent(d);
 }
 
+/*
+ * struct irq_chip timer_chip.irq_ack = timer_irq_ack()
+ */
 static void timer_irq_ack(struct irq_data *d)
 {
 	d = d->parent_data;
@@ -1286,6 +1919,10 @@ static void timer_irq_ack(struct irq_data *d)
 		d->chip->irq_ack(d);
 }
 
+/*
+ * 在以下使用timer_chip:
+ *   - arch/arm64/kvm/arch_timer.c|1383| <<timer_irq_domain_alloc>> return irq_domain_set_hwirq_and_chip(domain, virq, hwirq, &timer_chip, NULL);
+ */
 static struct irq_chip timer_chip = {
 	.name			= "KVM",
 	.irq_ack		= timer_irq_ack,
@@ -1311,11 +1948,20 @@ static void timer_irq_domain_free(struct irq_domain *domain, unsigned int virq,
 {
 }
 
+/*
+ * 在以下使用timer_domain_ops:
+ *   - arch/arm64/kvm/arch_timer.c|1853| <<kvm_irq_init>> domain = irq_domain_create_hierarchy(data->domain, 0, NR_KVM_TIMERS, fwnode, &timer_domain_ops, NULL);
+ */
 static const struct irq_domain_ops timer_domain_ops = {
 	.alloc	= timer_irq_domain_alloc,
 	.free	= timer_irq_domain_free,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1839| <<kvm_irq_init>> kvm_irq_fixup_flags(host_vtimer_irq, &host_vtimer_irq_flags);
+ *   - arch/arm64/kvm/arch_timer.c|1866| <<kvm_irq_init>> kvm_irq_fixup_flags(host_ptimer_irq, &host_ptimer_irq_flags);
+ */
 static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 {
 	*flags = irq_get_trigger_type(virq);
@@ -1326,6 +1972,10 @@ static void kvm_irq_fixup_flags(unsigned int virq, u32 *flags)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1906| <<kvm_timer_hyp_init>> err = kvm_irq_init(info);
+ */
 static int kvm_irq_init(struct arch_timer_kvm_info *info)
 {
 	struct irq_domain *domain = NULL;
@@ -1374,11 +2024,28 @@ static int kvm_irq_init(struct arch_timer_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|2044| <<init_subsystems>> err = kvm_timer_hyp_init(vgic_present);
+ */
 int __init kvm_timer_hyp_init(bool has_gic)
 {
 	struct arch_timer_kvm_info *info;
 	int err;
 
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 *
+	 * 在以下使用arch_timer_kvm_info:
+	 *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+	 *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+	 *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+	 *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+	 */
 	info = arch_timer_get_kvm_info();
 	timecounter = &info->timecounter;
 
@@ -1452,6 +2119,10 @@ int __init kvm_timer_hyp_init(bool has_gic)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|407| <<kvm_arch_vcpu_destroy>> kvm_timer_vcpu_terminate(vcpu);
+ */
 void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1459,6 +2130,10 @@ void kvm_timer_vcpu_terminate(struct kvm_vcpu *vcpu)
 	soft_timer_cancel(&timer->bg_timer);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|2053| <<kvm_timer_enable>> if (!timer_irqs_are_valid(vcpu)) {
+ */
 static bool timer_irqs_are_valid(struct kvm_vcpu *vcpu)
 {
 	u32 ppis = 0;
@@ -1492,6 +2167,9 @@ static bool timer_irqs_are_valid(struct kvm_vcpu *vcpu)
 	return valid;
 }
 
+/*
+ * struct irq_ops arch_timer_irq_ops..get_input_level = kvm_arch_timer_get_input_level,
+ */
 static bool kvm_arch_timer_get_input_level(int vintid)
 {
 	struct kvm_vcpu *vcpu = kvm_get_running_vcpu();
@@ -1513,6 +2191,10 @@ static bool kvm_arch_timer_get_input_level(int vintid)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|615| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_timer_enable(vcpu);
+ */
 int kvm_timer_enable(struct kvm_vcpu *vcpu)
 {
 	struct arch_timer_cpu *timer = vcpu_timer(vcpu);
@@ -1537,6 +2219,17 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 
 	get_timer_map(vcpu, &map);
 
+	/*
+	 * 在以下使用arch_timer_irq_ops:
+	 *   - arch/arm64/kvm/arch_timer.c|800| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq,
+	 *                                                                                      timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|805| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq,
+	 *                                                                                      timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|1437| <<kvm_irq_init>> arch_timer_irq_ops.flags |= VGIC_IRQ_SW_RESAMPLE;
+	 *   - arch/arm64/kvm/arch_timer.c|1624| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+	 *   - arch/arm64/kvm/arch_timer.c|1632| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+	 */
+
 	ret = kvm_vgic_map_phys_irq(vcpu,
 				    map.direct_vtimer->host_timer_irq,
 				    timer_irq(map.direct_vtimer),
@@ -1559,6 +2252,10 @@ int kvm_timer_enable(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1864| <<cpu_hyp_init_features>> kvm_timer_init_vhe();
+ */
 /* If we have CNTPOFF, permanently set ECV to enable it */
 void kvm_timer_init_vhe(void)
 {
@@ -1658,6 +2355,10 @@ int kvm_arm_timer_has_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *attr)
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1619| <<kvm_arch_vm_ioctl(KVM_ARM_SET_COUNTER_OFFSET)>> return kvm_vm_ioctl_set_counter_offset(kvm, &offset);
+ */
 int kvm_vm_ioctl_set_counter_offset(struct kvm *kvm,
 				    struct kvm_arm_counter_offset *offset)
 {
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index d1cb298a5..b30ec87af 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -704,6 +704,12 @@ static void kvm_vcpu_sleep(struct kvm_vcpu *vcpu)
  * the vCPU is runnable.  The vCPU may or may not be scheduled out, depending
  * on when a wake event arrives, e.g. there may already be a pending wake event.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|739| <<kvm_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/handle_exit.c|147| <<kvm_handle_wfx>> kvm_vcpu_wfi(vcpu);
+ *   - arch/arm64/kvm/psci.c|49| <<kvm_psci_vcpu_suspend>> kvm_vcpu_wfi(vcpu);
+ */
 void kvm_vcpu_wfi(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -1653,6 +1659,16 @@ void unlock_all_vcpus(struct kvm *kvm)
 	unlock_vcpus(kvm, atomic_read(&kvm->online_vcpus) - 1);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1756| <<kvm_vm_ioctl_set_counter_offset>> if (lock_all_vcpus(kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-init.c|91| <<kvm_vgic_create>> if (!lock_all_vcpus(kvm))
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2064| <<vgic_its_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2772| <<vgic_its_ctrl>> if (!lock_all_vcpus(kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|265| <<vgic_set_common_attr>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|385| <<vgic_v2_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|546| <<vgic_v3_attr_regs_access>> if (!lock_all_vcpus(dev->kvm)) {
+ */
 /* Returns true if all vcpus were locked, false otherwise */
 bool lock_all_vcpus(struct kvm *kvm)
 {
@@ -2360,6 +2376,12 @@ static int __init init_hyp_mode(void)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/psci.c|72| <<kvm_psci_vcpu_on>> vcpu = kvm_mpidr_to_vcpu(kvm, cpu_id);
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|491| <<vgic_v3_parse_attr>> reg_attr->vcpu = kvm_mpidr_to_vcpu(dev->kvm, mpidr_reg);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|232| <<vgic_mmio_write_irouter>> irq->target_vcpu = kvm_mpidr_to_vcpu(vcpu->kvm, irq->mpidr);
+ */
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr)
 {
 	struct kvm_vcpu *vcpu;
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 20280a523..3eaf2d3d9 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -759,6 +759,10 @@ int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)
 	return kvm_arm_copy_sys_reg_indices(vcpu, uindices);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1443| <<kvm_arch_vcpu_ioctl(KVM_GET_ONE_REG)>> r = kvm_arm_get_reg(vcpu, &reg);
+ */
 int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
 {
 	/* We currently use nothing arch-specific in upper 32 bits */
diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 6dcd6604b..6d6cde176 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -226,6 +226,10 @@ static int kvm_handle_eret(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用arm_exit_handlers[]:
+ *   - arch/arm64/kvm/handle_exit.c|261| <<kvm_get_exit_handler>> return arm_exit_handlers[esr_ec];
+ */
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -253,6 +257,10 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|284| <<handle_trap_exceptions>> exit_handler = kvm_get_exit_handler(vcpu);
+ */
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u64 esr = kvm_vcpu_get_esr(vcpu);
@@ -267,6 +275,10 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
  * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
  * emulation first.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/handle_exit.c|315| <<handle_exit>> return handle_trap_exceptions(vcpu);
+ */
 static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
 {
 	int handled;
@@ -292,6 +304,10 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu)
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1065| <<kvm_arch_vcpu_ioctl_run>> ret = handle_exit(vcpu, ret);
+ */
 int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 {
 	struct kvm_run *run = vcpu->run;
@@ -335,6 +351,10 @@ int handle_exit(struct kvm_vcpu *vcpu, int exception_index)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1042| <<kvm_arch_vcpu_ioctl_run>> handle_exit_early(vcpu, ret);
+ */
 /* For exit types that need handling before we can be preempted */
 void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 {
@@ -356,6 +376,10 @@ void handle_exit_early(struct kvm_vcpu *vcpu, int exception_index)
 		kvm_handle_guest_serror(vcpu, kvm_vcpu_get_esr(vcpu));
 }
 
+/*
+ * 在以下使用nvhe_hyp_panic_handler():
+ *   - arch/arm64/kernel/image-vars.h|62| <<global>> KVM_NVHE_ALIAS(nvhe_hyp_panic_handler);
+ */
 void __noreturn __cold nvhe_hyp_panic_handler(u64 esr, u64 spsr,
 					      u64 elr_virt, u64 elr_phys,
 					      u64 par, uintptr_t vcpu,
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index f7a93ef29..e173e4136 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -990,6 +990,15 @@ static int stage2_map_walker(const struct kvm_pgtable_visit_ctx *ctx,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|413| <<__host_stage2_idmap>> return kvm_pgtable_stage2_map(&host_mmu.pgt, start, end - start, start,
+ *   - arch/arm64/kvm/mmu.c|1026| <<kvm_phys_addr_ioremap>> ret = kvm_pgtable_stage2_map(pgt, addr, PAGE_SIZE, pa, prot,
+ *   - arch/arm64/kvm/mmu.c|1530| <<user_mem_abort>> ret = kvm_pgtable_stage2_map(pgt, fault_ipa, vma_pagesize,
+ *   - arch/arm64/kvm/mmu.c|1754| <<kvm_set_spte_gfn>> kvm_pgtable_stage2_map(kvm->arch.mmu.pgt, range->start << PAGE_SHIFT,
+ *
+ * kvm_pgtable_stage2_map() - Install a mapping in a guest stage-2 page-table.
+ */
 int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			   u64 phys, enum kvm_pgtable_prot prot,
 			   void *mc, enum kvm_pgtable_walk_flags flags)
diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
index 6cb638b18..355c25814 100644
--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
+++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
@@ -220,6 +220,22 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 
 		write_gicreg(cpu_if->vgic_hcr & ~ICH_HCR_EN, ICH_HCR_EL2);
 
+		/*
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct vgic_cpu vgic_cpu;
+		 *       -> struct vgic_v3_cpu_if vgic_v3;
+		 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+		 *
+		 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+		 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+		 */
 		for (i = 0; i < used_lrs; i++) {
 			if (elrsr & (1 << i))
 				cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
@@ -231,6 +247,11 @@ void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|129| <<__hyp_vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ *   - arch/arm64/kvm/vgic/vgic.c|969| <<vgic_restore_state>> __vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
+ */
 void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 {
 	u64 used_lrs = cpu_if->used_lrs;
@@ -239,6 +260,25 @@ void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if)
 	if (used_lrs || cpu_if->its_vpe.its_vm) {
 		write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
 
+		/*
+		 * struct kvm_vcpu *vcpu:
+		 * -> struct kvm_vcpu_arch arch;
+		 *    -> struct vgic_cpu vgic_cpu;
+		 *       -> struct vgic_v3_cpu_if vgic_v3;
+		 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+		 *
+		 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+		 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+		 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+		 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+		 *
+		 * ICH_LR<n>_EL2, Interrupt Controller List Registers
+		 * Provides interrupt context information for the virtual CPU interface.
+		 */
 		for (i = 0; i < used_lrs; i++)
 			__gic_v3_set_lr(cpu_if->vgic_lr[i], i);
 	}
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c
index d3b4feed4..dcf53eeb3 100644
--- a/arch/arm64/kvm/mmu.c
+++ b/arch/arm64/kvm/mmu.c
@@ -1574,6 +1574,11 @@ static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)
  * space. The distinction is based on the IPA causing the fault and whether this
  * memory region has been registered as standard RAM by user space.
  */
+/*
+ * 在以下使用kvm_handle_guest_abort():
+ *   - arch/arm64/kvm/handle_exit.c|245| <<global>> [ESR_ELx_EC_IABT_LOW] = kvm_handle_guest_abort,
+ *   - arch/arm64/kvm/handle_exit.c|246| <<global>> [ESR_ELx_EC_DABT_LOW] = kvm_handle_guest_abort,
+ */
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu)
 {
 	unsigned long fault_status;
diff --git a/arch/arm64/kvm/pvtime.c b/arch/arm64/kvm/pvtime.c
index 4ceabaa4c..df44b075e 100644
--- a/arch/arm64/kvm/pvtime.c
+++ b/arch/arm64/kvm/pvtime.c
@@ -10,6 +10,14 @@
 
 #include <kvm/arm_hypercalls.h>
 
+/*
+ * 在以下使用KVM_REQ_RECORD_STEAL:
+ *   - arch/arm64/kvm/arm.c|454| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_RECORD_STEAL, vcpu);
+ *   - arch/arm64/kvm/arm.c|791| <<check_vcpu_requests>> if (kvm_check_request(KVM_REQ_RECORD_STEAL, vcpu))
+ *
+ * 处理KVM_REQ_RECORD_STEAL:
+ *   - arch/arm64/kvm/arm.c|792| <<check_vcpu_requests>> kvm_update_stolen_time(vcpu);
+ */
 void kvm_update_stolen_time(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -32,6 +40,10 @@ void kvm_update_stolen_time(struct kvm_vcpu *vcpu)
 	srcu_read_unlock(&kvm->srcu, idx);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hypercalls.c|338| <<kvm_smccc_call_handler>> val[0] = kvm_hypercall_pv_features(vcpu);
+ */
 long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu)
 {
 	u32 feature = smccc_get_arg1(vcpu);
@@ -48,6 +60,10 @@ long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/hypercalls.c|341| <<kvm_smccc_call_handler>> gpa = kvm_init_stolen_time(vcpu);
+ */
 gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu)
 {
 	struct pvclock_vcpu_stolen_time init_values = {};
@@ -67,11 +83,22 @@ gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu)
 	return base;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|283| <<kvm_vm_ioctl_check_extension>> r = kvm_arm_pvtime_supported();
+ *   - arch/arm64/kvm/pvtime.c|84| <<kvm_arm_pvtime_set_attr>> if (!kvm_arm_pvtime_supported() ||
+ *   - arch/arm64/kvm/pvtime.c|113| <<kvm_arm_pvtime_get_attr>> if (!kvm_arm_pvtime_supported() ||
+ *   - arch/arm64/kvm/pvtime.c|129| <<kvm_arm_pvtime_has_attr>> if (kvm_arm_pvtime_supported())
+ */
 bool kvm_arm_pvtime_supported(void)
 {
 	return !!sched_info_on();
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|973| <<kvm_arm_vcpu_arch_set_attr>> ret = kvm_arm_pvtime_set_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
@@ -104,6 +131,10 @@ int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|996| <<kvm_arm_vcpu_arch_get_attr>> ret = kvm_arm_pvtime_get_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
@@ -121,6 +152,10 @@ int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/guest.c|1019| <<kvm_arm_vcpu_arch_has_attr>> ret = kvm_arm_pvtime_has_attr(vcpu, attr);
+ */
 int kvm_arm_pvtime_has_attr(struct kvm_vcpu *vcpu,
 			    struct kvm_device_attr *attr)
 {
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 2ca2973ab..0d066b8a4 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -295,6 +295,15 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
  * The cp15_64 code makes sure this automatically works
  * for both AArch64 and AArch32 accesses.
  */
+/*
+ * 在以下使用access_gic_sgi():
+ *   - arch/arm64/kvm/sys_regs.c|2156| <<global>> { SYS_DESC(SYS_ICC_SGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2157| <<global>> { SYS_DESC(SYS_ICC_ASGI1R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2158| <<global>> { SYS_DESC(SYS_ICC_SGI0R_EL1), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2749| <<global>> { Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2752| <<global>> { Op1( 1), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ *   - arch/arm64/kvm/sys_regs.c|2753| <<global>> { Op1( 2), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },
+ */
 static bool access_gic_sgi(struct kvm_vcpu *vcpu,
 			   struct sys_reg_params *p,
 			   const struct sys_reg_desc *r)
@@ -1164,6 +1173,19 @@ static unsigned int ptrauth_visibility(const struct kvm_vcpu *vcpu,
 	__PTRAUTH_KEY(k ## KEYLO_EL1),					\
 	__PTRAUTH_KEY(k ## KEYHI_EL1)
 
+/*
+ * 在以下使用access_arch_timer():
+ *   - arch/arm64/kvm/sys_regs.c|2283| <<global>> { SYS_DESC(SYS_CNTPCT_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2284| <<global>> { SYS_DESC(SYS_CNTPCTSS_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2285| <<global>> { SYS_DESC(SYS_CNTP_TVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2286| <<global>> { SYS_DESC(SYS_CNTP_CTL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2287| <<global>> { SYS_DESC(SYS_CNTP_CVAL_EL0), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2654| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_TVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2655| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CTL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2737| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCT), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2741| <<global>> { SYS_DESC(SYS_AARCH32_CNTP_CVAL), access_arch_timer },
+ *   - arch/arm64/kvm/sys_regs.c|2742| <<global>> { SYS_DESC(SYS_AARCH32_CNTPCTSS), access_arch_timer },
+ */
 static bool access_arch_timer(struct kvm_vcpu *vcpu,
 			      struct sys_reg_params *p,
 			      const struct sys_reg_desc *r)
diff --git a/arch/arm64/kvm/vgic-sys-reg-v3.c b/arch/arm64/kvm/vgic-sys-reg-v3.c
index 9e7c486b4..499bd5544 100644
--- a/arch/arm64/kvm/vgic-sys-reg-v3.c
+++ b/arch/arm64/kvm/vgic-sys-reg-v3.c
@@ -10,6 +10,10 @@
 #include "vgic/vgic.h"
 #include "sys_regs.h"
 
+/*
+ * ICC_CTLR_EL1 controls aspects of the behavior of the GIC CPU interface and
+ * provides information about the features implemented.
+ */
 static int set_gic_ctlr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 			u64 val)
 {
@@ -297,6 +301,15 @@ static int get_gic_sre(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 	return 0;
 }
 
+/*
+ * 在以下使用gic_v3_icc_reg_descs[]:
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|344| <<vgic_v3_has_cpu_sysregs_attr>> if (get_reg_by_id(attr_to_id(attr->attr), gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|345| <<vgic_v3_has_cpu_sysregs_attr>> ARRAY_SIZE(gic_v3_icc_reg_descs)))
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|361| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_set_user(vcpu, &reg, gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|362| <<vgic_v3_cpu_sysregs_uaccess>> ARRAY_SIZE(gic_v3_icc_reg_descs));
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|364| <<vgic_v3_cpu_sysregs_uaccess>> return kvm_sys_reg_get_user(vcpu, &reg, gic_v3_icc_reg_descs,
+ *   - arch/arm64/kvm/vgic-sys-reg-v3.c|365| <<vgic_v3_cpu_sysregs_uaccess>> ARRAY_SIZE(gic_v3_icc_reg_descs));
+ */
 static const struct sys_reg_desc gic_v3_icc_reg_descs[] = {
 	{ SYS_DESC(SYS_ICC_PMR_EL1),
 	  .set_user = set_gic_pmr, .get_user = get_gic_pmr, },
@@ -348,6 +361,10 @@ int vgic_v3_has_cpu_sysregs_attr(struct kvm_vcpu *vcpu, struct kvm_device_attr *
 	return -ENXIO;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|566| <<vgic_v3_attr_regs_access>> ret = vgic_v3_cpu_sysregs_uaccess(vcpu, attr, is_write);
+ */
 int vgic_v3_cpu_sysregs_uaccess(struct kvm_vcpu *vcpu,
 				struct kvm_device_attr *attr,
 				bool is_write)
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index c8c3cb812..7b55305b5 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -580,6 +580,15 @@ int kvm_vgic_hyp_init(void)
 		ret = vgic_v2_probe(gic_kvm_info);
 		break;
 	case GIC_V3:
+		/*
+		 * 例子.
+		 * [   14.829157] kvm [1]: IPA Size Limit: 48 bits
+		 * [   14.833671] kvm [1]: GICv3: no GICV resource entry
+		 * [   14.838458] kvm [1]: disabling GICv2 emulation
+		 * [   14.842929] kvm [1]: GIC system register CPU interface enabled
+		 * [   14.848777] kvm [1]: vgic interrupt IRQ9
+		 * [   14.852918] kvm [1]: VHE mode initialized successfully
+		 */
 		ret = vgic_v3_probe(gic_kvm_info);
 		if (!ret) {
 			static_branch_enable(&kvm_vgic_global_state.gicv3_cpuif);
diff --git a/arch/arm64/kvm/vgic/vgic-mmio-v3.c b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
index 188d2187e..36eeeb17d 100644
--- a/arch/arm64/kvm/vgic/vgic-mmio-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-mmio-v3.c
@@ -1066,6 +1066,10 @@ static int match_mpidr(u64 sgi_aff, u16 sgi_cpu_mask, struct kvm_vcpu *vcpu)
  * check for matching ones. If this bit is set, we signal all, but not the
  * calling VCPU.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/sys_regs.c|338| <<access_gic_sgi>> vgic_v3_dispatch_sgi(vcpu, p->regval, g1);
+ */
 void vgic_v3_dispatch_sgi(struct kvm_vcpu *vcpu, u64 reg, bool allow_group1)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/arm64/kvm/vgic/vgic-v3.c b/arch/arm64/kvm/vgic/vgic-v3.c
index 3dfc8b84e..cd5a1896b 100644
--- a/arch/arm64/kvm/vgic/vgic-v3.c
+++ b/arch/arm64/kvm/vgic/vgic-v3.c
@@ -32,6 +32,10 @@ static bool lr_signals_eoi_mi(u64 lr_val)
 	       !(lr_val & ICH_LR_HW);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|899| <<vgic_fold_lr_state>> vgic_v3_fold_lr_state(vcpu);
+ */
 void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -103,6 +107,16 @@ void vgic_v3_fold_lr_state(struct kvm_vcpu *vcpu)
 	cpuif->used_lrs = 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|819| <<vgic_populate_lr>> vgic_v3_populate_lr(vcpu, irq, lr);
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> kvm_vgic_flush_hwstate()
+ *    -> vgic_flush_lr_state()
+ *       -> vgic_populate_lr()
+ *          -> vgic_v3_populate_lr()
+ */
 /* Requires the irq to be locked already */
 void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 {
@@ -183,6 +197,22 @@ void vgic_v3_populate_lr(struct kvm_vcpu *vcpu, struct vgic_irq *irq, int lr)
 
 	val |= (u64)irq->priority << ICH_LR_PRIORITY_SHIFT;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> struct vgic_cpu vgic_cpu;
+	 *       -> struct vgic_v3_cpu_if vgic_v3;
+	 *          -> u64 vgic_lr[VGIC_V3_MAX_LRS];
+	 *
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
 }
 
@@ -657,6 +687,11 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 
 	kvm_vgic_global_state.vcpu_base = 0;
 
+	/*
+	 * struct gic_kvm_info *info:
+	 * -> struct resource vcpu;
+	 *    -> resource_size_t start;
+	 */
 	if (!info->vcpu.start) {
 		kvm_info("GICv3: no GICV resource entry\n");
 	} else if (!has_v2) {
@@ -717,6 +752,10 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1127| <<kvm_vgic_load>> vgic_v3_load(vcpu);
+ */
 void vgic_v3_load(struct kvm_vcpu *vcpu)
 {
 	struct vgic_v3_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v3;
diff --git a/arch/arm64/kvm/vgic/vgic.c b/arch/arm64/kvm/vgic/vgic.c
index 8be4c1ebd..50032f3db 100644
--- a/arch/arm64/kvm/vgic/vgic.c
+++ b/arch/arm64/kvm/vgic/vgic.c
@@ -17,6 +17,74 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * SPI
+ * PPI
+ * SGI
+ * LPI
+ *
+ * GIC(Generic Interrupt Controller)是ARM公司提供的中断控制器统一架构,它详细定义了ARM平台上,
+ * 中断控制器的内部分发逻辑(Distributor)和CPU接口(CPU Interface).
+ *
+ * 目前GICv3引入了对ITS(Interrupt Translation Service)的支持,而GICv4则引入了
+ * LPIs(Locality-specific Peripheral Interrupts)中断的透传功能.
+ *
+ * 从GICv2开始,支持中断虚拟化硬件扩展.vGIC为每个CPU引入了一个vGIC CPU接口和相应的hypervisor控制接口,
+ * 虚拟机可以被配置成直接使用vGIC CPU接口.当非安全物理中断发送给某个CPU CORE后,可以触发该CPU CORE
+ * 陷入到EL2模式,并允许VMM中的中断处理程序,中断处理程序会根据该物理中断对应的VMID等信息,
+ * 通过List Registers,配置并发送虚拟中断到vGIC CPU接口,并路由到相应的vCPU CORE上.
+ */
+/*
+ * https://www.zhihu.com/column/c_1520029500636696576
+ *
+ * 1. 外设触发irq中断,gic的distributor接收到该中断后,通过distributor的仲裁器确定其应该被发送的cpu.
+ *
+ * 2. 在target cpu确定以后,将中断发送给该cpu相关的redistributor
+ *
+ * 3. redistributor将中断发送到其对应的cpu interface上
+ *
+ * 4. 该中断被路由到EL2的hypervisor中断处理入口
+ *
+ * 5. hypervisor退出到host,然后重新使能中断,使中断再次触发,并由host中断处理函数执行中断处理
+ *
+ * 6. host通过写lr的方式向guest注入虚拟中断,并切入guest执行
+ *
+ * 7. 虚拟中断通过virq触发,并进入guest的中断入口函数
+ *
+ * 8. guest执行中断处理流程,并通过虚拟cpu interface执行中断应答以及EOI等操作
+ *
+ * 9. 若该虚拟中断与物理中断关联,则EOI操作将会作用到实际的物理中断上
+ *
+ * 10. 中断处理完成
+ *
+ *
+ * With GICv3, the GIC forwards a packet to the core describing the interrupt
+ * (ID, Group, Priority).  When, within the interrupt handler, software reads
+ * ICC_IARx_EL1 to acknowledge the interrupt, the core already has all the
+ * information it needs to return the interrupt ID (INTID).
+ *
+ * With GICv2, the equivalent step requires a read of an MMIO register.  That
+ * read has to get from the core to the GIC, and then back, before software knows
+ * the INTID.  How long will that take?  It depends on the specific system design
+ * and can also depend on how much contention there is on the memory system at
+ * the time.
+ *
+ * Another advantage is to software - in the interrupt handling path you don't
+ * need to know the address of the interrupt controller to perform the
+ * acknowledge.
+ *
+ * 在gicv2中,cpu interface的寄存器,是实现在gic内部的,因此当core收到一个中断时,
+ * 会通过axi总线(假设memory总线是axi总线,去访问cpu interface的寄存器.而中断在
+ * 一个soc系统中,是会频繁的产生的,这就意味着,core会频繁的去访问gic的寄存器,这样
+ * 会占用axi总线的带宽,总而会影响中断的实时响应.而且core通过axi总线去访问cpu
+ * interface寄存器,延迟,也比较大.
+ *
+ * 在gicv3中,将cpu interface从gic中抽离出来,实现在core内部,而不实现在gic中.
+ * core对cpu interface的访问,通过系统寄存器方式访问,也就是使用msr,mrs访问,那么core对
+ * cpu interface的寄存器访问,就加速了,而且还不占用axi总线带宽.这样core对中断的处理,
+ * 就加速了。
+ */
+
 struct vgic_global kvm_vgic_global_state __ro_after_init = {
 	.gicv3_cpuif = STATIC_KEY_FALSE_INIT,
 };
@@ -149,6 +217,11 @@ void vgic_put_irq(struct kvm *kvm, struct vgic_irq *irq)
 	raw_spin_unlock_irqrestore(&dist->lpi_list_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-init.c|379| <<kvm_vgic_vcpu_destroy>> vgic_flush_pending_lpis(vcpu);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|279| <<vgic_mmio_write_v3r_ctlr>> vgic_flush_pending_lpis(vcpu);
+ */
 void vgic_flush_pending_lpis(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
@@ -172,6 +245,18 @@ void vgic_flush_pending_lpis(struct kvm_vcpu *vcpu)
 
 void vgic_irq_set_phys_pending(struct vgic_irq *irq, bool pending)
 {
+	/*
+	 * irq_set_irqchip_state - set the state of a forwarded interrupt.
+	 * @irq: Interrupt line that is forwarded to a VM
+	 * @which: State to be restored (one of IRQCHIP_STATE_*)
+	 * @val: Value corresponding to @which
+	 *
+	 * This call sets the internal irqchip state of an interrupt,
+	 * depending on the value of @which.
+	 *
+	 * This function should be called with migration disabled if the
+	 * interrupt controller has per-cpu registers.
+	 */
 	WARN_ON(irq_set_irqchip_state(irq->host_irq,
 				      IRQCHIP_STATE_PENDING,
 				      pending));
@@ -213,6 +298,14 @@ void vgic_irq_set_phys_active(struct vgic_irq *irq, bool active)
  *
  * Requires the IRQ lock to be held.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|407| <<vgic_queue_irq_unlock>> vcpu = vgic_target_oracle(irq);
+ *   - arch/arm64/kvm/vgic/vgic.c|459| <<vgic_queue_irq_unlock>> if (unlikely(irq->vcpu || vcpu != vgic_target_oracle(irq))) {
+ *   - arch/arm64/kvm/vgic/vgic.c|742| <<vgic_prune_ap_list>> target_vcpu = vgic_target_oracle(irq);
+ *   - arch/arm64/kvm/vgic/vgic.c|801| <<vgic_prune_ap_list>> if (target_vcpu == vgic_target_oracle(irq)) {
+ *   - arch/arm64/kvm/vgic/vgic.c|928| <<vgic_flush_lr_state>> if (likely(vgic_target_oracle(irq) == vcpu)) {
+ */
 static struct kvm_vcpu *vgic_target_oracle(struct vgic_irq *irq)
 {
 	lockdep_assert_held(&irq->irq_lock);
@@ -333,6 +426,27 @@ static bool vgic_validate_injection(struct vgic_irq *irq, bool level, void *owne
  * Needs to be entered with the IRQ lock already held, but will return
  * with all locks dropped.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic-its.c|301| <<update_lpi_config>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|467| <<its_sync_lpi_pending_table>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|747| <<vgic_its_trigger_msi>> vgic_queue_irq_unlock(kvm, irq, flags); 
+ *   - arch/arm64/kvm/vgic/vgic-its.c|765| <<vgic_its_inject_cached_translation>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|157| <<vgic_mmio_write_sgir>> vgic_queue_irq_unlock(source_vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v2.c|264| <<vgic_mmio_write_sgipends>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|375| <<vgic_v3_uaccess_write_pending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|1127| <<vgic_v3_dispatch_sgi>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|85| <<vgic_mmio_write_group>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|159| <<vgic_mmio_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|200| <<vgic_uaccess_write_senable>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|341| <<vgic_mmio_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|368| <<vgic_uaccess_write_spending>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|603| <<vgic_mmio_change_active>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-mmio.c|828| <<vgic_write_irq_line_level_info>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v3.c|358| <<vgic_v3_lpi_sync_pending_status>> vgic_queue_irq_unlock(vcpu->kvm, irq, flags);
+ *   - arch/arm64/kvm/vgic/vgic-v4.c|476| <<kvm_vgic_v4_set_forwarding>> vgic_queue_irq_unlock(kvm, irq, flags);
+ *    - arch/arm64/kvm/vgic/vgic.c|552| <<kvm_vgic_inject_irq>> vgic_queue_irq_unlock(kvm, irq, flags);
+ */
 bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 			   unsigned long flags)
 {
@@ -402,6 +516,24 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
 		goto retry;
 	}
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
+
 	/*
 	 * Grab a reference to the irq to reflect the fact that it is
 	 * now in the ap_list.
@@ -436,6 +568,14 @@ bool vgic_queue_irq_unlock(struct kvm *kvm, struct vgic_irq *irq,
  * level-sensitive interrupts.  You can think of the level parameter as 1
  * being HIGH and 0 being LOW and all devices being active-HIGH.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|497| <<kvm_timer_update_irq>> ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/arm.c|1172| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, vcpu->vcpu_id, irq_num, level, NULL);
+ *   - arch/arm64/kvm/arm.c|1180| <<kvm_vm_ioctl_irq_line>> return kvm_vgic_inject_irq(kvm, 0, irq_num, level, NULL);
+ *   - arch/arm64/kvm/pmu-emul.c|346| <<kvm_pmu_update_state>> int ret = kvm_vgic_inject_irq(vcpu->kvm, vcpu->vcpu_id,
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|26| <<vgic_irqfd_set_irq>> return kvm_vgic_inject_irq(kvm, 0, spi_id, level, NULL);
+ */
 int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 			bool level, void *owner)
 {
@@ -454,6 +594,12 @@ int kvm_vgic_inject_irq(struct kvm *kvm, int cpuid, unsigned int intid,
 	if (!vcpu && intid < VGIC_NR_PRIVATE_IRQS)
 		return -EINVAL;
 
+	/*
+	 * 注释
+	 * This looks up the virtual interrupt ID to get the corresponding
+	 * struct vgic_irq. It also increases the refcount, so any caller is expected
+	 * to call vgic_put_irq() once it's finished with this IRQ.
+	 */
 	irq = vgic_get_irq(kvm, vcpu, intid);
 	if (!irq)
 		return -EINVAL;
@@ -513,6 +659,13 @@ static inline void kvm_vgic_unmap_irq(struct vgic_irq *irq)
 	irq->ops = NULL;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|797| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_vtimer->host_timer_irq, timer_irq(map->direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|802| <<kvm_timer_vcpu_load_nested_switch>> ret = kvm_vgic_map_phys_irq(vcpu, map->direct_ptimer->host_timer_irq, timer_irq(map->direct_ptimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1621| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_vtimer->host_timer_irq, timer_irq(map.direct_vtimer), &arch_timer_irq_ops);
+ *   - arch/arm64/kvm/arch_timer.c|1629| <<kvm_timer_enable>> ret = kvm_vgic_map_phys_irq(vcpu, map.direct_ptimer->host_timer_irq, timer_irq(map.direct_ptimer), &arch_timer_irq_ops);
+ */
 int kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, unsigned int host_irq,
 			  u32 vintid, struct irq_ops *ops)
 {
@@ -539,6 +692,11 @@ int kvm_vgic_map_phys_irq(struct kvm_vcpu *vcpu, unsigned int host_irq,
  * subsystems injecting mapped interrupts should reset their interrupt lines
  * when we are doing a reset of the VM.
  */
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|1402| <<kvm_timer_vcpu_reset>> kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_vtimer));
+ *   - arch/arm64/kvm/arch_timer.c|1404| <<kvm_timer_vcpu_reset>> kvm_vgic_reset_mapped_irq(vcpu, timer_irq(map.direct_ptimer));
+ */
 void kvm_vgic_reset_mapped_irq(struct kvm_vcpu *vcpu, u32 vintid)
 {
 	struct vgic_irq *irq = vgic_get_irq(vcpu->kvm, vcpu, vintid);
@@ -742,6 +900,10 @@ static inline void vgic_fold_lr_state(struct kvm_vcpu *vcpu)
 }
 
 /* Requires the irq_lock to be held. */
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|903| <<vgic_flush_lr_state>> vgic_populate_lr(vcpu, irq, count++);
+ */
 static inline void vgic_populate_lr(struct kvm_vcpu *vcpu,
 				    struct vgic_irq *irq, int lr)
 {
@@ -795,6 +957,10 @@ static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
 	return count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1004| <<kvm_vgic_flush_hwstate>> vgic_flush_lr_state(vcpu);
+ */
 /* Requires the VCPU's ap_list_lock to be held. */
 static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
@@ -855,6 +1021,11 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 		vcpu->arch.vgic_cpu.vgic_v3.used_lrs = count;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|956| <<kvm_vgic_sync_hwstate>> if (can_access_vgic_from_kernel())
+ *   - arch/arm64/kvm/vgic/vgic.c|1008| <<kvm_vgic_flush_hwstate>> if (can_access_vgic_from_kernel())
+ */
 static inline bool can_access_vgic_from_kernel(void)
 {
 	/*
@@ -895,6 +1066,10 @@ void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 	vgic_prune_ap_list(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/vgic/vgic.c|1009| <<kvm_vgic_flush_hwstate>> vgic_restore_state(vcpu);
+ */
 static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 {
 	if (!static_branch_unlikely(&kvm_vgic_global_state.gicv3_cpuif))
@@ -903,6 +1078,10 @@ static inline void vgic_restore_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_restore_state(&vcpu->arch.vgic_cpu.vgic_v3);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|952| <<kvm_arch_vcpu_ioctl_run>> kvm_vgic_flush_hwstate(vcpu);
+ */
 /* Flush our emulation state into the GIC hardware before entering the guest. */
 void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 {
@@ -937,6 +1116,10 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 		vgic_v4_commit(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|447| <<kvm_arch_vcpu_load>> kvm_vgic_load(vcpu);
+ */
 void kvm_vgic_load(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!vgic_initialized(vcpu->kvm)))
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3bc146dfd..297542cc4 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -80,7 +80,28 @@
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+ *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *
+ * 处理函数是kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+ *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理函数是kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
@@ -536,7 +557,23 @@ struct kvm_pmu {
 	 * filter changes.
 	 */
 	union {
+		/*
+		 * 在以下使用kvm_pmu->reprogram_pmi:
+		 *   - arch/x86/include/asm/kvm_host.h|539| <<global>> DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+		 *   - arch/x86/kvm/pmu.c|155| <<kvm_perf_overflow>> if (test_and_set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi))
+		 *   - arch/x86/kvm/pmu.c|455| <<reprogram_counter>> clear_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|464| <<kvm_pmu_handle_event>> for_each_set_bit(bit, pmu->reprogram_pmi, X86_PMC_IDX_MAX) {
+		 *   - arch/x86/kvm/pmu.c|468| <<kvm_pmu_handle_event>> clear_bit(bit, pmu->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.c|909| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) >
+		 *   - arch/x86/kvm/pmu.h|219| <<kvm_pmu_request_counter_reprogram>> set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+		 *   - arch/x86/kvm/pmu.h|231| <<reprogram_counters>> set_bit(bit, pmu->reprogram_pmi);
+		 */
 		DECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);
+		/*
+		 * 在以下使用kvm_pmu->_reprogram_pmi:
+		 *   - arch/x86/kvm/pmu.c|910| <<kvm_vm_ioctl_set_pmu_event_filter>> BUILD_BUG_ON(sizeof(((struct kvm_pmu *)0)->reprogram_pmi) > sizeof(((struct kvm_pmu *)0)->__reprogram_pmi));
+		 *   - arch/x86/kvm/pmu.c|913| <<kvm_vm_ioctl_set_pmu_event_filter>> atomic64_set(&vcpu_to_pmu(vcpu)->__reprogram_pmi, -1ull);
+		 */
 		atomic64_t __reprogram_pmi;
 	};
 	DECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index a983a1616..fbd98ea0d 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -1853,6 +1853,12 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1882| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|1897| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2859| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1866,6 +1872,14 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1935| <<start_sw_tscdeadline>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2047| <<start_sw_period>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2108| <<start_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2157| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic, false);
+ *   - arch/x86/kvm/lapic.c|2770| <<apic_timer_fn>> apic_timer_expired(apic, true);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1966,6 +1980,10 @@ static void update_target_expiration(struct kvm_lapic *apic, uint32_t old_diviso
 	apic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2198| <<__start_apic_timer>> && !set_target_expiration(apic, count_reg))
+ */
 static bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)
 {
 	ktime_t now;
@@ -2086,6 +2104,9 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (!ktimer->tscdeadline)
 		return false;
 
+	/*
+	 * vmx_set_hv_timer()
+	 */
 	if (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
@@ -2981,6 +3002,9 @@ int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	kvm_lapic_set_reg(apic, APIC_TMCCT, 0);
 	kvm_apic_update_apicv(vcpu);
 	if (apic->apicv_active) {
+		/*
+		 * vmx_apicv_post_state_restore()
+		 */
 		static_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);
 		static_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));
 		static_call_cond(kvm_x86_hwapic_isr_update)(apic_find_highest_isr(apic));
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index bf653df86..df690a799 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -101,6 +101,11 @@ static void kvm_pmi_trigger_fn(struct irq_work *irq_work)
 	kvm_pmu_deliver_pmi(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|158| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|417| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -139,8 +144,19 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		irq_work_queue(&pmc_to_pmu(pmc)->irq_work);
 	else
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   - arch/x86/kvm/pmu.c|225| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -157,6 +173,18 @@ static void kvm_perf_overflow(struct perf_event *perf_event,
 
 	__kvm_perf_overflow(pmc, true);
 
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -236,6 +264,10 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|420| <<reprogram_counter>> pmc_pause_counter(pmc);
+ */
 static void pmc_pause_counter(struct kvm_pmc *pmc)
 {
 	u64 counter = pmc->counter;
@@ -249,6 +281,10 @@ static void pmc_pause_counter(struct kvm_pmc *pmc)
 	pmc->is_paused = true;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|443| <<reprogram_counter>> if (pmc->current_config == new_config && pmc_resume_counter(pmc))
+ */
 static bool pmc_resume_counter(struct kvm_pmc *pmc)
 {
 	if (!pmc->perf_event)
@@ -401,6 +437,10 @@ static bool pmc_event_is_allowed(struct kvm_pmc *pmc)
 	       check_pmu_event_filter(pmc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|481| <<kvm_pmu_handle_event>> reprogram_counter(pmc);
+ */
 static void reprogram_counter(struct kvm_pmc *pmc)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -456,6 +496,10 @@ static void reprogram_counter(struct kvm_pmc *pmc)
 	pmc->prev_counter = 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|10592| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -469,6 +513,9 @@ void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
+		/*
+		 * 只在这里调用
+		 */
 		reprogram_counter(pmc);
 	}
 
@@ -547,6 +594,11 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|101| <<kvm_pmi_trigger_fn>> kvm_pmu_deliver_pmi(vcpu);
+ *   - arch/x86/kvm/x86.c|10594| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index 7d9ba301c..79ec053d9 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -214,12 +214,35 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 					     KVM_PMC_MAX_FIXED);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|746| <<kvm_pmu_incr_counter>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - rch/x86/kvm/svm/pmu.c|173| <<amd_pmu_set_msr(PMU_TYPE_EVNTSEL)>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|60| <<reprogram_fixed_counters>> kvm_pmu_request_counter_reprogram(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|426| <<intel_pmu_set_msr>> kvm_pmu_request_counter_reprogram(pmc);
+ */
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+	 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *
+	 * 处理函数是kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
+/*
+ * called b:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|384| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> reprogram_counters(pmu, diff);
+ */
 static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 {
 	int bit;
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 80c769c58..00c9614c5 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -634,6 +634,10 @@ static void intel_pmu_reset(struct kvm_vcpu *vcpu)
  *
  * Guest needs to re-enable LBR to resume branches recording.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/pmu_intel.c|655| <<intel_pmu_deliver_pmi>> intel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);
+ */
 static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 {
 	u64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);
@@ -644,6 +648,9 @@ static void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * struct kvm_pmu_ops intel_pmu_ops.deliver_pmi = intel_pmu_deliver_pmi()
+ */
 static void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	u8 version = vcpu_to_pmu(vcpu)->version;
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index df461f387..990ac55a2 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -8349,6 +8349,13 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+	 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c381770bc..5a32cf41a 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -3221,6 +3221,10 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
 		kvm_xen_update_tsc_info(v);
 	}
 
+	/*
+	 * struct kvm_vcpu_arch *vcpu:
+	 * -> struct pvclock_vcpu_time_info hv_clock;
+	 */
 	vcpu->hv_clock.tsc_timestamp = tsc_timestamp;
 	vcpu->hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
 	vcpu->last_guest_tsc = tsc_timestamp;
@@ -4936,6 +4940,11 @@ static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 	return kvm_apic_get_state(vcpu, s);
 }
 
+/*
+ * struct kvm_lapic_state {
+ *     char regs[KVM_APIC_REG_SIZE];
+ * };
+ */
 static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
@@ -10588,8 +10597,27 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 #endif
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+		 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 处理函数是kvm_pmu_handle_event()
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/include/asm/kvm_host.h|84| <<global>> #define KVM_REQ_PMI KVM_ARCH_REQ(11)
+		 *   - arch/x86/kvm/pmu.c|146| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8352| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|10593| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -12278,6 +12306,18 @@ void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
 	vcpu->arch.l1tf_flush_l1d = true;
 	if (pmu->version && unlikely(pmu->event_count)) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/include/asm/kvm_host.h|83| <<global>> #define KVM_REQ_PMU KVM_ARCH_REQ(10)
+		 *   - arch/x86/kvm/pmu.c|169| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|929| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|220| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|232| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|10591| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *   - arch/x86/kvm/x86.c|12281| <<kvm_arch_sched_in>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *
+		 * 处理函数是kvm_pmu_handle_event()
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 	static_call(kvm_x86_sched_in)(vcpu, cpu);
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index e733a2a19..4f6013726 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -83,8 +83,25 @@ static const char *arch_timer_ppi_names[ARCH_TIMER_MAX_TIMER_PPI] = {
 
 static struct clock_event_device __percpu *arch_timer_evt;
 
+/*
+ * 在以下设置arch_timer_uses_ppi:
+ *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+ *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+ */
 static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
 static bool arch_timer_c3stop __ro_after_init;
+/*
+ * 在以下使用arch_timer_mem_use_virtual:
+ *   - drivers/clocksource/arm_arch_timer.c|88| <<global>> static bool arch_timer_mem_use_virtual __ro_after_init;
+ *   - drivers/clocksource/arm_arch_timer.c|887| <<__arch_timer_setup>> if (arch_timer_mem_use_virtual) {
+ *   - drivers/clocksource/arm_arch_timer.c|1062| <<arch_timer_banner>> arch_timer_mem_use_virtual ? "virt" : "phys" :
+ *   - drivers/clocksource/arm_arch_timer.c|1325| <<arch_timer_mem_register>> if (arch_timer_mem_use_virtual)
+ *   - drivers/clocksource/arm_arch_timer.c|1544| <<arch_timer_mem_find_best_frame>> arch_timer_mem_use_virtual = true;
+ *   - drivers/clocksource/arm_arch_timer.c|1565| <<arch_timer_mem_frame_register>> if (arch_timer_mem_use_virtual)
+ *   - drivers/clocksource/arm_arch_timer.c|1572| <<arch_timer_mem_frame_register>> arch_timer_mem_use_virtual ? "virt" : "phys");
+ */
 static bool arch_timer_mem_use_virtual __ro_after_init;
 static bool arch_counter_suspend_stop __ro_after_init;
 #ifdef CONFIG_GENERIC_GETTIMEOFDAY
@@ -1083,13 +1100,43 @@ static noinstr u64 arch_counter_get_cntvct_mem(void)
 	return arch_counter_get_cnt_mem(arch_timer_mem, CNTVCT_LO);
 }
 
+/*
+ * struct arch_timer_kvm_info {
+ *     struct timecounter timecounter;
+ *     int virtual_irq;         
+ *     int physical_irq;
+ * };
+ *
+ * 在以下使用arch_timer_kvm_info:
+ *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+ *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+ *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+ *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+ */
 static struct arch_timer_kvm_info arch_timer_kvm_info;
 
 struct arch_timer_kvm_info *arch_timer_get_kvm_info(void)
 {
+	/*
+	 * struct arch_timer_kvm_info {
+	 *     struct timecounter timecounter;
+	 *     int virtual_irq;
+	 *     int physical_irq;
+	 * };
+	 *
+	 * 在以下使用arch_timer_kvm_info:
+	 *   - drivers/clocksource/arm_arch_timer.c|1090| <<arch_timer_get_kvm_info>> return &arch_timer_kvm_info;
+	 *   - drivers/clocksource/arm_arch_timer.c|1146| <<arch_counter_register>> timecounter_init(&arch_timer_kvm_info.timecounter, &cyclecounter, start_count);
+	 *   - drivers/clocksource/arm_arch_timer.c|1393| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.virtual_irq = arch_timer_ppi[ARCH_TIMER_VIRT_PPI];
+	 *   - drivers/clocksource/arm_arch_timer.c|1395| <<arch_timer_populate_kvm_info>> arch_timer_kvm_info.physical_irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
+	 */
 	return &arch_timer_kvm_info;
 }
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1348| <<arch_timer_common_init>> arch_counter_register(arch_timers_present);
+ */
 static void __init arch_counter_register(unsigned type)
 {
 	u64 (*scr)(void);
@@ -1100,12 +1147,22 @@ static void __init arch_counter_register(unsigned type)
 	if (type & ARCH_TIMER_TYPE_CP15) {
 		u64 (*rd)(void);
 
+		/*
+		 * 在以下设置arch_timer_uses_ppi:
+		 *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+		 *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+		 *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+		 *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+		 */
 		if ((IS_ENABLED(CONFIG_ARM64) && !is_hyp_mode_available()) ||
 		    arch_timer_uses_ppi == ARCH_TIMER_VIRT_PPI) {
 			if (arch_timer_counter_has_wa()) {
 				rd = arch_counter_get_cntvct_stable;
 				scr = raw_counter_get_cntvct_stable;
 			} else {
+				/*
+				 * kvm似乎是这里
+				 */
 				rd = arch_counter_get_cntvct;
 				scr = arch_counter_get_cntvct;
 			}
@@ -1206,6 +1263,11 @@ static void __init arch_timer_cpu_pm_deinit(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/clocksource/arm_arch_timer.c|1477| <<arch_timer_of_init>> ret = arch_timer_register();
+ *   - drivers/clocksource/arm_arch_timer.c|1800| <<arch_timer_acpi_init>> ret = arch_timer_register();
+ */
 static int __init arch_timer_register(void)
 {
 	int err;
@@ -1217,6 +1279,13 @@ static int __init arch_timer_register(void)
 		goto out;
 	}
 
+	/*
+	 * 在以下设置arch_timer_uses_ppi:
+	 *   - drivers/clocksource/arm_arch_timer.c|86| <<global>> static enum arch_timer_ppi_nr arch_timer_uses_ppi __ro_after_init = ARCH_TIMER_VIRT_PPI;
+	 *   - drivers/clocksource/arm_arch_timer.c|1464| <<arch_timer_of_init>> arch_timer_uses_ppi = ARCH_TIMER_PHYS_SECURE_PPI;
+	 *   - drivers/clocksource/arm_arch_timer.c|1466| <<arch_timer_of_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+	 *   - drivers/clocksource/arm_arch_timer.c|1788| <<arch_timer_acpi_init>> arch_timer_uses_ppi = arch_timer_select_ppi();
+	 */
 	ppi = arch_timer_ppi[arch_timer_uses_ppi];
 	switch (arch_timer_uses_ppi) {
 	case ARCH_TIMER_VIRT_PPI:
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 49d1d6acf..66bff6e9a 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -842,6 +842,11 @@ static ssize_t tap_put_user(struct tap_queue *q,
 	return ret ? ret : total;
 }
 
+/*
+ * called by:
+ *   - drivers/net/tap.c|904| <<tap_read_iter>> ret = tap_do_read(q, to, noblock, NULL);
+ *   - drivers/net/tap.c|1259| <<tap_recvmsg>> ret = tap_do_read(q, &m->msg_iter, flags & MSG_DONTWAIT, skb);
+ */
 static ssize_t tap_do_read(struct tap_queue *q,
 			   struct iov_iter *to,
 			   int noblock, struct sk_buff *skb)
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index f2ed7167c..96e1a89f6 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -35,6 +35,14 @@
 
 #include "vhost.h"
 
+/*
+ * 在以下使用experimental_zcopytx:
+ *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+ *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+ *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+ *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+ *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+ */
 static int experimental_zcopytx = 0;
 module_param(experimental_zcopytx, int, 0444);
 MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
@@ -326,11 +334,19 @@ static void vhost_net_tx_packet(struct vhost_net *net)
 	net->tx_zcopy_err = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|369| <<vhost_zerocopy_signal_used>> vhost_net_tx_err(net);
+ */
 static void vhost_net_tx_err(struct vhost_net *net)
 {
 	++net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|922| <<handle_tx_zerocopy>> && vhost_net_tx_select_zcopy(net);
+ */
 static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 {
 	/* TX flush waits for outstanding DMAs to be done.
@@ -340,12 +356,32 @@ static bool vhost_net_tx_select_zcopy(struct vhost_net *net)
 		net->tx_packets / 64 >= net->tx_zcopy_err;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|1002| <<handle_tx>> if (vhost_sock_zcopy(sock))
+ *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+ *
+ * experimental_zcopytx似乎一直是false
+ */
 static bool vhost_sock_zcopy(struct socket *sock)
 {
+	/*
+	 * 在以下使用experimental_zcopytx:
+	 *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+	 *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+	 *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+	 *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+	 *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+	 */
 	return unlikely(experimental_zcopytx) &&
 		sock_flag(sock->sk, SOCK_ZEROCOPY);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|724| <<vhost_net_build_xdp>> int headroom = vhost_sock_xdp(sock) ? XDP_PACKET_HEADROOM : 0;
+ */
 static bool vhost_sock_xdp(struct socket *sock)
 {
 	return sock_flag(sock->sk, SOCK_XDP);
@@ -459,6 +495,22 @@ static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 	nvq->done_idx = 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|587| <<vhost_net_tx_get_vq_desc>> vhost_tx_batch(net, tnvq, vhost_vq_get_backend(tvq), msghdr);
+ *   - drivers/vhost/net.c|791| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|820| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|830| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *   - drivers/vhost/net.c|856| <<handle_tx_copy>> vhost_tx_batch(net, nvq, sock, &msg);
+ *
+ * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+ *
+ * 2. 否则, 是下面的path
+ * handle_tx_copy() or handle_tx_zerocopy()
+ * -> get_tx_bufs()
+ *    -> vhost_net_tx_get_vq_desc()
+ *       -> vhost_tx_batch()
+ */
 static void vhost_tx_batch(struct vhost_net *net,
 			   struct vhost_net_virtqueue *nvq,
 			   struct socket *sock,
@@ -569,6 +621,10 @@ static void vhost_net_busy_poll(struct vhost_net *net,
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|630| <<get_tx_bufs>> ret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);
+ */
 static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				    struct vhost_net_virtqueue *tnvq,
 				    unsigned int *out_num, unsigned int *in_num,
@@ -582,6 +638,15 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 				  out_num, in_num, NULL, NULL);
 
 	if (r == tvq->num && tvq->busyloop_timeout) {
+		/*
+		 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+		 *
+		 * 2. 否则, 是下面的path
+		 * handle_tx_copy() or handle_tx_zerocopy()
+		 * -> get_tx_bufs()
+		 *    -> vhost_net_tx_get_vq_desc()
+		 *       -> vhost_tx_batch()
+		 */
 		/* Flush batched packets first */
 		if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
 			vhost_tx_batch(net, tnvq,
@@ -618,6 +683,11 @@ static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
 	return iov_iter_count(iter);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|793| <<handle_tx_copy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ *   - drivers/vhost/net.c|887| <<handle_tx_zerocopy>> head = get_tx_bufs(net, nvq, &msg, &out, &in, &len,
+ */
 static int get_tx_bufs(struct vhost_net *net,
 		       struct vhost_net_virtqueue *nvq,
 		       struct msghdr *msg,
@@ -766,6 +836,10 @@ static int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1005| <<handle_tx>> handle_tx_copy(net, sock);
+ */
 static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -787,6 +861,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 	do {
 		bool busyloop_intr = false;
 
+		/*
+		 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+		 *
+		 * 2. 否则, 是下面的path
+		 * handle_tx_copy() or handle_tx_zerocopy()
+		 * -> get_tx_bufs()
+		 *    -> vhost_net_tx_get_vq_desc()
+		 *       -> vhost_tx_batch()
+		 */
 		if (nvq->done_idx == VHOST_NET_BATCH)
 			vhost_tx_batch(net, nvq, sock, &msg);
 
@@ -817,12 +900,30 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 			if (!err) {
 				goto done;
 			} else if (unlikely(err != -ENOSPC)) {
+				/*
+				 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+				 *
+				 * 2. 否则, 是下面的path
+				 * handle_tx_copy() or handle_tx_zerocopy()
+				 * -> get_tx_bufs()
+				 *    -> vhost_net_tx_get_vq_desc()
+				 *       -> vhost_tx_batch()
+				 */
 				vhost_tx_batch(net, nvq, sock, &msg);
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
 			}
 
+			/*
+			 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+			 *
+			 * 2. 否则, 是下面的path
+			 * handle_tx_copy() or handle_tx_zerocopy()
+			 * -> get_tx_bufs()
+			 *    -> vhost_net_tx_get_vq_desc()
+			 *       -> vhost_tx_batch()
+			 */
 			/* We can't build XDP buff, go for single
 			 * packet path but let's flush batched
 			 * packets.
@@ -853,6 +954,15 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		++nvq->done_idx;
 	} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
 
+	/*
+	 * 1. handle_tx_copy()在四个地方调用vhost_tx_batch()
+	 *
+	 * 2. 否则, 是下面的path
+	 * handle_tx_copy() or handle_tx_zerocopy()
+	 * -> get_tx_bufs()
+	 *    -> vhost_net_tx_get_vq_desc()
+	 *       -> vhost_tx_batch()
+	 */
 	vhost_tx_batch(net, nvq, sock, &msg);
 }
 
@@ -965,6 +1075,11 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1284| <<handle_tx_kick>> handle_tx(net);
+ *   - drivers/vhost/net.c|1300| <<handle_tx_net>> handle_tx(net);
+ */
 static void handle_tx(struct vhost_net *net)
 {
 	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
@@ -982,6 +1097,14 @@ static void handle_tx(struct vhost_net *net)
 	vhost_disable_notify(&net->dev, vq);
 	vhost_net_disable_vq(net, vq);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|598| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+	 *   - drivers/vhost/net.c|1002| <<handle_tx>> if (vhost_sock_zcopy(sock))
+	 *   - drivers/vhost/net.c|1557| <<vhost_net_set_backend>> sock && vhost_sock_zcopy(sock));
+	 *
+	 * experimental_zcopytx似乎一直是false
+	 */
 	if (vhost_sock_zcopy(sock))
 		handle_tx_zerocopy(net, sock);
 	else
@@ -1258,6 +1381,10 @@ static void handle_rx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用handle_tx_kick():
+ *   - drivers/vhost/net.c|1349| <<vhost_net_open>> n->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;
+ */
 static void handle_tx_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1795,6 +1922,14 @@ static struct miscdevice vhost_net_misc = {
 
 static int __init vhost_net_init(void)
 {
+	/*
+	 * 在以下使用experimental_zcopytx:
+	 *   - drivers/vhost/net.c|38| <<global>> static int experimental_zcopytx = 0;
+	 *   - drivers/vhost/net.c|39| <<global>> module_param(experimental_zcopytx, int , 0444);
+	 *   - drivers/vhost/net.c|40| <<global>> MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
+	 *   - drivers/vhost/net.c|345| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+	 *   - drivers/vhost/net.c|1815| <<vhost_net_init>> if (experimental_zcopytx)
+	 */
 	if (experimental_zcopytx)
 		vhost_net_enable_zcopy(VHOST_NET_VQ_TX);
 	return misc_register(&vhost_net_misc);
diff --git a/include/kvm/arm_arch_timer.h b/include/kvm/arm_arch_timer.h
index bb3cb0058..815154c47 100644
--- a/include/kvm/arm_arch_timer.h
+++ b/include/kvm/arm_arch_timer.h
@@ -32,17 +32,55 @@ struct arch_timer_offset {
 	 * If set, pointer to one of the offsets in the kvm's offset
 	 * structure. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vm_offset:
+	 *   - arch/arm64/kvm/arch_timer.c|136| <<timer_get_offset>> if (ctxt->offset.vm_offset)
+	 *   - arch/arm64/kvm/arch_timer.c|137| <<timer_get_offset>> offset += *ctxt->offset.vm_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|190| <<timer_set_offset>> if (!ctxt->offset.vm_offset) {
+	 *   - arch/arm64/kvm/arch_timer.c|195| <<timer_set_offset>> WRITE_ONCE(*ctxt->offset.vm_offset, offset);
+	 *   - arch/arm64/kvm/arch_timer.c|1024| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1055| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1057| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|430| <<kvm_hyp_handle_cntpct>> if (ctxt->offset.vm_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|431| <<kvm_hyp_handle_cntpct>> val -= *kern_hyp_va(ctxt->offset.vm_offset);
+	 */
 	u64	*vm_offset;
 	/*
 	 * If set, pointer to one of the offsets in the vcpu's sysreg
 	 * array. If NULL, assume a zero offset.
 	 */
+	/*
+	 * 在以下使用arch_timer_offset->vcpu_offset:
+	 *   - arch/arm64/kvm/arch_timer.c|138| <<timer_get_offset>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/arch_timer.c|139| <<timer_get_offset>> offset += *ctxt->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|818| <<kvm_timer_vcpu_load_nested_switch>> offs->vcpu_offset = NULL;
+	 *   - arch/arm64/kvm/arch_timer.c|820| <<kvm_timer_vcpu_load_nested_switch>> offs->vcpu_offset = &__vcpu_sys_reg(vcpu, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1023| <<kvm_timer_vcpu_reset>> offs->vcpu_offset = &__vcpu_sys_reg(vcpu, CNTVOFF_EL2);
+	 *   - arch/arm64/kvm/arch_timer.c|1220| <<kvm_arm_timer_read>> val = *timer->offset.vcpu_offset;
+	 *   - arch/arm64/kvm/arch_timer.c|1274| <<kvm_arm_timer_write>> *timer->offset.vcpu_offset = val;
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|432| <<kvm_hyp_handle_cntpct>> if (ctxt->offset.vcpu_offset)
+	 *   - arch/arm64/kvm/hyp/include/hyp/switch.h|433| <<kvm_hyp_handle_cntpct>> val -= *kern_hyp_va(ctxt->offset.vcpu_offset);
+	 */
 	u64	*vcpu_offset;
 };
 
 struct arch_timer_vm_data {
+	/*
+	 * 在以下使用arch_timer_vm_data->voffset:
+	 *   - arch/arm64/kvm/arch_timer.c|1014| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.voffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1692| <<kvm_vm_ioctl_set_counter_offset>> kvm->arch.timer_data.voffset = offset->counter_offset;
+	 *   - arch/arm64/kvm/hypercalls.c|47| <<kvm_ptp_get_time>> cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.voffset;
+	 */
 	/* Offset applied to the virtual timer/counter */
 	u64	voffset;
+	/*
+	 * 在以下使用arch_timer_vm_data->poffset:
+	 *   - arch/arm64/kvm/arch_timer.c|983| <<kvm_timer_vcpu_reset>> offs->vm_offset = &vcpu->kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1016| <<timer_context_init>> ctxt->offset.vm_offset = &kvm->arch.timer_data.poffset;
+	 *   - arch/arm64/kvm/arch_timer.c|1693| <<kvm_vm_ioctl_set_counter_offset>> kvm->arch.timer_data.poffset = offset->counter_offset;
+	 *   - arch/arm64/kvm/hyp/nvhe/timer-sr.c|51| <<__timer_enable_traps>> !kern_hyp_va(vcpu->kvm)->arch.timer_data.poffset)
+	 *   - arch/arm64/kvm/hypercalls.c|50| <<kvm_ptp_get_time>> cycles = systime_snapshot.cycles - vcpu->kvm->arch.timer_data.poffset;
+	 */
 	/* Offset applied to the physical timer/counter */
 	u64	poffset;
 
@@ -124,6 +162,26 @@ void kvm_timer_init_vhe(void);
 #define vcpu_hvtimer(v)	(&(v)->arch.timer_cpu.timers[TIMER_HVTIMER])
 #define vcpu_hptimer(v)	(&(v)->arch.timer_cpu.timers[TIMER_HPTIMER])
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arch_timer.c|83| <<timer_get_ctl>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|102| <<timer_get_cval>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|148| <<timer_set_ctl>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|170| <<timer_set_cval>> switch (arch_timer_ctx_index(ctxt)) {
+ *   - arch/arm64/kvm/arch_timer.c|191| <<timer_set_offset>> WARN(offset, "timer %ld\n", arch_timer_ctx_index(ctxt));
+ *   - arch/arm64/kvm/arch_timer.c|430| <<kvm_timer_should_fire>> index = arch_timer_ctx_index(timer_ctx);
+ *   - arch/arm64/kvm/arch_timer.c|552| <<timer_save_state>> enum kvm_arch_timers index = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/arch_timer.c|666| <<timer_restore_state>> enum kvm_arch_timers index = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|215| <<__field>> __entry->direct_vtimer = arch_timer_ctx_index(map->direct_vtimer);
+ *   - arch/arm64/kvm/trace_arm.h|217| <<__field>> (map->direct_ptimer) ? arch_timer_ctx_index(map->direct_ptimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|219| <<__field>> (map->emul_vtimer) ? arch_timer_ctx_index(map->emul_vtimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|221| <<__field>> (map->emul_ptimer) ? arch_timer_ctx_index(map->emul_ptimer) : -1;
+ *   - arch/arm64/kvm/trace_arm.h|245| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|267| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|285| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - arch/arm64/kvm/trace_arm.h|301| <<__field>> __entry->timer_idx = arch_timer_ctx_index(ctx);
+ *   - include/kvm/arm_arch_timer.h|156| <<timer_irq>> #define timer_irq(ctx) (timer_vm_data(ctx)->ppi[arch_timer_ctx_index(ctx)])
+ */
 #define arch_timer_ctx_index(ctx)	((ctx) - vcpu_timer((ctx)->vcpu)->timers)
 
 #define timer_vm_data(ctx)		(&(ctx)->vcpu->kvm->arch.timer_data)
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index 5b27f94d4..10a69b328 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -309,6 +309,16 @@ struct vgic_v3_cpu_if {
 	u32		vgic_sre;	/* Restored only, change ignored */
 	u32		vgic_ap0r[4];
 	u32		vgic_ap1r[4];
+	/*
+	 * 在以下使用vgic_v3_cpu_if->vgic_lr[VGIC_V3_MAX_LRS]:
+	 *   - arch/arm64/kvm/hyp/nvhe/hyp-main.c|71| <<sync_hyp_vcpu>> host_cpu_if->vgic_lr[i] = hyp_cpu_if->vgic_lr[i];
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|225| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] &= ~ICH_LR_STATE;
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|227| <<__vgic_v3_save_state>> cpu_if->vgic_lr[i] = __gic_v3_get_lr(i);
+	 *   - arch/arm64/kvm/hyp/vgic-v3-sr.c|248| <<__vgic_v3_restore_state>> __gic_v3_set_lr(cpu_if->vgic_lr[i], i);
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|47| <<vgic_v3_fold_lr_state>> u64 val = cpuif->vgic_lr[lr];
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|197| <<vgic_v3_populate_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = val;
+	 *   - arch/arm64/kvm/vgic/vgic-v3.c|202| <<vgic_v3_clear_lr>> vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[lr] = 0;
+	 */
 	u64		vgic_lr[VGIC_V3_MAX_LRS];
 
 	/*
@@ -333,6 +343,23 @@ struct vgic_cpu {
 
 	raw_spinlock_t ap_list_lock;	/* Protects the ap_list */
 
+	/*
+	 * 在以下使用vgic_cpu->ap_list_head:
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|203| <<kvm_vgic_vcpu_init>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|381| <<kvm_vgic_vcpu_destroy>> INIT_LIST_HEAD(&vgic_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|211| <<vgic_flush_pending_lpis>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|356| <<vgic_sort_ap_list>> list_sort(NULL, &vgic_cpu->ap_list_head, vgic_irq_cmp);
+	 *   - arch/arm64/kvm/vgic/vgic.c|461| <<vgic_queue_irq_unlock>> list_add_tail(&irq->ap_list, &vcpu->arch.vgic_cpu.ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|716| <<vgic_prune_ap_list>> list_for_each_entry_safe(irq, tmp, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|788| <<vgic_prune_ap_list>> list_add_tail(&irq->ap_list, &new_cpu->ap_list_head);
+	 *   - arch/arm64/kvm/vgic/vgic.c|855| <<compute_ap_list_depth>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|887| <<vgic_flush_lr_state>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|913| <<vgic_flush_lr_state>> &vgic_cpu->ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|953| <<kvm_vgic_sync_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head))
+	 *   - arch/arm64/kvm/vgic/vgic.c|996| <<kvm_vgic_flush_hwstate>> if (list_empty(&vcpu->arch.vgic_cpu.ap_list_head) &&
+	 *   - arch/arm64/kvm/vgic/vgic.c|1002| <<kvm_vgic_flush_hwstate>> if (!list_empty(&vcpu->arch.vgic_cpu.ap_list_head)) {
+	 *   - arch/arm64/kvm/vgic/vgic.c|1066| <<kvm_vgic_vcpu_pending_irq>> list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
+	 */
 	/*
 	 * List of IRQs that this VCPU should consider because they are either
 	 * Active or Pending (hence the name; AP list), or because they recently
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index 88cbc1181..7308133be 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -1096,6 +1096,12 @@ static void clocksource_enqueue(struct clocksource *cs)
  * __clocksource_update_freq_hz() or __clocksource_update_freq_khz() helper
  * functions.
  */
+/*
+ * called by:
+ *   - include/linux/clocksource.h|256| <<__clocksource_update_freq_hz>> __clocksource_update_freq_scale(cs, 1, hz);
+ *   - include/linux/clocksource.h|261| <<__clocksource_update_freq_khz>> __clocksource_update_freq_scale(cs, 1000, khz);
+ *   - kernel/time/clocksource.c|1202| <<__clocksource_register_scale>> __clocksource_update_freq_scale(cs, scale, freq);
+ */
 void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq)
 {
 	u64 sec;
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index 266d02809..5db9424b3 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -937,6 +937,45 @@ ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs)
 }
 EXPORT_SYMBOL_GPL(ktime_mono_to_any);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2302| <<get_kvmclock_base_ns>> return ktime_to_ns(ktime_add(ktime_get_raw(), pvclock_gtod_data.offs_boot));
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|545| <<intel_dp_hdcp2_read_msg>> msg_end = ktime_add_ms(ktime_get_raw(),
+ *   - drivers/gpu/drm/i915/display/intel_dp_hdcp.c|562| <<intel_dp_hdcp2_read_msg>> msg_expired = ktime_after(ktime_get_raw(), msg_end);
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|78| <<live_nop_switch>> times[0] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|103| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|114| <<live_nop_switch>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gem/selftests/i915_gem_context.c|158| <<live_nop_switch>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/gt/intel_rps.c|1755| <<vlv_c0_read>> ei->ktime = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3749| <<nop_virtual_engine>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/gt/selftest_execlists.c|3804| <<nop_virtual_engine>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/i915_pmu.c|191| <<ktime_since_raw>> return ktime_to_ns(ktime_sub(ktime_get_raw(), kt));
+ *   - drivers/gpu/drm/i915/i915_pmu.c|267| <<init_rc6>> pmu->sleep_last[i] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_pmu.c|277| <<park_rc6>> pmu->sleep_last[gt->info.id] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/i915_utils.h|262| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/i915/i915_utils.h|267| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|593| <<live_nop_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|621| <<live_nop_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1077| <<live_empty_request>> times[1] = ktime_get_raw();
+ *   - drivers/gpu/drm/i915/selftests/i915_request.c|1090| <<live_empty_request>> times[1] = ktime_sub(ktime_get_raw(), times[1]);
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|312| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/v3d/v3d_drv.h|317| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|851| <<__wait_for>> const ktime_t end__ = ktime_add_ns(ktime_get_raw(), 1000ll * (US)); \
+ *   - drivers/gpu/drm/vc4/vc4_drv.h|856| <<__wait_for>> const bool expired__ = ktime_after(ktime_get_raw(), end__); \
+ *   - drivers/ptp/ptp_clockmatrix.c|438| <<_idtcm_gettime>> idtcm->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|750| <<_idtcm_set_dpll_hw_tod>> ktime_t diff = ktime_sub(ktime_get_raw(),
+ *   - drivers/ptp/ptp_clockmatrix.c|1006| <<set_tod_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_clockmatrix.c|1013| <<set_tod_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|393| <<_idt82p33_gettime>> idt82p33->start_time = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|433| <<_idt82p33_settime>> dynamic_overhead_ns = ktime_to_ns(ktime_get_raw())
+ *   - drivers/ptp/ptp_idt82p33.c|653| <<idt82p33_measure_one_byte_write_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|658| <<idt82p33_measure_one_byte_write_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|686| <<idt82p33_measure_one_byte_read_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|691| <<idt82p33_measure_one_byte_read_overhead>> stop = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|719| <<idt82p33_measure_tod_write_9_byte_overhead>> start = ktime_get_raw();
+ *   - drivers/ptp/ptp_idt82p33.c|730| <<idt82p33_measure_tod_write_9_byte_overhead>> stop = ktime_get_raw();
+ *   - include/linux/timekeeping.h|174| <<ktime_get_raw_ns>> return ktime_to_ns(ktime_get_raw());
+ */
 /**
  * ktime_get_raw - Returns the raw monotonic time in ktime_t format
  */
diff --git a/tools/testing/selftests/kvm/include/x86_64/processor.h b/tools/testing/selftests/kvm/include/x86_64/processor.h
index aa434c8f1..54a397550 100644
--- a/tools/testing/selftests/kvm/include/x86_64/processor.h
+++ b/tools/testing/selftests/kvm/include/x86_64/processor.h
@@ -957,6 +957,15 @@ static inline int __vcpu_set_cpuid(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|752| <<vcpu_init_cpuid>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|760| <<vcpu_set_cpuid_maxphyaddr>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|771| <<vcpu_clear_cpuid_entry>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/lib/x86_64/processor.c|789| <<vcpu_set_or_clear_cpuid_feature>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_features.c|486| <<guest_test_msrs_access>> vcpu_set_cpuid(vcpu);
+ *   - tools/testing/selftests/kvm/x86_64/hyperv_features.c|657| <<guest_test_hcalls_access>> vcpu_set_cpuid(vcpu);
+ */
 static inline void vcpu_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	TEST_ASSERT(vcpu->cpuid, "Must do vcpu_init_cpuid() first");
-- 
2.34.1

