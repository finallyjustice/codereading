From 54780be23b145172230162c8a9a3d7c298686930 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Thu, 21 Jan 2021 10:39:05 -0800
Subject: [PATCH 1/1] linux uek6 v5.4.17-2036.101.1

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/kvm/irq.c                    |  10 +
 arch/x86/kvm/lapic.c                  | 678 ++++++++++++++++++++++++++
 arch/x86/kvm/lapic.h                  | 115 +++++
 arch/x86/kvm/vmx/vmx.c                |  26 +
 arch/x86/kvm/vmx/vmx.h                |   8 +
 arch/x86/kvm/x86.c                    |  38 ++
 drivers/infiniband/core/cm.c          |   6 +
 drivers/infiniband/sw/rxe/rxe_net.c   |  55 +++
 drivers/infiniband/sw/rxe/rxe_verbs.c |  34 ++
 drivers/scsi/iscsi_tcp.c              | 105 ++++
 drivers/scsi/libiscsi.c               |   4 +
 drivers/scsi/scsi_transport_iscsi.c   |  10 +
 include/linux/kvm_host.h              |   4 +
 include/uapi/linux/kvm.h              |   7 +
 net/rds/af_rds.c                      |  49 ++
 net/rds/bind.c                        |   6 +
 net/rds/connection.c                  | 143 ++++++
 net/rds/ib.c                          |  10 +
 net/rds/ib.h                          |  21 +
 net/rds/ib_cm.c                       |  94 ++++
 net/rds/ib_rdma.c                     |   4 +
 net/rds/ib_recv.c                     |  46 ++
 net/rds/ib_ring.c                     |  13 +
 net/rds/ib_send.c                     |  44 ++
 net/rds/ib_stats.c                    |   3 +
 net/rds/info.c                        |   4 +
 net/rds/rdma.c                        |   6 +
 net/rds/rdma_transport.c              |   6 +
 net/rds/rds.h                         |  30 ++
 net/rds/recv.c                        |  62 +++
 net/rds/send.c                        |  90 ++++
 net/rds/threads.c                     |   4 +
 net/rds/transport.c                   |  14 +
 33 files changed, 1749 insertions(+)

diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index e330e7d125f7..98ae80007141 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -148,6 +148,16 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v)
 }
 EXPORT_SYMBOL_GPL(kvm_cpu_get_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8509| <<vcpu_run>> kvm_inject_pending_timer_irqs(vcpu);
+ *
+ * vcpu_run():
+ *   kvm_clear_request(KVM_REQ_PENDING_TIMER, vcpu);
+ *   if (kvm_cpu_has_pending_timer(vcpu))
+ *       kvm_inject_pending_timer_irqs(vcpu);
+ *       -> kvm_inject_apic_timer_irqs()
+ */
 void kvm_inject_pending_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu))
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 2c4daaf6c551..a9c4252959b2 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -57,12 +57,30 @@
 #define APIC_VERSION			(0x14UL | ((KVM_APIC_LVT_NUM - 1) << 16))
 #define LAPIC_MMIO_LENGTH		(1 << 12)
 /* followed define is not in apicdef.h */
+/*
+ * 在以下使用MAX_APIC_VECTOR:
+ *   - arch/x86/kvm/lapic.c|371| <<find_highest_vector>> for (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;
+ *   - arch/x86/kvm/lapic.c|387| <<count_vectors>> for (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {
+ *   - arch/x86/kvm/lapic.c|492| <<apic_set_isr>> BUG_ON(apic->isr_count > MAX_APIC_VECTOR);
+ */
 #define MAX_APIC_VECTOR			256
+/*
+ * 在以下使用APIC_VECTORS_PER_REG:
+ *   - arch/x86/kvm/lapic.c|371| <<find_highest_vector>> for (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;
+ *   - arch/x86/kvm/lapic.c|372| <<find_highest_vector>> vec >= 0; vec -= APIC_VECTORS_PER_REG) {
+ *   - arch/x86/kvm/lapic.c|387| <<count_vectors>> for (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {
+ */
 #define APIC_VECTORS_PER_REG		32
 
 #define APIC_BROADCAST			0xFF
 #define X2APIC_BROADCAST		0xFFFFFFFFul
 
+/*
+ * 在以下使用lapic_timer_advance_dynamic:
+ *   - arch/x86/kvm/lapic.c|1571| <<__kvm_wait_lapic_expire>> if (lapic_timer_advance_dynamic)
+ *   - arch/x86/kvm/lapic.c|2368| <<kvm_create_lapic>> lapic_timer_advance_dynamic = true;
+ *   - arch/x86/kvm/lapic.c|2371| <<kvm_create_lapic>> lapic_timer_advance_dynamic = false;
+ */
 static bool lapic_timer_advance_dynamic __read_mostly;
 #define LAPIC_TIMER_ADVANCE_ADJUST_MIN	100	/* clock cycles */
 #define LAPIC_TIMER_ADVANCE_ADJUST_MAX	10000	/* clock cycles */
@@ -71,11 +89,29 @@ static bool lapic_timer_advance_dynamic __read_mostly;
 /* step-by-step approximation to mitigate fluctuation */
 #define LAPIC_TIMER_ADVANCE_ADJUST_STEP 8
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|101| <<kvm_apic_pending_eoi>> return apic_test_vector(vector, apic->regs + APIC_ISR) ||
+ *   - arch/x86/kvm/lapic.c|102| <<kvm_apic_pending_eoi>> apic_test_vector(vector, apic->regs + APIC_IRR);
+ *   - arch/x86/kvm/lapic.c|1173| <<__apic_accept_irq>> if (apic_test_vector(vector, apic->regs + APIC_TMR) != !!trig_mode) {
+ *   - arch/x86/kvm/lapic.c|1272| <<kvm_ioapic_send_eoi>> if (apic_test_vector(vector, apic->regs + APIC_TMR))
+ *   - arch/x86/kvm/lapic.c|1625| <<lapic_timer_int_injected>> if (apic_test_vector(vec, bitmap))
+ */
 static inline int apic_test_vector(int vec, void *bitmap)
 {
 	return test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+ *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/ioapic.c|299| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+ *
+ * IRR: LAPIC已经收到中断但是还未提交CPU处理, 当前LAPIC接收到的中断请求
+ * ISR: CPU已经开始处理中断,但是尚未完成, 当前LAPIC送入CPU中(CPU正在处理)的中断请求
+ * EOI软件写入表示中断处理完成
+ */
 bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -84,19 +120,67 @@ bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 		apic_test_vector(vector, apic->regs + APIC_IRR);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|584| <<apic_set_isr>> if (__apic_test_and_set_vector(vec, apic->regs + APIC_ISR))
+ */
 static inline int __apic_test_and_set_vector(int vec, void *bitmap)
 {
 	return __test_and_set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|630| <<apic_clear_isr>> if (!__apic_test_and_clear_vector(vec, apic->regs + APIC_ISR))
+ */
 static inline int __apic_test_and_clear_vector(int vec, void *bitmap)
 {
 	return __test_and_clear_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * crash> apic_hw_disabled
+ * apic_hw_disabled = $4 = {
+ *   key = {
+ *     enabled = {
+ *       counter = 0
+ *   },
+ *
+ * 在以下使用apic_hw_disabled:
+ *   - arch/x86/kvm/lapic.c|2067| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2144| <<kvm_lapic_set_base>> static_key_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2146| <<kvm_lapic_set_base>> static_key_slow_inc(&apic_hw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2766| <<kvm_lapic_init>> jump_label_rate_limit(&apic_hw_disabled, HZ);
+ *   - arch/x86/kvm/lapic.c|2772| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.h|177| <<kvm_apic_hw_enabled>> if (static_key_false(&apic_hw_disabled.key))
+ */
 struct static_key_deferred apic_hw_disabled __read_mostly;
+/*
+ * crash> apic_sw_disabled
+ * apic_sw_disabled = $5 = {
+ *   key = {
+ *     enabled = {
+ *       counter = 0
+ *   },
+ *
+ * 在以下使用apic_sw_disabled:
+ *   - arch/x86/kvm/lapic.c|261| <<apic_set_spiv>> static_key_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|263| <<apic_set_spiv>> static_key_slow_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2070| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.c|2341| <<kvm_create_lapic>> static_key_slow_inc(&apic_sw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2767| <<kvm_lapic_init>> jump_label_rate_limit(&apic_sw_disabled, HZ);
+ *   - arch/x86/kvm/lapic.c|2773| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+ *   - arch/x86/kvm/lapic.h|186| <<kvm_apic_sw_enabled>> if (static_key_false(&apic_sw_disabled.key))
+ */
 struct static_key_deferred apic_sw_disabled __read_mostly;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1104| <<__apic_accept_irq>> if (unlikely(!apic_enabled(apic)))
+ *   - arch/x86/kvm/lapic.c|2486| <<apic_has_pending_timer>> if (apic_enabled(apic) && apic_lvt_enabled(apic, APIC_LVTT))
+ *
+ * kvm_apic_sw_enabled()和kvm_apic_ww_enabled()同时返回true
+ */
 static inline int apic_enabled(struct kvm_lapic *apic)
 {
 	return kvm_apic_sw_enabled(apic) &&	kvm_apic_hw_enabled(apic);
@@ -109,22 +193,57 @@ static inline int apic_enabled(struct kvm_lapic *apic)
 	(LVT_MASK | APIC_MODE_MASK | APIC_INPUT_POLARITY | \
 	 APIC_LVT_REMOTE_IRR | APIC_LVT_LEVEL_TRIGGER)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|286| <<recalculate_apic_map>> max_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));
+ *   - arch/x86/kvm/lapic.c|309| <<recalculate_apic_map>> x2apic_id = kvm_x2apic_id(apic);
+ *   - arch/x86/kvm/lapic.c|873| <<kvm_apic_match_physical_addr>> return mda == kvm_x2apic_id(apic);
+ *   - arch/x86/kvm/lapic.c|881| <<kvm_apic_match_physical_addr>> if (kvm_x2apic_id(apic) > 0xff && mda == kvm_x2apic_id(apic))
+ */
 static inline u32 kvm_x2apic_id(struct kvm_lapic *apic)
 {
 	return apic->vcpu->vcpu_id;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|176| <<kvm_use_posted_timer_interrupt>> return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
+ *   - arch/x86/kvm/lapic.c|2801| <<__kvm_migrate_apic_timer>> kvm_can_post_timer_interrupt(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|7277| <<vmx_set_hv_timer>> kvm_can_post_timer_interrupt(vcpu))
+ */
 bool kvm_can_post_timer_interrupt(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用pi_inject_timer:
+	 *   - arch/x86/kvm/x86.c|162| <<global>> module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
+	 *   - arch/x86/kvm/lapic.c|119| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
+	 *   - arch/x86/kvm/x86.c|7352| <<kvm_arch_init>> if (pi_inject_timer == -1)
+	 *   - arch/x86/kvm/x86.c|7353| <<kvm_arch_init>> pi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);
+	 *
+	 * pi_inject_timer测试是0
+	 */
 	return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
 }
 EXPORT_SYMBOL_GPL(kvm_can_post_timer_interrupt);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1701| <<apic_timer_expired>> if (kvm_use_posted_timer_interrupt(apic->vcpu)) {
+ */
 static bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 一个设置IN_GUEST_MODE的例子:
+	 *   - arch/x86/kvm/x86.c|8298| <<vcpu_enter_guest>> vcpu->mode = IN_GUEST_MODE;
+	 */
 	return kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|337| <<recalculate_apic_map>> if (!kvm_apic_map_get_logical_dest(new, ldr, &cluster, &mask))
+ *   - arch/x86/kvm/lapic.c|1046| <<kvm_apic_map_get_dest_lapic>> if (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,
+ */
 static inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,
 		u32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {
 	switch (map->mode) {
@@ -169,6 +288,16 @@ static void kvm_apic_map_free(struct rcu_head *rcu)
 	kvfree(map);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|385| <<apic_set_spiv>> recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|392| <<kvm_apic_set_xapic_id>> recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|398| <<kvm_apic_set_ldr>> recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|414| <<kvm_apic_set_x2apic_id>> recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2317| <<kvm_lapic_reg_write>> recalculate_apic_map(apic->vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|2578| <<kvm_lapic_set_base>> recalculate_apic_map(vcpu->kvm);
+ *   - arch/x86/kvm/lapic.c|3013| <<kvm_apic_set_state>> recalculate_apic_map(vcpu->kvm);
+ */
 static void recalculate_apic_map(struct kvm *kvm)
 {
 	struct kvm_apic_map *new, *old = NULL;
@@ -255,8 +384,25 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 
 	kvm_lapic_set_reg(apic, APIC_SPIV, val);
 
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|302| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|303| <<apic_set_spiv>> apic->sw_enabled = enabled;\
+	 *   - arch/x86/kvm/lapic.c|2262| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|237| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	if (enabled != apic->sw_enabled) {
 		apic->sw_enabled = enabled;
+		/*
+		 * 在以下使用apic_sw_disabled:
+		 *   - arch/x86/kvm/lapic.c|261| <<apic_set_spiv>> static_key_slow_dec_deferred(&apic_sw_disabled);
+		 *   - arch/x86/kvm/lapic.c|263| <<apic_set_spiv>> static_key_slow_inc(&apic_sw_disabled.key);
+		 *   - arch/x86/kvm/lapic.c|2070| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_sw_disabled);
+		 *   - arch/x86/kvm/lapic.c|2341| <<kvm_create_lapic>> static_key_slow_inc(&apic_sw_disabled.key);
+		 *   - arch/x86/kvm/lapic.c|2767| <<kvm_lapic_init>> jump_label_rate_limit(&apic_sw_disabled, HZ);
+		 *   - arch/x86/kvm/lapic.c|2773| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+		 *   - arch/x86/kvm/lapic.h|186| <<kvm_apic_sw_enabled>> if (static_key_false(&apic_sw_disabled.key))
+		 */
 		if (enabled)
 			static_key_slow_dec_deferred(&apic_sw_disabled);
 		else
@@ -294,6 +440,10 @@ static inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)
 	recalculate_apic_map(apic->vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2721| <<apic_has_pending_timer>> if (apic_enabled(apic) && apic_lvt_enabled(apic, APIC_LVTT))
+ */
 static inline int apic_lvt_enabled(struct kvm_lapic *apic, int lvt_type)
 {
 	return !(kvm_lapic_get_reg(apic, lvt_type) & APIC_LVT_MASKED);
@@ -304,8 +454,24 @@ static inline int apic_lvt_vector(struct kvm_lapic *apic, int lvt_type)
 	return kvm_lapic_get_reg(apic, lvt_type) & APIC_VECTOR_MASK;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1543| <<kvm_apic_inject_pending_timer_irqs>> if (apic_lvtt_oneshot(apic)) {
+ *   - arch/x86/kvm/lapic.c|1681| <<start_sw_period>> if (apic_lvtt_oneshot(apic))
+ *   - arch/x86/kvm/lapic.c|1761| <<start_sw_timer>> if (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
+ *   - arch/x86/kvm/lapic.c|1832| <<start_apic_timer>> if ((apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))
+ *   - arch/x86/kvm/lapic.c|2090| <<kvm_set_lapic_tscdeadline_msr>> if (!kvm_apic_present(vcpu) || apic_lvtt_oneshot(apic) ||
+ */
 static inline int apic_lvtt_oneshot(struct kvm_lapic *apic)
 {
+	/*
+	 * 在以下使用kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|355| <<apic_lvtt_oneshot>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;
+	 *   - arch/x86/kvm/lapic.c|360| <<apic_lvtt_period>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
+	 *   - arch/x86/kvm/lapic.c|365| <<apic_lvtt_tscdeadline>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
+	 *   - arch/x86/kvm/lapic.c|1470| <<apic_update_lvtt>> if (apic->lapic_timer.timer_mode != timer_mode) {
+	 *   - arch/x86/kvm/lapic.c|1478| <<apic_update_lvtt>> apic->lapic_timer.timer_mode = timer_mode;
+	 */
 	return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;
 }
 
@@ -324,6 +490,13 @@ static inline int apic_lvt_nmi_mode(u32 lvt_val)
 	return (lvt_val & (APIC_MODE_MASK | APIC_LVT_MASKED)) == APIC_DM_NMI;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|235| <<kvm_vcpu_ioctl_set_cpuid>> kvm_apic_set_version(vcpu);
+ *   - arch/x86/kvm/cpuid.c|258| <<kvm_vcpu_ioctl_set_cpuid2>> kvm_apic_set_version(vcpu);
+ *   - arch/x86/kvm/lapic.c|2650| <<kvm_lapic_reset>> kvm_apic_set_version(apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|3014| <<kvm_apic_set_state>> kvm_apic_set_version(vcpu);
+ */
 void kvm_apic_set_version(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -355,6 +528,11 @@ static const unsigned int apic_lvt_mask[KVM_APIC_LVT_NUM] = {
 	LVT_MASK		/* LVTERR */
 };
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|563| <<apic_search_irr>> return find_highest_vector(apic->regs + APIC_IRR);
+ *   - arch/x86/kvm/lapic.c|643| <<apic_find_highest_isr>> result = find_highest_vector(apic->regs + APIC_ISR);
+ */
 static int find_highest_vector(void *bitmap)
 {
 	int vec;
@@ -414,6 +592,33 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * kvm_apic_update_irr
+ * apic_has_interrupt_for_ppr
+ * kvm_apic_has_interrupt
+ * kvm_cpu_has_interrupt
+ * kvm_arch_vcpu_runnable
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_apic_update_irr
+ * vcpu_enter_guest
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6228| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -463,6 +668,10 @@ static inline void apic_clear_irr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|3029| <<kvm_get_apic_interrupt>> apic_set_isr(vector, apic);
+ */
 static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu;
@@ -535,6 +744,12 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm.c|7548| <<global>> .sync_pir_to_irr = kvm_lapic_find_highest_irr,
+ *   - arch/x86/kvm/vmx/vmx.c|6245| <<vmx_sync_pir_to_irr>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ *   - arch/x86/kvm/x86.c|7626| <<update_cr8_intercept>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ */
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
 	/* This may race with setting of irr in __apic_accept_irq() and
@@ -550,6 +765,17 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|656| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|1458| <<kvm_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|77| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|99| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|803| <<kvm_pv_send_ipi>> count += kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|816| <<kvm_pv_send_ipi>> count += kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1164| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1177| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
@@ -1023,6 +1249,11 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|642| <<kvm_apic_set_irq>> return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
+ *   - arch/x86/kvm/lapic.c|2525| <<kvm_apic_local_deliver>> return __apic_accept_irq(apic, mode, vector, 1, trig_mode,
+ */
 static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map)
@@ -1158,6 +1389,38 @@ static void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)
 	kvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);
 }
 
+/*
+ * 这个因为apicv所以不会调用
+ * apic_set_eoi
+ * vcpu_enter_guest
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * apic_set_eoi
+ * kvm_x2apic_msr_write
+ * kvm_set_msr_common
+ * vmx_set_msr
+ * __kvm_set_msr
+ * kvm_emulate_wrmsr
+ * vmx_handle_exit
+ * vcpu_enter_guest
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2151| <<kvm_lapic_reg_write>> apic_set_eoi(apic);
+ *   - arch/x86/kvm/lapic.c|2933| <<apic_sync_pv_eoi_from_guest>> vector = apic_set_eoi(apic);
+ */
 static int apic_set_eoi(struct kvm_lapic *apic)
 {
 	int vector = apic_find_highest_isr(apic);
@@ -1291,6 +1554,11 @@ static u32 __apic_read(struct kvm_lapic *apic, unsigned int offset)
 	return val;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1428| <<apic_mmio_read>> struct kvm_lapic *apic = to_lapic(this);
+ *   - arch/x86/kvm/lapic.c|2077| <<apic_mmio_write>> struct kvm_lapic *apic = to_lapic(this);
+ */
 static inline struct kvm_lapic *to_lapic(struct kvm_io_device *dev)
 {
 	return container_of(dev, struct kvm_lapic, dev);
@@ -1416,11 +1684,26 @@ static void limit_periodic_timer_frequency(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   -  arch/x86/kvm/lapic.c|1951| <<kvm_lapic_reg_write>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|1996| <<kvm_lapic_reg_write>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2238| <<kvm_lapic_reset>> apic_update_lvtt(apic);
+ *   - arch/x86/kvm/lapic.c|2514| <<kvm_apic_set_state>> apic_update_lvtt(apic);
+ */
 static void apic_update_lvtt(struct kvm_lapic *apic)
 {
 	u32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &
 			apic->lapic_timer.timer_mode_mask;
 
+	/*
+	 * 在以下使用kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|355| <<apic_lvtt_oneshot>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;
+	 *   - arch/x86/kvm/lapic.c|360| <<apic_lvtt_period>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
+	 *   - arch/x86/kvm/lapic.c|365| <<apic_lvtt_tscdeadline>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
+	 *   - arch/x86/kvm/lapic.c|1470| <<apic_update_lvtt>> if (apic->lapic_timer.timer_mode != timer_mode) {
+	 *   - arch/x86/kvm/lapic.c|1478| <<apic_update_lvtt>> apic->lapic_timer.timer_mode = timer_mode;
+	 */
 	if (apic->lapic_timer.timer_mode != timer_mode) {
 		if (apic_lvtt_tscdeadline(apic) != (timer_mode ==
 				APIC_LVT_TIMER_TSCDEADLINE)) {
@@ -1533,6 +1816,20 @@ void kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1675| <<apic_timer_expired>> kvm_apic_inject_pending_timer_irqs(apic);
+ *   - arch/x86/kvm/lapic.c|2620| <<kvm_inject_apic_timer_irqs>> kvm_apic_inject_pending_timer_irqs(apic);
+ *
+ * vcpu_run():
+ *   kvm_clear_request(KVM_REQ_PENDING_TIMER, vcpu);
+ *   if (kvm_cpu_has_pending_timer(vcpu))
+ *       kvm_inject_pending_timer_irqs(vcpu);
+ *       -> kvm_inject_apic_timer_irqs()
+ *          -> kvm_apic_inject_pending_timer_irqs()
+ *             -> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *          -> atomic_set(&apic->lapic_timer.pending, 0);
+ */
 static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1546,6 +1843,38 @@ static void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * [0] apic_timer_expired
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] apic_timer_expired
+ * [0] __hrtimer_run_queues
+ * [0] hrtimer_interrupt
+ * [0] smp_apic_timer_interrupt
+ * [0] apic_timer_interrupt
+ * [0] cpuidle_enter_state
+ * [0] cpuidle_enter
+ * [0] call_cpuidle
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1679| <<start_sw_tscdeadline>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|1763| <<start_sw_period>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|1829| <<start_hv_timer>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|1892| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic);
+ *   - arch/x86/kvm/lapic.c|2416| <<apic_timer_fn>> apic_timer_expired(apic);
+ */
 static void apic_timer_expired(struct kvm_lapic *apic)
 {
 	struct kvm_vcpu *vcpu = apic->vcpu;
@@ -1554,6 +1883,18 @@ static void apic_timer_expired(struct kvm_lapic *apic)
 	if (atomic_read(&apic->lapic_timer.pending))
 		return;
 
+	/*
+	 * 主要在以下使用kvm_timer:
+	 *   - arch/x86/kvm/lapic.c|1637| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1781| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1788| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 *   - arch/x86/kvm/lapic.c|1834| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1849| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1888| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1915| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 */
 	if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
 		ktimer->expired_tscdeadline = ktimer->tscdeadline;
 
@@ -1568,6 +1909,10 @@ static void apic_timer_expired(struct kvm_lapic *apic)
 	kvm_set_pending_timer(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2045| <<start_sw_timer>> start_sw_tscdeadline(apic);
+ */
 static void start_sw_tscdeadline(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1669,6 +2014,10 @@ static void advance_periodic_target_expiration(struct kvm_lapic *apic)
 		nsec_to_cycles(apic->vcpu, delta);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1846| <<start_sw_timer>> start_sw_period(apic);
+ */
 static void start_sw_period(struct kvm_lapic *apic)
 {
 	if (!apic->lapic_timer.period)
@@ -1703,9 +2052,27 @@ static void cancel_hv_timer(struct kvm_lapic *apic)
 	WARN_ON(preemptible());
 	WARN_ON(!apic->lapic_timer.hv_timer_in_use);
 	kvm_x86_ops->cancel_hv_timer(apic->vcpu);
+	/*
+	 * 主要在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1637| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1781| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1788| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 *   - arch/x86/kvm/lapic.c|1834| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1849| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1888| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1915| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 */
 	apic->lapic_timer.hv_timer_in_use = false;
 }
 
+/*
+ * EXIT_REASON_PREEMPTION_TIMER-->handle_preemption_timer()
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1850| <<restart_apic_timer>> if (!start_hv_timer(apic))
+ */
 static bool start_hv_timer(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
@@ -1719,9 +2086,24 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	if (!ktimer->tscdeadline)
 		return false;
 
+	/*
+	 * vmx_set_hv_timer()
+	 */
 	if (kvm_x86_ops->set_hv_timer(vcpu, ktimer->tscdeadline, &expired))
 		return false;
 
+	/*
+	 * 主要在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1637| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1781| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1788| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 *   - arch/x86/kvm/lapic.c|1834| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1849| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1888| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1915| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 */
 	ktimer->hv_timer_in_use = true;
 	hrtimer_cancel(&ktimer->timer);
 
@@ -1748,11 +2130,31 @@ static bool start_hv_timer(struct kvm_lapic *apic)
 	return true;
 }
 
+/*
+ * start_sw_timer
+ * vmx_pre_block
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by;
+ *   - arch/x86/kvm/lapic.c|1860| <<restart_apic_timer>> start_sw_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1899| <<kvm_lapic_switch_to_sw_timer>> start_sw_timer(apic);
+ */
 static void start_sw_timer(struct kvm_lapic *apic)
 {
 	struct kvm_timer *ktimer = &apic->lapic_timer;
 
 	WARN_ON(preemptible());
+	/*
+	 * 主要在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 */
 	if (apic->lapic_timer.hv_timer_in_use)
 		cancel_hv_timer(apic);
 	if (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))
@@ -1765,6 +2167,41 @@ static void start_sw_timer(struct kvm_lapic *apic)
 	trace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);
 }
 
+/*
+ * [0] restart_apic_timer
+ * [0] vmx_post_block
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] restart_apic_timer
+ * [0] kvm_lapic_reg_write
+ * [0] kvm_x2apic_msr_write
+ * [0] kvm_set_msr_common
+ * [0] vmx_set_msr
+ * [0] __kvm_set_msr
+ * [0] kvm_emulate_wrmsr
+ * [0] vmx_handle_exit
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1879| <<kvm_lapic_expired_hv_timer>> restart_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1888| <<kvm_lapic_switch_to_hv_timer>> restart_apic_timer(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|1909| <<kvm_lapic_restart_hv_timer>> restart_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|1926| <<start_apic_timer>> restart_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2061| <<kvm_lapic_reg_write>> restart_apic_timer(apic);
+ */
 static void restart_apic_timer(struct kvm_lapic *apic)
 {
 	preempt_disable();
@@ -1778,12 +2215,42 @@ static void restart_apic_timer(struct kvm_lapic *apic)
 	preempt_enable();
 }
 
+/*
+ * [0] kvm_lapic_expired_hv_timer
+ * [0] vcpu_enter_guest
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5570| <<handle_preemption_timer>> kvm_lapic_expired_hv_timer(vcpu);
+ *
+ * EXIT_REASON_PREEMPTION_TIMER
+ * ->handle_preemption_timer()
+ *   -> kvm_lapic_expired_hv_timer()
+ */
 void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	preempt_disable();
 	/* If the preempt notifier has already run, it also called apic_timer_expired */
+	/*
+	 * 主要在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1637| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1781| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1788| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 *   - arch/x86/kvm/lapic.c|1834| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1849| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1888| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1915| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 */
 	if (!apic->lapic_timer.hv_timer_in_use)
 		goto out;
 	WARN_ON(swait_active(&vcpu->wq));
@@ -1799,18 +2266,67 @@ void kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);
 
+/*
+ * kvm_lapic_switch_to_hv_timer
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7523| <<vmx_post_block>> kvm_lapic_switch_to_hv_timer(vcpu);
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> vcpu_run()
+ *    -> vcpu_block()
+ *       -> kvm_x86_ops->pre_block() = vmx_pre_block()
+ *          -> kvm_lapic_switch_to_sw_timer()
+ *       -> kvm_vcpu_block()
+ *       -> kvm_x86_ops->post_block = vmx_post_block()
+ *          -> kvm_lapic_switch_to_hv_timer()
+ */
 void kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)
 {
 	restart_apic_timer(vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_switch_to_hv_timer);
 
+/*
+ * kvm_lapic_switch_to_sw_timer
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7493| <<vmx_pre_block>> kvm_lapic_switch_to_sw_timer(vcpu);
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> vcpu_run()
+ *    -> vcpu_block()
+ *       -> kvm_x86_ops->pre_block() = vmx_pre_block()
+ *          -> kvm_lapic_switch_to_sw_timer()
+ *       -> kvm_vcpu_block()
+ *       -> kvm_x86_ops->post_block = vmx_post_block()
+ *          -> kvm_lapic_switch_to_hv_timer()
+ */
 void kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
 	preempt_disable();
 	/* Possibly the TSC deadline timer is not enabled yet */
+	/*
+	 * 主要在以下设置kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 */
 	if (apic->lapic_timer.hv_timer_in_use)
 		start_sw_timer(apic);
 	preempt_enable();
@@ -1825,6 +2341,12 @@ void kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)
 	restart_apic_timer(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2043| <<kvm_lapic_reg_write>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2180| <<kvm_set_lapic_tscdeadline_msr>> start_apic_timer(apic);
+ *   - arch/x86/kvm/lapic.c|2605| <<kvm_apic_set_state>> start_apic_timer(apic);
+ */
 static void start_apic_timer(struct kvm_lapic *apic)
 {
 	atomic_set(&apic->lapic_timer.pending, 0);
@@ -2113,6 +2635,13 @@ u64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)
 	return (tpr & 0xf0) >> 4;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2259| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu, APIC_DEFAULT_PHYS_BASE |
+ *   - arch/x86/kvm/lapic.c|2294| <<kvm_lapic_reset>> kvm_lapic_set_base(vcpu,
+ *   - arch/x86/kvm/lapic.c|2561| <<kvm_apic_set_state>> kvm_lapic_set_base(vcpu, vcpu->arch.apic_base);
+ *   - arch/x86/kvm/x86.c|375| <<kvm_set_apic_base>> kvm_lapic_set_base(vcpu, msr_info->data);
+ */
 void kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)
 {
 	u64 old_value = vcpu->arch.apic_base;
@@ -2169,6 +2698,29 @@ void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_apic_update_apicv);
 
+/*
+ * [0] kvm_lapic_reset
+ * [0] kvm_arch_vcpu_setup
+ * [0] kvm_vm_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] kvm_lapic_reset
+ * [0] kvm_apic_accept_events
+ * [0] kvm_arch_vcpu_ioctl_run
+ * [0] kvm_vcpu_ioctl
+ * [0] do_vfs_ioctl
+ * [0] ksys_ioctl
+ * [0] __x64_sys_ioctl
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|9320| <<kvm_vcpu_reset>> kvm_lapic_reset(vcpu, init_event);
+ */
 void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2189,6 +2741,13 @@ void kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	for (i = 0; i < KVM_APIC_LVT_NUM; i++)
 		kvm_lapic_set_reg(apic, APIC_LVTT + 0x10 * i, APIC_LVT_MASKED);
+	/*
+	 * 在以下调用apic_update_lvtt():
+	 *   -  arch/x86/kvm/lapic.c|1951| <<kvm_lapic_reg_write>> apic_update_lvtt(apic);
+	 *   - arch/x86/kvm/lapic.c|1996| <<kvm_lapic_reg_write>> apic_update_lvtt(apic);
+	 *   - arch/x86/kvm/lapic.c|2238| <<kvm_lapic_reset>> apic_update_lvtt(apic);
+	 *   - arch/x86/kvm/lapic.c|2514| <<kvm_apic_set_state>> apic_update_lvtt(apic);
+	 */
 	apic_update_lvtt(apic);
 	if (kvm_vcpu_is_reset_bsp(vcpu) &&
 	    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_LINT0_REENABLED))
@@ -2241,6 +2800,10 @@ static bool lapic_is_periodic(struct kvm_lapic *apic)
 	return apic_lvtt_period(apic);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq.c|25| <<kvm_cpu_has_pending_timer>> return apic_has_pending_timer(vcpu);
+ */
 int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2251,6 +2814,22 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * kvm_apic_local_deliver
+ * kvm_inject_pending_timer_irqs
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1649| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - arch/x86/kvm/lapic.c|2536| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|337| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2279,11 +2858,23 @@ static const struct kvm_io_device_ops apic_mmio_ops = {
 	.write    = apic_mmio_write,
 };
 
+/*
+ * 在以下使用apic_timer_fn():
+ *   - arch/x86/kvm/lapic.c|2365| <<kvm_create_lapic>> apic->lapic_timer.timer.function = apic_timer_fn;
+ */
 static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 {
 	struct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);
 	struct kvm_lapic *apic = container_of(ktimer, struct kvm_lapic, lapic_timer);
 
+	/*
+	 * 在以下调用apic_timer_expired():
+	 *   - arch/x86/kvm/lapic.c|1679| <<start_sw_tscdeadline>> apic_timer_expired(apic);
+	 *   - arch/x86/kvm/lapic.c|1763| <<start_sw_period>> apic_timer_expired(apic);
+	 *   - arch/x86/kvm/lapic.c|1829| <<start_hv_timer>> apic_timer_expired(apic);
+	 *   - arch/x86/kvm/lapic.c|1892| <<kvm_lapic_expired_hv_timer>> apic_timer_expired(apic);
+	 *   - arch/x86/kvm/lapic.c|2416| <<apic_timer_fn>> apic_timer_expired(apic);
+	 */
 	apic_timer_expired(apic);
 
 	if (lapic_is_periodic(apic)) {
@@ -2294,6 +2885,10 @@ static enum hrtimer_restart apic_timer_fn(struct hrtimer *data)
 		return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9574| <<kvm_arch_vcpu_init>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 {
 	struct kvm_lapic *apic;
@@ -2304,6 +2899,14 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	if (!apic)
 		goto nomem;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *    -> u64 apic_base;
+	 *    -> struct kvm_lapic *apic;
+	 *       -> void *regs;
+	 *       -> gpa_t vapic_addr;
+	 */
 	vcpu->arch.apic = apic;
 
 	apic->regs = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);
@@ -2319,6 +2922,12 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	apic->lapic_timer.timer.function = apic_timer_fn;
 	if (timer_advance_ns == -1) {
 		apic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;
+		/*
+		 * 在以下使用lapic_timer_advance_dynamic:
+		 *   - arch/x86/kvm/lapic.c|1571| <<__kvm_wait_lapic_expire>> if (lapic_timer_advance_dynamic)
+		 *   - arch/x86/kvm/lapic.c|2368| <<kvm_create_lapic>> lapic_timer_advance_dynamic = true;
+		 *   - arch/x86/kvm/lapic.c|2371| <<kvm_create_lapic>> lapic_timer_advance_dynamic = false;
+		 */
 		lapic_timer_advance_dynamic = true;
 	} else {
 		apic->lapic_timer.timer_advance_ns = timer_advance_ns;
@@ -2330,6 +2939,16 @@ int kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)
 	 * thinking that APIC state has changed.
 	 */
 	vcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;
+	/*
+	 * 在以下使用apic_sw_disabled:
+	 *   - arch/x86/kvm/lapic.c|261| <<apic_set_spiv>> static_key_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|263| <<apic_set_spiv>> static_key_slow_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2070| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2341| <<kvm_create_lapic>> static_key_slow_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2767| <<kvm_lapic_init>> jump_label_rate_limit(&apic_sw_disabled, HZ);
+	 *   - arch/x86/kvm/lapic.c|2773| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.h|186| <<kvm_apic_sw_enabled>> if (static_key_false(&apic_sw_disabled.key))
+	 */
 	static_key_slow_inc(&apic_sw_disabled.key); /* sw disabled at reset */
 	kvm_iodevice_init(&apic->dev, &apic_mmio_ops);
 
@@ -2366,6 +2985,27 @@ int kvm_apic_accept_pic_intr(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * kvm_inject_apic_timer_irqs
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/irq.c|154| <<kvm_inject_pending_timer_irqs>> kvm_inject_apic_timer_irqs(vcpu);
+ *
+ * vcpu_run():
+ *   kvm_clear_request(KVM_REQ_PENDING_TIMER, vcpu);
+ *   if (kvm_cpu_has_pending_timer(vcpu))
+ *       kvm_inject_pending_timer_irqs(vcpu);
+ *       -> kvm_inject_apic_timer_irqs()
+ *          -> kvm_apic_inject_pending_timer_irqs()
+ *          -> atomic_set(&apic->lapic_timer.pending, 0);
+ */
 void kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -2507,6 +3147,10 @@ void __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)
  * last entry. If yes, set EOI on guests's behalf.
  * Clear PV EOI in guest memory in any case.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2942| <<kvm_lapic_sync_from_vapic>> apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);
+ */
 static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 					struct kvm_lapic *apic)
 {
@@ -2537,6 +3181,11 @@ static void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,
 	trace_kvm_pv_eoi(apic, vector);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8436| <<vcpu_enter_guest>> kvm_lapic_sync_from_vapic(vcpu);
+ *   - arch/x86/kvm/x86.c|8445| <<vcpu_enter_guest>> kvm_lapic_sync_from_vapic(vcpu);
+ */
 void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
 {
 	u32 data;
@@ -2689,6 +3338,12 @@ int kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)
 	return 0;
 }
 
+/*
+ * calledMSR_KVM_PV_EOI_EN:
+ *   - arch/x86/kvm/hyperv.c|1123| <<kvm_hv_set_msr>> if (kvm_lapic_enable_pv_eoi(vcpu, 0, 0))
+ *   - arch/x86/kvm/hyperv.c|1141| <<kvm_hv_set_msr>> if (kvm_lapic_enable_pv_eoi(vcpu,
+ *   - arch/x86/kvm/x86.c|2910| <<kvm_set_msr_common(处理MSR_KVM_PV_EOI_EN)>> if (kvm_lapic_enable_pv_eoi(vcpu, data, sizeof(u8)))
+ */
 int kvm_lapic_enable_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)
 {
 	u64 addr = data & ~KVM_MSR_ENABLED;
@@ -2752,10 +3407,33 @@ void kvm_apic_accept_events(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|7358| <<kvm_arch_init>> kvm_lapic_init();
+ */
 void kvm_lapic_init(void)
 {
 	/* do not patch jump label more than once per second */
+	/*
+	 * 在以下使用apic_hw_disabled:
+	 *   - arch/x86/kvm/lapic.c|2067| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_hw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2144| <<kvm_lapic_set_base>> static_key_slow_dec_deferred(&apic_hw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2146| <<kvm_lapic_set_base>> static_key_slow_inc(&apic_hw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2766| <<kvm_lapic_init>> jump_label_rate_limit(&apic_hw_disabled, HZ);
+	 *   - arch/x86/kvm/lapic.c|2772| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+	 *   - arch/x86/kvm/lapic.h|177| <<kvm_apic_hw_enabled>> if (static_key_false(&apic_hw_disabled.key))
+	 */
 	jump_label_rate_limit(&apic_hw_disabled, HZ);
+	/*
+	 * 在以下使用apic_sw_disabled:
+	 *   - arch/x86/kvm/lapic.c|261| <<apic_set_spiv>> static_key_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|263| <<apic_set_spiv>> static_key_slow_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2070| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2341| <<kvm_create_lapic>> static_key_slow_inc(&apic_sw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2767| <<kvm_lapic_init>> jump_label_rate_limit(&apic_sw_disabled, HZ);
+	 *   - arch/x86/kvm/lapic.c|2773| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_sw_disabled);
+	 *   - arch/x86/kvm/lapic.h|186| <<kvm_apic_sw_enabled>> if (static_key_false(&apic_sw_disabled.key))
+	 */
 	jump_label_rate_limit(&apic_sw_disabled, HZ);
 }
 
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index e800cced3c47..c6fadff82ffa 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -28,22 +28,77 @@ struct kvm_timer {
 	struct hrtimer timer;
 	s64 period; 				/* unit: ns */
 	ktime_t target_expiration;
+	/*
+	 * 在以下使用kvm_timer->timer_mode:
+	 *   - arch/x86/kvm/lapic.c|355| <<apic_lvtt_oneshot>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;
+	 *   - arch/x86/kvm/lapic.c|360| <<apic_lvtt_period>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;
+	 *   - arch/x86/kvm/lapic.c|365| <<apic_lvtt_tscdeadline>> return apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;
+	 *   - arch/x86/kvm/lapic.c|1470| <<apic_update_lvtt>> if (apic->lapic_timer.timer_mode != timer_mode) {
+	 *   - arch/x86/kvm/lapic.c|1478| <<apic_update_lvtt>> apic->lapic_timer.timer_mode = timer_mode;
+	 */
 	u32 timer_mode;
 	u32 timer_mode_mask;
+	/*
+	 * 在以下使用kvm_timer->tscdeadline:
+	 *   - arch/x86/kvm/lapic.c|1567| <<apic_update_lvtt>> apic->lapic_timer.tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1693| <<kvm_apic_inject_pending_timer_irqs>> ktimer->tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1695| <<kvm_apic_inject_pending_timer_irqs>> ktimer->tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1753| <<apic_timer_expired>> ktimer->expired_tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1769| <<start_sw_tscdeadline>> u64 guest_tsc, tscdeadline = ktimer->tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|1777| <<start_sw_tscdeadline>> if (unlikely(!tscdeadline || !this_tsc_khz))
+	 *   - arch/x86/kvm/lapic.c|1785| <<start_sw_tscdeadline>> ns = (tscdeadline - guest_tsc) * 1000000ULL;
+	 *   - arch/x86/kvm/lapic.c|1788| <<start_sw_tscdeadline>> if (likely(tscdeadline > guest_tsc) &&
+	 *   - arch/x86/kvm/lapic.c|1817| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *   - arch/x86/kvm/lapic.c|1833| <<set_target_expiration>> apic->lapic_timer.tscdeadline = 0;
+	 *   - arch/x86/kvm/lapic.c|1839| <<set_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+	 *   - arch/x86/kvm/lapic.c|1863| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
+	 *   - arch/x86/kvm/lapic.c|1924| <<start_hv_timer>> if (!ktimer->tscdeadline)
+	 *   - arch/x86/kvm/lapic.c|1927| <<start_hv_timer>> if (kvm_x86_ops->set_hv_timer(vcpu, ktimer->tscdeadline, &expired))
+	 *   - arch/x86/kvm/lapic.c|2362| <<kvm_get_lapic_tscdeadline_msr>> return apic->lapic_timer.tscdeadline;
+	 *   - arch/x86/kvm/lapic.c|2374| <<kvm_set_lapic_tscdeadline_msr>> apic->lapic_timer.tscdeadline = data;
+	 */
 	u64 tscdeadline;
 	u64 expired_tscdeadline;
 	u32 timer_advance_ns;
 	s64 advance_expire_delta;
 	atomic_t pending;			/* accumulated triggered timers */
+	/*
+	 * 主要在以下使用kvm_timer->hv_timer_in_use:
+	 *   - arch/x86/kvm/lapic.c|1637| <<apic_timer_expired>> if (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1781| <<kvm_lapic_hv_timer_in_use>> return vcpu->arch.apic->lapic_timer.hv_timer_in_use;
+	 *   - arch/x86/kvm/lapic.c|1788| <<cancel_hv_timer>> WARN_ON(!apic->lapic_timer.hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1790| <<cancel_hv_timer>> apic->lapic_timer.hv_timer_in_use = false;
+	 *   - arch/x86/kvm/lapic.c|1813| <<start_hv_timer>> ktimer->hv_timer_in_use = true;
+	 *   - arch/x86/kvm/lapic.c|1834| <<start_hv_timer>> trace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);
+	 *   - arch/x86/kvm/lapic.c|1849| <<start_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1888| <<kvm_lapic_expired_hv_timer>> if (!apic->lapic_timer.hv_timer_in_use)
+	 *   - arch/x86/kvm/lapic.c|1915| <<kvm_lapic_switch_to_sw_timer>> if (apic->lapic_timer.hv_timer_in_use)
+	 */
 	bool hv_timer_in_use;
 };
 
 struct kvm_lapic {
+	/*
+	 * 在以下使用kvm_lapic->base_address:
+	 *   - arch/x86/kvm/lapic.c|1421| <<apic_mmio_in_range>> return addr >= apic->base_address &&
+	 *   - arch/x86/kvm/lapic.c|1422| <<apic_mmio_in_range>> addr < apic->base_address + LAPIC_MMIO_LENGTH;
+	 *   - arch/x86/kvm/lapic.c|1429| <<apic_mmio_read>> u32 offset = address - apic->base_address;
+	 *   - arch/x86/kvm/lapic.c|2078| <<apic_mmio_write>> unsigned int offset = address - apic->base_address;
+	 *   - arch/x86/kvm/lapic.c|2235| <<kvm_lapic_set_base>> apic->base_address = apic->vcpu->arch.apic_base &
+	 *   - arch/x86/kvm/lapic.c|2239| <<kvm_lapic_set_base>> apic->base_address != APIC_DEFAULT_PHYS_BASE)
+	 */
 	unsigned long base_address;
 	struct kvm_io_device dev;
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下使用kvm_lapic->sw_enabled:
+	 *   - arch/x86/kvm/lapic.c|302| <<apic_set_spiv>> if (enabled != apic->sw_enabled) {
+	 *   - arch/x86/kvm/lapic.c|303| <<apic_set_spiv>> apic->sw_enabled = enabled;\
+	 *   - arch/x86/kvm/lapic.c|2262| <<kvm_free_lapic>> if (!apic->sw_enabled)
+	 *   - arch/x86/kvm/lapic.h|237| <<kvm_apic_sw_enabled>> return apic->sw_enabled;
+	 */
 	bool sw_enabled;
 	bool irr_pending;
 	bool lvt0_in_nmi_mode;
@@ -163,6 +218,18 @@ static inline void kvm_lapic_set_reg(struct kvm_lapic *apic, int reg_off, u32 va
 
 extern struct static_key kvm_no_apic_vcpu;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|139| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|1434| <<apic_mmio_read>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|1522| <<lapic_timer_int_injected>> if (kvm_apic_hw_enabled(apic)) {
+ *   - arch/x86/kvm/lapic.c|2084| <<apic_mmio_write>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2356| <<kvm_apic_local_deliver>> if (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {
+ *   - arch/x86/kvm/lapic.c|2478| <<kvm_apic_has_interrupt>> if (!kvm_apic_hw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2490| <<kvm_apic_accept_pic_intr>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ *   - arch/x86/kvm/lapic.h|201| <<kvm_apic_present>> return lapic_in_kernel(vcpu) && kvm_apic_hw_enabled(vcpu->arch.apic);
+ *   - arch/x86/kvm/x86.c|8076| <<vcpu_load_eoi_exitmap>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ */
 static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 {
 	if (static_key_false(&kvm_no_apic_vcpu))
@@ -170,10 +237,49 @@ static inline bool lapic_in_kernel(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * 在以下使用apic_hw_disabled:
+ *   - arch/x86/kvm/lapic.c|2067| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2144| <<kvm_lapic_set_base>> static_key_slow_dec_deferred(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.c|2146| <<kvm_lapic_set_base>> static_key_slow_inc(&apic_hw_disabled.key);
+ *   - arch/x86/kvm/lapic.c|2766| <<kvm_lapic_init>> jump_label_rate_limit(&apic_hw_disabled, HZ);
+ *   - arch/x86/kvm/lapic.c|2772| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+ *   - arch/x86/kvm/lapic.h|177| <<kvm_apic_hw_enabled>> if (static_key_false(&apic_hw_disabled.key))
+ */
 extern struct static_key_deferred apic_hw_disabled;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|139| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|1439| <<apic_mmio_read>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|1527| <<lapic_timer_int_injected>> if (kvm_apic_hw_enabled(apic)) {
+ *   - arch/x86/kvm/lapic.c|2206| <<apic_mmio_write>> if (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {
+ *   - arch/x86/kvm/lapic.c|2497| <<kvm_apic_local_deliver>> if (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {
+ *   - arch/x86/kvm/lapic.c|2627| <<kvm_apic_has_interrupt>> if (!kvm_apic_hw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2639| <<kvm_apic_accept_pic_intr>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ *   - arch/x86/kvm/lapic.h|243| <<kvm_apic_present>> return lapic_in_kernel(vcpu) && kvm_apic_hw_enabled(vcpu->arch.apic);
+ *   - arch/x86/kvm/x86.c|8076| <<vcpu_load_eoi_exitmap>> if (!kvm_apic_hw_enabled(vcpu->arch.apic))
+ */
 static inline int kvm_apic_hw_enabled(struct kvm_lapic *apic)
 {
+	/*
+	 * crash> apic_hw_disabled
+	 * apic_hw_disabled = $4 = {
+	 *   key = {
+	 *     enabled = {
+	 *       counter = 0
+	 *   },
+	 *
+	 * 在以下使用apic_hw_disabled:
+	 *   - arch/x86/kvm/lapic.c|2067| <<kvm_free_lapic>> static_key_slow_dec_deferred(&apic_hw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2144| <<kvm_lapic_set_base>> static_key_slow_dec_deferred(&apic_hw_disabled);
+	 *   - arch/x86/kvm/lapic.c|2146| <<kvm_lapic_set_base>> static_key_slow_inc(&apic_hw_disabled.key);
+	 *   - arch/x86/kvm/lapic.c|2766| <<kvm_lapic_init>> jump_label_rate_limit(&apic_hw_disabled, HZ);
+	 *   - arch/x86/kvm/lapic.c|2772| <<kvm_lapic_exit>> static_key_deferred_flush(&apic_hw_disabled);
+	 *   - arch/x86/kvm/lapic.h|177| <<kvm_apic_hw_enabled>> if (static_key_false(&apic_hw_disabled.key))
+	 *
+	 * apic_base = 0xfee00c00 -> MSR_IA32_APICBASE_ENABLE is set!!!
+	 */
 	if (static_key_false(&apic_hw_disabled.key))
 		return apic->vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE;
 	return MSR_IA32_APICBASE_ENABLE;
@@ -181,6 +287,15 @@ static inline int kvm_apic_hw_enabled(struct kvm_lapic *apic)
 
 extern struct static_key_deferred apic_sw_disabled;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|78| <<kvm_irq_delivery_to_apic>> } else if (kvm_apic_sw_enabled(vcpu->arch.apic)) {
+ *   - arch/x86/kvm/lapic.c|139| <<apic_enabled>> return kvm_apic_sw_enabled(apic) && kvm_apic_hw_enabled(apic);
+ *   - arch/x86/kvm/lapic.c|263| <<recalculate_apic_map>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2014| <<kvm_lapic_reg_write>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.c|2025| <<kvm_lapic_reg_write>> if (!kvm_apic_sw_enabled(apic))
+ *   - arch/x86/kvm/lapic.h|206| <<kvm_lapic_enabled>> return kvm_apic_present(vcpu) && kvm_apic_sw_enabled(vcpu->arch.apic);
+ */
 static inline bool kvm_apic_sw_enabled(struct kvm_lapic *apic)
 {
 	if (static_key_false(&apic_sw_disabled.key))
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index a4564713a470..10670b144c98 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -6511,6 +6511,10 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 					msrs[i].host, false);
 }
 
+/*
+ * 在以下使用vmx_update_hv_timer() --> enable_preemption_timer为true的情况:
+ *   - arch/x86/kvm/vmx/vmx.c|6605| <<vmx_vcpu_run>> vmx_update_hv_timer(vcpu);
+ */
 static void vmx_update_hv_timer(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7480,6 +7484,17 @@ static int pi_pre_block(struct kvm_vcpu *vcpu)
 	return (vcpu->pre_pcpu == -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8453| <<vcpu_block>> (!kvm_x86_ops->pre_block || kvm_x86_ops->pre_block(vcpu) == 0)) {
+ *
+ * struct kvm_x86_ops vmx_x86_ops.pre_block = vmx_pre_block()
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> vcpu_run()
+ *    -> vcpu_block()
+ *       -> kvm_x86_ops->pre_block() = vmx_pre_block()
+ */
 static int vmx_pre_block(struct kvm_vcpu *vcpu)
 {
 	if (pi_pre_block(vcpu))
@@ -7502,6 +7517,17 @@ static void pi_post_block(struct kvm_vcpu *vcpu)
 	local_irq_enable();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8467| <<vcpu_block>> kvm_x86_ops->post_block(vcpu);
+ *
+ * struct kvm_x86_ops vmx_x86_ops.post_block = vmx_post_block()
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> vcpu_run()
+ *    -> vcpu_block()
+ *       -> kvm_x86_ops->post_block = vmx_post_block()
+ */
 static void vmx_post_block(struct kvm_vcpu *vcpu)
 {
 	if (kvm_x86_ops->set_hv_timer)
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 299640a78208..09acf2e2543b 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -260,6 +260,14 @@ struct vcpu_vmx {
 	unsigned int ple_window;
 	bool ple_window_dirty;
 
+	/*
+	 * 在以下设置vcpu_vmx->req_immediate_exit:
+	 *   - arch/x86/kvm/vmx/vmx.c|1138| <<vmx_prepare_switch_to_guest>> vmx->req_immediate_exit = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|7161| <<vmx_request_immediate_exit>> to_vmx(vcpu)->req_immediate_exit = true;
+	 * 在以下使用vcpu_vmx->req_immediate_exit:
+	 *   - arch/x86/kvm/vmx/vmx.c|5568| <<handle_preemption_timer>> if (!vmx->req_immediate_exit &&
+	 *   - arch/x86/kvm/vmx/vmx.c|6520| <<vmx_update_hv_timer>> if (vmx->req_immediate_exit) {
+	 */
 	bool req_immediate_exit;
 
 	/* Support for PML */
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 74f19e21350f..aa18b6002e0b 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -145,6 +145,11 @@ module_param(tsc_tolerance_ppm, uint, S_IRUGO | S_IWUSR);
  * advancement entirely.  Any other value is used as-is and disables adaptive
  * tuning, i.e. allows priveleged userspace to set an exact advancement time.
  */
+/*
+ * 在以下使用lapic_timer_advance_ns:
+ *   - arch/x86/kvm/x86.c|149| <<global>> module_param(lapic_timer_advance_ns, int , S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/x86.c|9574| <<kvm_arch_vcpu_init>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+ */
 static int __read_mostly lapic_timer_advance_ns = -1;
 module_param(lapic_timer_advance_ns, int, S_IRUGO | S_IWUSR);
 
@@ -158,6 +163,13 @@ EXPORT_SYMBOL_GPL(enable_vmware_backdoor);
 static bool __read_mostly force_emulation_prefix = false;
 module_param(force_emulation_prefix, bool, S_IRUGO);
 
+/*
+ * 在以下使用pi_inject_timer:
+ *   - arch/x86/kvm/x86.c|162| <<global>> module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
+ *   - arch/x86/kvm/lapic.c|119| <<kvm_can_post_timer_interrupt>> return pi_inject_timer && kvm_vcpu_apicv_active(vcpu);
+ *   - arch/x86/kvm/x86.c|7352| <<kvm_arch_init>> if (pi_inject_timer == -1)
+ *   - arch/x86/kvm/x86.c|7353| <<kvm_arch_init>> pi_inject_timer = housekeeping_enabled(HK_FLAG_TIMER);
+ */
 int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, S_IRUGO | S_IWUSR);
 
@@ -1702,6 +1714,10 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1709| <<apic_timer_expired>> kvm_set_pending_timer(vcpu);
+ */
 void kvm_set_pending_timer(struct kvm_vcpu *vcpu)
 {
 	kvm_make_request(KVM_REQ_PENDING_TIMER, vcpu);
@@ -8431,6 +8447,14 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8505| <<vcpu_run>> r = vcpu_block(kvm, vcpu);
+ *
+ * kvm_arch_vcpu_ioctl_run()
+ * -> vcpu_run()
+ *    -> vcpu_block()
+ */
 static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 {
 	if (!kvm_arch_vcpu_runnable(vcpu) &&
@@ -8465,6 +8489,11 @@ static inline int vcpu_block(struct kvm *kvm, struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|8506| <<vcpu_run>> if (kvm_vcpu_running(vcpu)) {
+ *   - arch/x86/kvm/x86.c|10110| <<kvm_arch_vcpu_runnable>> return kvm_vcpu_running(vcpu) || kvm_vcpu_has_events(vcpu);
+ */
 static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu) && kvm_x86_ops->check_nested_events)
@@ -8474,6 +8503,10 @@ static inline bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 		!vcpu->arch.apf.halted);
 }
 
+/*
+ * x86调用的例子:
+ *   - arch/x86/kvm/x86.c|8729| <<kvm_arch_vcpu_ioctl_run>> r = vcpu_run(vcpu);
+ */
 static int vcpu_run(struct kvm_vcpu *vcpu)
 {
 	int r;
@@ -9564,6 +9597,11 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 		goto fail_free_pio_data;
 
 	if (irqchip_in_kernel(vcpu->kvm)) {
+		/*
+		 * 在以下使用lapic_timer_advance_ns:
+		 *   - arch/x86/kvm/x86.c|149| <<global>> module_param(lapic_timer_advance_ns, int , S_IRUGO | S_IWUSR);
+		 *   - arch/x86/kvm/x86.c|9574| <<kvm_arch_vcpu_init>> r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
+		 */
 		r = kvm_create_lapic(vcpu, lapic_timer_advance_ns);
 		if (r < 0)
 			goto fail_mmu_destroy;
diff --git a/drivers/infiniband/core/cm.c b/drivers/infiniband/core/cm.c
index 54507569475a..744a2a23f1d9 100644
--- a/drivers/infiniband/core/cm.c
+++ b/drivers/infiniband/core/cm.c
@@ -3879,6 +3879,12 @@ static void cm_send_handler(struct ib_mad_agent *mad_agent,
 	}
 }
 
+/*
+ * 在以下使用cm_work_handler():
+ *   - drivers/infiniband/core/cm.c|959| <<cm_create_timewait_info>> INIT_DELAYED_WORK(&timewait_info->work.work, cm_work_handler);
+ *   - drivers/infiniband/core/cm.c|3982| <<cm_establish>> INIT_DELAYED_WORK(&work->work, cm_work_handler);
+ *   - drivers/infiniband/core/cm.c|4114| <<cm_recv_handler>> INIT_DELAYED_WORK(&work->work, cm_work_handler);
+ */
 static void cm_work_handler(struct work_struct *_work)
 {
 	struct cm_work *work = container_of(_work, struct cm_work, work.work);
diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 312c2fc961c0..d520995b5fbd 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -421,6 +421,61 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 	rxe_drop_ref(qp);
 }
 
+/*
+ * 几个例子
+ *
+ * [0] rxe_send
+ * [0] rxe_do_task
+ * [0] rxe_run_task
+ * [0] rxe_post_send
+ * [0] ib_uverbs_post_send
+ * [0] ib_uverbs_write
+ * [0] __vfs_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] __x64_sys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] rxe_send
+ * [0] rxe_responder
+ * [0] rxe_do_task
+ * [0] rxe_run_task
+ * [0] rxe_resp_queue_pkt
+ * [0] rxe_rcv
+ * [0] rxe_udp_encap_recv
+ * [0] udp_queue_rcv_one_skb
+ * [0] udp_queue_rcv_skb
+ * [0] udp_unicast_rcv_skb
+ * [0] __udp4_lib_rcv
+ * [0] udp_rcv
+ * [0] ip_protocol_deliver_rcu
+ * [0] ip_local_deliver_finish
+ * [0] ip_local_deliver
+ * [0] ip_rcv_finish
+ * [0] ip_rcv
+ * [0] __netif_receive_skb_one_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] receive_buf
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ *
+ * [0] rxe_send
+ * [0] rxe_do_task
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 int rxe_send(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	int err;
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.c b/drivers/infiniband/sw/rxe/rxe_verbs.c
index 36b4ab8ea4e6..a6de8278766d 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@ -689,6 +689,10 @@ static int post_one_send(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|758| <<rxe_post_send>> return rxe_post_send_kernel(qp, wr, bad_wr);
+ */
 static int rxe_post_send_kernel(struct rxe_qp *qp, const struct ib_send_wr *wr,
 				const struct ib_send_wr **bad_wr)
 {
@@ -735,6 +739,36 @@ static int rxe_post_send_kernel(struct rxe_qp *qp, const struct ib_send_wr *wr,
 	return err;
 }
 
+/*
+ * [0] rxe_post_send
+ * [0] rds_ib_rx
+ * [0] rds_ib_tasklet_fn_recv
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] native_safe_halt
+ * [0] default_idle
+ * [0] arch_cpu_idle
+ * [0] default_idle_call
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * [0] rxe_post_send
+ * [0] rds_send_xmit
+ * [0] rds_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] __x64_sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int rxe_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 			 const struct ib_send_wr **bad_wr)
 {
diff --git a/drivers/scsi/iscsi_tcp.c b/drivers/scsi/iscsi_tcp.c
index b5dd1caae5e9..f71e4f75391d 100644
--- a/drivers/scsi/iscsi_tcp.c
+++ b/drivers/scsi/iscsi_tcp.c
@@ -47,6 +47,12 @@ MODULE_LICENSE("GPL");
 
 static struct scsi_transport_template *iscsi_sw_tcp_scsi_transport;
 static struct scsi_host_template iscsi_sw_tcp_sht;
+/*
+ * 在以下使用iscsi_sw_tcp_transport:
+ *   - drivers/scsi/iscsi_tcp.c|939| <<iscsi_sw_tcp_session_create>> cls_session = iscsi_session_setup(&iscsi_sw_tcp_transport, shost,
+ *   - drivers/scsi/iscsi_tcp.c|1114| <<iscsi_sw_tcp_init>> &iscsi_sw_tcp_transport);
+ *   - drivers/scsi/iscsi_tcp.c|1123| <<iscsi_sw_tcp_exit>> iscsi_unregister_transport(&iscsi_sw_tcp_transport);
+ */
 static struct iscsi_transport iscsi_sw_tcp_transport;
 
 static unsigned int iscsi_max_lun = ~0;
@@ -58,6 +64,19 @@ module_param_named(debug_iscsi_tcp, iscsi_sw_tcp_dbg, int,
 MODULE_PARM_DESC(debug_iscsi_tcp, "Turn on debugging for iscsi_tcp module "
 		 "Set to 1 to turn on, and zero to turn off. Default is off.");
 
+/*
+ * called by:
+ *   - drivers/scsi/iscsi_tcp.c|87| <<iscsi_sw_tcp_recv>> ISCSI_SW_TCP_DBG(conn, "in %d bytes\n", skb->len - offset);
+ *   - drivers/scsi/iscsi_tcp.c|96| <<iscsi_sw_tcp_recv>> ISCSI_SW_TCP_DBG(conn, "read %d bytes status %d\n",
+ *   - drivers/scsi/iscsi_tcp.c|118| <<iscsi_sw_sk_state_check>> ISCSI_SW_TCP_DBG(conn, "TCP_CLOSE|TCP_CLOSE_WAIT\n");
+ *   - drivers/scsi/iscsi_tcp.c|207| <<iscsi_sw_tcp_write_space>> ISCSI_SW_TCP_DBG(conn, "iscsi_write_space\n");
+ *   - drivers/scsi/iscsi_tcp.c|342| <<iscsi_sw_tcp_xmit>> ISCSI_SW_TCP_DBG(conn, "xmit %d bytes\n", consumed);
+ *   - drivers/scsi/iscsi_tcp.c|350| <<iscsi_sw_tcp_xmit>> ISCSI_SW_TCP_DBG(conn, "Error sending PDU, errno=%d\n", rc);
+ *   - drivers/scsi/iscsi_tcp.c|409| <<iscsi_sw_tcp_send_hdr_done>> ISCSI_SW_TCP_DBG(tcp_conn->iscsi_conn,
+ *   - drivers/scsi/iscsi_tcp.c|422| <<iscsi_sw_tcp_send_hdr_prep>> ISCSI_SW_TCP_DBG(conn, "%s\n", conn->hdrdgst_en ?
+ *   - drivers/scsi/iscsi_tcp.c|465| <<iscsi_sw_tcp_send_data_prep>> ISCSI_SW_TCP_DBG(conn, "offset=%d, datalen=%d %s\n", offset, len,
+ *   - drivers/scsi/iscsi_tcp.c|491| <<iscsi_sw_tcp_send_linear_data_prep>> ISCSI_SW_TCP_DBG(conn, "datalen=%zd %s\n", len, conn->datadgst_en ?
+ */
 #define ISCSI_SW_TCP_DBG(_conn, dbg_fmt, arg...)		\
 	do {							\
 		if (iscsi_sw_tcp_dbg)				\
@@ -77,6 +96,57 @@ MODULE_PARM_DESC(debug_iscsi_tcp, "Turn on debugging for iscsi_tcp module "
  * @offset: offset in skb
  * @len: skb->len - offset
  */
+/*
+ * 两个例子
+ *
+ * [0] iscsi_sw_tcp_recv
+ * [0] iscsi_sw_tcp_data_ready
+ * [0] tcp_data_queue
+ * [0] tcp_rcv_established
+ * [0] tcp_v4_do_rcv
+ * [0] tcp_v4_rcv
+ * [0] ip_protocol_deliver_rcu
+ * [0] ip_local_deliver_finish
+ * [0] ip_local_deliver
+ * [0] ip_rcv_finish
+ * [0] ip_rcv
+ * [0] __netif_receive_skb_one_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] receive_buf
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ *
+ * [0] iscsi_sw_tcp_recv
+ * [0] iscsi_sw_tcp_data_ready
+ * [0] tcp_rcv_established
+ * [0] tcp_v4_do_rcv
+ * [0] tcp_v4_rcv
+ * [0] ip_protocol_deliver_rcu
+ * [0] ip_local_deliver_finish
+ * [0] ip_local_deliver
+ * [0] ip_rcv_finish
+ * [0] ip_rcv
+ * [0] __netif_receive_skb_one_core
+ * [0] __netif_receive_skb
+ * [0] netif_receive_skb_internal
+ * [0] napi_gro_receive
+ * [0] receive_buf
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ *
+ * called by:
+ *   - drivers/scsi/iscsi_tcp.c|208| <<iscsi_sw_tcp_data_ready>> tcp_read_sock(sk, &rd_desc, iscsi_sw_tcp_recv);
+ */
 static int iscsi_sw_tcp_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,
 			     unsigned int offset, size_t len)
 {
@@ -305,6 +375,19 @@ static int iscsi_sw_tcp_xmit_segment(struct iscsi_tcp_conn *tcp_conn,
  * iscsi_sw_tcp_xmit - TCP transmit
  * @conn: iscsi connection
  **/
+/*
+ * [0] iscsi_sw_tcp_xmit
+ * [0] iscsi_sw_tcp_pdu_xmit
+ * [0] iscsi_xmit_task
+ * [0] iscsi_xmitworker
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/scsi/iscsi_tcp.c|385| <<iscsi_sw_tcp_pdu_xmit>> rc = iscsi_sw_tcp_xmit(conn);
+ */
 static int iscsi_sw_tcp_xmit(struct iscsi_conn *conn)
 {
 	struct iscsi_tcp_conn *tcp_conn = conn->dd_data;
@@ -540,6 +623,22 @@ static int iscsi_sw_tcp_pdu_alloc(struct iscsi_task *task, uint8_t opcode)
 	return 0;
 }
 
+/*
+ * [0] iscsi_sw_tcp_session_create
+ * [0] iscsi_if_recv_msg
+ * [0] iscsi_if_rx
+ * [0] netlink_unicast
+ * [0] netlink_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] __x64_sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct iscsi_transport iscsi_sw_tcp_transpor.create_conn = iscsi_sw_tcp_conn_create()
+ */
 static struct iscsi_cls_conn *
 iscsi_sw_tcp_conn_create(struct iscsi_cls_session *cls_session,
 			 uint32_t conn_idx)
@@ -989,6 +1088,12 @@ static struct scsi_host_template iscsi_sw_tcp_sht = {
 	.track_queue_depth	= 1,
 };
 
+/*
+ * 在以下使用iscsi_sw_tcp_transport:
+ *   - drivers/scsi/iscsi_tcp.c|939| <<iscsi_sw_tcp_session_create>> cls_session = iscsi_session_setup(&iscsi_sw_tcp_transport, shost,
+ *   - drivers/scsi/iscsi_tcp.c|1114| <<iscsi_sw_tcp_init>> &iscsi_sw_tcp_transport);
+ *   - drivers/scsi/iscsi_tcp.c|1123| <<iscsi_sw_tcp_exit>> iscsi_unregister_transport(&iscsi_sw_tcp_transport);
+ */
 static struct iscsi_transport iscsi_sw_tcp_transport = {
 	.owner			= THIS_MODULE,
 	.name			= "tcp",
diff --git a/drivers/scsi/libiscsi.c b/drivers/scsi/libiscsi.c
index 74fdb180d011..f23fff54e367 100644
--- a/drivers/scsi/libiscsi.c
+++ b/drivers/scsi/libiscsi.c
@@ -1582,6 +1582,10 @@ static int iscsi_data_xmit(struct iscsi_conn *conn)
 	return rc;
 }
 
+/*
+ * 在以下使用iscsi_xmitworker():
+ *   - drivers/scsi/libiscsi.c|2916| <<iscsi_conn_setup>> INIT_WORK(&conn->xmitwork, iscsi_xmitworker);
+ */
 static void iscsi_xmitworker(struct work_struct *work)
 {
 	struct iscsi_conn *conn =
diff --git a/drivers/scsi/scsi_transport_iscsi.c b/drivers/scsi/scsi_transport_iscsi.c
index 5de8ced5169e..d3904d76cd3d 100644
--- a/drivers/scsi/scsi_transport_iscsi.c
+++ b/drivers/scsi/scsi_transport_iscsi.c
@@ -4530,6 +4530,16 @@ int iscsi_unregister_transport(struct iscsi_transport *tt)
 }
 EXPORT_SYMBOL_GPL(iscsi_unregister_transport);
 
+/*
+ * called by:
+ *   - drivers/scsi/iscsi_tcp.c|67| <<ISCSI_SW_TCP_DBG>> iscsi_dbg_trace(trace_iscsi_dbg_sw_tcp, \
+ *   - drivers/scsi/libiscsi.c|59| <<ISCSI_DBG_CONN>> iscsi_dbg_trace(trace_iscsi_dbg_conn, \
+ *   - drivers/scsi/libiscsi.c|70| <<ISCSI_DBG_SESSION>> iscsi_dbg_trace(trace_iscsi_dbg_session, \
+ *   - drivers/scsi/libiscsi.c|81| <<ISCSI_DBG_EH>> iscsi_dbg_trace(trace_iscsi_dbg_eh, \
+ *   - drivers/scsi/libiscsi_tcp.c|58| <<ISCSI_DBG_TCP>> iscsi_dbg_trace(trace_iscsi_dbg_tcp, \
+ *   - drivers/scsi/scsi_transport_iscsi.c|63| <<ISCSI_DBG_TRANS_SESSION>> iscsi_dbg_trace(trace_iscsi_dbg_trans_session, \
+ *   - drivers/scsi/scsi_transport_iscsi.c|74| <<ISCSI_DBG_TRANS_CONN>> iscsi_dbg_trace(trace_iscsi_dbg_trans_conn, \
+ */
 void iscsi_dbg_trace(void (*trace)(struct device *dev, struct va_format *),
 		     struct device *dev, const char *fmt, ...)
 {
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 07bc7dbba047..998b4219435c 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -219,6 +219,10 @@ int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);
 
 enum {
 	OUTSIDE_GUEST_MODE,
+	/*
+	 * 一个设置IN_GUEST_MODE的例子:
+	 *   - arch/x86/kvm/x86.c|8298| <<vcpu_enter_guest>> vcpu->mode = IN_GUEST_MODE;
+	 */
 	IN_GUEST_MODE,
 	EXITING_GUEST_MODE,
 	READING_SHADOW_PAGE_TABLES,
diff --git a/include/uapi/linux/kvm.h b/include/uapi/linux/kvm.h
index e735bc4075dc..2b7376ed5079 100644
--- a/include/uapi/linux/kvm.h
+++ b/include/uapi/linux/kvm.h
@@ -531,6 +531,13 @@ struct kvm_vapic_addr {
 #define KVM_MP_STATE_RUNNABLE          0
 #define KVM_MP_STATE_UNINITIALIZED     1
 #define KVM_MP_STATE_INIT_RECEIVED     2
+/*
+ * 在以下使用KVM_MP_STATE_HALTED:
+ *   - arch/x86/kvm/vmx/nested.c|3709| <<sync_vmcs02_to_vmcs12>> if (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)
+ *   - arch/x86/kvm/x86.c|7413| <<kvm_vcpu_halt>> vcpu->arch.mp_state = KVM_MP_STATE_HALTED;
+ *   - arch/x86/kvm/x86.c|8471| <<vcpu_block>> case KVM_MP_STATE_HALTED:
+ *   - arch/x86/kvm/x86.c|8889| <<kvm_arch_vcpu_ioctl_get_mpstate>> if (vcpu->arch.mp_state == KVM_MP_STATE_HALTED &&
+ */
 #define KVM_MP_STATE_HALTED            3
 #define KVM_MP_STATE_SIPI_RECEIVED     4
 #define KVM_MP_STATE_STOPPED           5
diff --git a/net/rds/af_rds.c b/net/rds/af_rds.c
index af4481d6386c..da830ea964fa 100644
--- a/net/rds/af_rds.c
+++ b/net/rds/af_rds.c
@@ -77,6 +77,14 @@ EXPORT_SYMBOL(rds_str_array);
 /* this is just used for stats gathering :/ */
 static DEFINE_SPINLOCK(rds_sock_lock);
 static unsigned long rds_sock_count;
+/*
+ * 在以下使用rds_sock_list:
+ *   - net/rds/af_rds.c|987| <<__rds_create>> list_add_tail(&rs->rs_item, &rds_sock_list);
+ *   - net/rds/af_rds.c|1076| <<rds_sock_inc_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+ *   - net/rds/af_rds.c|1112| <<rds6_sock_inc_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+ *   - net/rds/af_rds.c|1147| <<rds_sock_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+ *   - net/rds/af_rds.c|1181| <<rds6_sock_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+ */
 static LIST_HEAD(rds_sock_list);
 DECLARE_WAIT_QUEUE_HEAD(rds_poll_waitq);
 
@@ -944,6 +952,13 @@ static void rds_sock_destruct(struct sock *sk)
 	    &rs->rs_item != rs->rs_item.prev));
 }
 
+/*
+ * [0] rds_create
+ * [0] __sys_socket
+ * [0] __x64_sys_socket
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int __rds_create(struct socket *sock, struct sock *sk, int protocol)
 {
 	struct rds_sock *rs;
@@ -954,6 +969,10 @@ static int __rds_create(struct socket *sock, struct sock *sk, int protocol)
 	sk->sk_protocol		= protocol;
 	sk->sk_destruct		= rds_sock_destruct;
 
+	/*
+	 * struct rds_sock:
+	 *   -> struct sock rs_sk;
+	 */
 	rs = rds_sk_to_rs(sk);
 	spin_lock_init(&rs->rs_lock);
 	rwlock_init(&rs->rs_recv_lock);
@@ -971,6 +990,15 @@ static int __rds_create(struct socket *sock, struct sock *sk, int protocol)
 	rs->rs_rx_traces = 0;
 
 	spin_lock_init(&rs->rs_snd_lock);
+	/*
+	 * 在以下使用rds_sock->rs_buf_info_tbl:
+	 *   - net/rds/af_rds.c|128| <<rds_release>> rhashtable_free_and_destroy(&rs->rs_buf_info_tbl, rds_buf_info_free,
+	 *   - net/rds/af_rds.c|753| <<rds_add_buf_info>> info = rhashtable_lookup_fast(&rs->rs_buf_info_tbl, addr,
+	 *   - net/rds/af_rds.c|780| <<rds_add_buf_info>> *ret = rhashtable_insert_fast(&rs->rs_buf_info_tbl,
+	 *   - net/rds/af_rds.c|796| <<rds_add_buf_info>> info = rhashtable_lookup_fast(&rs->rs_buf_info_tbl, addr,
+	 *   - net/rds/af_rds.c|974| <<__rds_create>> ret = rhashtable_init(&rs->rs_buf_info_tbl, &rs_buf_info_params);
+	 *   - net/rds/rds.h|975| <<rds_get_buf_info>> return rhashtable_lookup_fast(&rs->rs_buf_info_tbl, addr,
+	 */
 	ret = rhashtable_init(&rs->rs_buf_info_tbl, &rs_buf_info_params);
 	if (ret)
 		return ret;
@@ -984,6 +1012,14 @@ static int __rds_create(struct socket *sock, struct sock *sk, int protocol)
 	}
 
 	spin_lock_bh(&rds_sock_lock);
+	/*
+	 * 在以下使用rds_sock_list:
+	 *   - net/rds/af_rds.c|987| <<__rds_create>> list_add_tail(&rs->rs_item, &rds_sock_list);
+	 *   - net/rds/af_rds.c|1076| <<rds_sock_inc_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+	 *   - net/rds/af_rds.c|1112| <<rds6_sock_inc_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+	 *   - net/rds/af_rds.c|1147| <<rds_sock_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+	 *   - net/rds/af_rds.c|1181| <<rds6_sock_info>> list_for_each_entry(rs, &rds_sock_list, rs_item) {
+	 */
 	list_add_tail(&rs->rs_item, &rds_sock_list);
 	rds_sock_count++;
 	spin_unlock_bh(&rds_sock_lock);
@@ -991,6 +1027,13 @@ static int __rds_create(struct socket *sock, struct sock *sk, int protocol)
 	return 0;
 }
 
+/*
+ * [0] rds_create
+ * [0] __sys_socket
+ * [0] __x64_sys_socket
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int rds_create(struct net *net, struct socket *sock, int protocol, int kern)
 {
 	struct sock *sk;
@@ -1000,6 +1043,12 @@ static int rds_create(struct net *net, struct socket *sock, int protocol, int ke
 	    (protocol && IPPROTO_OKA != protocol))
 		return -ESOCKTNOSUPPORT;
 
+	/*
+	 * struct rds_sock:
+	 *   -> struct sock rs_sk;
+	 *
+	 * struct proto rds_proto.obj_size = sizeof(struct rds_sock)
+	 */
 	sk = sk_alloc(net, AF_RDS, GFP_KERNEL, &rds_proto, kern);
 	if (!sk)
 		return -ENOMEM;
diff --git a/net/rds/bind.c b/net/rds/bind.c
index c37dd96f7ac8..97ff83660ff7 100644
--- a/net/rds/bind.c
+++ b/net/rds/bind.c
@@ -184,6 +184,12 @@ void rds_remove_bound(struct rds_sock *rs)
 	write_unlock_irqrestore(&bucket->lock, flags);
 }
 
+/*
+ * [0] rds_bind
+ * [0] __x64_sys_bind
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 int rds_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)
 {
 	struct sock *sk = sock->sk;
diff --git a/net/rds/connection.c b/net/rds/connection.c
index 5882adf51029..90377ef881e4 100644
--- a/net/rds/connection.c
+++ b/net/rds/connection.c
@@ -55,6 +55,11 @@ static struct kmem_cache *rds_conn_slab;
 	 (head) < rds_conn_hash + ARRAY_SIZE(rds_conn_hash);	\
 	 (head)++)
 
+/*
+ * called by:
+ *   - net/rds/connection.c|236| <<__rds_conn_create>> struct hlist_head *head = rds_conn_bucket(laddr, faddr, tos);
+ *   - net/rds/connection.c|446| <<rds_conn_find>> struct hlist_head *head = rds_conn_bucket(laddr, faddr, tos);
+ */
 static struct hlist_head *rds_conn_bucket(const struct in6_addr *laddr,
 					  const struct in6_addr *faddr,
 					  u8 tos)
@@ -79,12 +84,32 @@ static struct hlist_head *rds_conn_bucket(const struct in6_addr *laddr,
 	return &rds_conn_hash[hash & RDS_CONNECTION_HASH_MASK];
 }
 
+/*
+ * 在以下调用rds_conn_info_set():
+ *   - net/rds/connection.c|945| <<rds_conn_info_visitor>> rds_conn_info_set(cinfo->flags, test_bit(RDS_IN_XMIT, &cp->cp_flags),
+ *   - net/rds/connection.c|948| <<rds_conn_info_visitor>> rds_conn_info_set(cinfo->flags,
+ *   - net/rds/connection.c|951| <<rds_conn_info_visitor>> rds_conn_info_set(cinfo->flags,
+ *   - net/rds/connection.c|954| <<rds_conn_info_visitor>> rds_conn_info_set(cinfo->flags, cp->cp_pending_flush,
+ *   - net/rds/connection.c|977| <<fill_path_info>> rds_conn_info_set(pinfo->flags,
+ *   - net/rds/connection.c|980| <<fill_path_info>> rds_conn_info_set(pinfo->flags,
+ *   - net/rds/connection.c|983| <<fill_path_info>> rds_conn_info_set(pinfo->flags, cp->cp_pending_flush,
+ *   - net/rds/connection.c|1022| <<rds6_conn_info_visitor>> rds_conn_info_set(cinfo6->flags, test_bit(RDS_IN_XMIT, &cp->cp_flags),
+ *   - net/rds/connection.c|1025| <<rds6_conn_info_visitor>> rds_conn_info_set(cinfo6->flags,
+ *   - net/rds/connection.c|1028| <<rds6_conn_info_visitor>> rds_conn_info_set(cinfo6->flags,
+ *   - net/rds/connection.c|1031| <<rds6_conn_info_visitor>> rds_conn_info_set(cinfo6->flags, cp->cp_pending_flush,
+ */
 #define rds_conn_info_set(var, test, suffix) do {		\
 	if (test)						\
 		var |= RDS_INFO_CONNECTION_FLAG_##suffix;	\
 } while (0)
 
 /* rcu read lock must be held or the connection spinlock */
+/*
+ * called by:
+ *   - net/rds/connection.c|247| <<__rds_conn_create>> conn = rds_conn_lookup(net, head, laddr, faddr, trans, tos, dev_if);
+ *   - net/rds/connection.c|386| <<__rds_conn_create>> found = rds_conn_lookup(net, head, laddr, faddr, trans, tos,
+ *   - net/rds/connection.c|449| <<rds_conn_find>> conn = rds_conn_lookup(net, head, laddr, faddr, trans, tos, dev_if);
+ */
 static struct rds_connection *rds_conn_lookup(struct net *net,
 					      struct hlist_head *head,
 					      const struct in6_addr *laddr,
@@ -224,6 +249,19 @@ static void __rds_conn_path_init(struct rds_connection *conn,
  * For now they are not garbage collected once they're created.  They
  * are torn down as the module is removed, if ever.
  */
+/*
+ * 猜测这里的conn是:
+ * # rds-info -n
+ *
+ * RDS Connections:
+ *       LocalAddr      RemoteAddr  Tos           NextTX           NextRX Flgs
+ * 100.100.231.248 100.100.231.251    0           789534         17396635 --C-
+ *
+ *
+ * called by:
+ *   - net/rds/connection.c|458| <<rds_conn_create>> return __rds_conn_create(net, laddr, faddr, trans, gfp, tos, 0, dev_if);
+ *   - net/rds/connection.c|478| <<rds_conn_create_outgoing>> return __rds_conn_create(net, laddr, faddr, trans, gfp, tos, 1, dev_if);
+ */
 static struct rds_connection *__rds_conn_create(struct net *net,
 						const struct in6_addr *laddr,
 						const struct in6_addr *faddr,
@@ -340,6 +378,19 @@ static struct rds_connection *__rds_conn_create(struct net *net,
 			cp->cp_wq = rds_cp_wqs[cp_wqs_inx];
 		}
 	}
+
+	/*
+	 * [0] rds_ib_conn_alloc
+	 * [0] rds_conn_create_outgoing
+	 * [0] rds_sendmsg
+	 * [0] sock_sendmsg
+	 * [0] ____sys_sendmsg
+	 * [0] ___sys_sendmsg
+	 * [0] __sys_sendmsg
+	 * [0] __x64_sys_sendmsg
+	 * [0] do_syscall_64
+	 * [0] entry_SYSCALL_64_after_hwframe
+	 */
 	ret = trans->conn_alloc(conn, gfp);
 	if (ret) {
 		kfree(conn->c_path);
@@ -417,6 +468,11 @@ static struct rds_connection *__rds_conn_create(struct net *net,
 	return conn;
 }
 
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|1458| <<rds_ib_cm_handle_connect>> conn = rds_conn_create(&init_net, daddr6, saddr6,
+ *   - net/rds/tcp_listen.c|192| <<rds_tcp_accept_one>> conn = rds_conn_create(sock_net(sock->sk),
+ */
 struct rds_connection *rds_conn_create(struct net *net,
 				       const struct in6_addr *laddr,
 				       const struct in6_addr *faddr,
@@ -427,6 +483,16 @@ struct rds_connection *rds_conn_create(struct net *net,
 }
 EXPORT_SYMBOL_GPL(rds_conn_create);
 
+/*
+ * [0] rds_conn_create_outgoing
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] __x64_sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 struct rds_connection *rds_conn_create_outgoing(struct net *net,
 						struct in6_addr *laddr,
 						struct in6_addr *faddr,
@@ -453,6 +519,10 @@ struct rds_connection *rds_conn_find(struct net *net, struct in6_addr *laddr,
 }
 EXPORT_SYMBOL_GPL(rds_conn_find);
 
+/*
+ * called by:
+ *   - net/rds/threads.c|430| <<rds_shutdown_worker>> rds_conn_shutdown(cp, restart);
+ */
 void rds_conn_shutdown(struct rds_conn_path *cp, int restart)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -780,6 +850,11 @@ static void rds6_conn_message_info_retrans(struct socket *sock,
 }
 #endif
 
+/*
+ * called by:
+ *   - net/rds/ib.c|864| <<rds_ib_ic_info>> rds_for_each_conn_info(sock, len, iter, lens,
+ *   - net/rds/ib.c|877| <<rds6_ib_ic_info>> rds_for_each_conn_info(sock, len, iter, lens,
+ */
 void rds_for_each_conn_info(struct socket *sock, unsigned int len,
 			    struct rds_info_iterator *iter,
 			    struct rds_info_lengths *lens,
@@ -1142,6 +1217,35 @@ char *conn_drop_reason_str(enum rds_conn_drop_src reason)
 /*
  * Force a disconnect
  */
+/*
+ * [0] rds_conn_path_drop
+ * [0] rds_rdma_cm_event_handler_cmn
+ * [0] rds_rdma_cm_event_handler
+ * [0] cma_ib_handler
+ * [0] cm_process_work
+ * [0] cm_work_handler
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - net/rds/af_rds.c|451| <<rds_user_conn_paths_drop>> rds_conn_path_drop(cp, DR_USER_RESET);
+ *   - net/rds/af_rds.c|456| <<rds_user_conn_paths_drop>> rds_conn_path_drop(cp, DR_USER_RESET);
+ *   - net/rds/connection.c|534| <<rds_conn_shutdown>> rds_conn_path_drop(cp, DR_INV_CONN_STATE);
+ *   - net/rds/connection.c|569| <<rds_conn_shutdown>> rds_conn_path_drop(cp, DR_DOWN_TRANSITION_FAIL);
+ *   - net/rds/connection.c|618| <<rds_conn_path_destroy>> rds_conn_path_drop(cp, DR_CONN_DESTROY);
+ *   - net/rds/connection.c|1261| <<rds_conn_drop>> rds_conn_path_drop(&conn->c_path[0], reason);
+ *   - net/rds/connection.c|1277| <<rds_conn_drop>> rds_conn_path_drop(peer->c_path + 0, reason);
+ *   - net/rds/tcp.c|659| <<rds_tcp_sysctl_reset>> rds_conn_path_drop(tc->t_cpath, DR_USER_RESET);
+ *   - net/rds/tcp_connect.c|67| <<rds_tcp_state_change>> rds_conn_path_drop(cp, DR_TCP_STATE_CLOSE);
+ *   - net/rds/tcp_connect.c|75| <<rds_tcp_state_change>> rds_conn_path_drop(cp, DR_TCP_STATE_CLOSE);
+ *   - net/rds/tcp_send.c|165| <<rds_tcp_xmit>> rds_conn_path_drop(cp, DR_TCP_STATE_CLOSE);
+ *   - net/rds/threads.c|113| <<rds_connect_path_complete>> rds_conn_path_drop(cp, DR_IB_NOT_CONNECTING_STATE);
+ *   - net/rds/threads.c|262| <<rds_connect_worker>> rds_conn_path_drop(cp, DR_CONN_CONNECT_FAIL);
+ *   - net/rds/threads.c|359| <<rds_hb_worker>> rds_conn_path_drop(cp, DR_HB_TIMEOUT);
+ *   - net/rds/threads.c|395| <<rds_reconnect_timeout>> rds_conn_path_drop(cp, DR_RECONNECT_TIMEOUT);
+ */
 void rds_conn_path_drop(struct rds_conn_path *cp, int reason)
 {
 	unsigned long now = get_seconds();
@@ -1157,6 +1261,13 @@ void rds_conn_path_drop(struct rds_conn_path *cp, int reason)
 		cp->cp_connection_reset = ktime_get_real_seconds();
 		/* Restart counting */
 		cp->cp_connection_attempts = 0;
+		/*
+		 * [329683.387224] RDS/IB: connection <::ffff:xxx.xxx.xxx.251,::ffff:xxx.xxx.xxx.20,0> dropped due to 'RDMA device removal'
+		 * [329683.391885] RDS/IB: connection <::ffff:xxx.xxx.xxx.251,::ffff:xxx.xxx.xxx.248,0> dropped due to 'RDMA device removal'
+		 *
+		 * [317830.182577] RDS/IB: connection <::ffff:xxx.xxx.xxx.251,::ffff:xxx.xxx.xxx.248,0> dropped due to 'DISCONNECTED event'
+		 * [317830.211642] RDS/IB: Passive conn 000000008ef4824b i_cm_id 00000000c91f483b, frag 16KB, connected <::ffff:xxx.xxx.xxx.251,::ffff:xxx.xxx.xxx.248,0> version 4.1
+		 */
 		if (conn->c_trans->t_type != RDS_TRANS_TCP)
 			printk(KERN_INFO "RDS/IB: connection <%pI6c,%pI6c,%d> dropped due to '%s'\n",
 			       &conn->c_laddr,
@@ -1198,6 +1309,38 @@ void rds_conn_path_drop(struct rds_conn_path *cp, int reason)
 }
 EXPORT_SYMBOL_GPL(rds_conn_path_drop);
 
+/*
+ * called by:
+ *   - net/rds/af_rds.c|488| <<rds_user_reset>> rds_conn_drop(conn, DR_USER_RESET);
+ *   - net/rds/ib.c|457| <<__rds_ib_dev_shutdown>> rds_conn_drop(ic->conn, DR_RDMA_DEV_REM);
+ *   - net/rds/ib_cm.c|354| <<rds_ib_cm_connect_complete>> rds_conn_drop(conn, DR_IB_CONN_DROP_RACE);
+ *   - net/rds/ib_cm.c|738| <<rds_ib_qp_event_handler>> rds_conn_drop(conn, DR_IB_QP_EVENT);
+ *   - net/rds/ib_cm.c|1517| <<rds_ib_cm_handle_connect>> rds_conn_drop(conn, DR_IB_REQ_WHILE_CONN_UP);
+ *   - net/rds/ib_cm.c|1535| <<rds_ib_cm_handle_connect>> rds_conn_drop(conn, DR_IB_REQ_WHILE_CONNECTING);
+ *   - net/rds/ib_cm.c|1546| <<rds_ib_cm_handle_connect>> rds_conn_drop(conn, DR_IB_REQ_WHILE_CONNECTING);
+ *   - net/rds/ib_cm.c|1602| <<rds_ib_cm_handle_connect>> rds_conn_drop(conn, DR_IB_PAS_SETUP_QP_FAIL);
+ *   - net/rds/ib_cm.c|1620| <<rds_ib_cm_handle_connect>> rds_conn_drop(conn, DR_IB_RDMA_ACCEPT_FAIL);
+ *   - net/rds/ib_cm.c|1716| <<rds_ib_cm_initiate_connect>> rds_conn_drop(conn, DR_IB_ACT_SETUP_QP_FAIL);
+ *   - net/rds/ib_cm.c|1726| <<rds_ib_cm_initiate_connect>> rds_conn_drop(conn, DR_IB_RDMA_CONNECT_FAIL);
+ *   - net/rds/ib_recv.c|612| <<rds_ib_recv_refill>> rds_conn_drop(conn, DR_IB_POST_RECV_FAIL);
+ *   - net/rds/ib_recv.c|911| <<rds_ib_send_ack>> rds_conn_drop(ic->conn, DR_IB_SEND_ACK_FAIL);
+ *   - net/rds/ib_recv.c|1107| <<rds_ib_process_recv>> rds_conn_drop(conn, DR_IB_HEADER_MISSING);
+ *   - net/rds/ib_recv.c|1118| <<rds_ib_process_recv>> rds_conn_drop(conn, DR_IB_HEADER_CORRUPTED);
+ *   - net/rds/ib_recv.c|1184| <<rds_ib_process_recv>> rds_conn_drop(conn, DR_IB_FRAG_HEADER_MISMATCH);
+ *   - net/rds/ib_recv.c|1387| <<rds_ib_recv_cqe_handler>> rds_conn_drop(conn, DR_IB_RECV_COMP_ERR);
+ *   - net/rds/ib_send.c|397| <<rds_ib_send_cqe_handler>> rds_conn_drop(conn, DR_IB_SEND_COMP_ERR);
+ *   - net/rds/ib_send.c|877| <<rds_ib_xmit>> rds_conn_drop(ic->conn, DR_IB_POST_SEND_FAIL);
+ *   - net/rds/rdma_transport.c|211| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_RESOLVE_ROUTE_FAIL);
+ *   - net/rds/rdma_transport.c|244| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_RDMA_CM_ID_MISMATCH);
+ *   - net/rds/rdma_transport.c|257| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_ROUTE_ERR);
+ *   - net/rds/rdma_transport.c|273| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_ADDR_ERR);
+ *   - net/rds/rdma_transport.c|287| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_CONNECT_ERR);
+ *   - net/rds/rdma_transport.c|313| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_CONSUMER_DEFINED_REJ);
+ *   - net/rds/rdma_transport.c|332| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_REJECTED_EVENT);
+ *   - net/rds/rdma_transport.c|348| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_ADDR_CHANGE);
+ *   - net/rds/rdma_transport.c|360| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_DISCONNECTED_EVENT);
+ 32 net/rds/rdma_transport.c|367| <<rds_rdma_cm_event_handler_cmn>> rds_conn_drop(conn, DR_IB_TIMEWAIT_EXIT);
+ */
 void rds_conn_drop(struct rds_connection *conn, int reason)
 {
 	WARN_ON(conn->c_trans->t_mp_capable);
diff --git a/net/rds/ib.c b/net/rds/ib.c
index e34f19ced7f2..b967f497da72 100644
--- a/net/rds/ib.c
+++ b/net/rds/ib.c
@@ -1318,6 +1318,16 @@ void rds_ib_exit(void)
 	}
 }
 
+/*
+ * 在以下使用rds_ib_transport:
+ *   - net/rds/ib.c|761| <<rds_ib_conn_info_visitor>> if (conn->c_trans != &rds_ib_transport)
+ *   - net/rds/ib.c|815| <<rds6_ib_conn_info_visitor>> if (conn->c_trans != &rds_ib_transport)
+ *   - net/rds/ib.c|1221| <<rds_ib_init>> ret = rds_trans_register(&rds_ib_transport);
+ *   - net/rds/ib.c|1263| <<rds_ib_exit>> rds_trans_unregister(&rds_ib_transport);
+ *   - net/rds/ib.c|1307| <<rds_ib_exit>> WARN_ON(atomic_read(&rds_ib_transport.t_conn_count));
+ *   - net/rds/ib_cm.c|1433| <<rds_ib_cm_handle_connect>> &rds_ib_transport, dp_cmn->ricpc_tos,
+ *   - net/rds/rdma_transport.c|106| <<rds_rdma_cm_event_handler_cmn>> struct rds_transport *trans = &rds_ib_transport;
+ */
 struct rds_transport rds_ib_transport = {
 	.laddr_check		= rds_ib_laddr_check,
 	.xmit_path_complete	= rds_ib_xmit_path_complete,
diff --git a/net/rds/ib.h b/net/rds/ib.h
index eef1fd91f686..9a4321a746c8 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -201,6 +201,15 @@ struct rds_ib_connection {
 	struct rds_connection	*conn;
 
 	/* alphabet soup, IBTA style */
+	/*
+	 * 在以下设置rds_ib_connection->i_cm_id:
+	 *   - net/rds/ib_cm.c|1562| <<rds_ib_cm_handle_connect>> ic->i_cm_id = cm_id;
+	 *   - net/rds/ib_cm.c|1726| <<rds_ib_conn_path_connect>> ic->i_cm_id = rds_ib_rdma_create_id(rds_conn_net(conn),
+	 *   - net/rds/ib_cm.c|1731| <<rds_ib_conn_path_connect>> ic->i_cm_id = NULL;
+	 *   - net/rds/ib_cm.c|1777| <<rds_ib_conn_path_connect>> ic->i_cm_id = NULL;
+	 *   - net/rds/ib_cm.c|1857| <<rds_ib_conn_path_shutdown>> ic->i_cm_id = NULL;
+	 *   - net/rds/rdma_transport.c|210| <<rds_rdma_cm_event_handler_cmn>> ibic->i_cm_id = NULL;
+	 */
 	struct rdma_cm_id	*i_cm_id;
 	struct rds_connection	*i_cm_id_ctx;
 	struct ib_pd		*i_pd;
@@ -284,6 +293,12 @@ struct rds_ib_connection {
 	struct completion       i_last_wqe_complete;
 
 	/* Active Bonding */
+	/*
+	 * 在以下使用rds_ib_connection->i_active_side:
+	 *   - net/rds/ib_cm.c|336| <<rds_ib_cm_connect_complete>> ic->i_active_side ? "Active " : "Passive",
+	 *   - net/rds/ib_cm.c|1699| <<rds_ib_cm_initiate_connect>> ic->i_active_side = 1;
+	 *   - net/rds/ib_cm.c|1902| <<rds_ib_conn_path_shutdown>> ic->i_active_side = 0;
+	 */
 	unsigned int		i_active_side;
 
 	int			i_scq_vector;
@@ -304,6 +319,12 @@ struct rds_ib_connection {
 	struct rds_ib_device	*i_saved_rds_ibdev;
 
 	/* qp number info */
+	/*
+	 * 在以下使用rds_ib_connection->i_qp_num:
+	 *   - net/rds/ib.c|791| <<rds_ib_conn_info_visitor>> iinfo->qp_num = ic->i_qp_num;
+	 *   - net/rds/ib.c|842| <<rds6_ib_conn_info_visitor>> iinfo6->qp_num = ic->i_qp_num;
+	 *   - net/rds/ib_cm.c|379| <<rds_ib_cm_connect_complete>> ic->i_qp_num = ic->i_cm_id->qp->qp_num;
+	 */
 	s32			i_qp_num;
 	s32			i_dst_qp_num;
 };
diff --git a/net/rds/ib_cm.c b/net/rds/ib_cm.c
index 952d65a77e91..c1dab8986748 100644
--- a/net/rds/ib_cm.c
+++ b/net/rds/ib_cm.c
@@ -261,6 +261,20 @@ static int rds_ib_match_acl(struct rdma_cm_id *cm_id,
  * Connection established.
  * We get here for both outgoing and incoming connection.
  */
+/*
+ * [0] rds_ib_cm_connect_complete
+ * [0] rds_rdma_cm_event_handler
+ * [0] cma_ib_handler
+ * [0] cm_process_work
+ * [0] cm_rep_handler
+ * [0] cm_work_handler
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * struct rds_transport rds_ib_transport.cm_connect_complete = rds_ib_cm_connect_complete()
+ */
 void rds_ib_cm_connect_complete(struct rds_connection *conn, struct rdma_cm_event *event)
 {
 	struct rds_ib_connection *ic = conn->c_transport_data;
@@ -315,6 +329,14 @@ void rds_ib_cm_connect_complete(struct rds_connection *conn, struct rdma_cm_even
 		}
 	}
 
+	/*
+	 * 在以下使用rds_ib_connection->i_active_side:
+	 *   - net/rds/ib_cm.c|336| <<rds_ib_cm_connect_complete>> ic->i_active_side ? "Active " : "Passive",
+	 *   - net/rds/ib_cm.c|1699| <<rds_ib_cm_initiate_connect>> ic->i_active_side = 1;
+	 *   - net/rds/ib_cm.c|1902| <<rds_ib_conn_path_shutdown>> ic->i_active_side = 0;
+	 *
+	 * RDS/IB: Active  conn 00000000c0404405 i_cm_id 000000002e971272, frag 16KB, connected <::ffff:100.100.231.248,::ffff:100.100.231.251,0> version 4.1
+	 */
 	printk(KERN_NOTICE "RDS/IB: %s conn %p i_cm_id %p, frag %dKB, connected <%pI6c,%pI6c,%d> version %u.%u%s%s\n",
 	       ic->i_active_side ? "Active " : "Passive",
 	       conn, ic->i_cm_id, ic->i_frag_sz / SZ_1K,
@@ -535,6 +557,11 @@ static void poll_scq(struct rds_ib_connection *ic, struct ib_cq *cq,
 	}
 }
 
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|638| <<rds_ib_rx>> poll_rcq(ic, ic->i_rcq, ic->i_recv_wc, &ack_state);
+ *   - net/rds/ib_cm.c|640| <<rds_ib_rx>> poll_rcq(ic, ic->i_rcq, ic->i_recv_wc, &ack_state);
+ */
 static void poll_rcq(struct rds_ib_connection *ic, struct ib_cq *cq,
 		     struct ib_wc *wcs,
 		     struct rds_ib_ack_state *ack_state)
@@ -599,6 +626,24 @@ void rds_ib_tasklet_fn_send(unsigned long data)
  * routines that reach into rds_rdma_free_op()
  * where irqs_disabled() warning is asserted!
  */
+/*
+ * 一个例子:
+ * [0] rds_ib_rx
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] native_safe_halt
+ * [0] default_idle
+ * [0] arch_cpu_idle
+ * [0] default_idle_call
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ */
 static void rds_ib_rx(struct rds_ib_connection *ic)
 {
 	struct rds_connection *conn = ic->conn;
@@ -987,6 +1032,20 @@ static void __rds_rdma_conn_dev_rele(struct rds_ib_connection *ic)
  * This needs to be very careful to not leave IS_ERR pointers around for
  * cleanup to trip over.
  */
+/*
+ * [0] rds_ib_setup_qp
+ * [0] rds_rdma_cm_event_handler_cmn
+ * [0] rds_rdma_cm_event_handler
+ * [0] cma_work_handler
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - net/rds/ib_cm.c|1549| <<rds_ib_cm_handle_connect>> err = rds_ib_setup_qp(conn);
+ *   - net/rds/ib_cm.c|1653| <<rds_ib_cm_initiate_connect>> ret = rds_ib_setup_qp(conn);
+ */
 static int rds_ib_setup_qp(struct rds_connection *conn)
 {
 	struct rds_ib_connection *ic = conn->c_transport_data;
@@ -1595,6 +1654,17 @@ void rds_ib_conn_destroy_init(struct rds_connection *conn)
 	queue_delayed_work(rds_aux_wq, &work->work, 0);
 }
 
+/*
+ * [0] rds_ib_cm_initiate_connect
+ * [0] rds_rdma_cm_event_handler
+ * [0] cma_work_handler
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * struct rds_transport rds_ib_transport.cm_initiate_connect = rds_ib_cm_initiate_connect()
+ */
 int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id, bool isv6)
 {
 	struct rds_connection *conn = rds_ib_get_conn(cm_id);
@@ -1755,6 +1825,12 @@ int rds_ib_conn_path_connect(struct rds_conn_path *cp)
  * so that it can be called at any point during startup.  In fact it
  * can be called multiple times for a given connection.
  */
+/*
+ * called by:
+ *   - net/rds/connection.c|547| <<rds_conn_shutdown>> conn->c_trans->conn_path_shutdown(cp);
+ *
+ * struct rds_transport rds_ib_transport.conn_path_shutdown = rds_ib_conn_path_shutdown()
+ */
 void rds_ib_conn_path_shutdown(struct rds_conn_path *cp)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -1871,12 +1947,27 @@ void rds_ib_conn_path_shutdown(struct rds_conn_path *cp)
 	ic->i_active_side = 0;
 }
 
+/*
+ * [0] rds_ib_conn_alloc
+ * [0] rds_conn_create_outgoing
+ * [0] rds_sendmsg
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] __x64_sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp)
 {
 	struct rds_ib_connection *ic;
 	unsigned long flags;
 
 	/* XXX too lazy? */
+	/*
+	 * !!! 这里分配的rds_ib_connection !!!
+	 */
 	ic = kzalloc(sizeof(struct rds_ib_connection), gfp);
 	if (!ic)
 		return -ENOMEM;
@@ -1906,6 +1997,9 @@ int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp)
 	rds_ib_init_ic_frag(ic);
 
 	ic->conn = conn;
+	/*
+	 * struct rds_ib_connection *ic;
+	 */
 	conn->c_transport_data = ic;
 
 	init_completion(&ic->i_last_wqe_complete);
diff --git a/net/rds/ib_rdma.c b/net/rds/ib_rdma.c
index 9fd03928a783..d155b66df109 100644
--- a/net/rds/ib_rdma.c
+++ b/net/rds/ib_rdma.c
@@ -1582,6 +1582,10 @@ void rds_ib_fcq_handler(struct rds_ib_device *rds_ibdev, struct ib_wc *wc)
 	complete(&ibmr->wr_comp);
 }
 
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|555| <<poll_scq>> rds_ib_mr_cqe_handler(ic, wc);
+ */
 void rds_ib_mr_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc)
 {
 	struct rds_ib_mr *ibmr;
diff --git a/net/rds/ib_recv.c b/net/rds/ib_recv.c
index 107880943691..9da80df68666 100644
--- a/net/rds/ib_recv.c
+++ b/net/rds/ib_recv.c
@@ -1086,6 +1086,10 @@ static void rds_ib_cong_recv(struct rds_connection *conn,
 	rds_cong_map_updated(map, uncongested);
 }
 
+/*
+ * called by:
+ *   - net/rds/ib_recv.c|1341| <<rds_ib_recv_cqe_handler>> rds_ib_process_recv(conn, recv, wc->byte_len, state);
+ */
 static void rds_ib_process_recv(struct rds_connection *conn,
 				struct rds_ib_recv_work *recv, u32 data_len,
 				struct rds_ib_ack_state *state)
@@ -1305,6 +1309,37 @@ void rds_ib_srq_process_recv(struct rds_connection *conn,
 	}
 }
 
+/*
+ * 两个用的多的例子
+ * [0] rds_ib_recv_cqe_handler
+ * [0] rds_ib_rx
+ * [0] rds_ib_tasklet_fn_recv
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] rds_ib_recv_cqe_handler
+ * [0] rds_ib_rx
+ * [0] rds_ib_tasklet_fn_recv
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] native_safe_halt
+ * [0] default_idle
+ * [0] arch_cpu_idle
+ * [0] default_idle_call
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ */
 void rds_ib_recv_cqe_handler(struct rds_ib_connection *ic,
 			     struct ib_wc *wc,
 			     struct rds_ib_ack_state *state)
@@ -1494,6 +1529,9 @@ static void rds_ib_srq_clear_ring(struct rds_ib_device *rds_ibdev)
 }
 
 
+/*
+ * struct rds_transport rds_ib_transport.recv_path = rds_ib_recv_path()
+ */
 int rds_ib_recv_path(struct rds_conn_path *cp)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -1510,6 +1548,10 @@ int rds_ib_recv_path(struct rds_conn_path *cp)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - net/rds/ib.c|1210| <<rds_ib_init>> ret = rds_ib_recv_init();
+ */
 int rds_ib_recv_init(void)
 {
 	struct sysinfo si;
@@ -1585,6 +1627,10 @@ static void rds_ib_srq_event(struct ib_event *event,
 }
 
 /* Setup SRQ for a device */
+/*
+ * called by:
+ *   - net/rds/ib.c|1149| <<rds_ib_add_one>> if (rds_ib_srq_init(rds_ibdev))
+ */
 int rds_ib_srq_init(struct rds_ib_device *rds_ibdev)
 {
 	struct ib_srq_init_attr srq_init_attr = {
diff --git a/net/rds/ib_ring.c b/net/rds/ib_ring.c
index b66cd4244d30..81d0b0ede0b1 100644
--- a/net/rds/ib_ring.c
+++ b/net/rds/ib_ring.c
@@ -93,6 +93,15 @@ static int __rds_ib_ring_empty(struct rds_ib_work_ring *ring)
 	return __rds_ib_ring_used(ring) == 0;
 }
 
+/*
+ * called by:
+ *   - net/rds/ib_recv.c|581| <<rds_ib_recv_refill>> && rds_ib_ring_alloc(&ic->i_recv_ring, 1, &pos)) {
+ *   - net/rds/ib_send.c|602| <<rds_ib_xmit>> work_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);
+ *   - net/rds/ib_send.c|610| <<rds_ib_xmit>> work_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);
+ *   - net/rds/ib_send.c|877| <<rds_ib_xmit_atomic>> work_alloc = rds_ib_ring_alloc(&ic->i_send_ring, 1, &pos);
+ *   - net/rds/ib_send.c|993| <<rds_ib_xmit_rdma>> work_alloc = rds_ib_ring_alloc(&ic->i_send_ring, i, &pos);
+ *   - net/rds/ib_send.c|1060| <<rds_ib_xmit_rdma>> rcomp_alloc = rds_ib_ring_alloc(&ic->i_send_ring, 1,
+ */
 u32 rds_ib_ring_alloc(struct rds_ib_work_ring *ring, u32 val, u32 *pos)
 {
 	u32 ret = 0, avail;
@@ -153,6 +162,10 @@ u32 rds_ib_ring_oldest(struct rds_ib_work_ring *ring)
  * returns the number of completed work requests.
  */
 
+/*
+ * called by:
+ *   - net/rds/ib_send.c|313| <<rds_ib_send_cqe_handler>> completed = rds_ib_ring_completed(&ic->i_send_ring, wc->wr_id, oldest);
+ */
 u32 rds_ib_ring_completed(struct rds_ib_work_ring *ring, u32 wr_id, u32 oldest)
 {
 	u32 ret;
diff --git a/net/rds/ib_send.c b/net/rds/ib_send.c
index 78d376b0d234..465e5b4b51fe 100644
--- a/net/rds/ib_send.c
+++ b/net/rds/ib_send.c
@@ -284,6 +284,28 @@ static void rds_ib_sub_signaled(struct rds_ib_connection *ic, int nr)
  * unallocs the next free entry in the ring it doesn't alter which is
  * the next to be freed, which is what this is concerned with.
  */
+/*
+ * 一个例子
+ * [0] rds_ib_send_cqe_handler
+ * [0] rds_ib_tasklet_fn_send
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] native_safe_halt
+ * [0] default_idle
+ * [0] arch_cpu_idle
+ * [0] default_idle_call
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] start_secondary
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - net/rds/ib_cm.c|553| <<poll_scq>> rds_ib_send_cqe_handler(ic, wc);
+ */
 void rds_ib_send_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc)
 {
 	struct rds_connection *conn = ic->conn;
@@ -561,6 +583,9 @@ static inline int rds_ib_set_wr_signal_state(struct rds_ib_connection *ic,
  * per connection.  This makes sure that the tx ring alloc/unalloc pairs
  * don't get out of sync and confuse the ring.
  */
+/*
+ * struct rds_transport rds_ib_transport.xmit = rds_ib_xmit()
+ */
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off)
 {
@@ -946,6 +971,9 @@ int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op)
 	return ret;
 }
 
+/*
+ * struct rds_transport rds_ib_transport.xmit_rdma = rds_ib_xmit_rdma()
+ */
 int rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op)
 {
 	struct rds_ib_connection *ic = conn->c_transport_data;
@@ -1003,6 +1031,9 @@ int rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op)
 	num_sge = op->op_count;
 
 	for (i = 0; i < work_alloc && scat != &op->op_sg[op->op_count]; i++) {
+		/*
+		 * struct rds_ib_send_work *send
+		 */
 		send->s_wr.send_flags = 0;
 		send->s_queued = jiffies;
 		send->s_op = NULL;
@@ -1091,6 +1122,19 @@ int rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op)
 		atomic_add(nr_sig, &ic->i_signaled_sends);
 
 	failed_wr = &first->s_wr;
+	/*
+	 * 在以下设置rds_ib_connection->i_cm_id:
+	 *   - net/rds/ib_cm.c|1562| <<rds_ib_cm_handle_connect>> ic->i_cm_id = cm_id;
+	 *   - net/rds/ib_cm.c|1726| <<rds_ib_conn_path_connect>> ic->i_cm_id = rds_ib_rdma_create_id(rds_conn_net(conn),
+	 *   - net/rds/ib_cm.c|1731| <<rds_ib_conn_path_connect>> ic->i_cm_id = NULL;
+	 *   - net/rds/ib_cm.c|1777| <<rds_ib_conn_path_connect>> ic->i_cm_id = NULL;
+	 *   - net/rds/ib_cm.c|1857| <<rds_ib_conn_path_shutdown>> ic->i_cm_id = NULL;
+	 *   - net/rds/rdma_transport.c|210| <<rds_rdma_cm_event_handler_cmn>> ibic->i_cm_id = NULL;
+	 *
+	 * struct rds_ib_connection:
+	 *  -> struct rdma_cm_id *i_cm_id;
+	 *      -> struct ib_qp *qp;
+	 */
 	ret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);
 	rdsdebug("ic %p first %p (wr %p) ret %d wr %p\n", ic,
 		 first, &first->s_wr, ret, failed_wr);
diff --git a/net/rds/ib_stats.c b/net/rds/ib_stats.c
index 0c0e6a31af55..2fbbb89a4f9a 100644
--- a/net/rds/ib_stats.c
+++ b/net/rds/ib_stats.c
@@ -97,6 +97,9 @@ static char *rds_ib_stat_names[] = {
 	"ib_recv_nmb_cache_removed",
 };
 
+/*
+ * struct rds_transport rds_ib_transport.stats_info_copy = rds_ib_stats_info_copy()
+ */
 unsigned int rds_ib_stats_info_copy(struct rds_info_iterator *iter,
 				    unsigned int avail)
 {
diff --git a/net/rds/info.c b/net/rds/info.c
index 8f810439f29d..d7ef2132007b 100644
--- a/net/rds/info.c
+++ b/net/rds/info.c
@@ -153,6 +153,10 @@ EXPORT_SYMBOL_GPL(rds_info_copy);
  * On success it returns the positive number of bytes of each array element
  * in the snapshot.
  */
+/*
+ * called by:
+ *   - net/rds/af_rds.c|704| <<rds_getsockopt>> ret = rds_info_getsockopt(sock, optname, optval,
+ */
 int rds_info_getsockopt(struct socket *sock, int optname, char __user *optval,
 			int __user *optlen)
 {
diff --git a/net/rds/rdma.c b/net/rds/rdma.c
index 2ac87c5a4cce..945eee5fde50 100644
--- a/net/rds/rdma.c
+++ b/net/rds/rdma.c
@@ -154,6 +154,12 @@ EXPORT_SYMBOL_GPL(rds_rdma_drop_keys);
 /*
  * Helper function to pin user pages.
  */
+/*
+ * called by:
+ *   - net/rds/rdma.c|254| <<__rds_rdma_map>> ret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);
+ *   - net/rds/rdma.c|656| <<rds_cmsg_rdma_args>> ret = rds_pin_pages(vec.addr, nr, pages, !op->op_write);
+ *   - net/rds/rdma.c|829| <<rds_cmsg_atomic>> ret = rds_pin_pages(args->local_addr, 1, &page, 1);
+ */
 static int rds_pin_pages(unsigned long user_addr, unsigned int nr_pages,
 			struct page **pages, int write)
 {
diff --git a/net/rds/rdma_transport.c b/net/rds/rdma_transport.c
index adcb7f2e9940..e8a6fa175c89 100644
--- a/net/rds/rdma_transport.c
+++ b/net/rds/rdma_transport.c
@@ -383,6 +383,12 @@ static int rds_rdma_cm_event_handler_cmn(struct rdma_cm_id *cm_id,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - net/rds/ib.c|927| <<rds_ib_laddr_check_cm>> cm_id = rds_ib_rdma_create_id(net, rds_rdma_cm_event_handler, &dummy_ic,
+ *   - net/rds/ib_cm.c|1723| <<rds_ib_conn_path_connect>> handler = rds_rdma_cm_event_handler;
+ *   - net/rds/rdma_transport.c|467| <<rds_rdma_listen_init>> ret = rds_rdma_listen_init_common(rds_rdma_cm_event_handler,
+ */
 int rds_rdma_cm_event_handler(struct rdma_cm_id *cm_id,
 			      struct rdma_cm_event *event)
 {
diff --git a/net/rds/rds.h b/net/rds/rds.h
index 020f94bd7f71..1383d0fa30a9 100644
--- a/net/rds/rds.h
+++ b/net/rds/rds.h
@@ -92,6 +92,15 @@ enum {
 	RDS_RTD_TCP			= 1 << 28,	/* 0x10000000 */
 };
 
+/*
+ * # echo "ibase=2;111111111111" | bc > /sys/module/rds/parameters/rds_rt_debug_bitmap
+ *
+ * # echo "ibase=2;100000001011" | bc > /sys/module/rds/parameters/rds_rt_debug_bitmap
+ *
+ * # echo "ibase=2;000000000000" | bc > /sys/module/rds/parameters/rds_rt_debug_bitmap
+ *
+ * # echo "ibase=2;11111111111111111111" | bc > /sys/module/rds/parameters/rds_rt_debug_bitmap
+ */
 #define rds_rtd(enabling_bit, format, arg...)				     \
 	do { if (likely(!(enabling_bit & kernel_rds_rt_debug_bitmap))) break;\
 		trace_printk("%d: " format, __LINE__, ## arg);		     \
@@ -814,6 +823,12 @@ struct rds_sock {
 	 * around. This helps avoid costly lookups.
 	 */
 	struct rds_connection	*rs_conn;
+	/*
+	 * 在以下使用rds_sock->rs_conn_path:
+	 *   - net/rds/af_rds.c|988| <<__rds_create>> rs->rs_conn_path = NULL;
+	 *   - net/rds/send.c|1493| <<rds_sendmsg>> cpath = rs->rs_conn_path;
+	 *   - net/rds/send.c|1525| <<rds_sendmsg>> rs->rs_conn_path = cpath;
+	 */
 	struct rds_conn_path	*rs_conn_path;
 
 	/* flag indicating we were congested or not */
@@ -840,9 +855,24 @@ struct rds_sock {
 	 * newline
 	 */
 	spinlock_t		rs_snd_lock;
+	/*
+	 * 在以下使用rds_sock->rs_send_queue:
+	 *   - net/rds/af_rds.c|960| <<__rds_create>> INIT_LIST_HEAD(&rs->rs_send_queue);
+	 *   - net/rds/send.c|961| <<rds_send_drop_to>> list_for_each_entry_safe(rm, tmp, &rs->rs_send_queue, m_sock_item) {
+	 *   - net/rds/send.c|1090| <<rds_send_queue_rm>> list_add_tail(&rm->m_sock_item, &rs->rs_send_queue);
+	 */
 	struct list_head	rs_send_queue;
 	u32			rs_snd_bytes; /* Total bytes to all peers */
 	u32			rs_buf_info_dest_cnt;
+	/*
+	 * 在以下使用rds_sock->rs_buf_info_tbl:
+	 *   - net/rds/af_rds.c|128| <<rds_release>> rhashtable_free_and_destroy(&rs->rs_buf_info_tbl, rds_buf_info_free,
+	 *   - net/rds/af_rds.c|753| <<rds_add_buf_info>> info = rhashtable_lookup_fast(&rs->rs_buf_info_tbl, addr,
+	 *   - net/rds/af_rds.c|780| <<rds_add_buf_info>> *ret = rhashtable_insert_fast(&rs->rs_buf_info_tbl,
+	 *   - net/rds/af_rds.c|796| <<rds_add_buf_info>> info = rhashtable_lookup_fast(&rs->rs_buf_info_tbl, addr,
+	 *   - net/rds/af_rds.c|974| <<__rds_create>> ret = rhashtable_init(&rs->rs_buf_info_tbl, &rs_buf_info_params);
+	 *   - net/rds/rds.h|975| <<rds_get_buf_info>> return rhashtable_lookup_fast(&rs->rs_buf_info_tbl, addr,
+	 */
 	struct rhashtable	rs_buf_info_tbl;
 
 	/*
diff --git a/net/rds/recv.c b/net/rds/recv.c
index 24d8920aa863..8a8703f1071c 100644
--- a/net/rds/recv.c
+++ b/net/rds/recv.c
@@ -344,6 +344,50 @@ static void rds_start_mprds(struct rds_connection *conn)
  * conn.  This lets loopback, who only has one conn for both directions,
  * tell us which roles the addrs in the conn are playing for this message.
  */
+/*
+ * 代表的两个例子
+ *
+ * [0] rds_recv_incoming
+ * [0] poll_rcq
+ * [0] rds_ib_rx
+ * [0] rds_ib_tasklet_fn_recv
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] run_ksoftirqd
+ * [0] smpboot_thread_fn
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * [0] rds_recv_incoming
+ * [0] poll_rcq
+ * [0] rds_ib_rx
+ * [0] rds_ib_tasklet_fn_recv
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] native_safe_halt
+ * [0] default_idle
+ * [0] arch_cpu_idle
+ * [0] default_idle_call
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] rest_init
+ * [0] arch_call_rest_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - net/rds/ib_recv.c|1201| <<rds_ib_process_recv>> rds_recv_incoming(conn, &conn->c_faddr, &conn->c_laddr,
+ *   - net/rds/ib_recv.c|1298| <<rds_ib_srq_process_recv>> rds_recv_incoming(conn, &conn->c_faddr, &conn->c_laddr,
+ *   - net/rds/loop.c|85| <<rds_loop_xmit>> rds_recv_incoming(conn, &conn->c_laddr, &conn->c_faddr, &rm->m_inc,
+ *   - net/rds/tcp_recv.c|243| <<rds_tcp_data_recv>> rds_recv_incoming(conn, &conn->c_faddr,
+ */
 void rds_recv_incoming(struct rds_connection *conn, struct in6_addr *saddr,
 		       struct in6_addr *daddr,
 		       struct rds_incoming *inc, gfp_t gfp)
@@ -614,6 +658,15 @@ rds_recv_forward(struct rds_conn_path *cp, struct rds_incoming *inc,
 		       inc, gfp, NULL);
 }
 
+/*
+ * called by:
+ *   - net/rds/recv.c|380| <<rds_recv_incoming>> rds_recv_local(cp, saddr, daddr, inc, gfp, rs);
+ *   - net/rds/recv.c|399| <<rds_recv_incoming>> rds_recv_local(cp, saddr, daddr, inc, gfp, rs);
+ *   - net/rds/recv.c|467| <<rds_recv_incoming>> rds_recv_local(cp, saddr, daddr, inc, gfp, NULL);
+ *   - net/rds/recv.c|476| <<rds_recv_incoming>> rds_recv_local(cp, saddr, daddr, inc, gfp, NULL);
+ *   - net/rds/recv.c|543| <<rds_recv_route>> rds_recv_local(&nconn->c_path[0],
+ *   - net/rds/recv.c|613| <<rds_recv_forward>> rds_recv_local(&inc->i_oconn->c_path[0], &org->saddr, &org->daddr,
+ */
 static void
 rds_recv_local(struct rds_conn_path *cp, struct in6_addr *saddr,
 	       struct in6_addr *daddr, struct rds_incoming *inc, gfp_t gfp,
@@ -984,6 +1037,15 @@ static int rds_cmsg_recv(struct rds_incoming *inc, struct msghdr *msg,
 	return ret;
 }
 
+/*
+ * [0] rds_recvmsg
+ * [0] ____sys_recvmsg
+ * [0] ___sys_recvmsg
+ * [0] __sys_recvmsg
+ * [0] __x64_sys_recvmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 int rds_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,
 		int msg_flags)
 {
diff --git a/net/rds/send.c b/net/rds/send.c
index 5f77c549043a..f91efa2f37df 100644
--- a/net/rds/send.c
+++ b/net/rds/send.c
@@ -182,8 +182,46 @@ static void release_in_xmit(struct rds_conn_path *cp)
  *      - small message latency is higher behind queued large messages
  *      - large message latency isn't starved by intervening small sends
  */
+/*
+ * [0] rds_send_xmit
+ * [0] sock_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] __x64_sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] rds_send_xmit
+ * [0] tasklet_action_common.isra.18
+ * [0] tasklet_action
+ * [0] __do_softirq
+ * [0] irq_exit
+ * [0] do_IRQ
+ * [0] ret_from_intr
+ * [0] native_safe_halt
+ * [0] default_idle
+ * [0] arch_cpu_idle
+ * [0] default_idle_call
+ * [0] do_idle
+ * [0] cpu_startup_entry
+ * [0] rest_init
+ * [0] arch_call_rest_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] secondary_startup_64
+ *
+ * called by:
+ *   - net/rds/ib_cm.c|593| <<rds_ib_tasklet_fn_send>> rds_send_xmit(&ic->conn->c_path[0]);
+ *   - net/rds/send.c|1613| <<rds_sendmsg>> ret = rds_send_xmit(cpath);
+ *   - net/rds/threads.c|285| <<rds_send_worker>> ret = rds_send_xmit(cp);
+ */
 int rds_send_xmit(struct rds_conn_path *cp)
 {
+	/*
+	 * 似乎一个connection可以有多个path
+	 */
 	struct rds_connection *conn = cp->cp_conn;
 	struct rds_message *rm;
 	unsigned long flags;
@@ -1017,6 +1055,12 @@ void rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in6 *dest)
  * possible that another thread can race with us and remove the
  * message from the flow with RDS_CANCEL_SENT_TO.
  */
+/*
+ * called by:
+ *   - net/rds/send.c|1607| <<rds_sendmsg>> while (!rds_send_queue_rm(rs, conn, cpath, rm, rs->rs_bound_port,
+ *   - net/rds/send.c|1621| <<rds_sendmsg>> rds_send_queue_rm(rs, conn, cpath, rm,
+ *   - net/rds/send.c|1776| <<rds_send_internal>> if (!rds_send_queue_rm(rs, conn, &conn->c_path[0], rm,
+ */
 static int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,
 			     struct rds_conn_path *cp,
 			     struct rds_message *rm, __be16 sport,
@@ -1235,6 +1279,17 @@ static int rds_send_mprds_hash(struct rds_sock *rs, struct rds_connection *conn)
 	return hash;
 }
 
+/*
+ * [0] rds_sendmsg
+ * [0] ____sys_sendmsg
+ * [0] ___sys_sendmsg
+ * [0] __sys_sendmsg
+ * [0] __x64_sys_sendmsg
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * struct proto_ops rds_proto_ops.sendmsg = rds_sendmsg()
+ */
 int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 {
 	struct sock *sk = sock->sk;
@@ -1391,6 +1446,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 		goto out;
 	}
 
+	/*
+	 * Check if there is a rs_buf_info associated with the given address.  If not,
+	 * add one to the rds_sock.  The found or added rs_buf_info is returned.  If
+	 * there is no rs_buf_info found and a new rs_buf_info cannot be allocated,
+	 * NULL is returned and ret is set to the error.  Once an address' rs_buf_info
+	 * is added, it will not be removed until the rs_sock is closed.
+	 */
 	bufi = rds_add_buf_info(rs, &daddr, &ret, GFP_KERNEL);
 	if (!bufi)
 		goto out;
@@ -1454,6 +1516,18 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 		conn = rs->rs_conn;
 		cpath = rs->rs_conn_path;
 	} else {
+		/*
+		 * [0] rds_conn_create_outgoing
+		 * [0] sock_sendmsg
+		 * [0] ____sys_sendmsg
+		 * [0] ___sys_sendmsg
+		 * [0] __sys_sendmsg
+		 * [0] __x64_sys_sendmsg
+		 * [0] do_syscall_64
+		 * [0] entry_SYSCALL_64_after_hwframe
+		 *
+		 * 似乎两个ip之间就调用一次
+		 */
 		conn = rds_conn_create_outgoing(sock_net(sock->sk),
 						&rs->rs_bound_addr, &daddr,
 						rs->rs_transport, rs->rs_tos,
@@ -1463,6 +1537,10 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 			ret = PTR_ERR(conn);
 			goto out;
 		}
+		/*
+		 * struct rds_connection:
+		 * -> struct rds_transport *c_trans;
+		 */
 		if (conn->c_trans->t_mp_capable) {
 			/* c_npaths == 0 if we have not talked to this peer
 			 * before.  Initiate a connection request to the
@@ -1484,9 +1562,18 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 			cpath = &conn->c_path[0];
 		}
 		rs->rs_conn = conn;
+		/*
+		 * 在以下使用rds_sock->rs_conn_path:
+		 *   - net/rds/af_rds.c|988| <<__rds_create>> rs->rs_conn_path = NULL;
+		 *   - net/rds/send.c|1493| <<rds_sendmsg>> cpath = rs->rs_conn_path;
+		 *   - net/rds/send.c|1525| <<rds_sendmsg>> rs->rs_conn_path = cpath;
+		 */
 		rs->rs_conn_path = cpath;
 	}
 
+	/*
+	 * struct rds_message *rm = NULL;
+	 */
 	rm->m_conn_path = cpath;
 
 	/* Parse any control messages the user may have included. */
@@ -1580,6 +1667,9 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 	if (!dport)
 		rds_stats_inc(s_send_ping);
 
+	/*
+	 * 发送的核心函数!!!
+	 */
 	ret = rds_send_xmit(cpath);
 	if (ret == -ENOMEM || ret == -EAGAIN)
 		rds_cond_queue_send_work(cpath, 1);
diff --git a/net/rds/threads.c b/net/rds/threads.c
index 0d3afb063ad3..32f3b794db94 100644
--- a/net/rds/threads.c
+++ b/net/rds/threads.c
@@ -396,6 +396,10 @@ void rds_reconnect_timeout(struct work_struct *work)
 	}
 }
 
+/*
+ * 在以下使用rds_shutdown_worker():
+ *   - net/rds/connection.c|223| <<__rds_conn_path_init>> INIT_WORK(&cp->cp_down_w, rds_shutdown_worker);
+ */
 void rds_shutdown_worker(struct work_struct *work)
 {
 	struct rds_conn_path *cp = container_of(work,
diff --git a/net/rds/transport.c b/net/rds/transport.c
index 5454ec07fba5..7efdfd01f1ac 100644
--- a/net/rds/transport.c
+++ b/net/rds/transport.c
@@ -47,6 +47,11 @@ static char * const rds_trans_modules[] = {
 static struct rds_transport *transports[RDS_TRANS_COUNT];
 static DECLARE_RWSEM(rds_trans_sem);
 
+/*
+ * called by:
+ *   - net/rds/ib.c|1221| <<rds_ib_init>> ret = rds_trans_register(&rds_ib_transport);
+ *   - net/rds/tcp.c|724| <<rds_tcp_init>> ret = rds_trans_register(&rds_tcp_transport);
+ */
 int rds_trans_register(struct rds_transport *trans)
 {
 	BUG_ON(strlen(trans->t_name) + 1 > TRANSNAMSIZ);
@@ -84,6 +89,11 @@ void rds_trans_put(struct rds_transport *trans)
 		module_put(trans->t_owner);
 }
 
+/*
+ * called by:
+ *   - net/rds/bind.c|285| <<rds_bind>> trans = rds_trans_get_preferred(sock_net(sock->sk),
+ *   - net/rds/connection.c|329| <<__rds_conn_create>> loop_trans = rds_trans_get_preferred(net, faddr, conn->c_dev_if);
+ */
 struct rds_transport *rds_trans_get_preferred(struct net *net,
 					      const struct in6_addr *addr,
 					      __u32 scope_id)
@@ -114,6 +124,10 @@ struct rds_transport *rds_trans_get_preferred(struct net *net,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - net/rds/af_rds.c|574| <<rds_set_transport>> rs->rs_transport = rds_trans_get(t_type);
+ */
 struct rds_transport *rds_trans_get(int t_type)
 {
 	struct rds_transport *ret = NULL;
-- 
2.17.1

