From 6752fa3582e81f6ca885b251a58d652ccceb62d9 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 31 Oct 2023 07:17:13 -0700
Subject: [PATCH 1/1] qemu for v8.1.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 accel/accel-blocker.c          |  92 ++++++++++++++++++
 accel/kvm/kvm-all.c            |  11 +++
 block/block-backend.c          |  21 ++++
 block/monitor/block-hmp-cmds.c |  90 +++++++++++++++++
 block/qapi.c                   |  25 +++++
 hw/intc/arm_gicv3_its.c        |   7 ++
 hw/intc/arm_gicv3_its_common.c |   3 +
 hw/intc/arm_gicv3_its_kvm.c    |  10 ++
 hw/net/igb.c                   |  11 +++
 hw/net/igbvf.c                 |   6 ++
 hw/nvme/ctrl.c                 |  21 ++++
 hw/nvme/dif.c                  |   5 +
 hw/nvme/ns.c                   |  13 +++
 hw/nvme/nvme.h                 |  61 ++++++++++++
 hw/pci/pcie_sriov.c            |  73 ++++++++++++++
 hw/scsi/virtio-scsi.c          | 172 +++++++++++++++++++++++++++++++++
 hw/vfio/common.c               |   4 +
 hw/virtio/virtio.c             |  52 ++++++++++
 include/hw/core/cpu.h          |  10 ++
 include/qemu/futex.h           |  18 ++++
 migration/migration-stats.c    |  32 ++++++
 migration/migration.c          |  32 ++++++
 migration/migration.h          |  10 ++
 softmmu/balloon.c              |   4 +
 softmmu/dma-helpers.c          |   9 ++
 target/arm/kvm64.c             |   4 +
 target/i386/kvm/kvm.c          |   4 +
 util/aio-wait.c                |  20 ++++
 util/iov.c                     |  25 +++++
 util/lockcnt.c                 |  52 ++++++++++
 util/qemu-thread-posix.c       | 129 +++++++++++++++++++++++++
 31 files changed, 1026 insertions(+)

diff --git a/accel/accel-blocker.c b/accel/accel-blocker.c
index 1e7f42346..24e422582 100644
--- a/accel/accel-blocker.c
+++ b/accel/accel-blocker.c
@@ -30,25 +30,67 @@
 #include "hw/core/cpu.h"
 #include "sysemu/accel-blocker.h"
 
+/*
+ * 在以下使用accel_in_ioctl_lock:
+ *   - accel/accel-blocker.c|38| <<accel_blocker_init>> qemu_lockcnt_init(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|49| <<accel_ioctl_begin>> qemu_lockcnt_inc(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|58| <<accel_ioctl_end>> qemu_lockcnt_dec(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|97| <<accel_has_to_wait>> return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|114| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|149| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&accel_in_ioctl_lock);
+ */
 static QemuLockCnt accel_in_ioctl_lock;
+/*
+ * 在以下使用accel_in_ioctl_event:
+ *   - accel/accel-blocker.c|39| <<accel_blocker_init>> qemu_event_init(&accel_in_ioctl_event, false);
+ *   - accel/accel-blocker.c|60| <<accel_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|81| <<accel_cpu_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|120| <<accel_ioctl_inhibit_begin>> qemu_event_reset(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|137| <<accel_ioctl_inhibit_begin>> qemu_event_wait(&accel_in_ioctl_event);
+ */
 static QemuEvent accel_in_ioctl_event;
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2488| <<kvm_init>> accel_blocker_init();
+ */
 void accel_blocker_init(void)
 {
+    /*
+     * 设置成0
+     */
     qemu_lockcnt_init(&accel_in_ioctl_lock);
     qemu_event_init(&accel_in_ioctl_event, false);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3167| <<kvm_vm_ioctl>> accel_ioctl_begin();
+ *   - accel/kvm/kvm-all.c|3207| <<kvm_device_ioctl>> accel_ioctl_begin();
+ */
 void accel_ioctl_begin(void)
 {
     if (likely(qemu_mutex_iothread_locked())) {
         return;
     }
 
+    /*
+     * struct QemuLockCnt {
+     * #ifndef CONFIG_LINUX
+     *     QemuMutex mutex;
+     * #endif
+     *     unsigned count;
+     * };
+     */
     /* block if lock is taken in kvm_ioctl_inhibit_begin() */
     qemu_lockcnt_inc(&accel_in_ioctl_lock);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3169| <<kvm_vm_ioctl>> accel_ioctl_end();
+ *   - accel/kvm/kvm-all.c|3209| <<kvm_device_ioctl>> accel_ioctl_end();
+ */
 void accel_ioctl_end(void)
 {
     if (likely(qemu_mutex_iothread_locked())) {
@@ -60,6 +102,10 @@ void accel_ioctl_end(void)
     qemu_event_set(&accel_in_ioctl_event);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3187| <<kvm_vcpu_ioctl>> accel_cpu_ioctl_begin(cpu);
+ */
 void accel_cpu_ioctl_begin(CPUState *cpu)
 {
     if (unlikely(qemu_mutex_iothread_locked())) {
@@ -70,6 +116,10 @@ void accel_cpu_ioctl_begin(CPUState *cpu)
     qemu_lockcnt_inc(&cpu->in_ioctl_lock);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3189| <<kvm_vcpu_ioctl>> accel_cpu_ioctl_end(cpu);
+ */
 void accel_cpu_ioctl_end(CPUState *cpu)
 {
     if (unlikely(qemu_mutex_iothread_locked())) {
@@ -81,12 +131,33 @@ void accel_cpu_ioctl_end(CPUState *cpu)
     qemu_event_set(&accel_in_ioctl_event);
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|122| <<accel_ioctl_inhibit_begin>> if (accel_has_to_wait()) {
+ */
 static bool accel_has_to_wait(void)
 {
     CPUState *cpu;
     bool needs_to_wait = false;
 
     CPU_FOREACH(cpu) {
+        /*
+	 * qemu_lockcnt_count: query a LockCnt's count.
+	 * @lockcnt: the lockcnt to query.
+	 *
+	 * Note that the count can change at any time.  Still, while the
+	 * lockcnt is locked, one can usefully check whether the count
+	 * is non-zero.
+	 *
+	 * 在以下使用CPUState->in_ioctl_lock:
+	 *   - accel/accel-blocker.c|116| <<accel_cpu_ioctl_begin>> qemu_lockcnt_inc(&cpu->in_ioctl_lock);
+	 *   - accel/accel-blocker.c|129| <<accel_cpu_ioctl_end>> qemu_lockcnt_dec(&cpu->in_ioctl_lock);
+	 *   - accel/accel-blocker.c|144| <<accel_has_to_wait>> if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
+	 *   - accel/accel-blocker.c|170| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&cpu->in_ioctl_lock);
+	 *   - accel/accel-blocker.c|217| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&cpu->in_ioctl_lock);
+	 *   - hw/core/cpu-common.c|238| <<cpu_common_initfn>> qemu_lockcnt_init(&cpu->in_ioctl_lock);
+	 *   - hw/core/cpu-common.c|250| <<cpu_common_finalize>> qemu_lockcnt_destroy(&cpu->in_ioctl_lock);
+	 */
         if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
             /* exit the ioctl, if vcpu is running it */
             qemu_cpu_kick(cpu);
@@ -94,9 +165,22 @@ static bool accel_has_to_wait(void)
         }
     }
 
+    /*
+     * 在以下使用accel_in_ioctl_lock:
+     *   - accel/accel-blocker.c|38| <<accel_blocker_init>> qemu_lockcnt_init(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|49| <<accel_ioctl_begin>> qemu_lockcnt_inc(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|58| <<accel_ioctl_end>> qemu_lockcnt_dec(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|97| <<accel_has_to_wait>> return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|114| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|149| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&accel_in_ioctl_lock);
+     */
     return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1597| <<kvm_region_commit>> accel_ioctl_inhibit_begin();
+ */
 void accel_ioctl_inhibit_begin(void)
 {
     CPUState *cpu;
@@ -119,6 +203,10 @@ void accel_ioctl_inhibit_begin(void)
         /* Reset event to FREE. */
         qemu_event_reset(&accel_in_ioctl_event);
 
+	/*
+	 * called by:
+	 *   - accel/accel-blocker.c|122| <<accel_ioctl_inhibit_begin>> if (accel_has_to_wait()) {
+	 */
         if (accel_has_to_wait()) {
             /*
              * If event is still FREE, and there are ioctls still in progress,
@@ -142,6 +230,10 @@ void accel_ioctl_inhibit_begin(void)
     }
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1621| <<kvm_region_commit>> accel_ioctl_inhibit_end();
+ */
 void accel_ioctl_inhibit_end(void)
 {
     CPUState *cpu;
diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index 7b3da8dc3..37faac95e 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -2038,6 +2038,11 @@ static KVMMSIRoute *kvm_lookup_msi_route(KVMState *s, MSIMessage msg)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/i386/kvm/apic.c|192| <<kvm_send_msi>> ret = kvm_irqchip_send_msi(kvm_state, *msg);
+ *   - target/i386/kvm/xen-emu.c|447| <<kvm_xen_inject_vcpu_callback_vector>> kvm_irqchip_send_msi(kvm_state, msg);
+ */
 int kvm_irqchip_send_msi(KVMState *s, MSIMessage msg)
 {
     struct kvm_msi msi;
@@ -2050,6 +2055,12 @@ int kvm_irqchip_send_msi(KVMState *s, MSIMessage msg)
         msi.flags = 0;
         memset(msi.pad, 0, sizeof(msi.pad));
 
+	/*
+	 * 在以下使用KVM_SIGNAL_MSI:
+	 *   - linux-headers/linux/kvm.h|1523| <<global>> #define KVM_SIGNAL_MSI _IOW(KVMIO, 0xa5, struct kvm_msi)
+	 *   - accel/kvm/kvm-all.c|2053| <<kvm_irqchip_send_msi>> return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
+	 *   - hw/intc/arm_gicv3_its_kvm.c|65| <<kvm_its_send_msi>> return kvm_vm_ioctl(kvm_state, KVM_SIGNAL_MSI, &msi);
+	 */
         return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
     }
 
diff --git a/block/block-backend.c b/block/block-backend.c
index 4009ed5fe..1e153a46b 100644
--- a/block/block-backend.c
+++ b/block/block-backend.c
@@ -110,6 +110,14 @@ static const AIOCBInfo block_backend_aiocb_info = {
 static void drive_info_del(DriveInfo *dinfo);
 static BlockBackend *bdrv_first_blk(BlockDriverState *bs);
 
+/*
+ * 在以下block_backends:
+ *   - block/block-backend.c|114| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) block_backends = QTAILQ_HEAD_INITIALIZER(block_backends);
+ *   - block/block-backend.c|115| <<QTAILQ_HEAD>> QTAILQ_HEAD_INITIALIZER(block_backends);
+ *   - block/block-backend.c|378| <<blk_new>> QTAILQ_INSERT_TAIL(&block_backends, blk, link);
+ *   - block/block-backend.c|507| <<blk_delete>> QTAILQ_REMOVE(&block_backends, blk, link);
+ *   - block/block-backend.c|569| <<blk_all_next>> : QTAILQ_FIRST(&block_backends);
+ */
 /* All BlockBackends. Protected by BQL. */
 static QTAILQ_HEAD(, BlockBackend) block_backends =
     QTAILQ_HEAD_INITIALIZER(block_backends);
@@ -565,6 +573,14 @@ void blk_unref(BlockBackend *blk)
 BlockBackend *blk_all_next(BlockBackend *blk)
 {
     GLOBAL_STATE_CODE();
+    /*
+     * 在以下block_backends:
+     *   - block/block-backend.c|114| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) block_backends = QTAILQ_HEAD_INITIALIZER(block_backends);
+     *   - block/block-backend.c|115| <<QTAILQ_HEAD>> QTAILQ_HEAD_INITIALIZER(block_backends);
+     *   - block/block-backend.c|378| <<blk_new>> QTAILQ_INSERT_TAIL(&block_backends, blk, link);
+     *   - block/block-backend.c|507| <<blk_delete>> QTAILQ_REMOVE(&block_backends, blk, link);
+     *   - block/block-backend.c|569| <<blk_all_next>> : QTAILQ_FIRST(&block_backends);
+     */
     return blk ? QTAILQ_NEXT(blk, link)
                : QTAILQ_FIRST(&block_backends);
 }
@@ -776,6 +792,11 @@ BlockBackend *blk_by_name(const char *name)
 BlockDriverState *blk_bs(BlockBackend *blk)
 {
     IO_CODE();
+    /*
+     * BlockBackend *blk:
+     * -> BdrvChild *root;
+     *    -> BlockDriverState *bs;
+     */
     return blk->root ? blk->root->bs : NULL;
 }
 
diff --git a/block/monitor/block-hmp-cmds.c b/block/monitor/block-hmp-cmds.c
index ca2599de4..513c98b71 100644
--- a/block/monitor/block-hmp-cmds.c
+++ b/block/monitor/block-hmp-cmds.c
@@ -624,6 +624,23 @@ fail:
     hmp_handle_error(mon, err);
 }
 
+/*
+ * (qemu) info block
+ * ide0-hd0 (#block199): ol7.qcow2 (qcow2)
+ *     Attached to:      /machine/unattached/device[21]
+ *     Cache mode:       writeback
+ *
+ * sd0: [not inserted]
+ *     Removable device: not locked, tray closed
+ *
+ * drive01: test01.qcow2 (qcow2)
+ *     Attached to:      /machine/peripheral-anon/device[0]
+ *     Cache mode:       writeback, direct
+ *
+ * drive02: test02.qcow2 (qcow2)
+ *     Attached to:      /machine/peripheral-anon/device[1]
+ *     Cache mode:       writeback, direct
+ */
 static void print_block_info(Monitor *mon, BlockInfo *info,
                              BlockDeviceInfo *inserted, bool verbose)
 {
@@ -738,6 +755,36 @@ static void print_block_info(Monitor *mon, BlockInfo *info,
     }
 }
 
+/*
+ * # @BlockInfo:
+ * #
+ * # Block device information.  This structure describes a virtual device
+ * # and the backing device associated with it.
+ * #
+ * # @device: The device name associated with the virtual device.
+ * #
+ * # @qdev: The qdev ID, or if no ID is assigned, the QOM path of the
+ * #     block device.  (since 2.10)
+ * #
+ * # @type: This field is returned only for compatibility reasons, it
+ * #     should not be used (always returns 'unknown')
+ * #
+ * # @removable: True if the device supports removable media.
+ * #
+ * # @locked: True if the guest has locked this device from having its
+ * #     media removed
+ * #
+ * # @tray_open: True if the device's tray is open (only present if it
+ * #     has a tray)
+ * #
+ * # @io-status: @BlockDeviceIoStatus.  Only present if the device
+ * #     supports it and the VM is configured to stop on errors
+ * #     (supported device models: virtio-blk, IDE, SCSI except
+ * #     scsi-generic)
+ * #
+ * # @inserted: @BlockDeviceInfo describing the device if media is
+ * #     present
+ */
 void hmp_info_block(Monitor *mon, const QDict *qdict)
 {
     BlockInfoList *block_list, *info;
@@ -754,6 +801,9 @@ void hmp_info_block(Monitor *mon, const QDict *qdict)
         block_list = NULL;
     }
 
+    /*
+     * BlockInfoList *block_list, *info;
+     */
     for (info = block_list; info; info = info->next) {
         if (device && strcmp(device, info->value->device)) {
             continue;
@@ -791,6 +841,46 @@ void hmp_info_block(Monitor *mon, const QDict *qdict)
     qapi_free_BlockDeviceInfoList(blockdev_list);
 }
 
+/*
+ * # @BlockStats:
+ * #
+ * # Statistics of a virtual block device or a block backing device.
+ * #
+ * # @device: If the stats are for a virtual block device, the name
+ * #     corresponding to the virtual block device.
+ * #
+ * # @node-name: The node name of the device.  (Since 2.3)
+ * #
+ * # @qdev: The qdev ID, or if no ID is assigned, the QOM path of the
+ * #     block device.  (since 3.0)
+ * #
+ * # @stats: A @BlockDeviceStats for the device.
+ * #
+ * # @driver-specific: Optional driver-specific stats.  (Since 4.2)
+ * #
+ * # @parent: This describes the file block device if it has one.
+ * #     Contains recursively the statistics of the underlying protocol
+ * #     (e.g. the host file for a qcow2 image).  If there is no
+ * #     underlying protocol, this field is omitted
+ * #
+ * # @backing: This describes the backing block device if it has one.
+ * #     (Since 2.0)
+ *
+ *
+ * # @query-blockstats:
+ * #
+ * # Query the @BlockStats for all virtual block devices.
+ * #
+ * # @query-nodes: If true, the command will query all the block nodes
+ * #     that have a node name, in a list which will include "parent"
+ * #     information, but not "backing". If false or omitted, the
+ * #     behavior is as before - query all the device backends,
+ * #     recursively including their "parent" and "backing". Filter nodes
+ * #     that were created implicitly are skipped over in this mode.
+ * #     (Since 2.3)
+ * #
+ * # Returns: A list of @BlockStats for each virtual block devices.
+ */
 void hmp_info_blockstats(Monitor *mon, const QDict *qdict)
 {
     BlockStatsList *stats_list, *stats;
diff --git a/block/qapi.c b/block/qapi.c
index f34f95e0e..c18ef612c 100644
--- a/block/qapi.c
+++ b/block/qapi.c
@@ -723,6 +723,31 @@ BlockInfoList *qmp_query_block(Error **errp)
     return head;
 }
 
+/*
+ * # @BlockStats:
+ * #
+ * # Statistics of a virtual block device or a block backing device.
+ * #
+ * # @device: If the stats are for a virtual block device, the name
+ * #     corresponding to the virtual block device.
+ * #
+ * # @node-name: The node name of the device.  (Since 2.3)
+ * #
+ * # @qdev: The qdev ID, or if no ID is assigned, the QOM path of the
+ * #     block device.  (since 3.0)
+ * #
+ * # @stats: A @BlockDeviceStats for the device.
+ * #
+ * # @driver-specific: Optional driver-specific stats.  (Since 4.2)
+ * #
+ * # @parent: This describes the file block device if it has one.
+ * #     Contains recursively the statistics of the underlying protocol
+ * #     (e.g. the host file for a qcow2 image).  If there is no
+ * #     underlying protocol, this field is omitted
+ * #
+ * # @backing: This describes the backing block device if it has one.
+ * #     (Since 2.0)
+ */
 BlockStatsList *qmp_query_blockstats(bool has_query_nodes,
                                      bool query_nodes,
                                      Error **errp)
diff --git a/hw/intc/arm_gicv3_its.c b/hw/intc/arm_gicv3_its.c
index 43dfd7a35..57118e5e8 100644
--- a/hw/intc/arm_gicv3_its.c
+++ b/hw/intc/arm_gicv3_its.c
@@ -360,6 +360,13 @@ out:
  * The string @who is purely for the LOG_GUEST_ERROR messages,
  * and should indicate the name of the calling function or similar.
  */
+/*
+ * called by:
+ *   - hw/intc/arm_gicv3_its.c|525| <<do_process_its_cmd>> cmdres = lookup_ite(s, __func__, devid, eventid, &ite, &dte);
+ *   - hw/intc/arm_gicv3_its.c|903| <<process_movi>> cmdres = lookup_ite(s, __func__, devid, eventid, &old_ite, &dte);
+ *   - hw/intc/arm_gicv3_its.c|1114| <<process_vmovi>> cmdres = lookup_ite(s, __func__, devid, eventid, &ite, &dte);
+ *   - hw/intc/arm_gicv3_its.c|1203| <<process_inv>> cmdres = lookup_ite(s, __func__, devid, eventid, &ite, &dte);
+ */
 static ItsCmdResult lookup_ite(GICv3ITSState *s, const char *who,
                                uint32_t devid, uint32_t eventid, ITEntry *ite,
                                DTEntry *dte)
diff --git a/hw/intc/arm_gicv3_its_common.c b/hw/intc/arm_gicv3_its_common.c
index abaf77057..53869f748 100644
--- a/hw/intc/arm_gicv3_its_common.c
+++ b/hw/intc/arm_gicv3_its_common.c
@@ -74,6 +74,9 @@ static MemTxResult gicv3_its_trans_read(void *opaque, hwaddr offset,
     return MEMTX_OK;
 }
 
+/*
+ * MemoryRegionOps gicv3_its_trans_ops.write_with_attrs = gicv3_its_trans_write()
+ */
 static MemTxResult gicv3_its_trans_write(void *opaque, hwaddr offset,
                                          uint64_t value, unsigned size,
                                          MemTxAttrs attrs)
diff --git a/hw/intc/arm_gicv3_its_kvm.c b/hw/intc/arm_gicv3_its_kvm.c
index 7eda9fb86..da275b839 100644
--- a/hw/intc/arm_gicv3_its_kvm.c
+++ b/hw/intc/arm_gicv3_its_kvm.c
@@ -41,6 +41,10 @@ struct KVMARMITSClass {
 };
 
 
+/*
+ * 在以下使用kvm_its_send_msi():
+ *   - hw/intc/arm_gicv3_its_kvm.c|254| <<kvm_arm_its_class_init>> icc->send_msi = kvm_its_send_msi;
+ */
 static int kvm_its_send_msi(GICv3ITSState *s, uint32_t value, uint16_t devid)
 {
     struct kvm_msi msi;
@@ -62,6 +66,12 @@ static int kvm_its_send_msi(GICv3ITSState *s, uint32_t value, uint16_t devid)
     msi.devid = devid;
     memset(msi.pad, 0, sizeof(msi.pad));
 
+    /*
+     * 在以下使用KVM_SIGNAL_MSI:
+     *   - linux-headers/linux/kvm.h|1523| <<global>> #define KVM_SIGNAL_MSI _IOW(KVMIO, 0xa5, struct kvm_msi)
+     *   - accel/kvm/kvm-all.c|2053| <<kvm_irqchip_send_msi>> return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
+     *   - hw/intc/arm_gicv3_its_kvm.c|65| <<kvm_its_send_msi>> return kvm_vm_ioctl(kvm_state, KVM_SIGNAL_MSI, &msi);
+     */
     return kvm_vm_ioctl(kvm_state, KVM_SIGNAL_MSI, &msi);
 }
 
diff --git a/hw/net/igb.c b/hw/net/igb.c
index 8ff832acf..1659cfbf8 100644
--- a/hw/net/igb.c
+++ b/hw/net/igb.c
@@ -433,10 +433,21 @@ static void igb_pci_realize(PCIDevice *pci_dev, Error **errp)
 
     pcie_ari_init(pci_dev, 0x150);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|436| <<igb_pci_realize>> pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
+     *   - hw/nvme/ctrl.c|8050| <<nvme_init_sriov>> pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
+     */
     pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
         IGB_82576_VF_DEV_ID, IGB_MAX_VF_FUNCTIONS, IGB_MAX_VF_FUNCTIONS,
         IGB_VF_OFFSET, IGB_VF_STRIDE);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|440| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
+     *   - hw/net/igb.c|443| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MSIX_BAR_IDX,
+     *   - hw/nvme/ctrl.c|8054| <<nvme_init_sriov>> pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+     */
     pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
         PCI_BASE_ADDRESS_MEM_TYPE_64 | PCI_BASE_ADDRESS_MEM_PREFETCH,
         IGBVF_MMIO_SIZE);
diff --git a/hw/net/igbvf.c b/hw/net/igbvf.c
index d55e1e8a6..4f9324e19 100644
--- a/hw/net/igbvf.c
+++ b/hw/net/igbvf.c
@@ -247,6 +247,12 @@ static void igbvf_pci_realize(PCIDevice *dev, Error **errp)
 
     memory_region_init_io(&s->mmio, OBJECT(dev), &mmio_ops, s, "igbvf-mmio",
         IGBVF_MMIO_SIZE);
+    /*
+     * called by:
+     *   - hw/net/igbvf.c|250| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
+     *   - hw/net/igbvf.c|253| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MSIX_BAR_IDX, &s->msix);
+     *   - hw/nvme/ctrl.c|8117| <<nvme_init_pci>> pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
+     */
     pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
 
     memory_region_init(&s->msix, OBJECT(dev), "igbvf-msix", IGBVF_MSIX_SIZE);
diff --git a/hw/nvme/ctrl.c b/hw/nvme/ctrl.c
index 539d27355..79258e5a8 100644
--- a/hw/nvme/ctrl.c
+++ b/hw/nvme/ctrl.c
@@ -8038,6 +8038,10 @@ static uint64_t nvme_bar_size(unsigned total_queues, unsigned total_irqs,
     return bar_size;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|8145| <<nvme_init_pci>> nvme_init_sriov(n, pci_dev, 0x120);
+ */
 static void nvme_init_sriov(NvmeCtrl *n, PCIDevice *pci_dev, uint16_t offset)
 {
     uint16_t vf_dev_id = n->params.use_intel_id ?
@@ -8047,10 +8051,21 @@ static void nvme_init_sriov(NvmeCtrl *n, PCIDevice *pci_dev, uint16_t offset)
                                       le16_to_cpu(cap->vifrsm),
                                       NULL, NULL);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|436| <<igb_pci_realize>> pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
+     *   - hw/nvme/ctrl.c|8050| <<nvme_init_sriov>> pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
+     */
     pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
                        n->params.sriov_max_vfs, n->params.sriov_max_vfs,
                        NVME_VF_OFFSET, NVME_VF_STRIDE);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|440| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
+     *   - hw/net/igb.c|443| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MSIX_BAR_IDX,
+     *   - hw/nvme/ctrl.c|8054| <<nvme_init_sriov>> pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+     */
     pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
                               PCI_BASE_ADDRESS_MEM_TYPE_64, bar_size);
 }
@@ -8114,6 +8129,12 @@ static bool nvme_init_pci(NvmeCtrl *n, PCIDevice *pci_dev, Error **errp)
     memory_region_add_subregion(&n->bar0, 0, &n->iomem);
 
     if (pci_is_vf(pci_dev)) {
+        /*
+	 * called by:
+	 *   - hw/net/igbvf.c|250| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
+	 *   - hw/net/igbvf.c|253| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MSIX_BAR_IDX, &s->msix);
+	 *   - hw/nvme/ctrl.c|8117| <<nvme_init_pci>> pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
+	 */
         pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
     } else {
         pci_register_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
diff --git a/hw/nvme/dif.c b/hw/nvme/dif.c
index 01b19c337..fc0b746c7 100644
--- a/hw/nvme/dif.c
+++ b/hw/nvme/dif.c
@@ -535,6 +535,11 @@ out:
     nvme_dif_rw_cb(ctx, ret);
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|3459| <<nvme_read>> return nvme_dif_rw(n, req);
+ *   - hw/nvme/ctrl.c|3630| <<nvme_do_write>> return nvme_dif_rw(n, req);
+ */
 uint16_t nvme_dif_rw(NvmeCtrl *n, NvmeRequest *req)
 {
     NvmeRwCmd *rw = (NvmeRwCmd *)&req->cmd;
diff --git a/hw/nvme/ns.c b/hw/nvme/ns.c
index 44aba8f4d..18c93a8cc 100644
--- a/hw/nvme/ns.c
+++ b/hw/nvme/ns.c
@@ -148,6 +148,10 @@ lbaf_found:
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ns.c|656| <<nvme_ns_setup>> if (nvme_ns_init_blk(ns, errp)) {
+ */
 static int nvme_ns_init_blk(NvmeNamespace *ns, Error **errp)
 {
     bool read_only;
@@ -394,6 +398,10 @@ static NvmeRuHandle *nvme_find_ruh_by_attr(NvmeEnduranceGroup *endgrp,
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ns.c|671| <<nvme_ns_setup>> if (!nvme_ns_init_fdp(ns, errp)) {
+ */
 static bool nvme_ns_init_fdp(NvmeNamespace *ns, Error **errp)
 {
     NvmeEnduranceGroup *endgrp = ns->endgrp;
@@ -647,6 +655,11 @@ static int nvme_ns_check_constraints(NvmeNamespace *ns, Error **errp)
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|8345| <<nvme_realize>> if (nvme_ns_setup(ns, errp)) {
+ *   - hw/nvme/ns.c|743| <<nvme_ns_realize>> if (nvme_ns_setup(ns, errp)) {
+ */
 int nvme_ns_setup(NvmeNamespace *ns, Error **errp)
 {
     if (nvme_ns_check_constraints(ns, errp)) {
diff --git a/hw/nvme/nvme.h b/hw/nvme/nvme.h
index 5f2ae7b28..d83d038a6 100644
--- a/hw/nvme/nvme.h
+++ b/hw/nvme/nvme.h
@@ -609,6 +609,40 @@ typedef enum NvmeResetType {
     NVME_RESET_CONTROLLER = 1,
 } NvmeResetType;
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|3346| <<nvme_do_flush>> iocb->ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|3386| <<nvme_flush>> iocb->ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|4443| <<nvme_io_cmd>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|4735| <<nvme_smart_info>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|4744| <<nvme_smart_info>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|5410| <<nvme_identify_ns>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5464| <<nvme_identify_ctrl_list>> if (attached && !nvme_ns(ctrl, nsid)) {
+ *   - hw/nvme/ctrl.c|5520| <<nvme_identify_ns_csi>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5567| <<nvme_identify_nslist>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|5615| <<nvme_identify_nslist_csi>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|5664| <<nvme_identify_ns_descr_list>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5921| <<nvme_get_feature>> if (!nvme_ns(n, nsid)) {
+ *   - hw/nvme/ctrl.c|5965| <<nvme_get_feature>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5975| <<nvme_get_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6012| <<nvme_get_feature>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6168| <<nvme_set_feature>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6211| <<nvme_set_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6232| <<nvme_set_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6279| <<nvme_set_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6333| <<nvme_update_dmrsl>> NvmeNamespace *ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6406| <<nvme_ns_attachment>> if (nvme_ns(ctrl, nsid)) {
+ *   - hw/nvme/ctrl.c|6420| <<nvme_ns_attachment>> if (!nvme_ns(ctrl, nsid)) {
+ *   - hw/nvme/ctrl.c|6572| <<nvme_do_format>> iocb->ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6634| <<nvme_format>> iocb->ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6900| <<nvme_directive_receive>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|7093| <<nvme_ctrl_reset>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|7169| <<nvme_ctrl_shutdown>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|7184| <<nvme_select_iocs>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|8363| <<nvme_exit>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ns.c|749| <<nvme_ns_realize>> if (nvme_ns(n, i) || nvme_subsys_ns(subsys, i)) {
+ *   - hw/nvme/ns.c|762| <<nvme_ns_realize>> if (nvme_ns(n, nsid) || nvme_subsys_ns(subsys, nsid)) {
+ */
 static inline NvmeNamespace *nvme_ns(NvmeCtrl *n, uint32_t nsid)
 {
     if (!nsid || nsid > NVME_MAX_NAMESPACES) {
@@ -626,6 +660,17 @@ static inline NvmeCQueue *nvme_cq(NvmeRequest *req)
     return n->cq[sq->cqid];
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|2207| <<nvme_rw_cb>> status = nvme_map_mdata(nvme_ctrl(req), nlb, req);
+ *   - hw/nvme/ctrl.c|2327| <<nvme_compare_mdata_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|2415| <<nvme_compare_data_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|2557| <<nvme_dsm_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|3337| <<nvme_do_flush>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|3725| <<nvme_open_zone>> return nvme_zrm_open_flags(nvme_ctrl(req), ns, zone, flags);
+ *   - hw/nvme/ctrl.c|6559| <<nvme_do_format>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/dif.c|428| <<nvme_dif_rw_check_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ */
 static inline NvmeCtrl *nvme_ctrl(NvmeRequest *req)
 {
     NvmeSQueue *sq = req->sq;
@@ -641,6 +686,17 @@ static inline uint16_t nvme_cid(NvmeRequest *req)
     return le16_to_cpu(req->cqe.cid);
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|7070| <<nvme_activate_virt_res>> sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|7143| <<nvme_ctrl_reset>> sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|7202| <<nvme_start_ctrl>> NvmeSecCtrlEntry *sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|7549| <<nvme_mmio_read>> if (pci_is_vf(PCI_DEVICE(n)) && !nvme_sctrl(n)->scs &&
+ *   - hw/nvme/ctrl.c|7725| <<nvme_mmio_write>> if (pci_is_vf(PCI_DEVICE(n)) && !nvme_sctrl(n)->scs &&
+ *   - hw/nvme/ctrl.c|7916| <<nvme_init_state>> sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|8169| <<nvme_init_ctrl>> NvmeSecCtrlEntry *sctrl = nvme_sctrl(n);
+ *   - hw/nvme/subsys.c|58| <<nvme_subsys_register_ctrl>> NvmeSecCtrlEntry *sctrl = nvme_sctrl(n);
+ */
 static inline NvmeSecCtrlEntry *nvme_sctrl(NvmeCtrl *n)
 {
     PCIDevice *pci_dev = &n->parent_obj;
@@ -653,6 +709,11 @@ static inline NvmeSecCtrlEntry *nvme_sctrl(NvmeCtrl *n)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|6716| <<nvme_assign_virt_res_to_sec>> sctrl = nvme_sctrl_for_cntlid(n, cntlid);
+ *   - hw/nvme/ctrl.c|6751| <<nvme_virt_set_state>> sctrl = nvme_sctrl_for_cntlid(n, cntlid);
+ */
 static inline NvmeSecCtrlEntry *nvme_sctrl_for_cntlid(NvmeCtrl *n,
                                                       uint16_t cntlid)
 {
diff --git a/hw/pci/pcie_sriov.c b/hw/pci/pcie_sriov.c
index 76a3b6917..33c0388ea 100644
--- a/hw/pci/pcie_sriov.c
+++ b/hw/pci/pcie_sriov.c
@@ -24,6 +24,11 @@ static PCIDevice *register_vf(PCIDevice *pf, int devfn,
                               const char *name, uint16_t vf_num);
 static void unregister_vfs(PCIDevice *dev);
 
+/*
+ * called by:
+ *   - hw/net/igb.c|436| <<igb_pci_realize>> pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
+ *   - hw/nvme/ctrl.c|8050| <<nvme_init_sriov>> pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
+ */
 void pcie_sriov_pf_init(PCIDevice *dev, uint16_t offset,
                         const char *vfname, uint16_t vf_dev_id,
                         uint16_t init_vfs, uint16_t total_vfs,
@@ -78,6 +83,12 @@ void pcie_sriov_pf_exit(PCIDevice *dev)
     dev->exp.sriov_pf.vfname = NULL;
 }
 
+/*
+ * called by:
+ *   - hw/net/igb.c|440| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
+ *   - hw/net/igb.c|443| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MSIX_BAR_IDX,
+ *   - hw/nvme/ctrl.c|8054| <<nvme_init_sriov>> pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+ */
 void pcie_sriov_pf_init_vf_bar(PCIDevice *dev, int region_num,
                                uint8_t type, dma_addr_t size)
 {
@@ -105,6 +116,12 @@ void pcie_sriov_pf_init_vf_bar(PCIDevice *dev, int region_num,
     dev->exp.sriov_pf.vf_bar_type[region_num] = type;
 }
 
+/*
+ * called by:
+ *   - hw/net/igbvf.c|250| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
+ *   - hw/net/igbvf.c|253| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MSIX_BAR_IDX, &s->msix);
+ *   - hw/nvme/ctrl.c|8117| <<nvme_init_pci>> pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
+ */
 void pcie_sriov_vf_register_bar(PCIDevice *dev, int region_num,
                                 MemoryRegion *memory)
 {
@@ -141,6 +158,10 @@ void pcie_sriov_vf_register_bar(PCIDevice *dev, int region_num,
     }
 }
 
+/*
+ * called by:
+ *   - hw/pci/pcie_sriov.c|186| <<register_vfs>> dev->exp.sriov_pf.vf[i] = register_vf(dev, devfn, dev->exp.sriov_pf.vfname, i);
+ */
 static PCIDevice *register_vf(PCIDevice *pf, int devfn, const char *name,
                               uint16_t vf_num)
 {
@@ -163,6 +184,10 @@ static PCIDevice *register_vf(PCIDevice *pf, int devfn, const char *name,
     return dev;
 }
 
+/*
+ * called by:
+ *   - hw/pci/pcie_sriov.c|267| <<pcie_sriov_config_write>> register_vfs(dev);
+ */
 static void register_vfs(PCIDevice *dev)
 {
     uint16_t num_vfs;
@@ -219,6 +244,11 @@ static void unregister_vfs(PCIDevice *dev)
     pci_set_word(dev->config + dev->exp.sriov_cap + PCI_SRIOV_NUM_VF, 0);
 }
 
+/*
+ * called by:
+ *   - hw/pci/pci.c|1625| <<pci_default_write_config>> pcie_sriov_config_write(d, addr, val_in, l);
+ *   - hw/pci/pcie_sriov.c|286| <<pcie_sriov_pf_disable_vfs>> pcie_sriov_config_write(dev, sriov_cap + PCI_SRIOV_CTRL, val, 1);
+ */
 void pcie_sriov_config_write(PCIDevice *dev, uint32_t address,
                              uint32_t val, int len)
 {
@@ -250,6 +280,11 @@ void pcie_sriov_config_write(PCIDevice *dev, uint32_t address,
 }
 
 
+/*
+ * called by:
+ *   - hw/net/igb.c|482| <<igb_qdev_reset_hold>> pcie_sriov_pf_disable_vfs(d);
+ *   - hw/nvme/ctrl.c|7126| <<nvme_ctrl_reset>> pcie_sriov_pf_disable_vfs(pci_dev);
+ */
 /* Reset SR/IOV VF Enable bit to trigger an unregister of all VFs */
 void pcie_sriov_pf_disable_vfs(PCIDevice *dev)
 {
@@ -263,6 +298,9 @@ void pcie_sriov_pf_disable_vfs(PCIDevice *dev)
     }
 }
 
+/*
+ * 似乎没有调用
+ */
 /* Add optional supported page sizes to the mask of supported page sizes */
 void pcie_sriov_pf_add_sup_pgsize(PCIDevice *dev, uint16_t opt_sup_pgsize)
 {
@@ -282,17 +320,47 @@ void pcie_sriov_pf_add_sup_pgsize(PCIDevice *dev, uint16_t opt_sup_pgsize)
 }
 
 
+/*
+ * called by:
+ *   - hw/net/igbvf.c|214| <<igbvf_mmio_read>> addr = vf_to_pf_addr(addr, pcie_sriov_vf_number(vf), false);
+ *   - hw/net/igbvf.c|224| <<igbvf_mmio_write>> addr = vf_to_pf_addr(addr, pcie_sriov_vf_number(vf), true);
+ *   - hw/nvme/nvme.h|650| <<nvme_sctrl>> return &pf->sec_ctrl_list.sec[pcie_sriov_vf_number(pci_dev)];
+ */
 uint16_t pcie_sriov_vf_number(PCIDevice *dev)
 {
     assert(pci_is_vf(dev));
     return dev->exp.sriov_vf.vf_number;
 }
 
+/*
+ * called by:
+ *   - hw/net/igbvf.c|212| <<igbvf_mmio_read>> PCIDevice *pf = pcie_sriov_get_pf(vf);
+ *   - hw/net/igbvf.c|222| <<igbvf_mmio_write>> PCIDevice *pf = pcie_sriov_get_pf(vf);
+ *   - hw/nvme/ctrl.c|8314| <<nvme_realize>> NvmeCtrl *pn = NVME(pcie_sriov_get_pf(pci_dev));
+ *   - hw/nvme/nvme.h|647| <<nvme_sctrl>> NvmeCtrl *pf = NVME(pcie_sriov_get_pf(pci_dev));
+ */
 PCIDevice *pcie_sriov_get_pf(PCIDevice *dev)
 {
+    /*
+     * PCIDevice *dev:
+     * -> PCIExpressDevice exp;
+     *    -> uint16_t sriov_cap;
+     *    -> PCIESriovPF sriov_pf;
+     *    -> PCIESriovVF sriov_vf;
+     *       -> PCIDevice *pf;
+     *       -> uint16_t vf_number;
+     */
     return dev->exp.sriov_vf.pf;
 }
 
+/*
+ * called by:
+ *   - hw/net/igb_core.c|124| <<igb_msix_notify>> dev = pcie_sriov_get_vf_at_index(core->owner, vfn);
+ *   - hw/net/igb_core.c|849| <<igb_txdesc_writeback>> d = pcie_sriov_get_vf_at_index(core->owner, txi->idx % 8);
+ *   - hw/net/igb_core.c|894| <<igb_start_xmit>> d = pcie_sriov_get_vf_at_index(core->owner, txi->idx % 8);
+ *   - hw/net/igb_core.c|1573| <<igb_write_packet_to_guest>> d = pcie_sriov_get_vf_at_index(core->owner, rxi->idx % 8);
+ *   - hw/nvme/ctrl.c|6758| <<nvme_virt_set_state>> sn = NVME(pcie_sriov_get_vf_at_index(pci, vf_index));
+ */
 PCIDevice *pcie_sriov_get_vf_at_index(PCIDevice *dev, int n)
 {
     assert(!pci_is_vf(dev));
@@ -302,6 +370,11 @@ PCIDevice *pcie_sriov_get_vf_at_index(PCIDevice *dev, int n)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/net/igb_core.c|123| <<igb_msix_notify>> if (vfn < pcie_sriov_num_vfs(core->owner)) {
+ *   - hw/net/igb_core.c|2650| <<igb_get_status>> uint16_t num_vfs = pcie_sriov_num_vfs(core->owner);
+ */
 uint16_t pcie_sriov_num_vfs(PCIDevice *dev)
 {
     return dev->exp.sriov_pf.num_vfs;
diff --git a/hw/scsi/virtio-scsi.c b/hw/scsi/virtio-scsi.c
index 45b95ea07..ef5743e2c 100644
--- a/hw/scsi/virtio-scsi.c
+++ b/hw/scsi/virtio-scsi.c
@@ -81,6 +81,11 @@ static inline SCSIDevice *virtio_scsi_device_get(VirtIOSCSI *s, uint8_t *lun)
     return scsi_device_get(&s->bus, 0, lun[1], virtio_scsi_get_lun(lun));
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|244| <<virtio_scsi_pop_req>> virtio_scsi_init_req(s, vq, req);
+ *   - hw/scsi/virtio-scsi.c|273| <<virtio_scsi_load_request>> virtio_scsi_init_req(s, vs->cmd_vqs[n], req);
+ */
 static void virtio_scsi_init_req(VirtIOSCSI *s, VirtQueue *vq, VirtIOSCSIReq *req)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
@@ -90,6 +95,12 @@ static void virtio_scsi_init_req(VirtIOSCSI *s, VirtQueue *vq, VirtIOSCSIReq *re
     req->vq = vq;
     req->dev = s;
     qemu_sglist_init(&req->qsgl, DEVICE(s), 8, vdev->dma_as);
+    /*
+     * 有些可以查看virtqueue_alloc_element()
+     *
+     * VirtIOSCSIReq *req:
+     * -> QEMUIOVector resp_iov;
+     */
     qemu_iovec_init(&req->resp_iov, 1);
     memset((uint8_t *)req + zero_skip, 0, sizeof(*req) - zero_skip);
 }
@@ -122,6 +133,14 @@ static void virtio_scsi_complete_req(VirtIOSCSIReq *req)
     virtio_scsi_free_req(req);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|547| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|555| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|565| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|775| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|974| <<virtio_scsi_push_event>> virtio_scsi_bad_req(req);
+ */
 static void virtio_scsi_bad_req(VirtIOSCSIReq *req)
 {
     virtio_error(VIRTIO_DEVICE(req->dev), "wrong size for virtio-scsi headers");
@@ -152,12 +171,42 @@ static size_t qemu_sgl_concat(VirtIOSCSIReq *req, struct iovec *iov,
     return copied;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|261| <<virtio_scsi_load_request>> if (virtio_scsi_parse_req(req, sizeof(VirtIOSCSICmdReq) + vs->cdb_size,
+ *   - hw/scsi/virtio-scsi.c|565| <<virtio_scsi_handle_ctrl_req>> if (virtio_scsi_parse_req(req, sizeof(VirtIOSCSICtrlTMFReq),
+ *   - hw/scsi/virtio-scsi.c|575| <<virtio_scsi_handle_ctrl_req>> if (virtio_scsi_parse_req(req, sizeof(VirtIOSCSICtrlANReq),
+ *   - hw/scsi/virtio-scsi.c|788| <<virtio_scsi_handle_cmd_req_prepare>> rc = virtio_scsi_parse_req(req, sizeof(VirtIOSCSICmdReq) + vs->cdb_size,
+ *   - hw/scsi/virtio-scsi.c|1022| <<virtio_scsi_push_event>> if (virtio_scsi_parse_req(req, 0, sizeof(VirtIOSCSIEvent))) {
+ */
 static int virtio_scsi_parse_req(VirtIOSCSIReq *req,
                                  unsigned req_size, unsigned resp_size)
 {
     VirtIODevice *vdev = (VirtIODevice *) req->dev;
     size_t in_size, out_size;
 
+    /*
+     * VirtIOSCSIReq *req:
+     * -> VirtQueueElement elem;
+     *    -> unsigned int index;
+     *    -> unsigned int len;
+     *    -> unsigned int ndescs;
+     *    -> unsigned int out_num;
+     *    -> unsigned int in_num;
+     *    -> hwaddr *in_addr;
+     *    -> hwaddr *out_addr;
+     *    -> struct iovec *in_sg;
+     *    -> struct iovec *out_sg;
+     *
+     *   union {
+     *       VirtIOSCSICmdReq      cmd;
+     *       VirtIOSCSICtrlTMFReq  tmf;
+     *       VirtIOSCSICtrlANReq   an;
+     *   } req;
+     *
+     *
+     * iov_to_buf_full()
+     */
     if (iov_to_buf(req->elem.out_sg, req->elem.out_num, 0,
                    &req->req, req_size) < req_size) {
         return -EINVAL;
@@ -210,11 +259,23 @@ static int virtio_scsi_parse_req(VirtIOSCSIReq *req,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|607| <<virtio_scsi_handle_ctrl_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+ *   - hw/scsi/virtio-scsi.c|862| <<virtio_scsi_handle_cmd_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+ *   - hw/scsi/virtio-scsi.c|1011| <<virtio_scsi_push_event>> req = virtio_scsi_pop_req(s, vs->event_vq);
+ */
 static VirtIOSCSIReq *virtio_scsi_pop_req(VirtIOSCSI *s, VirtQueue *vq)
 {
     VirtIOSCSICommon *vs = (VirtIOSCSICommon *)s;
     VirtIOSCSIReq *req;
 
+    /*
+     * 在以下使用VirtIOSCSICommon->cdb_size:
+     *   - hw/scsi/virtio-scsi.c|973| <<virtio_scsi_set_config>> vs->cdb_size = virtio_ldl_p(vdev, &scsiconf->cdb_size);
+     *   - hw/scsi/virtio-scsi.c|1001| <<virtio_scsi_reset>> vs->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     *   - hw/scsi/virtio-scsi.c|1303| <<virtio_scsi_common_realize>> s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     */
     req = virtqueue_pop(vq, sizeof(VirtIOSCSIReq) + vs->cdb_size);
     if (!req) {
         return NULL;
@@ -536,6 +597,10 @@ fail:
     return ret;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|596| <<virtio_scsi_handle_ctrl_vq>> virtio_scsi_handle_ctrl_req(s, req);
+ */
 static void virtio_scsi_handle_ctrl_req(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     VirtIODevice *vdev = (VirtIODevice *)s;
@@ -628,6 +693,15 @@ static void virtio_scsi_handle_ctrl(VirtIODevice *vdev, VirtQueue *vq)
     virtio_scsi_release(s);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|698| <<virtio_scsi_command_failed>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|724| <<virtio_scsi_command_complete>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|765| <<virtio_scsi_request_cancelled>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|775| <<virtio_scsi_fail_cmd_req>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|813| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|825| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_complete_cmd_req(req);
+ */
 static void virtio_scsi_complete_cmd_req(VirtIOSCSIReq *req)
 {
     trace_virtio_scsi_cmd_resp(virtio_scsi_get_lun(req->req.cmd.lun),
@@ -753,18 +827,73 @@ static void virtio_scsi_request_cancelled(SCSIRequest *r)
     virtio_scsi_complete_cmd_req(req);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|772| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_fail_cmd_req(req);
+ */
 static void virtio_scsi_fail_cmd_req(VirtIOSCSIReq *req)
 {
     req->resp.cmd.response = VIRTIO_SCSI_S_FAILURE;
     virtio_scsi_complete_cmd_req(req);
 }
 
+/*
+ * struct virtio_scsi_cmd_req {
+ *     uint8_t lun[8];         // Logical Unit Number
+ *     __virtio64 tag;         // Command identifier
+ *     uint8_t task_attr;              // Task attribute
+ *     uint8_t prio;           // SAM command priority field
+ *     uint8_t crn;
+ *     uint8_t cdb[VIRTIO_SCSI_CDB_SIZE];
+ * } QEMU_PACKED;
+ *
+ * struct virtio_scsi_cmd_resp {
+ *     __virtio32 sense_len;           // Sense data length
+ *     __virtio32 resid;               // Residual bytes in data buffer
+ *     __virtio16 status_qualifier;    // Status qualifier
+ *     uint8_t status;         // Command completion status
+ *     uint8_t response;               // Response values
+ *     uint8_t sense[VIRTIO_SCSI_SENSE_SIZE];
+ * } QEMU_PACKED;
+ *
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|831| <<virtio_scsi_handle_cmd_vq>> ret = virtio_scsi_handle_cmd_req_prepare(s, req);
+ */
 static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     VirtIOSCSICommon *vs = &s->parent_obj;
     SCSIDevice *d;
     int rc;
 
+    /*
+     * VirtIOSCSIReq *req:
+     * -> VirtQueueElement elem;
+     *    -> unsigned int index;
+     *    -> unsigned int len;
+     *    -> unsigned int ndescs;
+     *    -> unsigned int out_num;
+     *    -> unsigned int in_num;
+     *    -> hwaddr *in_addr;
+     *    -> hwaddr *out_addr;
+     *    -> struct iovec *in_sg;
+     *    -> struct iovec *out_sg;
+     *
+     * struct iovec {
+     *     void *iov_base;
+     *     size_t iov_len;
+     * };
+     *
+     * 在以下设置vs->cdb_size:
+     *   - hw/scsi/virtio-scsi.c|1048| <<virtio_scsi_set_config>> vs->cdb_size = virtio_ldl_p(vdev, &scsiconf->cdb_size);
+     *   - hw/scsi/virtio-scsi.c|1076| <<virtio_scsi_reset>> vs->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     *   - hw/scsi/virtio-scsi.c|1378| <<virtio_scsi_common_realize>> s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     *
+     *
+     * 在以下设置vs->sense_size:
+     *   - hw/scsi/virtio-scsi.c|1056| <<virtio_scsi_set_config>> vs->sense_size = virtio_ldl_p(vdev, &scsiconf->sense_size);
+     *   - hw/scsi/virtio-scsi.c|1084| <<virtio_scsi_reset>> vs->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE;
+     *   - hw/scsi/virtio-scsi.c|1386| <<virtio_scsi_common_realize>> s->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE;
+     */
     rc = virtio_scsi_parse_req(req, sizeof(VirtIOSCSICmdReq) + vs->cdb_size,
                                sizeof(VirtIOSCSICmdResp) + vs->sense_size);
     if (rc < 0) {
@@ -772,6 +901,14 @@ static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
             virtio_scsi_fail_cmd_req(req);
             return -ENOTSUP;
         } else {
+            /*
+	     * called by:
+	     *   - hw/scsi/virtio-scsi.c|547| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|555| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|565| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|775| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|974| <<virtio_scsi_push_event>> virtio_scsi_bad_req(req);
+	     */
             virtio_scsi_bad_req(req);
             return -EINVAL;
         }
@@ -804,6 +941,10 @@ static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|885| <<virtio_scsi_handle_cmd_vq>> virtio_scsi_handle_cmd_req_submit(s, req);
+ */
 static void virtio_scsi_handle_cmd_req_submit(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     SCSIRequest *sreq = req->sreq;
@@ -814,6 +955,10 @@ static void virtio_scsi_handle_cmd_req_submit(VirtIOSCSI *s, VirtIOSCSIReq *req)
     scsi_req_unref(sreq);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|867| <<virtio_scsi_handle_cmd>> virtio_scsi_handle_cmd_vq(s, vq);
+ */
 static void virtio_scsi_handle_cmd_vq(VirtIOSCSI *s, VirtQueue *vq)
 {
     VirtIOSCSIReq *req, *next;
@@ -854,6 +999,16 @@ static void virtio_scsi_handle_cmd_vq(VirtIOSCSI *s, VirtQueue *vq)
     }
 }
 
+/*
+ * 在以下使用virtio_scsi_handle_cmd():
+ *   - hw/scsi/virtio-scsi.c|1280| <<virtio_scsi_device_realize>> virtio_scsi_handle_cmd,
+ *
+ * 1281     virtio_scsi_common_realize(dev,
+ * 1282                                virtio_scsi_handle_ctrl,
+ * 1283                                virtio_scsi_handle_event,
+ * 1284                                virtio_scsi_handle_cmd,
+ * 1285                                &err);
+ */
 static void virtio_scsi_handle_cmd(VirtIODevice *vdev, VirtQueue *vq)
 {
     /* use non-QOM casts in the data path */
@@ -887,6 +1042,10 @@ static void virtio_scsi_get_config(VirtIODevice *vdev,
     virtio_stl_p(vdev, &scsiconf->max_lun, VIRTIO_SCSI_MAX_LUN);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|1493| <<virtio_scsi_class_init>> vdc->set_config = virtio_scsi_set_config;
+ */
 static void virtio_scsi_set_config(VirtIODevice *vdev,
                                    const uint8_t *config)
 {
@@ -945,6 +1104,13 @@ typedef struct {
     };
 } VirtIOSCSIEventInfo;
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|1003| <<virtio_scsi_handle_event_vq>> virtio_scsi_push_event(s, &info);
+ *   - hw/scsi/virtio-scsi.c|1037| <<virtio_scsi_change>> virtio_scsi_push_event(s, &info);
+ *   - hw/scsi/virtio-scsi.c|1082| <<virtio_scsi_hotplug>> virtio_scsi_push_event(s, &info);
+ *   - hw/scsi/virtio-scsi.c|1114| <<virtio_scsi_hotunplug>> virtio_scsi_push_event(s, &info);
+ */
 static void virtio_scsi_push_event(VirtIOSCSI *s,
                                    const VirtIOSCSIEventInfo *info)
 {
@@ -1187,6 +1353,12 @@ static struct SCSIBusInfo virtio_scsi_scsi_info = {
     .drained_end = virtio_scsi_drained_end,
 };
 
+/*
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|195| <<vhost_scsi_realize>> virtio_scsi_common_realize(dev, vhost_dummy_handle_output, vhost_dummy_handle_output, vhost_dummy_handle_output, &err);
+ *   - hw/scsi/vhost-user-scsi.c|106| <<vhost_user_scsi_realize>> virtio_scsi_common_realize(dev, vhost_dummy_handle_output, vhost_dummy_handle_output, vhost_dummy_handle_output, &err);
+ *   - hw/scsi/virtio-scsi.c|1287| <<virtio_scsi_device_realize>> virtio_scsi_common_realize(dev, virtio_scsi_handle_ctrl, virtio_scsi_handle_event, virtio_scsi_handle_cmd, &err);
+ */
 void virtio_scsi_common_realize(DeviceState *dev,
                                 VirtIOHandleOutput ctrl,
                                 VirtIOHandleOutput evt,
diff --git a/hw/vfio/common.c b/hw/vfio/common.c
index 9aac21abb..bab853252 100644
--- a/hw/vfio/common.c
+++ b/hw/vfio/common.c
@@ -1542,6 +1542,10 @@ static void vfio_device_feature_dma_logging_start_destroy(
     g_free(feature);
 }
 
+/*
+ * called by:
+ *   - hw/vfio/common.c|1593| <<vfio_listener_log_global_start>> ret = vfio_devices_dma_logging_start(container);
+ */
 static int vfio_devices_dma_logging_start(VFIOContainer *container)
 {
     struct vfio_device_feature *feature;
diff --git a/hw/virtio/virtio.c b/hw/virtio/virtio.c
index 309038fd4..84e24177e 100644
--- a/hw/virtio/virtio.c
+++ b/hw/virtio/virtio.c
@@ -1365,6 +1365,16 @@ int virtqueue_avail_bytes(VirtQueue *vq, unsigned int in_bytes,
     return in_bytes <= in_total && out_bytes <= out_total;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1597| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
+ *   - hw/virtio/virtio.c|1606| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
+ *   - hw/virtio/virtio.c|1731| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
+ *   - hw/virtio/virtio.c|1740| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
+ *   - subprojects/libvhost-user/libvhost-user.c|2524| <<virtqueue_map_desc>> virtqueue_map_desc(VuDev *dev,
+ *   - subprojects/libvhost-user/libvhost-user.c|2628| <<vu_queue_map_desc>> if (!virtqueue_map_desc(dev, &in_num, iov + out_num,
+ *   - subprojects/libvhost-user/libvhost-user.c|2639| <<vu_queue_map_desc>> if (!virtqueue_map_desc(dev, &out_num, iov,
+ */
 static bool virtqueue_map_desc(VirtIODevice *vdev, unsigned int *p_num_sg,
                                hwaddr *addr, struct iovec *iov,
                                unsigned int max_num_sg, bool is_write,
@@ -1461,9 +1471,28 @@ void virtqueue_map(VirtIODevice *vdev, VirtQueueElement *elem)
                                                                         false);
 }
 
+/*
+ * typedef struct VirtQueueElement
+ * {
+ *     unsigned int index;
+ *     unsigned int len;
+ *     unsigned int ndescs;
+ *     unsigned int out_num;
+ *     unsigned int in_num;
+ *     hwaddr *in_addr;
+ *     hwaddr *out_addr;
+ *     struct iovec *in_sg;
+ *     struct iovec *out_sg;
+ * } VirtQueueElement;
+ *
+ * sz可以是sizeof(VirtIOSCSIReq) + vs->cdb_size
+ */
 static void *virtqueue_alloc_element(size_t sz, unsigned out_num, unsigned in_num)
 {
     VirtQueueElement *elem;
+    /*
+     * 注意这里的sz !!!
+     */
     size_t in_addr_ofs = QEMU_ALIGN_UP(sz, __alignof__(elem->in_addr[0]));
     size_t out_addr_ofs = in_addr_ofs + in_num * sizeof(elem->in_addr[0]);
     size_t out_addr_end = out_addr_ofs + out_num * sizeof(elem->out_addr[0]);
@@ -1483,6 +1512,9 @@ static void *virtqueue_alloc_element(size_t sz, unsigned out_num, unsigned in_nu
     return elem;
 }
 
+/*
+ * 一个例子: sz可以是sizeof(VirtIOSCSIReq) + vs->cdb_size
+ */
 static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
 {
     unsigned int i, head, max;
@@ -1538,6 +1570,9 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
     }
 
     desc_cache = &caches->desc;
+    /*
+     * VRingDesc desc;
+     */
     vring_split_desc_read(vdev, &desc, desc_cache, i);
     if (desc.flags & VRING_DESC_F_INDIRECT) {
         if (!desc.len || (desc.len % sizeof(VRingDesc))) {
@@ -1564,6 +1599,11 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         bool map_ok;
 
         if (desc.flags & VRING_DESC_F_WRITE) {
+            /*
+	     * desc.addr是VM的物理地址
+	     *
+	     * hwaddr addr[VIRTQUEUE_MAX_SIZE];
+	     */
             map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
                                         iov + out_num,
                                         VIRTQUEUE_MAX_SIZE - out_num, true,
@@ -1594,11 +1634,17 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         goto err_undo_map;
     }
 
+    /*
+     * sz可以是sizeof(VirtIOSCSIReq) + vs->cdb_size
+     */
     /* Now copy what we have collected and mapped */
     elem = virtqueue_alloc_element(sz, out_num, in_num);
     elem->index = head;
     elem->ndescs = 1;
     for (i = 0; i < out_num; i++) {
+        /*
+	 * 这里应该是copy, 不是指针
+	 */
         elem->out_addr[i] = addr[i];
         elem->out_sg[i] = iov[i];
     }
@@ -1887,6 +1933,12 @@ typedef struct VirtQueueElementOld {
     struct iovec out_sg[VIRTQUEUE_MAX_SIZE];
 } VirtQueueElementOld;
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1480| <<virtio_blk_load_device>> req = qemu_get_virtqueue_element(vdev, f, sizeof(VirtIOBlockReq));
+ *   - hw/char/virtio-serial-bus.c|785| <<fetch_active_ports_list>> qemu_get_virtqueue_element(vdev, f, sizeof(VirtQueueElement));
+ *   - hw/scsi/virtio-scsi.c|301| <<virtio_scsi_load_request>> req = qemu_get_virtqueue_element(vdev, f, sizeof(VirtIOSCSIReq) + vs->cdb_size);
+ */
 void *qemu_get_virtqueue_element(VirtIODevice *vdev, QEMUFile *f, size_t sz)
 {
     VirtQueueElement *elem;
diff --git a/include/hw/core/cpu.h b/include/hw/core/cpu.h
index fdcbe8735..54221cc23 100644
--- a/include/hw/core/cpu.h
+++ b/include/hw/core/cpu.h
@@ -402,6 +402,16 @@ struct CPUState {
     uint64_t dirty_pages;
     int kvm_vcpu_stats_fd;
 
+    /*
+     * 在以下使用CPUState->in_ioctl_lock:
+     *   - accel/accel-blocker.c|116| <<accel_cpu_ioctl_begin>> qemu_lockcnt_inc(&cpu->in_ioctl_lock);
+     *   - accel/accel-blocker.c|129| <<accel_cpu_ioctl_end>> qemu_lockcnt_dec(&cpu->in_ioctl_lock);
+     *   - accel/accel-blocker.c|144| <<accel_has_to_wait>> if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
+     *   - accel/accel-blocker.c|170| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&cpu->in_ioctl_lock);
+     *   - accel/accel-blocker.c|217| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&cpu->in_ioctl_lock);
+     *   - hw/core/cpu-common.c|238| <<cpu_common_initfn>> qemu_lockcnt_init(&cpu->in_ioctl_lock);
+     *   - hw/core/cpu-common.c|250| <<cpu_common_finalize>> qemu_lockcnt_destroy(&cpu->in_ioctl_lock);
+     */
     /* Use by accel-block: CPU is executing an ioctl() */
     QemuLockCnt in_ioctl_lock;
 
diff --git a/include/qemu/futex.h b/include/qemu/futex.h
index 91ae88966..af55aefb4 100644
--- a/include/qemu/futex.h
+++ b/include/qemu/futex.h
@@ -19,11 +19,29 @@
 
 #define qemu_futex(...)              syscall(__NR_futex, __VA_ARGS__)
 
+/*
+ * called by:
+ *   - tests/unit/test-aio-multithread.c|331| <<mcs_mutex_unlock>> qemu_futex_wake(&nodes[next].locked, 1);
+ *   - util/lockcnt.c|116| <<lockcnt_wake>> qemu_futex_wake(&lockcnt->count, 1);
+ *   - util/qemu-thread-posix.c|413| <<qemu_event_set>> qemu_futex_wake(ev, INT_MAX);
+ *
+ * FUTEX_WAIT: 如果futex word中仍然保存着参数val给定的值,
+ * 那么当前线程则进入睡眠,等待FUTEX_WAKE的操作唤醒它.
+ *
+ * FUTEX_WAKE: 最多唤醒val个等待在futex word上的线程.
+ * Val或者等于1(唤醒1个等待线程)或者等于INT_MAX(唤醒全部等待线程)
+ */
 static inline void qemu_futex_wake(void *f, int n)
 {
     qemu_futex(f, FUTEX_WAKE, n, NULL, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - tests/unit/test-aio-multithread.c|308| <<mcs_mutex_lock>> qemu_futex_wait(&nodes[id].locked, 1);
+ *   - util/lockcnt.c|102| <<qemu_lockcnt_cmpxchg_or_wait>> qemu_futex_wait(&lockcnt->count, *val);
+ *   - util/qemu-thread-posix.c|474| <<qemu_event_wait>> qemu_futex_wait(ev, EV_BUSY);
+ */
 static inline void qemu_futex_wait(void *f, unsigned val)
 {
     while (qemu_futex(f, FUTEX_WAIT, (int) val, NULL, NULL, 0)) {
diff --git a/migration/migration-stats.c b/migration/migration-stats.c
index 095d6d75b..b1c94341b 100644
--- a/migration/migration-stats.c
+++ b/migration/migration-stats.c
@@ -18,15 +18,43 @@
 
 MigrationAtomicStats mig_stats;
 
+/*
+ * (gdb) bt
+ * #0  migration_rate_exceeded (f=f@entry=0x55718cc4c630) at ../migration/migration-stats.c:23
+ * #1  0x000055718a2e6546 in ram_save_iterate (f=0x55718cc4c630, opaque=<optimized out>) at ../migration/ram.c:3133
+ * #2  0x000055718a190939 in qemu_savevm_state_iterate (f=0x55718cc4c630, postcopy=postcopy@entry=false) at ../migration/savevm.c:1348
+ * #3  0x000055718a180fb1 in migration_iteration_run (s=0x55718c6d51c0) at ../migration/migration.c:2785
+ * #4  migration_thread (opaque=opaque@entry=0x55718c6d51c0) at ../migration/migration.c:3017
+ * #5  0x000055718a4b40d9 in qemu_thread_start (args=0x7f680ad0e8d0) at ../util/qemu-thread-posix.c:541
+ * #6  0x00007f69c3936ea5 in start_thread () at /lib64/libpthread.so.0
+ * #7  0x00007f69c365f9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - hw/ppc/spapr.c|2175| <<htab_save_first_pass>> } while ((index < htabslots) && !migration_rate_exceeded(f));
+ *   - hw/ppc/spapr.c|2246| <<htab_save_later_pass>> } while ((examined < htabslots) && (!migration_rate_exceeded(f) || final));
+ *   - hw/s390x/s390-stattrib.c|212| <<cmma_save>> while (final ? 1 : migration_rate_exceeded(f) == 0) {
+ *   - migration/block-dirty-bitmap.c|709| <<bulk_phase>> if (limit && migration_rate_exceeded(f)) {
+ *   - migration/block.c|629| <<flush_blks>> if (migration_rate_exceeded(f)) {
+ *   - migration/migration.c|2894| <<migration_rate_limit>> if (migration_rate_exceeded(s->to_dst_file)) {
+ *   - migration/migration.c|3016| <<migration_thread>> if (urgent || !migration_rate_exceeded(s->to_dst_file)) {
+ *   - migration/ram.c|3133| <<ram_save_iterate>> while ((ret = migration_rate_exceeded(f)) == 0 ||
+ *   - migration/savevm.c|1341| <<qemu_savevm_state_iterate>> if (migration_rate_exceeded(f)) {
+ */
 bool migration_rate_exceeded(QEMUFile *f)
 {
     if (qemu_file_get_error(f)) {
         return true;
     }
 
+    /*
+     * Amount of transferred data at the start of current cycle.
+     */
     uint64_t rate_limit_start = stat64_get(&mig_stats.rate_limit_start);
     uint64_t rate_limit_current = migration_transferred_bytes(f);
     uint64_t rate_limit_used = rate_limit_current - rate_limit_start;
+    /*
+     * Maximum amount of data we can send in a cycle.
+     */
     uint64_t rate_limit_max = stat64_get(&mig_stats.rate_limit_max);
 
     if (rate_limit_max == RATE_LIMIT_DISABLED) {
@@ -53,6 +81,10 @@ void migration_rate_set(uint64_t limit)
     stat64_set(&mig_stats.rate_limit_max, limit / XFER_LIMIT_RATIO);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2717| <<migration_update_counters>> migration_rate_reset(s->to_dst_file);
+ */
 void migration_rate_reset(QEMUFile *f)
 {
     stat64_set(&mig_stats.rate_limit_start, migration_transferred_bytes(f));
diff --git a/migration/migration.c b/migration/migration.c
index 5528acb65..5656b73de 100644
--- a/migration/migration.c
+++ b/migration/migration.c
@@ -2307,6 +2307,10 @@ static int migration_maybe_pause(MigrationState *s,
  *
  * @s: Current migration state
  */
+/*
+ * called by:
+ *   - migration/migration.c|2775| <<migration_iteration_run>> migration_completion(s);
+ */
 static void migration_completion(MigrationState *s)
 {
     int ret;
@@ -2679,6 +2683,11 @@ static void update_iteration_initial_status(MigrationState *s)
     s->iteration_initial_pages = ram_get_total_transferred_pages();
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2893| <<migration_rate_limit>> migration_update_counters(s, now);
+ *   - migration/migration.c|3191| <<bg_migration_thread>> migration_update_counters(s, qemu_clock_get_ms(QEMU_CLOCK_REALTIME));
+ */
 static void migration_update_counters(MigrationState *s,
                                       int64_t current_time)
 {
@@ -2747,6 +2756,14 @@ typedef enum {
  * Return true if continue to the next iteration directly, false
  * otherwise.
  */
+/*
+ * (gdb) bt
+ * #0  migration_iteration_run (s=s@entry=0x55e44417c1c0) at ../migration/migration.c:2751
+ * #1  0x000055e4428a7ef7 in migration_thread (opaque=opaque@entry=0x55e44417c1c0) at ../migration/migration.c:3022
+ * #2  0x000055e442bda0e9 in qemu_thread_start (args=0x7f362623c8d0) at ../util/qemu-thread-posix.c:541
+ * #3  0x00007f362c9e3ea5 in start_thread () at /lib64/libpthread.so.0
+ * #4  0x00007f362c70c9fd in clone () at /lib64/libc.so.6
+ */
 static MigIterateState migration_iteration_run(MigrationState *s)
 {
     uint64_t must_precopy, can_postcopy;
@@ -2765,6 +2782,16 @@ static MigIterateState migration_iteration_run(MigrationState *s)
         trace_migrate_pending_exact(pending_size, must_precopy, can_postcopy);
     }
 
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1426| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|2702| <<migration_update_counters>> s->threshold_size = bandwidth * migrate_downtime_limit();
+     *   - migration/migration.c|2727| <<migration_update_counters>> bandwidth, s->threshold_size);
+     *   - migration/migration.c|2767| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|2773| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|2780| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover &&
+     *   - migration/ram.c|3302| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     */
     if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
         trace_migration_thread_low_pending(pending_size);
         migration_completion(s);
@@ -2883,6 +2910,11 @@ void migration_consume_urgent_request(void)
     qemu_sem_wait(&migrate_get_current()->rate_limit_sem);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3042| <<migration_thread>> urgent = migration_rate_limit();
+ *   - migration/ram.c|2333| <<ram_save_host_page>> migration_rate_limit();
+ */
 /* Returns true if the rate limiting was broken by an urgent request */
 bool migration_rate_limit(void)
 {
diff --git a/migration/migration.h b/migration/migration.h
index 6eea18db3..c289bcd8f 100644
--- a/migration/migration.h
+++ b/migration/migration.h
@@ -285,6 +285,16 @@ struct MigrationState {
      * this threshold; it's calculated from the requested downtime and
      * measured bandwidth
      */
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1426| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|2702| <<migration_update_counters>> s->threshold_size = bandwidth * migrate_downtime_limit();
+     *   - migration/migration.c|2727| <<migration_update_counters>> bandwidth, s->threshold_size);
+     *   - migration/migration.c|2767| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|2773| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|2780| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover &&
+     *   - migration/ram.c|3302| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     */
     int64_t threshold_size;
 
     /* params from 'migrate-set-parameters' */
diff --git a/softmmu/balloon.c b/softmmu/balloon.c
index e0e8969a4..b889ef84b 100644
--- a/softmmu/balloon.c
+++ b/softmmu/balloon.c
@@ -52,6 +52,10 @@ static bool have_balloon(Error **errp)
     return true;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio-balloon.c|863| <<virtio_balloon_device_realize>> ret = qemu_add_balloon_handler(virtio_balloon_to_target, virtio_balloon_stat, s);
+ */
 int qemu_add_balloon_handler(QEMUBalloonEvent *event_func,
                              QEMUBalloonStatus *stat_func, void *opaque)
 {
diff --git a/softmmu/dma-helpers.c b/softmmu/dma-helpers.c
index 246396480..370819362 100644
--- a/softmmu/dma-helpers.c
+++ b/softmmu/dma-helpers.c
@@ -219,6 +219,15 @@ static const AIOCBInfo dma_aiocb_info = {
     .get_aio_context    = dma_get_aio_context,
 };
 
+/*
+ * called by:
+ *   - hw/ide/core.c|959| <<ide_dma_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk),
+ *   - hw/ide/macio.c|193| <<pmac_ide_transfer_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk), &s->sg,
+ *   - hw/scsi/scsi-disk.c|429| <<scsi_do_read>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *   - hw/scsi/scsi-disk.c|591| <<scsi_write_data>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *   - softmmu/dma-helpers.c|262| <<dma_blk_read>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ *   - softmmu/dma-helpers.c|280| <<dma_blk_write>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ */
 BlockAIOCB *dma_blk_io(AioContext *ctx,
     QEMUSGList *sg, uint64_t offset, uint32_t align,
     DMAIOFunc *io_func, void *io_func_opaque,
diff --git a/target/arm/kvm64.c b/target/arm/kvm64.c
index 94bbd9661..91f676373 100644
--- a/target/arm/kvm64.c
+++ b/target/arm/kvm64.c
@@ -1007,6 +1007,10 @@ static int kvm_arch_get_sve(CPUState *cs)
     return 0;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2851| <<do_kvm_cpu_synchronize_state>> kvm_arch_get_registers(cpu);
+ */
 int kvm_arch_get_registers(CPUState *cs)
 {
     struct kvm_one_reg reg;
diff --git a/target/i386/kvm/kvm.c b/target/i386/kvm/kvm.c
index ebfaf3d24..57244954e 100644
--- a/target/i386/kvm/kvm.c
+++ b/target/i386/kvm/kvm.c
@@ -4419,6 +4419,10 @@ static int kvm_get_apic(X86CPU *cpu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|4788| <<kvm_arch_put_registers>> ret = kvm_put_vcpu_events(x86_cpu, level);
+ */
 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
 {
     CPUState *cs = CPU(cpu);
diff --git a/util/aio-wait.c b/util/aio-wait.c
index b5336cf5f..180fca2f2 100644
--- a/util/aio-wait.c
+++ b/util/aio-wait.c
@@ -33,6 +33,26 @@ static void dummy_bh_cb(void *opaque)
     /* The point is to make AIO_WAIT_WHILE()'s aio_poll() return */
 }
 
+/*
+ * called by:
+ *   - block/block-backend.c|1533| <<blk_dec_in_flight>> aio_wait_kick();
+ *   - block/export/fuse.c|308| <<read_from_fuse_export>> aio_wait_kick();
+ *   - block/export/vduse-blk.c|55| <<vduse_blk_inflight_dec>> aio_wait_kick();
+ *   - block/graph-lock.c|236| <<bdrv_graph_co_rdlock>> aio_wait_kick();
+ *   - block/graph-lock.c|259| <<bdrv_graph_co_rdunlock>> aio_wait_kick();
+ *   - block/io.c|771| <<bdrv_wakeup>> aio_wait_kick();
+ *   - block/nvme.c|513| <<nvme_admin_cmd_sync_cb>> aio_wait_kick();
+ *   - block/qcow2.c|1926| <<qcow2_open_entry>> aio_wait_kick();
+ *   - block/throttle-groups.c|424| <<throttle_group_restart_queue_entry>> aio_wait_kick();
+ *   - blockjob.c|268| <<block_job_on_idle_locked>> aio_wait_kick();
+ *   - nbd/server.c|1529| <<nbd_request_put>> aio_wait_kick();
+ *   - nbd/server.c|2667| <<nbd_trip>> aio_wait_kick();
+ *   - net/colo-compare.c|789| <<_compare_chr_send>> aio_wait_kick();
+ *   - net/filter-mirror.c|103| <<filter_send_co>> aio_wait_kick();
+ *   - tests/unit/test-bdrv-drain.c|497| <<test_iothread_drain_co_entry>> aio_wait_kick();
+ *   - util/aio-wait.c|72| <<aio_wait_bh>> aio_wait_kick();
+ *   - util/vhost-user-server.c|224| <<vu_client_trip>> aio_wait_kick();
+ */
 void aio_wait_kick(void)
 {
     /*
diff --git a/util/iov.c b/util/iov.c
index 866fb577f..f89039cf3 100644
--- a/util/iov.c
+++ b/util/iov.c
@@ -40,6 +40,11 @@ size_t iov_from_buf_full(const struct iovec *iov, unsigned int iov_cnt,
     return done;
 }
 
+/*
+ * called by:
+ *   - block/mirror.c|1536| <<bdrv_mirror_top_pwritev>> iov_to_buf_full(qiov->iov, qiov->niov, 0, bounce_buf, bytes);
+ *   - include/qemu/iov.h|62| <<iov_to_buf>> return iov_to_buf_full(iov, iov_cnt, offset, buf, bytes);
+ */
 size_t iov_to_buf_full(const struct iovec *iov, const unsigned int iov_cnt,
                        size_t offset, void *buf, size_t bytes)
 {
@@ -47,6 +52,13 @@ size_t iov_to_buf_full(const struct iovec *iov, const unsigned int iov_cnt,
     unsigned int i;
     for (i = 0, done = 0; (offset || done < bytes) && i < iov_cnt; i++) {
         if (offset < iov[i].iov_len) {
+            /*
+	     * 如果所有的iov_len加起来不等于bytes???
+	     * bytes比较大??
+	     *
+	     * 如果函数的参数offset=0
+	     * 应该一直进入这个if吧
+	     */
             size_t len = MIN(iov[i].iov_len - offset, bytes - done);
             memcpy(buf + done, iov[i].iov_base + offset, len);
             done += len;
@@ -316,6 +328,16 @@ void qemu_iovec_add(QEMUIOVector *qiov, void *base, size_t len)
  * of src".
  * Only vector pointers are processed, not the actual data buffers.
  */
+/*
+ * called by:
+ *   - block/io.c|1654| <<bdrv_create_padded_qiov>> qemu_iovec_concat_iov(&pad->pre_collapse_qiov, iov, collapse_count, iov_offset, SIZE_MAX);
+ *   - block/io.c|1676| <<bdrv_create_padded_qiov>> qemu_iovec_concat_iov(&pad->local_qiov, iov, niov, iov_offset, bytes);
+ *   - block/vhdx.c|1412| <<vhdx_co_writev>> qemu_iovec_concat_iov(&hd_qiov, &iov1, 1, 0, iov1.iov_len);
+ *   - block/vhdx.c|1428| <<vhdx_co_writev>> qemu_iovec_concat_iov(&hd_qiov, &iov2, 1, 0, iov2.iov_len);
+ *   - hw/scsi/virtio-scsi.c|182| <<virtio_scsi_parse_req>> if (qemu_iovec_concat_iov(&req->resp_iov, req->elem.in_sg, req->elem.in_num, 0, resp_size) < resp_size) {
+ *   - util/iov.c|358| <<qemu_iovec_concat>> qemu_iovec_concat_iov(dst, src->iov, src->niov, soffset, sbytes);
+ *   - util/iov.c|463| <<qemu_iovec_init_slice>> qemu_iovec_concat_iov(qiov, slice_iov, slice_niov, slice_head, len);
+ */
 size_t qemu_iovec_concat_iov(QEMUIOVector *dst,
                              struct iovec *src_iov, unsigned int src_cnt,
                              size_t soffset, size_t sbytes)
@@ -329,6 +351,9 @@ size_t qemu_iovec_concat_iov(QEMUIOVector *dst,
     assert(dst->nalloc != -1);
     for (i = 0, done = 0; done < sbytes && i < src_cnt; i++) {
         if (soffset < src_iov[i].iov_len) {
+            /*
+	     * 如果soffset是0, 会一直进入这个if吧
+	     */
             size_t len = MIN(src_iov[i].iov_len - soffset, sbytes - done);
             qemu_iovec_add(dst, src_iov[i].iov_base + soffset, len);
             done += len;
diff --git a/util/lockcnt.c b/util/lockcnt.c
index 5da36946b..17037b2a3 100644
--- a/util/lockcnt.c
+++ b/util/lockcnt.c
@@ -30,6 +30,14 @@
 
 void qemu_lockcnt_init(QemuLockCnt *lockcnt)
 {
+    /*
+     * struct QemuLockCnt {
+     * #ifndef CONFIG_LINUX
+     *     QemuMutex mutex;
+     * #endif
+     *     unsigned count;
+     * };
+     */
     lockcnt->count = 0;
 }
 
@@ -53,6 +61,13 @@ void qemu_lockcnt_destroy(QemuLockCnt *lockcnt)
  * is set the caller has effectively acquired the lock.  If it returns
  * with the lock not taken, it must wake another futex waiter.
  */
+/*
+ * called by:
+ *   - util/lockcnt.c|139| <<qemu_lockcnt_inc>> if (qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, QEMU_LOCKCNT_COUNT_STEP,
+ *   - util/lockcnt.c|184| <<qemu_lockcnt_dec_and_lock>> if (qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, locked_state, &waited)) {
+ *   - util/lockcnt.c|225| <<qemu_lockcnt_dec_if_lock>> if (qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, locked_state, &waited)) {
+ *   - util/lockcnt.c|259| <<qemu_lockcnt_lock>> while (!qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, val + step, &waited)) {
+ */
 static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
                                          int new_if_free, bool *waited)
 {
@@ -61,6 +76,11 @@ static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
         int expected = *val;
 
         trace_lockcnt_fast_path_attempt(lockcnt, expected, new_if_free);
+        /*
+	 * 将old和ptr指向的内容比较,
+	 * 如果相等,则将new写入到ptr中,返回old,
+	 * 如果不相等,则返回ptr指向的内容.
+         */
         *val = qatomic_cmpxchg(&lockcnt->count, expected, new_if_free);
         if (*val == expected) {
             trace_lockcnt_fast_path_success(lockcnt, expected, new_if_free);
@@ -91,6 +111,16 @@ static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
         if ((*val & QEMU_LOCKCNT_STATE_MASK) == QEMU_LOCKCNT_STATE_WAITING) {
             *waited = true;
             trace_lockcnt_futex_wait(lockcnt, *val);
+            /*
+	     * QemuLockCnt *lockcnt
+	     *
+	     * struct QemuLockCnt {
+	     * #ifndef CONFIG_LINUX
+	     *     QemuMutex mutex;
+	     * #endif
+	     *     unsigned count;
+	     * };
+	     */
             qemu_futex_wait(&lockcnt->count, *val);
             *val = qatomic_read(&lockcnt->count);
             trace_lockcnt_futex_wait_resume(lockcnt, *val);
@@ -102,6 +132,14 @@ static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
     return false;
 }
 
+/*
+ * called by:
+ *   - util/lockcnt.c|153| <<qemu_lockcnt_inc>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|204| <<qemu_lockcnt_dec_and_lock>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|244| <<qemu_lockcnt_dec_if_lock>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|283| <<qemu_lockcnt_inc_and_unlock>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|301| <<qemu_lockcnt_unlock>> lockcnt_wake(lockcnt);
+ */
 static void lockcnt_wake(QemuLockCnt *lockcnt)
 {
     trace_lockcnt_futex_wake(lockcnt);
@@ -289,6 +327,20 @@ void qemu_lockcnt_unlock(QemuLockCnt *lockcnt)
     }
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|144| <<accel_has_to_wait>> if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
+ *   - accel/accel-blocker.c|151| <<accel_has_to_wait>> return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
+ *   - util/aio-posix.c|87| <<aio_remove_fd_handler>> if (qemu_lockcnt_count(&ctx->list_lock)) {
+ *   - util/aio-posix.c|528| <<run_poll_handlers>> assert(qemu_lockcnt_count(&ctx->list_lock) > 0);
+ *
+ * qemu_lockcnt_count: query a LockCnt's count.
+ * @lockcnt: the lockcnt to query.
+ *
+ * Note that the count can change at any time.  Still, while the
+ * lockcnt is locked, one can usefully check whether the count
+ * is non-zero.
+ */
 unsigned qemu_lockcnt_count(QemuLockCnt *lockcnt)
 {
     return qatomic_read(&lockcnt->count) >> QEMU_LOCKCNT_COUNT_SHIFT;
diff --git a/util/qemu-thread-posix.c b/util/qemu-thread-posix.c
index b2e26e212..27fb4e665 100644
--- a/util/qemu-thread-posix.c
+++ b/util/qemu-thread-posix.c
@@ -361,6 +361,18 @@ static inline void qemu_futex_wait(QemuEvent *ev, unsigned val)
 #define EV_FREE        1
 #define EV_BUSY       -1
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|63| <<accel_blocker_init>> qemu_event_init(&accel_in_ioctl_event, false);
+ *   - migration/colo.c|648| <<migrate_start_colo_process>> qemu_event_init(&s->colo_checkpoint_event, false);
+ *   - migration/migration.c|149| <<migration_object_init>> qemu_event_init(&current_incoming->main_thread_load_event, false);
+ *   - tests/unit/test-aio-multithread.c|75| <<create_aio_contexts>> qemu_event_init(&done_event, false);
+ *   - tests/unit/test-bdrv-drain.c|2056| <<main>> qemu_event_init(&done_event, false);
+ *   - util/qemu-timer.c|104| <<timerlist_new>> qemu_event_init(&timer_list->timers_done_ev, true);
+ *   - util/rcu.c|343| <<drain_call_rcu>> qemu_event_init(&rcu_drain.drain_complete_event, false);
+ *   - util/rcu.c|408| <<rcu_init_complete>> qemu_event_init(&rcu_gp_event, true);
+ *   - util/rcu.c|410| <<rcu_init_complete>> qemu_event_init(&rcu_call_ready_event, false);
+ */
 void qemu_event_init(QemuEvent *ev, bool init)
 {
 #ifndef __linux__
@@ -368,10 +380,26 @@ void qemu_event_init(QemuEvent *ev, bool init)
     pthread_cond_init(&ev->cond, NULL);
 #endif
 
+    /*
+     * struct QemuEvent {
+     * #ifndef __linux__
+     *     pthread_mutex_t lock;
+     *     pthread_cond_t cond;
+     * #endif
+     *     unsigned value;
+     *     bool initialized;
+     * };
+     */
     ev->value = (init ? EV_SET : EV_FREE);
     ev->initialized = true;
 }
 
+/*
+ * called by:
+ *   - migration/colo.c|632| <<colo_process_checkpoint>> qemu_event_destroy(&s->colo_checkpoint_event);
+ *   - tests/unit/test-aio-multithread.c|96| <<join_aio_contexts>> qemu_event_destroy(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|2118| <<main>> qemu_event_destroy(&done_event);
+ */
 void qemu_event_destroy(QemuEvent *ev)
 {
     assert(ev->initialized);
@@ -382,6 +410,35 @@ void qemu_event_destroy(QemuEvent *ev)
 #endif
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|91| <<accel_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|120| <<accel_cpu_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - include/qemu/rcu.h|117| <<rcu_read_unlock>> qemu_event_set(&rcu_gp_event);
+ *   - migration/colo.c|71| <<colo_checkpoint_notify>> qemu_event_set(&s->colo_checkpoint_event);
+ *   - migration/colo.c|808| <<colo_shutdown>> qemu_event_set(&s->colo_checkpoint_event);
+ *   - migration/savevm.c|2832| <<qemu_loadvm_state>> qemu_event_set(&mis->main_thread_load_event);
+ *   - tests/unit/test-aio-multithread.c|42| <<ctx_run_bh_cb>> qemu_event_set(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|504| <<test_iothread_aio_cb>> qemu_event_set(&done_event);
+ *   - util/qemu-timer.c|584| <<timerlist_run_timers>> qemu_event_set(&timer_list->timers_done_ev);
+ *   - util/rcu.c|313| <<call_rcu1>> qemu_event_set(&rcu_call_ready_event);
+ *   - util/rcu.c|325| <<drain_rcu_callback>> qemu_event_set(&event->drain_complete_event);
+ *
+ * struct QemuEvent {
+ * #ifndef __linux__
+ *     pthread_mutex_t lock;
+ *     pthread_cond_t cond;
+ * #endif
+ *     unsigned value;
+ *     bool initialized;
+ * };
+ *
+ * 核心思想:
+ * 如果已经是EV_SET了, 目的达到了, 什么也不用做.
+ * 如果不是EV_SET, 用xchg换成EV_SET:
+ *   - 如果旧的是EV_FREE, 什么也不做
+ *   - 如果旧的是EV_BUSY, wake up!
+ */
 void qemu_event_set(QemuEvent *ev)
 {
     assert(ev->initialized);
@@ -393,18 +450,53 @@ void qemu_event_set(QemuEvent *ev)
      * ev->value we need a full memory barrier here.
      */
     smp_mb();
+    /*
+     * #define EV_SET         0
+     * #define EV_FREE        1
+     * #define EV_BUSY       -1
+     */
     if (qatomic_read(&ev->value) != EV_SET) {
         int old = qatomic_xchg(&ev->value, EV_SET);
 
         /* Pairs with memory barrier in kernel futex_wait system call.  */
         smp_mb__after_rmw();
+        /*
+	 * #define EV_SET         0
+	 * #define EV_FREE        1
+	 * #define EV_BUSY       -1
+	 */
         if (old == EV_BUSY) {
+            /*
+	     * FUTEX_WAIT: 如果futex word中仍然保存着参数val给定的值,
+	     * 那么当前线程则进入睡眠,等待FUTEX_WAKE的操作唤醒它.
+	     *
+	     * FUTEX_WAKE: 最多唤醒val个等待在futex word上的线程.
+	     * Val或者等于1(唤醒1个等待线程)或者等于INT_MAX(唤醒全部等待线程)
+	     *
+	     * called by:
+	     *   - tests/unit/test-aio-multithread.c|331| <<mcs_mutex_unlock>> qemu_futex_wake(&nodes[next].locked, 1);
+	     *   - util/lockcnt.c|116| <<lockcnt_wake>> qemu_futex_wake(&lockcnt->count, 1);
+	     *   - util/qemu-thread-posix.c|413| <<qemu_event_set>> qemu_futex_wake(ev, INT_MAX);
+	     */
             /* There were waiters, wake them up.  */
             qemu_futex_wake(ev, INT_MAX);
         }
     }
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|178| <<accel_ioctl_inhibit_begin>> qemu_event_reset(&accel_in_ioctl_event);
+ *   - migration/colo.c|498| <<colo_do_checkpoint_transaction>> qemu_event_reset(&s->colo_checkpoint_event);
+ *   - migration/migration.c|257| <<migration_incoming_state_destroy>> qemu_event_reset(&mis->main_thread_load_event);
+ *   - tests/unit/test-aio-multithread.c|52| <<ctx_run>> qemu_event_reset(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|562| <<test_iothread_common>> qemu_event_reset(&done_event);
+ *   - util/qemu-thread-win32.c|295| <<qemu_event_reset>> void qemu_event_reset(QemuEvent *ev)
+ *   - util/qemu-timer.c|513| <<timerlist_run_timers>> qemu_event_reset(&timer_list->timers_done_ev);
+ *   - util/rcu.c|84| <<wait_for_readers>> qemu_event_reset(&rcu_gp_event);
+ *   - util/rcu.c|272| <<call_rcu_thread>> qemu_event_reset(&rcu_call_ready_event);
+ *   - util/rcu.c|291| <<call_rcu_thread>> qemu_event_reset(&rcu_call_ready_event);
+ */
 void qemu_event_reset(QemuEvent *ev)
 {
     assert(ev->initialized);
@@ -422,6 +514,28 @@ void qemu_event_reset(QemuEvent *ev)
     smp_mb__after_rmw();
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|196| <<accel_ioctl_inhibit_begin>> qemu_event_wait(&accel_in_ioctl_event);
+ *   - migration/colo.c|585| <<colo_process_checkpoint>> qemu_event_wait(&s->colo_checkpoint_event);
+ *   - migration/savevm.c|1956| <<postcopy_ram_listen_thread>> qemu_event_wait(&mis->main_thread_load_event);
+ *   - tests/unit/test-aio-multithread.c|54| <<ctx_run>> qemu_event_wait(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|599| <<test_iothread_common>> qemu_event_wait(&done_event);
+ *   - util/qemu-timer.c|167| <<qemu_clock_enable>> qemu_event_wait(&tl->timers_done_ev);
+ *   - util/rcu.c|136| <<wait_for_readers>> qemu_event_wait(&rcu_gp_event);
+ *   - util/rcu.c|278| <<call_rcu_thread>> qemu_event_wait(&rcu_call_ready_event);
+ *   - util/rcu.c|294| <<call_rcu_thread>> qemu_event_wait(&rcu_call_ready_event);
+ *   - util/rcu.c|364| <<drain_call_rcu>> qemu_event_wait(&rcu_drain.drain_complete_event);
+ *
+ * struct QemuEvent {
+ * #ifndef __linux__
+ *     pthread_mutex_t lock;
+ *     pthread_cond_t cond;
+ * #endif
+ *     unsigned value;
+ *     bool initialized;
+ * };
+ */
 void qemu_event_wait(QemuEvent *ev)
 {
     unsigned value;
@@ -438,6 +552,11 @@ void qemu_event_wait(QemuEvent *ev)
      * qemu_futex_wait() will ensure the check is done correctly.
      */
     value = qatomic_load_acquire(&ev->value);
+    /*
+     * #define EV_SET         0
+     * #define EV_FREE        1
+     * #define EV_BUSY       -1
+     */
     if (value != EV_SET) {
         if (value == EV_FREE) {
             /*
@@ -452,10 +571,20 @@ void qemu_event_wait(QemuEvent *ev)
              * like the load above.
              */
             if (qatomic_cmpxchg(&ev->value, EV_FREE, EV_BUSY) == EV_SET) {
+                /*
+		 * 这里多名ev->value又被设置成EV_SET了, 什么也不用做了
+		 */
                 return;
             }
         }
 
+	/*
+	 * FUTEX_WAIT: 如果futex word中仍然保存着参数val给定的值,
+	 * 那么当前线程则进入睡眠,等待FUTEX_WAKE的操作唤醒它.
+	 *
+	 * FUTEX_WAKE: 最多唤醒val个等待在futex word上的线程.
+	 * Val或者等于1(唤醒1个等待线程)或者等于INT_MAX(唤醒全部等待线程)
+	 */
         /*
          * This is the final check for a concurrent set, so it does need
          * a smp_mb() pairing with the second barrier of qemu_event_set().
-- 
2.34.1

