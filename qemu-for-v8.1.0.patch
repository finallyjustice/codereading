From 9166c2657f61627d06dedaff8ae13236fc19e385 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 25 Dec 2023 11:02:03 -0800
Subject: [PATCH 1/1] qemu for v8.1.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 accel/accel-blocker.c            |  92 ++++++++++++++++
 accel/kvm/kvm-all.c              |  15 +++
 block/block-backend.c            |  21 ++++
 block/monitor/block-hmp-cmds.c   |  90 +++++++++++++++
 block/qapi.c                     |  25 +++++
 cpu.c                            |   6 +
 cpus-common.c                    |  24 ++++
 hw/acpi/acpi_interface.c         |  14 +++
 hw/acpi/cpu.c                    |  36 ++++++
 hw/acpi/cpu_hotplug.c            |   9 ++
 hw/core/hotplug.c                |  10 ++
 hw/core/qdev-properties-system.c |   6 +
 hw/core/qdev-properties.c        |   4 +
 hw/i386/pc.c                     |   5 +
 hw/intc/arm_gicv3_its.c          |   7 ++
 hw/intc/arm_gicv3_its_common.c   |   3 +
 hw/intc/arm_gicv3_its_kvm.c      |  10 ++
 hw/net/igb.c                     |  11 ++
 hw/net/igbvf.c                   |   6 +
 hw/nvme/ctrl.c                   |  21 ++++
 hw/nvme/dif.c                    |   5 +
 hw/nvme/ns.c                     |  13 +++
 hw/nvme/nvme.h                   |  61 ++++++++++
 hw/pci/pci.c                     | 132 ++++++++++++++++++++++
 hw/pci/pcie_sriov.c              |  73 ++++++++++++
 hw/scsi/vhost-scsi.c             |  98 ++++++++++++++++
 hw/scsi/virtio-scsi.c            | 184 +++++++++++++++++++++++++++++++
 hw/vfio/common.c                 |   4 +
 hw/virtio/vhost-scsi-pci.c       |   9 ++
 hw/virtio/vhost.c                |  33 ++++++
 hw/virtio/virtio-pci.c           |   4 +
 hw/virtio/virtio.c               |  69 ++++++++++++
 include/hw/core/cpu.h            |  10 ++
 include/qemu/futex.h             |  18 +++
 migration/channel.c              |   7 ++
 migration/migration-stats.c      |  32 ++++++
 migration/migration.c            |  68 ++++++++++++
 migration/migration.h            |  18 +++
 migration/multifd.c              |  37 +++++++
 migration/options.c              |  18 +++
 migration/savevm.c               |   6 +
 migration/socket.c               |  12 ++
 monitor/fds.c                    |  15 +++
 softmmu/balloon.c                |   4 +
 softmmu/dma-helpers.c            |   9 ++
 softmmu/vl.c                     |  36 ++++++
 target/arm/kvm64.c               |   4 +
 target/i386/cpu.c                |  22 ++++
 target/i386/cpu.h                |  13 +++
 target/i386/kvm/kvm.c            |  57 ++++++++++
 util/aio-wait.c                  |  20 ++++
 util/iov.c                       |  25 +++++
 util/lockcnt.c                   |  52 +++++++++
 util/qemu-thread-posix.c         | 129 ++++++++++++++++++++++
 54 files changed, 1712 insertions(+)

diff --git a/accel/accel-blocker.c b/accel/accel-blocker.c
index 1e7f42346..24e422582 100644
--- a/accel/accel-blocker.c
+++ b/accel/accel-blocker.c
@@ -30,25 +30,67 @@
 #include "hw/core/cpu.h"
 #include "sysemu/accel-blocker.h"
 
+/*
+ * 在以下使用accel_in_ioctl_lock:
+ *   - accel/accel-blocker.c|38| <<accel_blocker_init>> qemu_lockcnt_init(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|49| <<accel_ioctl_begin>> qemu_lockcnt_inc(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|58| <<accel_ioctl_end>> qemu_lockcnt_dec(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|97| <<accel_has_to_wait>> return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|114| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&accel_in_ioctl_lock);
+ *   - accel/accel-blocker.c|149| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&accel_in_ioctl_lock);
+ */
 static QemuLockCnt accel_in_ioctl_lock;
+/*
+ * 在以下使用accel_in_ioctl_event:
+ *   - accel/accel-blocker.c|39| <<accel_blocker_init>> qemu_event_init(&accel_in_ioctl_event, false);
+ *   - accel/accel-blocker.c|60| <<accel_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|81| <<accel_cpu_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|120| <<accel_ioctl_inhibit_begin>> qemu_event_reset(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|137| <<accel_ioctl_inhibit_begin>> qemu_event_wait(&accel_in_ioctl_event);
+ */
 static QemuEvent accel_in_ioctl_event;
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2488| <<kvm_init>> accel_blocker_init();
+ */
 void accel_blocker_init(void)
 {
+    /*
+     * 设置成0
+     */
     qemu_lockcnt_init(&accel_in_ioctl_lock);
     qemu_event_init(&accel_in_ioctl_event, false);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3167| <<kvm_vm_ioctl>> accel_ioctl_begin();
+ *   - accel/kvm/kvm-all.c|3207| <<kvm_device_ioctl>> accel_ioctl_begin();
+ */
 void accel_ioctl_begin(void)
 {
     if (likely(qemu_mutex_iothread_locked())) {
         return;
     }
 
+    /*
+     * struct QemuLockCnt {
+     * #ifndef CONFIG_LINUX
+     *     QemuMutex mutex;
+     * #endif
+     *     unsigned count;
+     * };
+     */
     /* block if lock is taken in kvm_ioctl_inhibit_begin() */
     qemu_lockcnt_inc(&accel_in_ioctl_lock);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3169| <<kvm_vm_ioctl>> accel_ioctl_end();
+ *   - accel/kvm/kvm-all.c|3209| <<kvm_device_ioctl>> accel_ioctl_end();
+ */
 void accel_ioctl_end(void)
 {
     if (likely(qemu_mutex_iothread_locked())) {
@@ -60,6 +102,10 @@ void accel_ioctl_end(void)
     qemu_event_set(&accel_in_ioctl_event);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3187| <<kvm_vcpu_ioctl>> accel_cpu_ioctl_begin(cpu);
+ */
 void accel_cpu_ioctl_begin(CPUState *cpu)
 {
     if (unlikely(qemu_mutex_iothread_locked())) {
@@ -70,6 +116,10 @@ void accel_cpu_ioctl_begin(CPUState *cpu)
     qemu_lockcnt_inc(&cpu->in_ioctl_lock);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3189| <<kvm_vcpu_ioctl>> accel_cpu_ioctl_end(cpu);
+ */
 void accel_cpu_ioctl_end(CPUState *cpu)
 {
     if (unlikely(qemu_mutex_iothread_locked())) {
@@ -81,12 +131,33 @@ void accel_cpu_ioctl_end(CPUState *cpu)
     qemu_event_set(&accel_in_ioctl_event);
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|122| <<accel_ioctl_inhibit_begin>> if (accel_has_to_wait()) {
+ */
 static bool accel_has_to_wait(void)
 {
     CPUState *cpu;
     bool needs_to_wait = false;
 
     CPU_FOREACH(cpu) {
+        /*
+	 * qemu_lockcnt_count: query a LockCnt's count.
+	 * @lockcnt: the lockcnt to query.
+	 *
+	 * Note that the count can change at any time.  Still, while the
+	 * lockcnt is locked, one can usefully check whether the count
+	 * is non-zero.
+	 *
+	 * 在以下使用CPUState->in_ioctl_lock:
+	 *   - accel/accel-blocker.c|116| <<accel_cpu_ioctl_begin>> qemu_lockcnt_inc(&cpu->in_ioctl_lock);
+	 *   - accel/accel-blocker.c|129| <<accel_cpu_ioctl_end>> qemu_lockcnt_dec(&cpu->in_ioctl_lock);
+	 *   - accel/accel-blocker.c|144| <<accel_has_to_wait>> if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
+	 *   - accel/accel-blocker.c|170| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&cpu->in_ioctl_lock);
+	 *   - accel/accel-blocker.c|217| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&cpu->in_ioctl_lock);
+	 *   - hw/core/cpu-common.c|238| <<cpu_common_initfn>> qemu_lockcnt_init(&cpu->in_ioctl_lock);
+	 *   - hw/core/cpu-common.c|250| <<cpu_common_finalize>> qemu_lockcnt_destroy(&cpu->in_ioctl_lock);
+	 */
         if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
             /* exit the ioctl, if vcpu is running it */
             qemu_cpu_kick(cpu);
@@ -94,9 +165,22 @@ static bool accel_has_to_wait(void)
         }
     }
 
+    /*
+     * 在以下使用accel_in_ioctl_lock:
+     *   - accel/accel-blocker.c|38| <<accel_blocker_init>> qemu_lockcnt_init(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|49| <<accel_ioctl_begin>> qemu_lockcnt_inc(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|58| <<accel_ioctl_end>> qemu_lockcnt_dec(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|97| <<accel_has_to_wait>> return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|114| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&accel_in_ioctl_lock);
+     *   - accel/accel-blocker.c|149| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&accel_in_ioctl_lock);
+     */
     return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1597| <<kvm_region_commit>> accel_ioctl_inhibit_begin();
+ */
 void accel_ioctl_inhibit_begin(void)
 {
     CPUState *cpu;
@@ -119,6 +203,10 @@ void accel_ioctl_inhibit_begin(void)
         /* Reset event to FREE. */
         qemu_event_reset(&accel_in_ioctl_event);
 
+	/*
+	 * called by:
+	 *   - accel/accel-blocker.c|122| <<accel_ioctl_inhibit_begin>> if (accel_has_to_wait()) {
+	 */
         if (accel_has_to_wait()) {
             /*
              * If event is still FREE, and there are ioctls still in progress,
@@ -142,6 +230,10 @@ void accel_ioctl_inhibit_begin(void)
     }
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|1621| <<kvm_region_commit>> accel_ioctl_inhibit_end();
+ */
 void accel_ioctl_inhibit_end(void)
 {
     CPUState *cpu;
diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index 7b3da8dc3..31280b20b 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -2038,6 +2038,11 @@ static KVMMSIRoute *kvm_lookup_msi_route(KVMState *s, MSIMessage msg)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/i386/kvm/apic.c|192| <<kvm_send_msi>> ret = kvm_irqchip_send_msi(kvm_state, *msg);
+ *   - target/i386/kvm/xen-emu.c|447| <<kvm_xen_inject_vcpu_callback_vector>> kvm_irqchip_send_msi(kvm_state, msg);
+ */
 int kvm_irqchip_send_msi(KVMState *s, MSIMessage msg)
 {
     struct kvm_msi msi;
@@ -2050,6 +2055,12 @@ int kvm_irqchip_send_msi(KVMState *s, MSIMessage msg)
         msi.flags = 0;
         memset(msi.pad, 0, sizeof(msi.pad));
 
+	/*
+	 * 在以下使用KVM_SIGNAL_MSI:
+	 *   - linux-headers/linux/kvm.h|1523| <<global>> #define KVM_SIGNAL_MSI _IOW(KVMIO, 0xa5, struct kvm_msi)
+	 *   - accel/kvm/kvm-all.c|2053| <<kvm_irqchip_send_msi>> return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
+	 *   - hw/intc/arm_gicv3_its_kvm.c|65| <<kvm_its_send_msi>> return kvm_vm_ioctl(kvm_state, KVM_SIGNAL_MSI, &msi);
+	 */
         return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
     }
 
@@ -2860,6 +2871,10 @@ void kvm_cpu_synchronize_state(CPUState *cpu)
     }
 }
 
+/*
+ * 在以下使用do_kvm_cpu_synchronize_post_reset():
+ *   - accel/kvm/kvm-all.c|2882| <<kvm_cpu_synchronize_post_reset>> run_on_cpu(cpu, do_kvm_cpu_synchronize_post_reset, RUN_ON_CPU_NULL);
+ */
 static void do_kvm_cpu_synchronize_post_reset(CPUState *cpu, run_on_cpu_data arg)
 {
     kvm_arch_put_registers(cpu, KVM_PUT_RESET_STATE);
diff --git a/block/block-backend.c b/block/block-backend.c
index 4009ed5fe..1e153a46b 100644
--- a/block/block-backend.c
+++ b/block/block-backend.c
@@ -110,6 +110,14 @@ static const AIOCBInfo block_backend_aiocb_info = {
 static void drive_info_del(DriveInfo *dinfo);
 static BlockBackend *bdrv_first_blk(BlockDriverState *bs);
 
+/*
+ * 在以下block_backends:
+ *   - block/block-backend.c|114| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) block_backends = QTAILQ_HEAD_INITIALIZER(block_backends);
+ *   - block/block-backend.c|115| <<QTAILQ_HEAD>> QTAILQ_HEAD_INITIALIZER(block_backends);
+ *   - block/block-backend.c|378| <<blk_new>> QTAILQ_INSERT_TAIL(&block_backends, blk, link);
+ *   - block/block-backend.c|507| <<blk_delete>> QTAILQ_REMOVE(&block_backends, blk, link);
+ *   - block/block-backend.c|569| <<blk_all_next>> : QTAILQ_FIRST(&block_backends);
+ */
 /* All BlockBackends. Protected by BQL. */
 static QTAILQ_HEAD(, BlockBackend) block_backends =
     QTAILQ_HEAD_INITIALIZER(block_backends);
@@ -565,6 +573,14 @@ void blk_unref(BlockBackend *blk)
 BlockBackend *blk_all_next(BlockBackend *blk)
 {
     GLOBAL_STATE_CODE();
+    /*
+     * 在以下block_backends:
+     *   - block/block-backend.c|114| <<QTAILQ_HEAD>> static QTAILQ_HEAD(, BlockBackend) block_backends = QTAILQ_HEAD_INITIALIZER(block_backends);
+     *   - block/block-backend.c|115| <<QTAILQ_HEAD>> QTAILQ_HEAD_INITIALIZER(block_backends);
+     *   - block/block-backend.c|378| <<blk_new>> QTAILQ_INSERT_TAIL(&block_backends, blk, link);
+     *   - block/block-backend.c|507| <<blk_delete>> QTAILQ_REMOVE(&block_backends, blk, link);
+     *   - block/block-backend.c|569| <<blk_all_next>> : QTAILQ_FIRST(&block_backends);
+     */
     return blk ? QTAILQ_NEXT(blk, link)
                : QTAILQ_FIRST(&block_backends);
 }
@@ -776,6 +792,11 @@ BlockBackend *blk_by_name(const char *name)
 BlockDriverState *blk_bs(BlockBackend *blk)
 {
     IO_CODE();
+    /*
+     * BlockBackend *blk:
+     * -> BdrvChild *root;
+     *    -> BlockDriverState *bs;
+     */
     return blk->root ? blk->root->bs : NULL;
 }
 
diff --git a/block/monitor/block-hmp-cmds.c b/block/monitor/block-hmp-cmds.c
index ca2599de4..513c98b71 100644
--- a/block/monitor/block-hmp-cmds.c
+++ b/block/monitor/block-hmp-cmds.c
@@ -624,6 +624,23 @@ fail:
     hmp_handle_error(mon, err);
 }
 
+/*
+ * (qemu) info block
+ * ide0-hd0 (#block199): ol7.qcow2 (qcow2)
+ *     Attached to:      /machine/unattached/device[21]
+ *     Cache mode:       writeback
+ *
+ * sd0: [not inserted]
+ *     Removable device: not locked, tray closed
+ *
+ * drive01: test01.qcow2 (qcow2)
+ *     Attached to:      /machine/peripheral-anon/device[0]
+ *     Cache mode:       writeback, direct
+ *
+ * drive02: test02.qcow2 (qcow2)
+ *     Attached to:      /machine/peripheral-anon/device[1]
+ *     Cache mode:       writeback, direct
+ */
 static void print_block_info(Monitor *mon, BlockInfo *info,
                              BlockDeviceInfo *inserted, bool verbose)
 {
@@ -738,6 +755,36 @@ static void print_block_info(Monitor *mon, BlockInfo *info,
     }
 }
 
+/*
+ * # @BlockInfo:
+ * #
+ * # Block device information.  This structure describes a virtual device
+ * # and the backing device associated with it.
+ * #
+ * # @device: The device name associated with the virtual device.
+ * #
+ * # @qdev: The qdev ID, or if no ID is assigned, the QOM path of the
+ * #     block device.  (since 2.10)
+ * #
+ * # @type: This field is returned only for compatibility reasons, it
+ * #     should not be used (always returns 'unknown')
+ * #
+ * # @removable: True if the device supports removable media.
+ * #
+ * # @locked: True if the guest has locked this device from having its
+ * #     media removed
+ * #
+ * # @tray_open: True if the device's tray is open (only present if it
+ * #     has a tray)
+ * #
+ * # @io-status: @BlockDeviceIoStatus.  Only present if the device
+ * #     supports it and the VM is configured to stop on errors
+ * #     (supported device models: virtio-blk, IDE, SCSI except
+ * #     scsi-generic)
+ * #
+ * # @inserted: @BlockDeviceInfo describing the device if media is
+ * #     present
+ */
 void hmp_info_block(Monitor *mon, const QDict *qdict)
 {
     BlockInfoList *block_list, *info;
@@ -754,6 +801,9 @@ void hmp_info_block(Monitor *mon, const QDict *qdict)
         block_list = NULL;
     }
 
+    /*
+     * BlockInfoList *block_list, *info;
+     */
     for (info = block_list; info; info = info->next) {
         if (device && strcmp(device, info->value->device)) {
             continue;
@@ -791,6 +841,46 @@ void hmp_info_block(Monitor *mon, const QDict *qdict)
     qapi_free_BlockDeviceInfoList(blockdev_list);
 }
 
+/*
+ * # @BlockStats:
+ * #
+ * # Statistics of a virtual block device or a block backing device.
+ * #
+ * # @device: If the stats are for a virtual block device, the name
+ * #     corresponding to the virtual block device.
+ * #
+ * # @node-name: The node name of the device.  (Since 2.3)
+ * #
+ * # @qdev: The qdev ID, or if no ID is assigned, the QOM path of the
+ * #     block device.  (since 3.0)
+ * #
+ * # @stats: A @BlockDeviceStats for the device.
+ * #
+ * # @driver-specific: Optional driver-specific stats.  (Since 4.2)
+ * #
+ * # @parent: This describes the file block device if it has one.
+ * #     Contains recursively the statistics of the underlying protocol
+ * #     (e.g. the host file for a qcow2 image).  If there is no
+ * #     underlying protocol, this field is omitted
+ * #
+ * # @backing: This describes the backing block device if it has one.
+ * #     (Since 2.0)
+ *
+ *
+ * # @query-blockstats:
+ * #
+ * # Query the @BlockStats for all virtual block devices.
+ * #
+ * # @query-nodes: If true, the command will query all the block nodes
+ * #     that have a node name, in a list which will include "parent"
+ * #     information, but not "backing". If false or omitted, the
+ * #     behavior is as before - query all the device backends,
+ * #     recursively including their "parent" and "backing". Filter nodes
+ * #     that were created implicitly are skipped over in this mode.
+ * #     (Since 2.3)
+ * #
+ * # Returns: A list of @BlockStats for each virtual block devices.
+ */
 void hmp_info_blockstats(Monitor *mon, const QDict *qdict)
 {
     BlockStatsList *stats_list, *stats;
diff --git a/block/qapi.c b/block/qapi.c
index f34f95e0e..c18ef612c 100644
--- a/block/qapi.c
+++ b/block/qapi.c
@@ -723,6 +723,31 @@ BlockInfoList *qmp_query_block(Error **errp)
     return head;
 }
 
+/*
+ * # @BlockStats:
+ * #
+ * # Statistics of a virtual block device or a block backing device.
+ * #
+ * # @device: If the stats are for a virtual block device, the name
+ * #     corresponding to the virtual block device.
+ * #
+ * # @node-name: The node name of the device.  (Since 2.3)
+ * #
+ * # @qdev: The qdev ID, or if no ID is assigned, the QOM path of the
+ * #     block device.  (since 3.0)
+ * #
+ * # @stats: A @BlockDeviceStats for the device.
+ * #
+ * # @driver-specific: Optional driver-specific stats.  (Since 4.2)
+ * #
+ * # @parent: This describes the file block device if it has one.
+ * #     Contains recursively the statistics of the underlying protocol
+ * #     (e.g. the host file for a qcow2 image).  If there is no
+ * #     underlying protocol, this field is omitted
+ * #
+ * # @backing: This describes the backing block device if it has one.
+ * #     (Since 2.0)
+ */
 BlockStatsList *qmp_query_blockstats(bool has_query_nodes,
                                      bool query_nodes,
                                      Error **errp)
diff --git a/cpu.c b/cpu.c
index 1c948d116..334488b8e 100644
--- a/cpu.c
+++ b/cpu.c
@@ -257,6 +257,12 @@ void cpu_exec_initfn(CPUState *cpu)
 #endif
 }
 
+/*
+ * called by:
+ *   - bsd-user/main.c|452| <<main>> cpu_type = parse_cpu_option(cpu_model);
+ *   - linux-user/main.c|784| <<main>> cpu_type = parse_cpu_option(cpu_model);
+ *   - softmmu/vl.c|3633| <<qemu_init>> current_machine->cpu_type = parse_cpu_option(cpu_option);
+ */
 const char *parse_cpu_option(const char *cpu_option)
 {
     ObjectClass *oc;
diff --git a/cpus-common.c b/cpus-common.c
index 45c745ecf..b22ae64e4 100644
--- a/cpus-common.c
+++ b/cpus-common.c
@@ -81,6 +81,30 @@ unsigned int cpu_list_generation_id_get(void)
     return cpu_list_generation_id;
 }
 
+/*
+ * (gdb) bt
+ * #0  0x0000555555863ee0 in cpu_list_add (cpu=0x555556f6f8e0) at /home/opc/ext4/qemu/include/qemu/thread.h:122
+ * #1  0x0000555555beba88 in cpu_exec_realizefn (cpu=cpu@entry=0x555556f6f8e0, errp=errp@entry=0x7fffffffd720) at ../cpu-target.c:143
+ * #2  0x0000555555b4376f in x86_cpu_realizefn (dev=0x555556f6f8e0, errp=0x7fffffffd780) at ../target/i386/cpu.c:7308
+ * #3  0x0000555555c5ac2e in device_set_realized (obj=<optimized out>, value=<optimized out>, errp=0x7fffffffd800) at ../hw/core/qdev.c:510
+ * #4  0x0000555555c5e976 in property_set_bool (obj=0x555556f6f8e0, v=<optimized out>, name=<optimized out>, opaque=0x555556cf40c0, errp=0x7fffffffd800) at ../qom/object.c:2305
+ * #5  0x0000555555c61ae4 in object_property_set (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", v=v@entry=0x555556f78020, errp=0x7fffffffd800, errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/object.c:1435
+ * #6  0x0000555555c64f60 in object_property_set_qobject (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=0x555556f505a0, errp=errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/qom-qobject.c:28
+ * #7  0x0000555555c62105 in object_property_set_bool (obj=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=true, errp=errp@entry=0x555556c66b58 <error_fatal>)
+ *     at ../qom/object.c:1504
+ * #8  0x0000555555c5b5ae in qdev_realize (dev=<optimized out>, bus=bus@entry=0x0, errp=errp@entry=0x555556c66b58 <error_fatal>) at ../hw/core/qdev.c:292
+ * #9  0x0000555555b07ce1 in x86_cpu_new (x86ms=<optimized out>, apic_id=0, errp=0x555556c66b58 <error_fatal>) at /home/opc/ext4/qemu/include/hw/qdev-core.h:77
+ * #10 0x0000555555b07dc6 in x86_cpus_init (x86ms=x86ms@entry=0x555556f33c00, default_cpu_version=<optimized out>) at ../hw/i386/x86.c:148
+ * #11 0x0000555555b0e7db in pc_init1 (machine=0x555556f33c00, pci_type=0x555555f0cbf0 "i440FX", host_type=0x555555f0cc0f "i440FX-pcihost") at ../hw/i386/pc_piix.c:191
+ * #12 0x00005555558e8281 in machine_run_board_init (machine=0x555556f33c00, mem_path=<optimized out>, errp=<optimized out>) at ../hw/core/machine.c:1508
+ * #13 0x0000555555a80d85 in qmp_x_exit_preconfig () at ../system/vl.c:2610
+ * #14 0x0000555555a80d85 in qmp_x_exit_preconfig (errp=<optimized out>) at ../system/vl.c:2701
+ * #15 0x0000555555a8498b in qemu_init (errp=<optimized out>) at ../system/vl.c:3750
+ * #16 0x0000555555a8498b in qemu_init (argc=<optimized out>, argv=<optimized out>) at ../system/vl.c:3750
+ * #17 0x000055555585d559 in main (argc=<optimized out>, argv=<optimized out>) at ../system/main.c:47
+ */
 void cpu_list_add(CPUState *cpu)
 {
     QEMU_LOCK_GUARD(&qemu_cpu_list_lock);
diff --git a/hw/acpi/acpi_interface.c b/hw/acpi/acpi_interface.c
index 8637ff18f..aad08b2ab 100644
--- a/hw/acpi/acpi_interface.c
+++ b/hw/acpi/acpi_interface.c
@@ -4,6 +4,20 @@
 #include "qemu/module.h"
 #include "qemu/queue.h"
 
+/*
+ * called by:
+ *   - hw/acpi/cpu.c|263| <<acpi_cpu_plug_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_CPU_HOTPLUG_STATUS);
+ *   - hw/acpi/cpu.c|279| <<acpi_cpu_unplug_request_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_CPU_HOTPLUG_STATUS);
+ *   - hw/acpi/cpu_hotplug.c|81| <<legacy_acpi_cpu_plug_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_CPU_HOTPLUG_STATUS);
+ *   - hw/acpi/memory_hotplug.c|282| <<acpi_memory_plug_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_MEMORY_HOTPLUG_STATUS);
+ *   - hw/acpi/memory_hotplug.c|298| <<acpi_memory_unplug_request_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_MEMORY_HOTPLUG_STATUS);
+ *   - hw/acpi/nvdimm.c|886| <<nvdimm_acpi_plug_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_NVDIMM_HOTPLUG_STATUS);
+ *   - hw/acpi/pcihp.c|324| <<acpi_pcihp_device_plug_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_PCI_HOTPLUG_STATUS);
+ *   - hw/acpi/pcihp.c|371| <<acpi_pcihp_device_unplug_request_cb>> acpi_send_event(DEVICE(hotplug_dev), ACPI_PCI_HOTPLUG_STATUS);
+ *   - hw/acpi/vmgenid.c|161| <<vmgenid_update_guest>> acpi_send_event(DEVICE(obj), ACPI_VMGENID_CHANGE_STATUS);
+ *   - hw/arm/virt.c|931| <<virt_powerdown_req>> acpi_send_event(s->acpi_dev, ACPI_POWER_DOWN_STATUS);
+ *   - hw/loongarch/virt.c|347| <<virt_powerdown_req>> acpi_send_event(s->acpi_ged, ACPI_POWER_DOWN_STATUS);
+ */
 void acpi_send_event(DeviceState *dev, AcpiEventStatusBits event)
 {
     AcpiDeviceIfClass *adevc = ACPI_DEVICE_IF_GET_CLASS(dev);
diff --git a/hw/acpi/cpu.c b/hw/acpi/cpu.c
index 19c154d78..20951d396 100644
--- a/hw/acpi/cpu.c
+++ b/hw/acpi/cpu.c
@@ -247,6 +247,42 @@ static AcpiCpuStatus *get_cpu_status(CPUHotplugState *cpu_st, DeviceState *dev)
     return NULL;
 }
 
+/*
+ * (gdb) bt
+ * #0  acpi_cpu_plug_cb (hotplug_dev=0x55555726e350, cpu_st=0x55555726fb90, dev=0x555556d8c520, errp=0x7fffffffc438) at ../hw/acpi/cpu.c:255
+ * #1  0x00005555558ca4d7 in ich9_pm_device_plug_cb (hotplug_dev=0x55555726e350, dev=0x555556d8c520, errp=0x7fffffffc438) at ../hw/acpi/ich9.c:514
+ * #2  0x0000555555db4fa9 in hotplug_handler_plug (plug_handler=0x55555726e350, plugged_dev=0x555556d8c520, errp=0x7fffffffc438) at ../hw/core/hotplug.c:34
+ * #3  0x0000555555be65c7 in x86_cpu_plug (hotplug_dev=0x555556c45bf0, dev=0x555556d8c520, errp=0x7fffffffc530) at ../hw/i386/x86.c:207
+ * #4  0x0000555555c106a0 in pc_machine_device_plug_cb (hotplug_dev=0x555556c45bf0, dev=0x555556d8c520, errp=0x7fffffffc530) at ../hw/i386/pc.c:1533
+ * #5  0x0000555555db4fa9 in hotplug_handler_plug (plug_handler=0x555556c45bf0, plugged_dev=0x555556d8c520, errp=0x7fffffffc530) at ../hw/core/hotplug.c:34
+ * #6  0x0000555555db0d98 in device_set_realized (obj=0x555556d8c520, value=true, errp=0x7fffffffc870) at ../hw/core/qdev.c:567
+ * #7  0x0000555555dba87e in property_set_bool (obj=0x555556d8c520, v=0x5555572fdab0, name=0x5555561c8379 "realized", opaque=0x555556a108b0, errp=0x7fffffffc870) at ../qom/object.c:2285
+ * #8  0x0000555555db88c5 in object_property_set (obj=0x555556d8c520, name=0x5555561c8379 "realized", v=0x5555572fdab0, errp=0x7fffffffc870) at ../qom/object.c:1420
+ * #9  0x0000555555dbcc70 in object_property_set_qobject (obj=0x555556d8c520, name=0x5555561c8379 "realized", value=0x5555572fd960, errp=0x7fffffffc870) at ../qom/qom-qobject.c:28
+ * #10 0x0000555555db8c2a in object_property_set_bool (obj=0x555556d8c520, name=0x5555561c8379 "realized", value=true, errp=0x7fffffffc870) at ../qom/object.c:1489
+ * #11 0x0000555555db0395 in qdev_realize (dev=0x555556d8c520, bus=0x0, errp=0x7fffffffc870) at ../hw/core/qdev.c:292
+ * #12 0x0000555555b2d627 in qdev_device_add_from_qdict (opts=0x555556cae200, from_json=false, errp=0x7fffffffc870) at ../softmmu/qdev-monitor.c:714
+ * #13 0x0000555555b2d6c5 in qdev_device_add (opts=0x55555766e360, errp=0x7fffffffc870) at ../softmmu/qdev-monitor.c:733
+ * #14 0x0000555555b2dcd9 in qmp_device_add (qdict=0x555556ca9600, ret_data=0x0, errp=0x7fffffffc870) at ../softmmu/qdev-monitor.c:855
+ * #15 0x0000555555b2e0e9 in hmp_device_add (mon=0x555556c631d0, qdict=0x555556ca9600) at ../softmmu/qdev-monitor.c:963
+ * #16 0x0000555555b84d21 in handle_hmp_command_exec (mon=0x555556c631d0, cmd=0x55555689eda0 <hmp_cmds+1920>, qdict=0x555556ca9600) at ../monitor/hmp.c:1106
+ * #17 0x0000555555b84f4e in handle_hmp_command (mon=0x555556c631d0, cmdline=0x555556c962fb "host-x86_64-cpu,id=core6,socket-id=0,core-id=6,thread-id=0") at ../monitor/hmp.c:1158
+ * #18 0x0000555555b8243a in monitor_command_cb (opaque=0x555556c631d0, cmdline=0x555556c962f0 "device_add host-x86_64-cpu,id=core6,socket-id=0,core-id=6,thread-id=0", readline_opaque=0x0)
+ *                                              at ../monitor/hmp.c:47
+ * #19 0x0000555555fcd18c in readline_handle_byte (rs=0x555556c962f0, ch=13) at ../util/readline.c:419
+ * #20 0x0000555555b859f9 in monitor_read (opaque=0x555556c631d0, buf=0x7fffffffcad0 "\r\320\377\377\377\177", size=1) at ../monitor/hmp.c:1390
+ * #21 0x0000555555ed4e59 in qemu_chr_be_write_impl (s=0x555556c59fe0, buf=0x7fffffffcad0 "\r\320\377\377\377\177", len=1) at ../chardev/char.c:202
+ * #22 0x0000555555ed4ebd in qemu_chr_be_write (s=0x555556c59fe0, buf=0x7fffffffcad0 "\r\320\377\377\377\177", len=1) at ../chardev/char.c:214
+ * #23 0x0000555555ed786a in fd_chr_read (chan=0x555556c5a0a0, cond=G_IO_IN, opaque=0x555556c59fe0) at ../chardev/char-fd.c:72
+ * #24 0x0000555555dc9a0c in qio_channel_fd_source_dispatch (source=0x5555573140d0, callback=0x555555ed7740 <fd_chr_read>, user_data=0x555556c59fe0) at ../io/channel-watch.c:84
+ * #25 0x00007ffff69ef119 in g_main_context_dispatch () at /lib64/libglib-2.0.so.0
+ * #26 0x0000555555fb664b in glib_pollfds_poll () at ../util/main-loop.c:290
+ * #27 0x0000555555fb66c5 in os_host_main_loop_wait (timeout=499000000) at ../util/main-loop.c:313
+ * #28 0x0000555555fb67ca in main_loop_wait (nonblocking=0) at ../util/main-loop.c:592
+ * #29 0x0000555555b33a12 in qemu_main_loop () at ../softmmu/runstate.c:732
+ * #30 0x0000555555dac6a1 in qemu_default_main () at ../softmmu/main.c:37
+ * #31 0x0000555555dac6d7 in main (argc=20, argv=0x7fffffffdd98) at ../softmmu/main.c:48
+ */
 void acpi_cpu_plug_cb(HotplugHandler *hotplug_dev,
                       CPUHotplugState *cpu_st, DeviceState *dev, Error **errp)
 {
diff --git a/hw/acpi/cpu_hotplug.c b/hw/acpi/cpu_hotplug.c
index ff14c3f41..a3bf9e2cf 100644
--- a/hw/acpi/cpu_hotplug.c
+++ b/hw/acpi/cpu_hotplug.c
@@ -59,12 +59,21 @@ static const MemoryRegionOps AcpiCpuHotplug_ops = {
     },
 };
 
+/*
+ * called by:
+ *   - hw/acpi/cpu_hotplug.c|80| <<legacy_acpi_cpu_plug_cb>> acpi_set_cpu_present_bit(g, CPU(dev));
+ *   - hw/acpi/cpu_hotplug.c|95| <<legacy_acpi_cpu_hotplug_init>> acpi_set_cpu_present_bit(gpe_cpu, cpu);
+ */
 static void acpi_set_cpu_present_bit(AcpiCpuHotplug *g, CPUState *cpu)
 {
     CPUClass *k = CPU_GET_CLASS(cpu);
     int64_t cpu_id;
 
     cpu_id = k->get_arch_id(cpu);
+    /*
+     * // 256 CPU IDs, 8 bits per entry:
+     * #define ACPI_GPE_PROC_LEN 32
+     */
     if ((cpu_id / 8) >= ACPI_GPE_PROC_LEN) {
         object_property_set_bool(g->device, "cpu-hotplug-legacy", false,
                                  &error_abort);
diff --git a/hw/core/hotplug.c b/hw/core/hotplug.c
index 17ac98668..523db3dcb 100644
--- a/hw/core/hotplug.c
+++ b/hw/core/hotplug.c
@@ -24,6 +24,16 @@ void hotplug_handler_pre_plug(HotplugHandler *plug_handler,
     }
 }
 
+/*
+ * called by:
+ *   - hw/arm/virt.c|2731| <<virt_memory_plug>> hotplug_handler_plug(HOTPLUG_HANDLER(vms->acpi_dev),
+ *   - hw/core/qdev.c|567| <<device_set_realized>> hotplug_handler_plug(hotplug_ctrl, dev, &local_err);
+ *   - hw/i386/pc.c|1453| <<pc_memory_plug>> hotplug_handler_plug(x86ms->acpi_dev, dev, &error_abort);
+ *   - hw/i386/x86.c|207| <<x86_cpu_plug>> hotplug_handler_plug(x86ms->acpi_dev, dev, &local_err);
+ *   - hw/loongarch/virt.c|1039| <<virt_mem_plug>> hotplug_handler_plug(HOTPLUG_HANDLER(lams->acpi_ged),
+ *   - hw/net/virtio-net.c|3474| <<failover_replug_primary>> hotplug_handler_plug(hotplug_ctrl, dev, &err);
+ *   - hw/virtio/virtio-md-pci.c|61| <<virtio_md_pci_plug>> hotplug_handler_plug(bus_handler, dev, &local_err);
+ */
 void hotplug_handler_plug(HotplugHandler *plug_handler,
                           DeviceState *plugged_dev,
                           Error **errp)
diff --git a/hw/core/qdev-properties-system.c b/hw/core/qdev-properties-system.c
index 6d5d43eda..f75fb78f8 100644
--- a/hw/core/qdev-properties-system.c
+++ b/hw/core/qdev-properties-system.c
@@ -36,6 +36,12 @@
 #include "hw/i386/x86.h"
 #include "util/block-helpers.h"
 
+/*
+ * called by:
+ *   - hw/core/qdev-properties-system.c|105| <<set_drive_helper>> if (!check_prop_still_unset(obj, name, *ptr, str, true, errp)) {
+ *   - hw/core/qdev-properties-system.c|266| <<set_chr>> if (!check_prop_still_unset(obj, name, be->chr, str, false, errp)) {
+ *   - hw/core/qdev-properties-system.c|436| <<set_netdev>> if (!check_prop_still_unset(obj, name, ncs[i], str, false, errp)) {
+ */
 static bool check_prop_still_unset(Object *obj, const char *name,
                                    const void *old_val, const char *new_val,
                                    bool allow_override, Error **errp)
diff --git a/hw/core/qdev-properties.c b/hw/core/qdev-properties.c
index 357b8761b..bfca1da04 100644
--- a/hw/core/qdev-properties.c
+++ b/hw/core/qdev-properties.c
@@ -755,6 +755,10 @@ void qdev_prop_register_global(GlobalProperty *prop)
     g_ptr_array_add(global_props(), prop);
 }
 
+/*
+ * called by:
+ *   - hw/core/qdev-properties-system.c|43| <<check_prop_still_unset>> const GlobalProperty *prop = qdev_find_global_prop(obj, name);
+ */
 const GlobalProperty *qdev_find_global_prop(Object *obj,
                                             const char *name)
 {
diff --git a/hw/i386/pc.c b/hw/i386/pc.c
index 3109d5e0e..912b452a7 100644
--- a/hw/i386/pc.c
+++ b/hw/i386/pc.c
@@ -947,6 +947,11 @@ static hwaddr pc_max_used_gpa(PCMachineState *pcms, uint64_t pci_hole64_size)
 #define AMD_ABOVE_1TB_START  (AMD_HT_END + 1)
 #define AMD_HT_SIZE          (AMD_ABOVE_1TB_START - AMD_HT_START)
 
+/*
+ * called by:
+ *   - hw/i386/pc_piix.c|248| <<pc_init1>> pc_memory_init(pcms, system_memory, rom_memory, hole64_size);
+ *   - hw/i386/pc_q35.c|218| <<pc_q35_init>> pc_memory_init(pcms, system_memory, rom_memory, pci_hole64_size);
+ */
 void pc_memory_init(PCMachineState *pcms,
                     MemoryRegion *system_memory,
                     MemoryRegion *rom_memory,
diff --git a/hw/intc/arm_gicv3_its.c b/hw/intc/arm_gicv3_its.c
index 43dfd7a35..57118e5e8 100644
--- a/hw/intc/arm_gicv3_its.c
+++ b/hw/intc/arm_gicv3_its.c
@@ -360,6 +360,13 @@ out:
  * The string @who is purely for the LOG_GUEST_ERROR messages,
  * and should indicate the name of the calling function or similar.
  */
+/*
+ * called by:
+ *   - hw/intc/arm_gicv3_its.c|525| <<do_process_its_cmd>> cmdres = lookup_ite(s, __func__, devid, eventid, &ite, &dte);
+ *   - hw/intc/arm_gicv3_its.c|903| <<process_movi>> cmdres = lookup_ite(s, __func__, devid, eventid, &old_ite, &dte);
+ *   - hw/intc/arm_gicv3_its.c|1114| <<process_vmovi>> cmdres = lookup_ite(s, __func__, devid, eventid, &ite, &dte);
+ *   - hw/intc/arm_gicv3_its.c|1203| <<process_inv>> cmdres = lookup_ite(s, __func__, devid, eventid, &ite, &dte);
+ */
 static ItsCmdResult lookup_ite(GICv3ITSState *s, const char *who,
                                uint32_t devid, uint32_t eventid, ITEntry *ite,
                                DTEntry *dte)
diff --git a/hw/intc/arm_gicv3_its_common.c b/hw/intc/arm_gicv3_its_common.c
index abaf77057..53869f748 100644
--- a/hw/intc/arm_gicv3_its_common.c
+++ b/hw/intc/arm_gicv3_its_common.c
@@ -74,6 +74,9 @@ static MemTxResult gicv3_its_trans_read(void *opaque, hwaddr offset,
     return MEMTX_OK;
 }
 
+/*
+ * MemoryRegionOps gicv3_its_trans_ops.write_with_attrs = gicv3_its_trans_write()
+ */
 static MemTxResult gicv3_its_trans_write(void *opaque, hwaddr offset,
                                          uint64_t value, unsigned size,
                                          MemTxAttrs attrs)
diff --git a/hw/intc/arm_gicv3_its_kvm.c b/hw/intc/arm_gicv3_its_kvm.c
index 7eda9fb86..da275b839 100644
--- a/hw/intc/arm_gicv3_its_kvm.c
+++ b/hw/intc/arm_gicv3_its_kvm.c
@@ -41,6 +41,10 @@ struct KVMARMITSClass {
 };
 
 
+/*
+ * 在以下使用kvm_its_send_msi():
+ *   - hw/intc/arm_gicv3_its_kvm.c|254| <<kvm_arm_its_class_init>> icc->send_msi = kvm_its_send_msi;
+ */
 static int kvm_its_send_msi(GICv3ITSState *s, uint32_t value, uint16_t devid)
 {
     struct kvm_msi msi;
@@ -62,6 +66,12 @@ static int kvm_its_send_msi(GICv3ITSState *s, uint32_t value, uint16_t devid)
     msi.devid = devid;
     memset(msi.pad, 0, sizeof(msi.pad));
 
+    /*
+     * 在以下使用KVM_SIGNAL_MSI:
+     *   - linux-headers/linux/kvm.h|1523| <<global>> #define KVM_SIGNAL_MSI _IOW(KVMIO, 0xa5, struct kvm_msi)
+     *   - accel/kvm/kvm-all.c|2053| <<kvm_irqchip_send_msi>> return kvm_vm_ioctl(s, KVM_SIGNAL_MSI, &msi);
+     *   - hw/intc/arm_gicv3_its_kvm.c|65| <<kvm_its_send_msi>> return kvm_vm_ioctl(kvm_state, KVM_SIGNAL_MSI, &msi);
+     */
     return kvm_vm_ioctl(kvm_state, KVM_SIGNAL_MSI, &msi);
 }
 
diff --git a/hw/net/igb.c b/hw/net/igb.c
index 8ff832acf..1659cfbf8 100644
--- a/hw/net/igb.c
+++ b/hw/net/igb.c
@@ -433,10 +433,21 @@ static void igb_pci_realize(PCIDevice *pci_dev, Error **errp)
 
     pcie_ari_init(pci_dev, 0x150);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|436| <<igb_pci_realize>> pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
+     *   - hw/nvme/ctrl.c|8050| <<nvme_init_sriov>> pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
+     */
     pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
         IGB_82576_VF_DEV_ID, IGB_MAX_VF_FUNCTIONS, IGB_MAX_VF_FUNCTIONS,
         IGB_VF_OFFSET, IGB_VF_STRIDE);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|440| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
+     *   - hw/net/igb.c|443| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MSIX_BAR_IDX,
+     *   - hw/nvme/ctrl.c|8054| <<nvme_init_sriov>> pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+     */
     pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
         PCI_BASE_ADDRESS_MEM_TYPE_64 | PCI_BASE_ADDRESS_MEM_PREFETCH,
         IGBVF_MMIO_SIZE);
diff --git a/hw/net/igbvf.c b/hw/net/igbvf.c
index d55e1e8a6..4f9324e19 100644
--- a/hw/net/igbvf.c
+++ b/hw/net/igbvf.c
@@ -247,6 +247,12 @@ static void igbvf_pci_realize(PCIDevice *dev, Error **errp)
 
     memory_region_init_io(&s->mmio, OBJECT(dev), &mmio_ops, s, "igbvf-mmio",
         IGBVF_MMIO_SIZE);
+    /*
+     * called by:
+     *   - hw/net/igbvf.c|250| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
+     *   - hw/net/igbvf.c|253| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MSIX_BAR_IDX, &s->msix);
+     *   - hw/nvme/ctrl.c|8117| <<nvme_init_pci>> pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
+     */
     pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
 
     memory_region_init(&s->msix, OBJECT(dev), "igbvf-msix", IGBVF_MSIX_SIZE);
diff --git a/hw/nvme/ctrl.c b/hw/nvme/ctrl.c
index 539d27355..79258e5a8 100644
--- a/hw/nvme/ctrl.c
+++ b/hw/nvme/ctrl.c
@@ -8038,6 +8038,10 @@ static uint64_t nvme_bar_size(unsigned total_queues, unsigned total_irqs,
     return bar_size;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|8145| <<nvme_init_pci>> nvme_init_sriov(n, pci_dev, 0x120);
+ */
 static void nvme_init_sriov(NvmeCtrl *n, PCIDevice *pci_dev, uint16_t offset)
 {
     uint16_t vf_dev_id = n->params.use_intel_id ?
@@ -8047,10 +8051,21 @@ static void nvme_init_sriov(NvmeCtrl *n, PCIDevice *pci_dev, uint16_t offset)
                                       le16_to_cpu(cap->vifrsm),
                                       NULL, NULL);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|436| <<igb_pci_realize>> pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
+     *   - hw/nvme/ctrl.c|8050| <<nvme_init_sriov>> pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
+     */
     pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
                        n->params.sriov_max_vfs, n->params.sriov_max_vfs,
                        NVME_VF_OFFSET, NVME_VF_STRIDE);
 
+    /*
+     * called by:
+     *   - hw/net/igb.c|440| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
+     *   - hw/net/igb.c|443| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MSIX_BAR_IDX,
+     *   - hw/nvme/ctrl.c|8054| <<nvme_init_sriov>> pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+     */
     pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
                               PCI_BASE_ADDRESS_MEM_TYPE_64, bar_size);
 }
@@ -8114,6 +8129,12 @@ static bool nvme_init_pci(NvmeCtrl *n, PCIDevice *pci_dev, Error **errp)
     memory_region_add_subregion(&n->bar0, 0, &n->iomem);
 
     if (pci_is_vf(pci_dev)) {
+        /*
+	 * called by:
+	 *   - hw/net/igbvf.c|250| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
+	 *   - hw/net/igbvf.c|253| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MSIX_BAR_IDX, &s->msix);
+	 *   - hw/nvme/ctrl.c|8117| <<nvme_init_pci>> pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
+	 */
         pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
     } else {
         pci_register_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
diff --git a/hw/nvme/dif.c b/hw/nvme/dif.c
index 01b19c337..fc0b746c7 100644
--- a/hw/nvme/dif.c
+++ b/hw/nvme/dif.c
@@ -535,6 +535,11 @@ out:
     nvme_dif_rw_cb(ctx, ret);
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|3459| <<nvme_read>> return nvme_dif_rw(n, req);
+ *   - hw/nvme/ctrl.c|3630| <<nvme_do_write>> return nvme_dif_rw(n, req);
+ */
 uint16_t nvme_dif_rw(NvmeCtrl *n, NvmeRequest *req)
 {
     NvmeRwCmd *rw = (NvmeRwCmd *)&req->cmd;
diff --git a/hw/nvme/ns.c b/hw/nvme/ns.c
index 44aba8f4d..18c93a8cc 100644
--- a/hw/nvme/ns.c
+++ b/hw/nvme/ns.c
@@ -148,6 +148,10 @@ lbaf_found:
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ns.c|656| <<nvme_ns_setup>> if (nvme_ns_init_blk(ns, errp)) {
+ */
 static int nvme_ns_init_blk(NvmeNamespace *ns, Error **errp)
 {
     bool read_only;
@@ -394,6 +398,10 @@ static NvmeRuHandle *nvme_find_ruh_by_attr(NvmeEnduranceGroup *endgrp,
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ns.c|671| <<nvme_ns_setup>> if (!nvme_ns_init_fdp(ns, errp)) {
+ */
 static bool nvme_ns_init_fdp(NvmeNamespace *ns, Error **errp)
 {
     NvmeEnduranceGroup *endgrp = ns->endgrp;
@@ -647,6 +655,11 @@ static int nvme_ns_check_constraints(NvmeNamespace *ns, Error **errp)
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|8345| <<nvme_realize>> if (nvme_ns_setup(ns, errp)) {
+ *   - hw/nvme/ns.c|743| <<nvme_ns_realize>> if (nvme_ns_setup(ns, errp)) {
+ */
 int nvme_ns_setup(NvmeNamespace *ns, Error **errp)
 {
     if (nvme_ns_check_constraints(ns, errp)) {
diff --git a/hw/nvme/nvme.h b/hw/nvme/nvme.h
index 5f2ae7b28..d83d038a6 100644
--- a/hw/nvme/nvme.h
+++ b/hw/nvme/nvme.h
@@ -609,6 +609,40 @@ typedef enum NvmeResetType {
     NVME_RESET_CONTROLLER = 1,
 } NvmeResetType;
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|3346| <<nvme_do_flush>> iocb->ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|3386| <<nvme_flush>> iocb->ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|4443| <<nvme_io_cmd>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|4735| <<nvme_smart_info>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|4744| <<nvme_smart_info>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|5410| <<nvme_identify_ns>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5464| <<nvme_identify_ctrl_list>> if (attached && !nvme_ns(ctrl, nsid)) {
+ *   - hw/nvme/ctrl.c|5520| <<nvme_identify_ns_csi>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5567| <<nvme_identify_nslist>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|5615| <<nvme_identify_nslist_csi>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|5664| <<nvme_identify_ns_descr_list>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5921| <<nvme_get_feature>> if (!nvme_ns(n, nsid)) {
+ *   - hw/nvme/ctrl.c|5965| <<nvme_get_feature>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|5975| <<nvme_get_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6012| <<nvme_get_feature>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6168| <<nvme_set_feature>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6211| <<nvme_set_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6232| <<nvme_set_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6279| <<nvme_set_feature>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6333| <<nvme_update_dmrsl>> NvmeNamespace *ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6406| <<nvme_ns_attachment>> if (nvme_ns(ctrl, nsid)) {
+ *   - hw/nvme/ctrl.c|6420| <<nvme_ns_attachment>> if (!nvme_ns(ctrl, nsid)) {
+ *   - hw/nvme/ctrl.c|6572| <<nvme_do_format>> iocb->ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|6634| <<nvme_format>> iocb->ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|6900| <<nvme_directive_receive>> ns = nvme_ns(n, nsid);
+ *   - hw/nvme/ctrl.c|7093| <<nvme_ctrl_reset>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|7169| <<nvme_ctrl_shutdown>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|7184| <<nvme_select_iocs>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ctrl.c|8363| <<nvme_exit>> ns = nvme_ns(n, i);
+ *   - hw/nvme/ns.c|749| <<nvme_ns_realize>> if (nvme_ns(n, i) || nvme_subsys_ns(subsys, i)) {
+ *   - hw/nvme/ns.c|762| <<nvme_ns_realize>> if (nvme_ns(n, nsid) || nvme_subsys_ns(subsys, nsid)) {
+ */
 static inline NvmeNamespace *nvme_ns(NvmeCtrl *n, uint32_t nsid)
 {
     if (!nsid || nsid > NVME_MAX_NAMESPACES) {
@@ -626,6 +660,17 @@ static inline NvmeCQueue *nvme_cq(NvmeRequest *req)
     return n->cq[sq->cqid];
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|2207| <<nvme_rw_cb>> status = nvme_map_mdata(nvme_ctrl(req), nlb, req);
+ *   - hw/nvme/ctrl.c|2327| <<nvme_compare_mdata_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|2415| <<nvme_compare_data_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|2557| <<nvme_dsm_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|3337| <<nvme_do_flush>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/ctrl.c|3725| <<nvme_open_zone>> return nvme_zrm_open_flags(nvme_ctrl(req), ns, zone, flags);
+ *   - hw/nvme/ctrl.c|6559| <<nvme_do_format>> NvmeCtrl *n = nvme_ctrl(req);
+ *   - hw/nvme/dif.c|428| <<nvme_dif_rw_check_cb>> NvmeCtrl *n = nvme_ctrl(req);
+ */
 static inline NvmeCtrl *nvme_ctrl(NvmeRequest *req)
 {
     NvmeSQueue *sq = req->sq;
@@ -641,6 +686,17 @@ static inline uint16_t nvme_cid(NvmeRequest *req)
     return le16_to_cpu(req->cqe.cid);
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|7070| <<nvme_activate_virt_res>> sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|7143| <<nvme_ctrl_reset>> sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|7202| <<nvme_start_ctrl>> NvmeSecCtrlEntry *sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|7549| <<nvme_mmio_read>> if (pci_is_vf(PCI_DEVICE(n)) && !nvme_sctrl(n)->scs &&
+ *   - hw/nvme/ctrl.c|7725| <<nvme_mmio_write>> if (pci_is_vf(PCI_DEVICE(n)) && !nvme_sctrl(n)->scs &&
+ *   - hw/nvme/ctrl.c|7916| <<nvme_init_state>> sctrl = nvme_sctrl(n);
+ *   - hw/nvme/ctrl.c|8169| <<nvme_init_ctrl>> NvmeSecCtrlEntry *sctrl = nvme_sctrl(n);
+ *   - hw/nvme/subsys.c|58| <<nvme_subsys_register_ctrl>> NvmeSecCtrlEntry *sctrl = nvme_sctrl(n);
+ */
 static inline NvmeSecCtrlEntry *nvme_sctrl(NvmeCtrl *n)
 {
     PCIDevice *pci_dev = &n->parent_obj;
@@ -653,6 +709,11 @@ static inline NvmeSecCtrlEntry *nvme_sctrl(NvmeCtrl *n)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/nvme/ctrl.c|6716| <<nvme_assign_virt_res_to_sec>> sctrl = nvme_sctrl_for_cntlid(n, cntlid);
+ *   - hw/nvme/ctrl.c|6751| <<nvme_virt_set_state>> sctrl = nvme_sctrl_for_cntlid(n, cntlid);
+ */
 static inline NvmeSecCtrlEntry *nvme_sctrl_for_cntlid(NvmeCtrl *n,
                                                       uint16_t cntlid)
 {
diff --git a/hw/pci/pci.c b/hw/pci/pci.c
index 881d774fb..98e6c0fdd 100644
--- a/hw/pci/pci.c
+++ b/hw/pci/pci.c
@@ -1302,6 +1302,78 @@ static void pci_qdev_unrealize(DeviceState *dev)
     }
 }
 
+/*
+ * q35有两个ioport, MCH_HOST_BRIDGE_CONFIG_ADDR和MCH_HOST_BRIDGE_CONFIG_DATA
+ *
+ * pci的操作要通过这两个port,
+ * 比如先往MCH_HOST_BRIDGE_CONFIG_ADDR写入想操作设备的bdf和寄存器的offset,
+ * 然后把要写入寄存器的值写入MCH_HOST_BRIDGE_CONFIG_DATA
+ *
+ * 比如以下在seabios是想把bdf的config space的ofs区域(比如bar0的地址)写入addr:
+ *
+ * pci_config_writel(pci->bdf, ofs, addr);
+ *
+ * 实际实现(PORT_PCI_CMD就是MCH_HOST_BRIDGE_CONFIG_ADDR, PORT_PCI_DATA就是MCH_HOST_BRIDGE_CONFIG_DATA):
+ *
+ * void pci_config_writel(u16 bdf, u32 addr, u32 val)
+ * {
+ *     outl(0x80000000 | (bdf << 8) | (addr & 0xfc), PORT_PCI_CMD);
+ *     outl(val, PORT_PCI_DATA);
+ * }
+ *
+ *
+ * 假设guest或者bios想更新config space中某个bar的地址, 会调用上面的pci_config_writel()类似的函数
+ *
+ * qemu会拦截对MCH_HOST_BRIDGE_CONFIG_ADDR和MCH_HOST_BRIDGE_CONFIG_DATA的写,
+ * 根据bdf调用对于的pci设备的write_config, 比如pci_default_write_config()
+ *
+ * pci_default_write_config()会判断,
+ * 如果写的地址是一个bar地址, 会调用pci_update_mappings()
+ *
+ * 一个MemoryRegion, 如果其ram不是true(默认false), 就不会绑定真正的内存
+ * 这样guest写这个读bar的时候kvm就会trap到qemu,
+ * 调用PCIDevice->io_regions[i]的对应的MemoryRegion的read/write callback
+ *
+ *
+ * 比如nvme的某个bar的初始化, 里面n->iomem->ram是false!!!
+ *
+ * 1314     memory_region_init_io(&n->iomem, OBJECT(n), &nvme_mmio_ops, n,
+ * 1315                           "nvme", n->reg_size);
+ * 1316     pci_register_bar(&n->parent_obj, 0,
+ * 1317         PCI_BASE_ADDRESS_SPACE_MEMORY | PCI_BASE_ADDRESS_MEM_TYPE_64,
+ * 1318         &n->iomem);
+ *
+ * 一些使用的例子:
+ *   - hw/net/e1000.c|1676| <<pci_e1000_realize>> pci_register_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY, &d->mmio);
+ *   - hw/net/e1000.c|1678| <<pci_e1000_realize>> pci_register_bar(pci_dev, 1, PCI_BASE_ADDRESS_SPACE_IO, &d->io);
+ *   - hw/net/e1000e.c|438| <<e1000e_pci_realize>> pci_register_bar(pci_dev, E1000E_MMIO_IDX,
+ *   - hw/net/e1000e.c|447| <<e1000e_pci_realize>> pci_register_bar(pci_dev, E1000E_FLASH_IDX,
+ *   - hw/net/e1000e.c|452| <<e1000e_pci_realize>> pci_register_bar(pci_dev, E1000E_IO_IDX,
+ *   - hw/net/e1000e.c|457| <<e1000e_pci_realize>> pci_register_bar(pci_dev, E1000E_MSIX_IDX,
+ *   - hw/net/igb.c|389| <<igb_pci_realize>> pci_register_bar(pci_dev, E1000E_MMIO_IDX,
+ *   - hw/net/igb.c|398| <<igb_pci_realize>> pci_register_bar(pci_dev, E1000E_FLASH_IDX,
+ *   - hw/net/igb.c|403| <<igb_pci_realize>> pci_register_bar(pci_dev, E1000E_IO_IDX,
+ *   - hw/net/igb.c|408| <<igb_pci_realize>> pci_register_bar(pci_dev, E1000E_MSIX_IDX,
+ *   - hw/nvme/ctrl.c|7979| <<nvme_init_cmb>> pci_register_bar(pci_dev, NVME_CMB_BIR,
+ *   - hw/nvme/ctrl.c|8005| <<nvme_init_pmr>> pci_register_bar(pci_dev, NVME_PMR_BIR,
+ *   - hw/nvme/ctrl.c|8140| <<nvme_init_pci>> pci_register_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+ *   - hw/pci-bridge/cxl_downstream.c|187| <<cxl_dsp_realize>> pci_register_bar(d, CXL_COMPONENT_REG_BAR_IDX,
+ *   - hw/pci-bridge/cxl_root_port.c|183| <<cxl_rp_realize>> pci_register_bar(pci_dev, CXL_COMPONENT_REG_BAR_IDX,
+ *   - hw/pci-bridge/cxl_upstream.c|334| <<cxl_usp_realize>> pci_register_bar(d, CXL_COMPONENT_REG_BAR_IDX,
+ *   - hw/pci-bridge/pci_bridge_dev.c|111| <<pci_bridge_dev_realize>> pci_register_bar(dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+ *   - hw/pci-bridge/pcie_pci_bridge.c|84| <<pcie_pci_bridge_realize>> pci_register_bar(d, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+ *   - hw/pci-host/ppce500.c|427| <<e500_pcihost_bridge_realize>> pci_register_bar(d, 0, PCI_BASE_ADDRESS_SPACE_MEMORY, &b->bar0);
+ *   - hw/pci/msix.c|424| <<msix_init_exclusive_bar>> pci_register_bar(dev, bar_nr, PCI_BASE_ADDRESS_SPACE_MEMORY,
+ *   - hw/pci/pci.c|2446| <<pci_add_option_rom>> pci_register_bar(pdev, PCI_ROM_SLOT, 0, &pdev->rom);
+ *   - hw/remote/proxy.c|366| <<probe_pci_info>> pci_register_bar(dev, i, type, &pdev->region[i].mr);
+ *   - hw/vfio/pci.c|1013| <<vfio_pci_size_rom>> pci_register_bar(&vdev->pdev, PCI_ROM_SLOT,
+ *   - hw/vfio/pci.c|1714| <<vfio_bar_register>> pci_register_bar(&vdev->pdev, nr, bar->type, bar->mr);
+ *   - hw/virtio/virtio-pci.c|2022| <<virtio_pci_device_plugged>> pci_register_bar(&proxy->pci_dev, proxy->modern_io_bar_idx,
+ *   - hw/virtio/virtio-pci.c|2029| <<virtio_pci_device_plugged>> pci_register_bar(&proxy->pci_dev, proxy->modern_mem_bar_idx,
+ *   - hw/virtio/virtio-pci.c|2068| <<virtio_pci_device_plugged>> pci_register_bar(&proxy->pci_dev, proxy->legacy_io_bar_idx,
+ *
+ * 关于更新memory的地方, 参考pci_update_mappings()
+ */
 void pci_register_bar(PCIDevice *pci_dev, int region_num,
                       uint8_t type, MemoryRegion *memory)
 {
@@ -1321,6 +1393,10 @@ void pci_register_bar(PCIDevice *pci_dev, int region_num,
         pci_dev->config[PCI_HEADER_TYPE] & ~PCI_HEADER_TYPE_MULTI_FUNCTION;
     assert(hdr_type != PCI_HEADER_TYPE_BRIDGE || region_num < 2);
 
+    /*
+     * PCIDevice *pci_dev:
+     * -> PCIIORegion io_regions[PCI_NUM_REGIONS];
+     */
     r = &pci_dev->io_regions[region_num];
     r->addr = PCI_BAR_UNMAPPED;
     r->size = size;
@@ -1515,6 +1591,62 @@ pcibus_t pci_bar_address(PCIDevice *d,
     return new_addr;
 }
 
+/*
+ * 这是3.0.0的笔记
+ *
+ * nvme第一次到这里的例子
+ *   io_regions = {{
+ *       addr = 18446744073709551615,
+ *       size = 8192,
+ *       type = 4 '\004',
+ *       memory = 0x5555579aa870,   (name="nvme", ops=nvme_mmio_ops)
+ *       address_space = 0x5555568faff0  (name="pci", ops=unassigned_mem_ops, terminates = false)
+ *     }, {
+ *       addr = 0,
+ *       size = 0,
+ *       type = 0 '\000',
+ *       memory = 0x0,
+ *       address_space = 0x0
+ *     }, {
+ *       addr = 0,
+ *       size = 0,
+ *       type = 0 '\000',
+ *       memory = 0x0,
+ *       address_space = 0x0
+ *     }, {
+ *       addr = 0,
+ *       size = 0,
+ *       type = 0 '\000',
+ *       memory = 0x0,
+ *       address_space = 0x0
+ *     }, {
+ *       addr = 18446744073709551615,
+ *       size = 4096,
+ *       type = 0 '\000',
+ *       memory = 0x5555579aa430,  (name="nvme-msix", ops=unassigned_mem_ops, terminates=false)
+ *       address_space = 0x5555568faff0 (name="pci", ops=unassigned_mem_ops, terminates = false)
+ *     }, {
+ *       addr = 0,
+ *       size = 0,
+ *       type = 0 '\000',
+ *       memory = 0x0,
+ *       address_space = 0x0
+ *     }, {
+ *       addr = 0,
+ *       size = 0,
+ *       type = 0 '\000',
+ *       memory = 0x0,
+ *       address_space = 0x0
+ *     }},
+ *
+ * bar的地址应该是guest (比如BIOS)更新的
+ *
+ * called by:
+ *   - hw/pci/pci.c|406| <<pci_do_device_reset>> pci_update_mappings(dev);
+ *   - hw/pci/pci.c|659| <<get_pci_config_device>> pci_update_mappings(s);
+ *   - hw/pci/pci.c|1614| <<pci_default_write_config>> pci_update_mappings(d);
+ *   - hw/pci/pci.c|2844| <<pci_set_power>> pci_update_mappings(d);
+ */
 static void pci_update_mappings(PCIDevice *d)
 {
     PCIIORegion *r;
diff --git a/hw/pci/pcie_sriov.c b/hw/pci/pcie_sriov.c
index 76a3b6917..33c0388ea 100644
--- a/hw/pci/pcie_sriov.c
+++ b/hw/pci/pcie_sriov.c
@@ -24,6 +24,11 @@ static PCIDevice *register_vf(PCIDevice *pf, int devfn,
                               const char *name, uint16_t vf_num);
 static void unregister_vfs(PCIDevice *dev);
 
+/*
+ * called by:
+ *   - hw/net/igb.c|436| <<igb_pci_realize>> pcie_sriov_pf_init(pci_dev, IGB_CAP_SRIOV_OFFSET, TYPE_IGBVF,
+ *   - hw/nvme/ctrl.c|8050| <<nvme_init_sriov>> pcie_sriov_pf_init(pci_dev, offset, "nvme", vf_dev_id,
+ */
 void pcie_sriov_pf_init(PCIDevice *dev, uint16_t offset,
                         const char *vfname, uint16_t vf_dev_id,
                         uint16_t init_vfs, uint16_t total_vfs,
@@ -78,6 +83,12 @@ void pcie_sriov_pf_exit(PCIDevice *dev)
     dev->exp.sriov_pf.vfname = NULL;
 }
 
+/*
+ * called by:
+ *   - hw/net/igb.c|440| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MMIO_BAR_IDX,
+ *   - hw/net/igb.c|443| <<igb_pci_realize>> pcie_sriov_pf_init_vf_bar(pci_dev, IGBVF_MSIX_BAR_IDX,
+ *   - hw/nvme/ctrl.c|8054| <<nvme_init_sriov>> pcie_sriov_pf_init_vf_bar(pci_dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |
+ */
 void pcie_sriov_pf_init_vf_bar(PCIDevice *dev, int region_num,
                                uint8_t type, dma_addr_t size)
 {
@@ -105,6 +116,12 @@ void pcie_sriov_pf_init_vf_bar(PCIDevice *dev, int region_num,
     dev->exp.sriov_pf.vf_bar_type[region_num] = type;
 }
 
+/*
+ * called by:
+ *   - hw/net/igbvf.c|250| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MMIO_BAR_IDX, &s->mmio);
+ *   - hw/net/igbvf.c|253| <<igbvf_pci_realize>> pcie_sriov_vf_register_bar(dev, IGBVF_MSIX_BAR_IDX, &s->msix);
+ *   - hw/nvme/ctrl.c|8117| <<nvme_init_pci>> pcie_sriov_vf_register_bar(pci_dev, 0, &n->bar0);
+ */
 void pcie_sriov_vf_register_bar(PCIDevice *dev, int region_num,
                                 MemoryRegion *memory)
 {
@@ -141,6 +158,10 @@ void pcie_sriov_vf_register_bar(PCIDevice *dev, int region_num,
     }
 }
 
+/*
+ * called by:
+ *   - hw/pci/pcie_sriov.c|186| <<register_vfs>> dev->exp.sriov_pf.vf[i] = register_vf(dev, devfn, dev->exp.sriov_pf.vfname, i);
+ */
 static PCIDevice *register_vf(PCIDevice *pf, int devfn, const char *name,
                               uint16_t vf_num)
 {
@@ -163,6 +184,10 @@ static PCIDevice *register_vf(PCIDevice *pf, int devfn, const char *name,
     return dev;
 }
 
+/*
+ * called by:
+ *   - hw/pci/pcie_sriov.c|267| <<pcie_sriov_config_write>> register_vfs(dev);
+ */
 static void register_vfs(PCIDevice *dev)
 {
     uint16_t num_vfs;
@@ -219,6 +244,11 @@ static void unregister_vfs(PCIDevice *dev)
     pci_set_word(dev->config + dev->exp.sriov_cap + PCI_SRIOV_NUM_VF, 0);
 }
 
+/*
+ * called by:
+ *   - hw/pci/pci.c|1625| <<pci_default_write_config>> pcie_sriov_config_write(d, addr, val_in, l);
+ *   - hw/pci/pcie_sriov.c|286| <<pcie_sriov_pf_disable_vfs>> pcie_sriov_config_write(dev, sriov_cap + PCI_SRIOV_CTRL, val, 1);
+ */
 void pcie_sriov_config_write(PCIDevice *dev, uint32_t address,
                              uint32_t val, int len)
 {
@@ -250,6 +280,11 @@ void pcie_sriov_config_write(PCIDevice *dev, uint32_t address,
 }
 
 
+/*
+ * called by:
+ *   - hw/net/igb.c|482| <<igb_qdev_reset_hold>> pcie_sriov_pf_disable_vfs(d);
+ *   - hw/nvme/ctrl.c|7126| <<nvme_ctrl_reset>> pcie_sriov_pf_disable_vfs(pci_dev);
+ */
 /* Reset SR/IOV VF Enable bit to trigger an unregister of all VFs */
 void pcie_sriov_pf_disable_vfs(PCIDevice *dev)
 {
@@ -263,6 +298,9 @@ void pcie_sriov_pf_disable_vfs(PCIDevice *dev)
     }
 }
 
+/*
+ * 似乎没有调用
+ */
 /* Add optional supported page sizes to the mask of supported page sizes */
 void pcie_sriov_pf_add_sup_pgsize(PCIDevice *dev, uint16_t opt_sup_pgsize)
 {
@@ -282,17 +320,47 @@ void pcie_sriov_pf_add_sup_pgsize(PCIDevice *dev, uint16_t opt_sup_pgsize)
 }
 
 
+/*
+ * called by:
+ *   - hw/net/igbvf.c|214| <<igbvf_mmio_read>> addr = vf_to_pf_addr(addr, pcie_sriov_vf_number(vf), false);
+ *   - hw/net/igbvf.c|224| <<igbvf_mmio_write>> addr = vf_to_pf_addr(addr, pcie_sriov_vf_number(vf), true);
+ *   - hw/nvme/nvme.h|650| <<nvme_sctrl>> return &pf->sec_ctrl_list.sec[pcie_sriov_vf_number(pci_dev)];
+ */
 uint16_t pcie_sriov_vf_number(PCIDevice *dev)
 {
     assert(pci_is_vf(dev));
     return dev->exp.sriov_vf.vf_number;
 }
 
+/*
+ * called by:
+ *   - hw/net/igbvf.c|212| <<igbvf_mmio_read>> PCIDevice *pf = pcie_sriov_get_pf(vf);
+ *   - hw/net/igbvf.c|222| <<igbvf_mmio_write>> PCIDevice *pf = pcie_sriov_get_pf(vf);
+ *   - hw/nvme/ctrl.c|8314| <<nvme_realize>> NvmeCtrl *pn = NVME(pcie_sriov_get_pf(pci_dev));
+ *   - hw/nvme/nvme.h|647| <<nvme_sctrl>> NvmeCtrl *pf = NVME(pcie_sriov_get_pf(pci_dev));
+ */
 PCIDevice *pcie_sriov_get_pf(PCIDevice *dev)
 {
+    /*
+     * PCIDevice *dev:
+     * -> PCIExpressDevice exp;
+     *    -> uint16_t sriov_cap;
+     *    -> PCIESriovPF sriov_pf;
+     *    -> PCIESriovVF sriov_vf;
+     *       -> PCIDevice *pf;
+     *       -> uint16_t vf_number;
+     */
     return dev->exp.sriov_vf.pf;
 }
 
+/*
+ * called by:
+ *   - hw/net/igb_core.c|124| <<igb_msix_notify>> dev = pcie_sriov_get_vf_at_index(core->owner, vfn);
+ *   - hw/net/igb_core.c|849| <<igb_txdesc_writeback>> d = pcie_sriov_get_vf_at_index(core->owner, txi->idx % 8);
+ *   - hw/net/igb_core.c|894| <<igb_start_xmit>> d = pcie_sriov_get_vf_at_index(core->owner, txi->idx % 8);
+ *   - hw/net/igb_core.c|1573| <<igb_write_packet_to_guest>> d = pcie_sriov_get_vf_at_index(core->owner, rxi->idx % 8);
+ *   - hw/nvme/ctrl.c|6758| <<nvme_virt_set_state>> sn = NVME(pcie_sriov_get_vf_at_index(pci, vf_index));
+ */
 PCIDevice *pcie_sriov_get_vf_at_index(PCIDevice *dev, int n)
 {
     assert(!pci_is_vf(dev));
@@ -302,6 +370,11 @@ PCIDevice *pcie_sriov_get_vf_at_index(PCIDevice *dev, int n)
     return NULL;
 }
 
+/*
+ * called by:
+ *   - hw/net/igb_core.c|123| <<igb_msix_notify>> if (vfn < pcie_sriov_num_vfs(core->owner)) {
+ *   - hw/net/igb_core.c|2650| <<igb_get_status>> uint16_t num_vfs = pcie_sriov_num_vfs(core->owner);
+ */
 uint16_t pcie_sriov_num_vfs(PCIDevice *dev)
 {
     return dev->exp.sriov_pf.num_vfs;
diff --git a/hw/scsi/vhost-scsi.c b/hw/scsi/vhost-scsi.c
index 443f67daa..c66214d01 100644
--- a/hw/scsi/vhost-scsi.c
+++ b/hw/scsi/vhost-scsi.c
@@ -41,11 +41,23 @@ static const int kernel_feature_bits[] = {
     VHOST_INVALID_FEATURE_BIT
 };
 
+/*
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|118| <<vhost_scsi_start>> ret = vhost_scsi_set_endpoint(s);
+ */
 static int vhost_scsi_set_endpoint(VHostSCSI *s)
 {
     VirtIOSCSICommon *vs = VIRTIO_SCSI_COMMON(s);
     VHostSCSICommon *vsc = VHOST_SCSI_COMMON(s);
     const VhostOps *vhost_ops = vsc->dev.vhost_ops;
+    /*
+     * struct vhost_scsi_target {
+     *     int abi_version;
+     *     char vhost_wwpn[224]; // TRANSPORT_IQN_LEN
+     *     unsigned short vhost_tpgt;
+     *     unsigned short reserved;
+     * };
+     */
     struct vhost_scsi_target backend;
     int ret;
 
@@ -58,6 +70,10 @@ static int vhost_scsi_set_endpoint(VHostSCSI *s)
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|131| <<vhost_scsi_stop>> vhost_scsi_clear_endpoint(s);
+ */
 static void vhost_scsi_clear_endpoint(VHostSCSI *s)
 {
     VirtIOSCSICommon *vs = VIRTIO_SCSI_COMMON(s);
@@ -70,6 +86,31 @@ static void vhost_scsi_clear_endpoint(VHostSCSI *s)
     vhost_ops->vhost_scsi_clear_endpoint(&vsc->dev, &backend);
 }
 
+/*
+ * (gdb) bt
+ * #0  0x0000555555fc742a in error_vprepend (errp=0x7ffddaffc378, fmt=0x5555561522f8 "Error setting vhost-scsi endpoint", ap=0x7ffddaffc380) at ../util/error.c:142
+ * #1  0x0000555555fc7908 in error_reportf_err (err=0x0, fmt=0x5555561522f8 "Error setting vhost-scsi endpoint") at ../util/error.c:255
+ * #2  0x0000555555a9653e in vhost_scsi_start (s=0x555557225110) at ../hw/scsi/vhost-scsi.c:100
+ * #3  0x0000555555a96619 in vhost_scsi_set_status (vdev=0x555557225110, val=15 '\017') at ../hw/scsi/vhost-scsi.c:132
+ * #4  0x0000555555d33e8a in virtio_set_status (vdev=0x555557225110, val=15 '\017') at ../hw/virtio/virtio.c:2048
+ * #5  0x0000555555b011f5 in virtio_pci_common_write (opaque=0x55555721cd40, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1580
+ * #6  0x0000555555d622f4 in memory_region_write_accessor (mr=0x55555721d880, addr=20, value=0x7ffddaffc668, size=1, shift=0, mask=255, attrs=...) at ../system/memory.c:497
+ * #7  0x0000555555d62601 in access_with_adjusted_size (addr=20, value=0x7ffddaffc668, size=1, access_size_min=1, access_size_max=4, access_fn=0x555555d621fe <memory_region_write_accessor>,
+ *                                                     mr=0x55555721d880, attrs=...) at ../system/memory.c:573
+ * #8  0x0000555555d65696 in memory_region_dispatch_write (mr=0x55555721d880, addr=20, data=15, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #9  0x0000555555d72bc8 in flatview_write_continue (fv=0x7ffdd0160140, addr=34360786964, attrs=..., ptr=0x7ffff7e23028, len=1, addr1=20, l=1, mr=0x55555721d880) at ../system/physmem.c:2714
+ * #10 0x0000555555d72d2b in flatview_write (fv=0x7ffdd0160140, addr=34360786964, attrs=..., buf=0x7ffff7e23028, len=1) at ../system/physmem.c:2756
+ * #11 0x0000555555d730db in address_space_write (as=0x555556f305a0 <address_space_memory>, addr=34360786964, attrs=..., buf=0x7ffff7e23028, len=1) at ../system/physmem.c:2863
+ * #12 0x0000555555d73148 in address_space_rw (as=0x555556f305a0 <address_space_memory>, addr=34360786964, attrs=..., buf=0x7ffff7e23028, len=1, is_write=true) at ../system/physmem.c:2873
+ * #13 0x0000555555dc4b0e in kvm_cpu_exec (cpu=0x5555573111c0) at ../accel/kvm/kvm-all.c:2915
+ * #14 0x0000555555dc779d in kvm_vcpu_thread_fn (arg=0x5555573111c0) at ../accel/kvm/kvm-accel-ops.c:51
+ * #15 0x0000555555fc25db in qemu_thread_start (args=0x55555731a2a0) at ../util/qemu-thread-posix.c:541
+ * #16 0x00007ffff6296105 in start_thread () at /lib64/libpthread.so.0
+ * #17 0x00007ffff5fbeb2d in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|152| <<vhost_scsi_set_status>> ret = vhost_scsi_start(s);
+ */
 static int vhost_scsi_start(VHostSCSI *s)
 {
     int ret, abi_version;
@@ -102,6 +143,10 @@ static int vhost_scsi_start(VHostSCSI *s)
     return ret;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|158| <<vhost_scsi_set_status>> vhost_scsi_stop(s);
+ */
 static void vhost_scsi_stop(VHostSCSI *s)
 {
     VHostSCSICommon *vsc = VHOST_SCSI_COMMON(s);
@@ -110,6 +155,14 @@ static void vhost_scsi_stop(VHostSCSI *s)
     vhost_scsi_common_stop(vsc);
 }
 
+/*
+ * 调用的地方:
+ * - hw/virtio/virtio.c|2084| <<virtio_set_status>> k->set_status(vdev, val);
+ *
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|290| <<vhost_scsi_unrealize>> vhost_scsi_set_status(vdev, 0);
+ *   - hw/scsi/vhost-scsi.c|338| <<vhost_scsi_class_init>> vdc->set_status = vhost_scsi_set_status;
+ */
 static void vhost_scsi_set_status(VirtIODevice *vdev, uint8_t val)
 {
     VHostSCSI *s = VHOST_SCSI(vdev);
@@ -137,6 +190,18 @@ static void vhost_scsi_set_status(VirtIODevice *vdev, uint8_t val)
     }
 }
 
+/*
+ * 在以下使用vhost_dummy_handle_output():
+ *
+ * 217 static void vhost_scsi_realize(DeviceState *dev, Error **errp)
+ * 218 {
+ * ... ...
+ * 241     virtio_scsi_common_realize(dev,
+ * 242                                vhost_dummy_handle_output,
+ * 243                                vhost_dummy_handle_output,
+ * 244                                vhost_dummy_handle_output,
+ * 245                                &err);
+ */
 static void vhost_dummy_handle_output(VirtIODevice *vdev, VirtQueue *vq)
 {
 }
@@ -178,6 +243,21 @@ static void vhost_scsi_realize(DeviceState *dev, Error **errp)
     }
 
     if (vs->conf.vhostfd) {
+        /*
+	 * called by:
+	 *   - hw/remote/proxy.c|90| <<pci_proxy_dev_realize>> fd = monitor_fd_param(monitor_cur(), dev->fd, errp);
+	 *   - hw/remote/remote-obj.c|55| <<remote_object_set_fd>> fd = monitor_fd_param(monitor_cur(), str, errp);
+	 *   - hw/scsi/vhost-scsi.c|223| <<vhost_scsi_realize>> vhostfd = monitor_fd_param(monitor_cur(), vs->conf.vhostfd, errp);
+	 *   - hw/virtio/vhost-vsock.c|142| <<vhost_vsock_device_realize>> vhostfd = monitor_fd_param(monitor_cur(), vsock->conf.vhostfd, errp);
+	 *   - migration/fd.c|58| <<fd_start_incoming_migration>> int fd = monitor_fd_param(monitor_cur(), fdname, errp);
+	 *   - net/dgram.c|330| <<net_dgram_mcast_init>> fd = monitor_fd_param(monitor_cur(), local->u.fd.str, errp);
+	 *   - net/dgram.c|561| <<net_init_dgram>> fd = monitor_fd_param(monitor_cur(), local->u.fd.str, errp);
+	 *   - net/socket.c|711| <<net_init_socket>> fd = monitor_fd_param(monitor_cur(), sock->fd, errp);
+	 *   - net/tap.c|731| <<net_init_tap_one>> vhostfd = monitor_fd_param(monitor_cur(), vhostfdname, &err);
+	 *   - net/tap.c|848| <<net_init_tap>> fd = monitor_fd_param(monitor_cur(), tap->fd, errp);
+	 *   - net/tap.c|903| <<net_init_tap>> fd = monitor_fd_param(monitor_cur(), fds[i], errp);
+	 *   - net/vhost-vdpa.c|1491| <<net_init_vhost_vdpa>> vdpa_device_fd = monitor_fd_param(monitor_cur(), opts->vhostfd, errp);
+	 */
         vhostfd = monitor_fd_param(monitor_cur(), vs->conf.vhostfd, errp);
         if (vhostfd == -1) {
             error_prepend(errp, "vhost-scsi: unable to parse vhostfd: ");
@@ -192,6 +272,12 @@ static void vhost_scsi_realize(DeviceState *dev, Error **errp)
         }
     }
 
+    /*
+     * called by:
+     *   - hw/scsi/vhost-scsi.c|195| <<vhost_scsi_realize>> virtio_scsi_common_realize(dev, vhost_dummy_handle_output, vhost_dummy_handle_output, vhost_dummy_handle_output, &err);
+     *   - hw/scsi/vhost-user-scsi.c|106| <<vhost_user_scsi_realize>> virtio_scsi_common_realize(dev, vhost_dummy_handle_output, vhost_dummy_handle_output, vhost_dummy_handle_output, &err);
+     *   - hw/scsi/virtio-scsi.c|1287| <<virtio_scsi_device_realize>> virtio_scsi_common_realize(dev, virtio_scsi_handle_ctrl, virtio_scsi_handle_event, virtio_scsi_handle_cmd, &err);
+     */
     virtio_scsi_common_realize(dev,
                                vhost_dummy_handle_output,
                                vhost_dummy_handle_output,
@@ -273,6 +359,12 @@ static void vhost_scsi_unrealize(DeviceState *dev)
     virtio_scsi_common_unrealize(dev);
 }
 
+/*
+ * struct VHostSCSIPCI {
+ *     VirtIOPCIProxy parent_obj;
+ *     VHostSCSI vdev;
+ * };
+ */
 static struct vhost_dev *vhost_scsi_get_vhost(VirtIODevice *vdev)
 {
     VHostSCSI *s = VHOST_SCSI(vdev);
@@ -328,6 +420,12 @@ static void vhost_scsi_instance_init(Object *obj)
                                   DEVICE(vsc));
 }
 
+/*
+ * struct VHostSCSIPCI {
+ *     VirtIOPCIProxy parent_obj;
+ *     VHostSCSI vdev;
+ * };
+ */
 static const TypeInfo vhost_scsi_info = {
     .name = TYPE_VHOST_SCSI,
     .parent = TYPE_VHOST_SCSI_COMMON,
diff --git a/hw/scsi/virtio-scsi.c b/hw/scsi/virtio-scsi.c
index 45b95ea07..6af506499 100644
--- a/hw/scsi/virtio-scsi.c
+++ b/hw/scsi/virtio-scsi.c
@@ -81,6 +81,11 @@ static inline SCSIDevice *virtio_scsi_device_get(VirtIOSCSI *s, uint8_t *lun)
     return scsi_device_get(&s->bus, 0, lun[1], virtio_scsi_get_lun(lun));
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|244| <<virtio_scsi_pop_req>> virtio_scsi_init_req(s, vq, req);
+ *   - hw/scsi/virtio-scsi.c|273| <<virtio_scsi_load_request>> virtio_scsi_init_req(s, vs->cmd_vqs[n], req);
+ */
 static void virtio_scsi_init_req(VirtIOSCSI *s, VirtQueue *vq, VirtIOSCSIReq *req)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
@@ -90,6 +95,12 @@ static void virtio_scsi_init_req(VirtIOSCSI *s, VirtQueue *vq, VirtIOSCSIReq *re
     req->vq = vq;
     req->dev = s;
     qemu_sglist_init(&req->qsgl, DEVICE(s), 8, vdev->dma_as);
+    /*
+     * 有些可以查看virtqueue_alloc_element()
+     *
+     * VirtIOSCSIReq *req:
+     * -> QEMUIOVector resp_iov;
+     */
     qemu_iovec_init(&req->resp_iov, 1);
     memset((uint8_t *)req + zero_skip, 0, sizeof(*req) - zero_skip);
 }
@@ -122,6 +133,14 @@ static void virtio_scsi_complete_req(VirtIOSCSIReq *req)
     virtio_scsi_free_req(req);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|547| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|555| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|565| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|775| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_bad_req(req);
+ *   - hw/scsi/virtio-scsi.c|974| <<virtio_scsi_push_event>> virtio_scsi_bad_req(req);
+ */
 static void virtio_scsi_bad_req(VirtIOSCSIReq *req)
 {
     virtio_error(VIRTIO_DEVICE(req->dev), "wrong size for virtio-scsi headers");
@@ -152,12 +171,42 @@ static size_t qemu_sgl_concat(VirtIOSCSIReq *req, struct iovec *iov,
     return copied;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|261| <<virtio_scsi_load_request>> if (virtio_scsi_parse_req(req, sizeof(VirtIOSCSICmdReq) + vs->cdb_size,
+ *   - hw/scsi/virtio-scsi.c|565| <<virtio_scsi_handle_ctrl_req>> if (virtio_scsi_parse_req(req, sizeof(VirtIOSCSICtrlTMFReq),
+ *   - hw/scsi/virtio-scsi.c|575| <<virtio_scsi_handle_ctrl_req>> if (virtio_scsi_parse_req(req, sizeof(VirtIOSCSICtrlANReq),
+ *   - hw/scsi/virtio-scsi.c|788| <<virtio_scsi_handle_cmd_req_prepare>> rc = virtio_scsi_parse_req(req, sizeof(VirtIOSCSICmdReq) + vs->cdb_size,
+ *   - hw/scsi/virtio-scsi.c|1022| <<virtio_scsi_push_event>> if (virtio_scsi_parse_req(req, 0, sizeof(VirtIOSCSIEvent))) {
+ */
 static int virtio_scsi_parse_req(VirtIOSCSIReq *req,
                                  unsigned req_size, unsigned resp_size)
 {
     VirtIODevice *vdev = (VirtIODevice *) req->dev;
     size_t in_size, out_size;
 
+    /*
+     * VirtIOSCSIReq *req:
+     * -> VirtQueueElement elem;
+     *    -> unsigned int index;
+     *    -> unsigned int len;
+     *    -> unsigned int ndescs;
+     *    -> unsigned int out_num;
+     *    -> unsigned int in_num;
+     *    -> hwaddr *in_addr;
+     *    -> hwaddr *out_addr;
+     *    -> struct iovec *in_sg;
+     *    -> struct iovec *out_sg;
+     *
+     *   union {
+     *       VirtIOSCSICmdReq      cmd;
+     *       VirtIOSCSICtrlTMFReq  tmf;
+     *       VirtIOSCSICtrlANReq   an;
+     *   } req;
+     *
+     *
+     * iov_to_buf_full()
+     */
     if (iov_to_buf(req->elem.out_sg, req->elem.out_num, 0,
                    &req->req, req_size) < req_size) {
         return -EINVAL;
@@ -210,11 +259,23 @@ static int virtio_scsi_parse_req(VirtIOSCSIReq *req,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|607| <<virtio_scsi_handle_ctrl_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+ *   - hw/scsi/virtio-scsi.c|862| <<virtio_scsi_handle_cmd_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+ *   - hw/scsi/virtio-scsi.c|1011| <<virtio_scsi_push_event>> req = virtio_scsi_pop_req(s, vs->event_vq);
+ */
 static VirtIOSCSIReq *virtio_scsi_pop_req(VirtIOSCSI *s, VirtQueue *vq)
 {
     VirtIOSCSICommon *vs = (VirtIOSCSICommon *)s;
     VirtIOSCSIReq *req;
 
+    /*
+     * 在以下使用VirtIOSCSICommon->cdb_size:
+     *   - hw/scsi/virtio-scsi.c|973| <<virtio_scsi_set_config>> vs->cdb_size = virtio_ldl_p(vdev, &scsiconf->cdb_size);
+     *   - hw/scsi/virtio-scsi.c|1001| <<virtio_scsi_reset>> vs->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     *   - hw/scsi/virtio-scsi.c|1303| <<virtio_scsi_common_realize>> s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     */
     req = virtqueue_pop(vq, sizeof(VirtIOSCSIReq) + vs->cdb_size);
     if (!req) {
         return NULL;
@@ -536,6 +597,10 @@ fail:
     return ret;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|596| <<virtio_scsi_handle_ctrl_vq>> virtio_scsi_handle_ctrl_req(s, req);
+ */
 static void virtio_scsi_handle_ctrl_req(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     VirtIODevice *vdev = (VirtIODevice *)s;
@@ -628,6 +693,15 @@ static void virtio_scsi_handle_ctrl(VirtIODevice *vdev, VirtQueue *vq)
     virtio_scsi_release(s);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|698| <<virtio_scsi_command_failed>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|724| <<virtio_scsi_command_complete>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|765| <<virtio_scsi_request_cancelled>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|775| <<virtio_scsi_fail_cmd_req>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|813| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_complete_cmd_req(req);
+ *   - hw/scsi/virtio-scsi.c|825| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_complete_cmd_req(req);
+ */
 static void virtio_scsi_complete_cmd_req(VirtIOSCSIReq *req)
 {
     trace_virtio_scsi_cmd_resp(virtio_scsi_get_lun(req->req.cmd.lun),
@@ -753,18 +827,73 @@ static void virtio_scsi_request_cancelled(SCSIRequest *r)
     virtio_scsi_complete_cmd_req(req);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|772| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_fail_cmd_req(req);
+ */
 static void virtio_scsi_fail_cmd_req(VirtIOSCSIReq *req)
 {
     req->resp.cmd.response = VIRTIO_SCSI_S_FAILURE;
     virtio_scsi_complete_cmd_req(req);
 }
 
+/*
+ * struct virtio_scsi_cmd_req {
+ *     uint8_t lun[8];         // Logical Unit Number
+ *     __virtio64 tag;         // Command identifier
+ *     uint8_t task_attr;              // Task attribute
+ *     uint8_t prio;           // SAM command priority field
+ *     uint8_t crn;
+ *     uint8_t cdb[VIRTIO_SCSI_CDB_SIZE];
+ * } QEMU_PACKED;
+ *
+ * struct virtio_scsi_cmd_resp {
+ *     __virtio32 sense_len;           // Sense data length
+ *     __virtio32 resid;               // Residual bytes in data buffer
+ *     __virtio16 status_qualifier;    // Status qualifier
+ *     uint8_t status;         // Command completion status
+ *     uint8_t response;               // Response values
+ *     uint8_t sense[VIRTIO_SCSI_SENSE_SIZE];
+ * } QEMU_PACKED;
+ *
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|831| <<virtio_scsi_handle_cmd_vq>> ret = virtio_scsi_handle_cmd_req_prepare(s, req);
+ */
 static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     VirtIOSCSICommon *vs = &s->parent_obj;
     SCSIDevice *d;
     int rc;
 
+    /*
+     * VirtIOSCSIReq *req:
+     * -> VirtQueueElement elem;
+     *    -> unsigned int index;
+     *    -> unsigned int len;
+     *    -> unsigned int ndescs;
+     *    -> unsigned int out_num;
+     *    -> unsigned int in_num;
+     *    -> hwaddr *in_addr;
+     *    -> hwaddr *out_addr;
+     *    -> struct iovec *in_sg;
+     *    -> struct iovec *out_sg;
+     *
+     * struct iovec {
+     *     void *iov_base;
+     *     size_t iov_len;
+     * };
+     *
+     * 在以下设置vs->cdb_size:
+     *   - hw/scsi/virtio-scsi.c|1048| <<virtio_scsi_set_config>> vs->cdb_size = virtio_ldl_p(vdev, &scsiconf->cdb_size);
+     *   - hw/scsi/virtio-scsi.c|1076| <<virtio_scsi_reset>> vs->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     *   - hw/scsi/virtio-scsi.c|1378| <<virtio_scsi_common_realize>> s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
+     *
+     *
+     * 在以下设置vs->sense_size:
+     *   - hw/scsi/virtio-scsi.c|1056| <<virtio_scsi_set_config>> vs->sense_size = virtio_ldl_p(vdev, &scsiconf->sense_size);
+     *   - hw/scsi/virtio-scsi.c|1084| <<virtio_scsi_reset>> vs->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE;
+     *   - hw/scsi/virtio-scsi.c|1386| <<virtio_scsi_common_realize>> s->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE;
+     */
     rc = virtio_scsi_parse_req(req, sizeof(VirtIOSCSICmdReq) + vs->cdb_size,
                                sizeof(VirtIOSCSICmdResp) + vs->sense_size);
     if (rc < 0) {
@@ -772,6 +901,14 @@ static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
             virtio_scsi_fail_cmd_req(req);
             return -ENOTSUP;
         } else {
+            /*
+	     * called by:
+	     *   - hw/scsi/virtio-scsi.c|547| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|555| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|565| <<virtio_scsi_handle_ctrl_req>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|775| <<virtio_scsi_handle_cmd_req_prepare>> virtio_scsi_bad_req(req);
+	     *   - hw/scsi/virtio-scsi.c|974| <<virtio_scsi_push_event>> virtio_scsi_bad_req(req);
+	     */
             virtio_scsi_bad_req(req);
             return -EINVAL;
         }
@@ -804,6 +941,10 @@ static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|885| <<virtio_scsi_handle_cmd_vq>> virtio_scsi_handle_cmd_req_submit(s, req);
+ */
 static void virtio_scsi_handle_cmd_req_submit(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     SCSIRequest *sreq = req->sreq;
@@ -814,6 +955,10 @@ static void virtio_scsi_handle_cmd_req_submit(VirtIOSCSI *s, VirtIOSCSIReq *req)
     scsi_req_unref(sreq);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|867| <<virtio_scsi_handle_cmd>> virtio_scsi_handle_cmd_vq(s, vq);
+ */
 static void virtio_scsi_handle_cmd_vq(VirtIOSCSI *s, VirtQueue *vq)
 {
     VirtIOSCSIReq *req, *next;
@@ -854,6 +999,16 @@ static void virtio_scsi_handle_cmd_vq(VirtIOSCSI *s, VirtQueue *vq)
     }
 }
 
+/*
+ * 在以下使用virtio_scsi_handle_cmd():
+ *   - hw/scsi/virtio-scsi.c|1280| <<virtio_scsi_device_realize>> virtio_scsi_handle_cmd,
+ *
+ * 1281     virtio_scsi_common_realize(dev,
+ * 1282                                virtio_scsi_handle_ctrl,
+ * 1283                                virtio_scsi_handle_event,
+ * 1284                                virtio_scsi_handle_cmd,
+ * 1285                                &err);
+ */
 static void virtio_scsi_handle_cmd(VirtIODevice *vdev, VirtQueue *vq)
 {
     /* use non-QOM casts in the data path */
@@ -887,6 +1042,10 @@ static void virtio_scsi_get_config(VirtIODevice *vdev,
     virtio_stl_p(vdev, &scsiconf->max_lun, VIRTIO_SCSI_MAX_LUN);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|1493| <<virtio_scsi_class_init>> vdc->set_config = virtio_scsi_set_config;
+ */
 static void virtio_scsi_set_config(VirtIODevice *vdev,
                                    const uint8_t *config)
 {
@@ -945,6 +1104,13 @@ typedef struct {
     };
 } VirtIOSCSIEventInfo;
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|1003| <<virtio_scsi_handle_event_vq>> virtio_scsi_push_event(s, &info);
+ *   - hw/scsi/virtio-scsi.c|1037| <<virtio_scsi_change>> virtio_scsi_push_event(s, &info);
+ *   - hw/scsi/virtio-scsi.c|1082| <<virtio_scsi_hotplug>> virtio_scsi_push_event(s, &info);
+ *   - hw/scsi/virtio-scsi.c|1114| <<virtio_scsi_hotunplug>> virtio_scsi_push_event(s, &info);
+ */
 static void virtio_scsi_push_event(VirtIOSCSI *s,
                                    const VirtIOSCSIEventInfo *info)
 {
@@ -1187,6 +1353,12 @@ static struct SCSIBusInfo virtio_scsi_scsi_info = {
     .drained_end = virtio_scsi_drained_end,
 };
 
+/*
+ * called by:
+ *   - hw/scsi/vhost-scsi.c|195| <<vhost_scsi_realize>> virtio_scsi_common_realize(dev, vhost_dummy_handle_output, vhost_dummy_handle_output, vhost_dummy_handle_output, &err);
+ *   - hw/scsi/vhost-user-scsi.c|106| <<vhost_user_scsi_realize>> virtio_scsi_common_realize(dev, vhost_dummy_handle_output, vhost_dummy_handle_output, vhost_dummy_handle_output, &err);
+ *   - hw/scsi/virtio-scsi.c|1287| <<virtio_scsi_device_realize>> virtio_scsi_common_realize(dev, virtio_scsi_handle_ctrl, virtio_scsi_handle_event, virtio_scsi_handle_cmd, &err);
+ */
 void virtio_scsi_common_realize(DeviceState *dev,
                                 VirtIOHandleOutput ctrl,
                                 VirtIOHandleOutput evt,
@@ -1216,6 +1388,18 @@ void virtio_scsi_common_realize(DeviceState *dev,
                    "must be > 2", s->conf.virtqueue_size);
         return;
     }
+    /*
+     * struct VirtIOSCSICommon {
+     *     VirtIODevice parent_obj;
+     *     VirtIOSCSIConf conf;
+     *
+     *     uint32_t sense_size;
+     *     uint32_t cdb_size;
+     *     VirtQueue *ctrl_vq;
+     *     VirtQueue *event_vq;
+     *     VirtQueue **cmd_vqs;
+     * };
+     */
     s->cmd_vqs = g_new0(VirtQueue *, s->conf.num_queues);
     s->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE;
     s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;
diff --git a/hw/vfio/common.c b/hw/vfio/common.c
index 9aac21abb..bab853252 100644
--- a/hw/vfio/common.c
+++ b/hw/vfio/common.c
@@ -1542,6 +1542,10 @@ static void vfio_device_feature_dma_logging_start_destroy(
     g_free(feature);
 }
 
+/*
+ * called by:
+ *   - hw/vfio/common.c|1593| <<vfio_listener_log_global_start>> ret = vfio_devices_dma_logging_start(container);
+ */
 static int vfio_devices_dma_logging_start(VFIOContainer *container)
 {
     struct vfio_device_feature *feature;
diff --git a/hw/virtio/vhost-scsi-pci.c b/hw/virtio/vhost-scsi-pci.c
index 08980bc23..9b9053494 100644
--- a/hw/virtio/vhost-scsi-pci.c
+++ b/hw/virtio/vhost-scsi-pci.c
@@ -80,12 +80,21 @@ static void vhost_scsi_pci_instance_init(Object *obj)
 {
     VHostSCSIPCI *dev = VHOST_SCSI_PCI(obj);
 
+    /*
+     * 这里是更general的vhost-scsi
+     */
     virtio_instance_init_common(obj, &dev->vdev, sizeof(dev->vdev),
                                 TYPE_VHOST_SCSI);
     object_property_add_alias(obj, "bootindex", OBJECT(&dev->vdev),
                               "bootindex");
 }
 
+/*
+ * struct VHostSCSIPCI {
+ *     VirtIOPCIProxy parent_obj;
+ *     VHostSCSI vdev;
+ * };
+ */
 static const VirtioPCIDeviceTypeInfo vhost_scsi_pci_info = {
     .base_name             = TYPE_VHOST_SCSI_PCI,
     .generic_name          = "vhost-scsi-pci",
diff --git a/hw/virtio/vhost.c b/hw/virtio/vhost.c
index e2f6ffb44..b3ddd8094 100644
--- a/hw/virtio/vhost.c
+++ b/hw/virtio/vhost.c
@@ -1396,6 +1396,23 @@ static void vhost_virtqueue_cleanup(struct vhost_virtqueue *vq)
     }
 }
 
+/*
+ * called by:
+ *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev, options->opaque, options->backend_type, 0, &local_err);
+ *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev, &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/block/vhost-user-blk.c|342| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev, &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/net/vhost_net.c|203| <<vhost_net_init>> r = vhost_dev_init(&net->dev, options->opaque, options->backend_type, options->busyloop_timeout, &local_err);
+ *   - hw/scsi/vhost-scsi.c|276| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev, (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+ *   - hw/scsi/vhost-user-scsi.c|124| <<vhost_user_scsi_realize>> ret = vhost_dev_init(&vsc->dev, &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vdpa-dev.c|119| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev, &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+ *   - hw/virtio/vhost-user-fs.c|252| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev, &fs->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-gpio.c|235| <<vu_gpio_connect>> ret = vhost_dev_init(vhost_dev, &gpio->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-i2c.c|242| <<vu_i2c_device_realize>> ret = vhost_dev_init(&i2c->vhost_dev, &i2c->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-rng.c|219| <<vu_rng_device_realize>> ret = vhost_dev_init(&rng->vhost_dev, &rng->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev, &scmi->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-user-vsock.c|110| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev, &vsock->vhost_user, VHOST_BACKEND_TYPE_USER, 0, errp);
+ *   - hw/virtio/vhost-vsock.c|170| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev, (void *)(uintptr_t)vhostfd, VHOST_BACKEND_TYPE_KERNEL, 0, errp);
+ */
 int vhost_dev_init(struct vhost_dev *hdev, void *opaque,
                    VhostBackendType backend_type, uint32_t busyloop_timeout,
                    Error **errp)
@@ -1409,17 +1426,29 @@ int vhost_dev_init(struct vhost_dev *hdev, void *opaque,
     r = vhost_set_backend_type(hdev, backend_type);
     assert(r >= 0);
 
+    /*
+     * vhost_kernel_init() --> 没见到ioctl
+     */
     r = hdev->vhost_ops->vhost_backend_init(hdev, opaque, errp);
     if (r < 0) {
         goto fail;
     }
 
+    /*
+     * vhost_kernel_set_owner() --> VHOST_SET_OWNER
+     * Set current process as the (exclusive) owner of this file descriptor.  This
+     * must be called before any other vhost command.  Further calls to
+     * VHOST_OWNER_SET fail until VHOST_OWNER_RESET is called.
+     */
     r = hdev->vhost_ops->vhost_set_owner(hdev);
     if (r < 0) {
         error_setg_errno(errp, -r, "vhost_set_owner failed");
         goto fail;
     }
 
+    /*
+     * vhost_kernel_get_features() --> VHOST_GET_FEATURES
+     */
     r = hdev->vhost_ops->vhost_get_features(hdev, &features);
     if (r < 0) {
         error_setg_errno(errp, -r, "vhost_get_features failed");
@@ -1756,6 +1785,10 @@ int vhost_dev_get_config(struct vhost_dev *hdev, uint8_t *config,
     assert(hdev->vhost_ops);
 
     if (hdev->vhost_ops->vhost_get_config) {
+        /*
+	 * vhost_user_get_config()
+	 * vhost_vdpa_get_config()
+	 */
         return hdev->vhost_ops->vhost_get_config(hdev, config, config_len,
                                                  errp);
     }
diff --git a/hw/virtio/virtio-pci.c b/hw/virtio/virtio-pci.c
index edbc0daa1..8fa9b688d 100644
--- a/hw/virtio/virtio-pci.c
+++ b/hw/virtio/virtio-pci.c
@@ -1912,6 +1912,10 @@ static void virtio_pci_pre_plugged(DeviceState *d, Error **errp)
     virtio_add_feature(&vdev->host_features, VIRTIO_F_BAD_FEATURE);
 }
 
+/*
+ * 在以下使用virtio_pci_device_plugged():
+ *   - hw/virtio/virtio-pci.c|2522| <<virtio_pci_bus_class_init>> k->device_plugged = virtio_pci_device_plugged;
+ */
 /* This is called by virtio-bus just after the device is plugged. */
 static void virtio_pci_device_plugged(DeviceState *d, Error **errp)
 {
diff --git a/hw/virtio/virtio.c b/hw/virtio/virtio.c
index 309038fd4..398a3b4c0 100644
--- a/hw/virtio/virtio.c
+++ b/hw/virtio/virtio.c
@@ -1365,6 +1365,16 @@ int virtqueue_avail_bytes(VirtQueue *vq, unsigned int in_bytes,
     return in_bytes <= in_total && out_bytes <= out_total;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1597| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
+ *   - hw/virtio/virtio.c|1606| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
+ *   - hw/virtio/virtio.c|1731| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
+ *   - hw/virtio/virtio.c|1740| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
+ *   - subprojects/libvhost-user/libvhost-user.c|2524| <<virtqueue_map_desc>> virtqueue_map_desc(VuDev *dev,
+ *   - subprojects/libvhost-user/libvhost-user.c|2628| <<vu_queue_map_desc>> if (!virtqueue_map_desc(dev, &in_num, iov + out_num,
+ *   - subprojects/libvhost-user/libvhost-user.c|2639| <<vu_queue_map_desc>> if (!virtqueue_map_desc(dev, &out_num, iov,
+ */
 static bool virtqueue_map_desc(VirtIODevice *vdev, unsigned int *p_num_sg,
                                hwaddr *addr, struct iovec *iov,
                                unsigned int max_num_sg, bool is_write,
@@ -1461,9 +1471,28 @@ void virtqueue_map(VirtIODevice *vdev, VirtQueueElement *elem)
                                                                         false);
 }
 
+/*
+ * typedef struct VirtQueueElement
+ * {
+ *     unsigned int index;
+ *     unsigned int len;
+ *     unsigned int ndescs;
+ *     unsigned int out_num;
+ *     unsigned int in_num;
+ *     hwaddr *in_addr;
+ *     hwaddr *out_addr;
+ *     struct iovec *in_sg;
+ *     struct iovec *out_sg;
+ * } VirtQueueElement;
+ *
+ * sz可以是sizeof(VirtIOSCSIReq) + vs->cdb_size
+ */
 static void *virtqueue_alloc_element(size_t sz, unsigned out_num, unsigned in_num)
 {
     VirtQueueElement *elem;
+    /*
+     * 注意这里的sz !!!
+     */
     size_t in_addr_ofs = QEMU_ALIGN_UP(sz, __alignof__(elem->in_addr[0]));
     size_t out_addr_ofs = in_addr_ofs + in_num * sizeof(elem->in_addr[0]);
     size_t out_addr_end = out_addr_ofs + out_num * sizeof(elem->out_addr[0]);
@@ -1483,6 +1512,9 @@ static void *virtqueue_alloc_element(size_t sz, unsigned out_num, unsigned in_nu
     return elem;
 }
 
+/*
+ * 一个例子: sz可以是sizeof(VirtIOSCSIReq) + vs->cdb_size
+ */
 static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
 {
     unsigned int i, head, max;
@@ -1538,6 +1570,9 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
     }
 
     desc_cache = &caches->desc;
+    /*
+     * VRingDesc desc;
+     */
     vring_split_desc_read(vdev, &desc, desc_cache, i);
     if (desc.flags & VRING_DESC_F_INDIRECT) {
         if (!desc.len || (desc.len % sizeof(VRingDesc))) {
@@ -1564,6 +1599,11 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         bool map_ok;
 
         if (desc.flags & VRING_DESC_F_WRITE) {
+            /*
+	     * desc.addr是VM的物理地址
+	     *
+	     * hwaddr addr[VIRTQUEUE_MAX_SIZE];
+	     */
             map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
                                         iov + out_num,
                                         VIRTQUEUE_MAX_SIZE - out_num, true,
@@ -1594,11 +1634,17 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         goto err_undo_map;
     }
 
+    /*
+     * sz可以是sizeof(VirtIOSCSIReq) + vs->cdb_size
+     */
     /* Now copy what we have collected and mapped */
     elem = virtqueue_alloc_element(sz, out_num, in_num);
     elem->index = head;
     elem->ndescs = 1;
     for (i = 0; i < out_num; i++) {
+        /*
+	 * 这里应该是copy, 不是指针
+	 */
         elem->out_addr[i] = addr[i];
         elem->out_sg[i] = iov[i];
     }
@@ -1887,6 +1933,12 @@ typedef struct VirtQueueElementOld {
     struct iovec out_sg[VIRTQUEUE_MAX_SIZE];
 } VirtQueueElementOld;
 
+/*
+ * called by:
+ *   - hw/block/virtio-blk.c|1480| <<virtio_blk_load_device>> req = qemu_get_virtqueue_element(vdev, f, sizeof(VirtIOBlockReq));
+ *   - hw/char/virtio-serial-bus.c|785| <<fetch_active_ports_list>> qemu_get_virtqueue_element(vdev, f, sizeof(VirtQueueElement));
+ *   - hw/scsi/virtio-scsi.c|301| <<virtio_scsi_load_request>> req = qemu_get_virtqueue_element(vdev, f, sizeof(VirtIOSCSIReq) + vs->cdb_size);
+ */
 void *qemu_get_virtqueue_element(VirtIODevice *vdev, QEMUFile *f, size_t sz)
 {
     VirtQueueElement *elem;
@@ -2007,6 +2059,19 @@ static int virtio_validate_features(VirtIODevice *vdev)
     }
 }
 
+/*
+ * called by:
+ *   - hw/block/vhost-user-blk.c|525| <<vhost_user_blk_device_unrealize>> virtio_set_status(vdev, 0);
+ *   - hw/s390x/virtio-ccw.c|536| <<virtio_ccw_cb>> if (virtio_set_status(vdev, status) == 0) {
+ *   - hw/virtio/vdpa-dev.c|176| <<vhost_vdpa_device_unrealize>> virtio_set_status(vdev, 0);
+ *   - hw/virtio/virtio-mmio.c|429| <<virtio_mmio_write>> virtio_set_status(vdev, value & 0xff);
+ *   - hw/virtio/virtio-pci.c|431| <<virtio_ioport_write>> virtio_set_status(vdev, val & 0xFF);
+ *   - hw/virtio/virtio-pci.c|772| <<virtio_write_config>> virtio_set_status(vdev, vdev->status & ~VIRTIO_CONFIG_S_DRIVER_OK);
+ *   - hw/virtio/virtio-pci.c|1573| <<virtio_pci_common_write>> virtio_set_status(vdev, val & 0xFF);
+ *   - hw/virtio/virtio.c|2167| <<virtio_reset>> virtio_set_status(vdev, 0);
+ *   - hw/virtio/virtio.c|3181| <<virtio_vmstate_change>> virtio_set_status(vdev, vdev->status);
+ *   - hw/virtio/virtio.c|3189| <<virtio_vmstate_change>> virtio_set_status(vdev, vdev->status);
+ */
 int virtio_set_status(VirtIODevice *vdev, uint8_t val)
 {
     VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);
@@ -2315,6 +2380,10 @@ VirtQueue *virtio_add_queue(VirtIODevice *vdev, int queue_size,
     vdev->vq[i].handle_output = handle_output;
     vdev->vq[i].used_elems = g_new0(VirtQueueElement, queue_size);
 
+    /*
+     * VirtIODevice *vdev:
+     * -> VirtQueue *vq;
+     */
     return &vdev->vq[i];
 }
 
diff --git a/include/hw/core/cpu.h b/include/hw/core/cpu.h
index fdcbe8735..54221cc23 100644
--- a/include/hw/core/cpu.h
+++ b/include/hw/core/cpu.h
@@ -402,6 +402,16 @@ struct CPUState {
     uint64_t dirty_pages;
     int kvm_vcpu_stats_fd;
 
+    /*
+     * 在以下使用CPUState->in_ioctl_lock:
+     *   - accel/accel-blocker.c|116| <<accel_cpu_ioctl_begin>> qemu_lockcnt_inc(&cpu->in_ioctl_lock);
+     *   - accel/accel-blocker.c|129| <<accel_cpu_ioctl_end>> qemu_lockcnt_dec(&cpu->in_ioctl_lock);
+     *   - accel/accel-blocker.c|144| <<accel_has_to_wait>> if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
+     *   - accel/accel-blocker.c|170| <<accel_ioctl_inhibit_begin>> qemu_lockcnt_lock(&cpu->in_ioctl_lock);
+     *   - accel/accel-blocker.c|217| <<accel_ioctl_inhibit_end>> qemu_lockcnt_unlock(&cpu->in_ioctl_lock);
+     *   - hw/core/cpu-common.c|238| <<cpu_common_initfn>> qemu_lockcnt_init(&cpu->in_ioctl_lock);
+     *   - hw/core/cpu-common.c|250| <<cpu_common_finalize>> qemu_lockcnt_destroy(&cpu->in_ioctl_lock);
+     */
     /* Use by accel-block: CPU is executing an ioctl() */
     QemuLockCnt in_ioctl_lock;
 
diff --git a/include/qemu/futex.h b/include/qemu/futex.h
index 91ae88966..af55aefb4 100644
--- a/include/qemu/futex.h
+++ b/include/qemu/futex.h
@@ -19,11 +19,29 @@
 
 #define qemu_futex(...)              syscall(__NR_futex, __VA_ARGS__)
 
+/*
+ * called by:
+ *   - tests/unit/test-aio-multithread.c|331| <<mcs_mutex_unlock>> qemu_futex_wake(&nodes[next].locked, 1);
+ *   - util/lockcnt.c|116| <<lockcnt_wake>> qemu_futex_wake(&lockcnt->count, 1);
+ *   - util/qemu-thread-posix.c|413| <<qemu_event_set>> qemu_futex_wake(ev, INT_MAX);
+ *
+ * FUTEX_WAIT: 如果futex word中仍然保存着参数val给定的值,
+ * 那么当前线程则进入睡眠,等待FUTEX_WAKE的操作唤醒它.
+ *
+ * FUTEX_WAKE: 最多唤醒val个等待在futex word上的线程.
+ * Val或者等于1(唤醒1个等待线程)或者等于INT_MAX(唤醒全部等待线程)
+ */
 static inline void qemu_futex_wake(void *f, int n)
 {
     qemu_futex(f, FUTEX_WAKE, n, NULL, NULL, 0);
 }
 
+/*
+ * called by:
+ *   - tests/unit/test-aio-multithread.c|308| <<mcs_mutex_lock>> qemu_futex_wait(&nodes[id].locked, 1);
+ *   - util/lockcnt.c|102| <<qemu_lockcnt_cmpxchg_or_wait>> qemu_futex_wait(&lockcnt->count, *val);
+ *   - util/qemu-thread-posix.c|474| <<qemu_event_wait>> qemu_futex_wait(ev, EV_BUSY);
+ */
 static inline void qemu_futex_wait(void *f, unsigned val)
 {
     while (qemu_futex(f, FUTEX_WAIT, (int) val, NULL, NULL, 0)) {
diff --git a/migration/channel.c b/migration/channel.c
index ca3319a30..f8ed3eecc 100644
--- a/migration/channel.c
+++ b/migration/channel.c
@@ -59,6 +59,13 @@ void migration_channel_process_incoming(QIOChannel *ioc)
  * @hostname: Where we want to connect
  * @error: Error indicating failure to connect, free'd here
  */
+/*
+ * called by:
+ *   - migration/exec.c|62| <<exec_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+ *   - migration/fd.c|42| <<fd_start_outgoing_migration>> migration_channel_connect(s, ioc, NULL, NULL);
+ *   - migration/socket.c|107| <<socket_outgoing_migration>> migration_channel_connect(data->s, sioc, data->hostname, err);
+ *   - migration/tls.c|113| <<migration_tls_outgoing_handshake>> migration_channel_connect(s, ioc, NULL, err);
+ */
 void migration_channel_connect(MigrationState *s,
                                QIOChannel *ioc,
                                const char *hostname,
diff --git a/migration/migration-stats.c b/migration/migration-stats.c
index 095d6d75b..b1c94341b 100644
--- a/migration/migration-stats.c
+++ b/migration/migration-stats.c
@@ -18,15 +18,43 @@
 
 MigrationAtomicStats mig_stats;
 
+/*
+ * (gdb) bt
+ * #0  migration_rate_exceeded (f=f@entry=0x55718cc4c630) at ../migration/migration-stats.c:23
+ * #1  0x000055718a2e6546 in ram_save_iterate (f=0x55718cc4c630, opaque=<optimized out>) at ../migration/ram.c:3133
+ * #2  0x000055718a190939 in qemu_savevm_state_iterate (f=0x55718cc4c630, postcopy=postcopy@entry=false) at ../migration/savevm.c:1348
+ * #3  0x000055718a180fb1 in migration_iteration_run (s=0x55718c6d51c0) at ../migration/migration.c:2785
+ * #4  migration_thread (opaque=opaque@entry=0x55718c6d51c0) at ../migration/migration.c:3017
+ * #5  0x000055718a4b40d9 in qemu_thread_start (args=0x7f680ad0e8d0) at ../util/qemu-thread-posix.c:541
+ * #6  0x00007f69c3936ea5 in start_thread () at /lib64/libpthread.so.0
+ * #7  0x00007f69c365f9fd in clone () at /lib64/libc.so.6
+ *
+ * called by:
+ *   - hw/ppc/spapr.c|2175| <<htab_save_first_pass>> } while ((index < htabslots) && !migration_rate_exceeded(f));
+ *   - hw/ppc/spapr.c|2246| <<htab_save_later_pass>> } while ((examined < htabslots) && (!migration_rate_exceeded(f) || final));
+ *   - hw/s390x/s390-stattrib.c|212| <<cmma_save>> while (final ? 1 : migration_rate_exceeded(f) == 0) {
+ *   - migration/block-dirty-bitmap.c|709| <<bulk_phase>> if (limit && migration_rate_exceeded(f)) {
+ *   - migration/block.c|629| <<flush_blks>> if (migration_rate_exceeded(f)) {
+ *   - migration/migration.c|2894| <<migration_rate_limit>> if (migration_rate_exceeded(s->to_dst_file)) {
+ *   - migration/migration.c|3016| <<migration_thread>> if (urgent || !migration_rate_exceeded(s->to_dst_file)) {
+ *   - migration/ram.c|3133| <<ram_save_iterate>> while ((ret = migration_rate_exceeded(f)) == 0 ||
+ *   - migration/savevm.c|1341| <<qemu_savevm_state_iterate>> if (migration_rate_exceeded(f)) {
+ */
 bool migration_rate_exceeded(QEMUFile *f)
 {
     if (qemu_file_get_error(f)) {
         return true;
     }
 
+    /*
+     * Amount of transferred data at the start of current cycle.
+     */
     uint64_t rate_limit_start = stat64_get(&mig_stats.rate_limit_start);
     uint64_t rate_limit_current = migration_transferred_bytes(f);
     uint64_t rate_limit_used = rate_limit_current - rate_limit_start;
+    /*
+     * Maximum amount of data we can send in a cycle.
+     */
     uint64_t rate_limit_max = stat64_get(&mig_stats.rate_limit_max);
 
     if (rate_limit_max == RATE_LIMIT_DISABLED) {
@@ -53,6 +81,10 @@ void migration_rate_set(uint64_t limit)
     stat64_set(&mig_stats.rate_limit_max, limit / XFER_LIMIT_RATIO);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2717| <<migration_update_counters>> migration_rate_reset(s->to_dst_file);
+ */
 void migration_rate_reset(QEMUFile *f)
 {
     stat64_set(&mig_stats.rate_limit_start, migration_transferred_bytes(f));
diff --git a/migration/migration.c b/migration/migration.c
index 5528acb65..c972b9719 100644
--- a/migration/migration.c
+++ b/migration/migration.c
@@ -670,6 +670,10 @@ static bool migration_should_start_incoming(bool main_channel)
     return true;
 }
 
+/*
+ * called by:
+ *   - migration/channel.c|45| <<migration_channel_process_incoming>> migration_ioc_process_incoming(ioc, &local_err);
+ */
 void migration_ioc_process_incoming(QIOChannel *ioc, Error **errp)
 {
     MigrationIncomingState *mis = migration_incoming_get_current();
@@ -893,6 +897,11 @@ static bool migrate_show_downtime(MigrationState *s)
     return (s->state == MIGRATION_STATUS_COMPLETED) || migration_in_postcopy();
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1039| <<fill_source_migration_info>> populate_time_info(info, s);
+ *   - migration/migration.c|1049| <<fill_source_migration_info>> populate_time_info(info, s);
+ */
 static void populate_time_info(MigrationInfo *info, MigrationState *s)
 {
     info->has_status = true;
@@ -996,6 +1005,10 @@ static void populate_disk_info(MigrationInfo *info)
     }
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1106| <<qmp_query_migrate>> fill_source_migration_info(info);
+ */
 static void fill_source_migration_info(MigrationInfo *info)
 {
     MigrationState *s = migrate_get_current();
@@ -2307,6 +2320,10 @@ static int migration_maybe_pause(MigrationState *s,
  *
  * @s: Current migration state
  */
+/*
+ * called by:
+ *   - migration/migration.c|2775| <<migration_iteration_run>> migration_completion(s);
+ */
 static void migration_completion(MigrationState *s)
 {
     int ret;
@@ -2679,6 +2696,11 @@ static void update_iteration_initial_status(MigrationState *s)
     s->iteration_initial_pages = ram_get_total_transferred_pages();
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2893| <<migration_rate_limit>> migration_update_counters(s, now);
+ *   - migration/migration.c|3191| <<bg_migration_thread>> migration_update_counters(s, qemu_clock_get_ms(QEMU_CLOCK_REALTIME));
+ */
 static void migration_update_counters(MigrationState *s,
                                       int64_t current_time)
 {
@@ -2747,6 +2769,14 @@ typedef enum {
  * Return true if continue to the next iteration directly, false
  * otherwise.
  */
+/*
+ * (gdb) bt
+ * #0  migration_iteration_run (s=s@entry=0x55e44417c1c0) at ../migration/migration.c:2751
+ * #1  0x000055e4428a7ef7 in migration_thread (opaque=opaque@entry=0x55e44417c1c0) at ../migration/migration.c:3022
+ * #2  0x000055e442bda0e9 in qemu_thread_start (args=0x7f362623c8d0) at ../util/qemu-thread-posix.c:541
+ * #3  0x00007f362c9e3ea5 in start_thread () at /lib64/libpthread.so.0
+ * #4  0x00007f362c70c9fd in clone () at /lib64/libc.so.6
+ */
 static MigIterateState migration_iteration_run(MigrationState *s)
 {
     uint64_t must_precopy, can_postcopy;
@@ -2765,6 +2795,16 @@ static MigIterateState migration_iteration_run(MigrationState *s)
         trace_migrate_pending_exact(pending_size, must_precopy, can_postcopy);
     }
 
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1426| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|2702| <<migration_update_counters>> s->threshold_size = bandwidth * migrate_downtime_limit();
+     *   - migration/migration.c|2727| <<migration_update_counters>> bandwidth, s->threshold_size);
+     *   - migration/migration.c|2767| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|2773| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|2780| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover &&
+     *   - migration/ram.c|3302| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     */
     if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
         trace_migration_thread_low_pending(pending_size);
         migration_completion(s);
@@ -2883,6 +2923,11 @@ void migration_consume_urgent_request(void)
     qemu_sem_wait(&migrate_get_current()->rate_limit_sem);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3042| <<migration_thread>> urgent = migration_rate_limit();
+ *   - migration/ram.c|2333| <<ram_save_host_page>> migration_rate_limit();
+ */
 /* Returns true if the rate limiting was broken by an urgent request */
 bool migration_rate_limit(void)
 {
@@ -2960,6 +3005,10 @@ static void qemu_savevm_wait_unplug(MigrationState *s, int old_state,
  * Master migration thread on the source VM.
  * It drives the migration and pumps the data down the outgoing channel.
  */
+/*
+ * 在以下使用migration_thread():
+ *   - migration/migration.c|3366| <<migrate_fd_connect>> qemu_thread_create(&s->thread, "live_migration", migration_thread, s, QEMU_THREAD_JOINABLE);
+ */
 static void *migration_thread(void *opaque)
 {
     MigrationState *s = opaque;
@@ -3050,6 +3099,17 @@ static void *migration_thread(void *opaque)
     return NULL;
 }
 
+/*
+ * 在以下使用MigrationState->vm_start_bh:
+ *   - migration/migration.c|1403| <<migrate_init>> s->vm_start_bh = 0;
+ *   - migration/migration.c|3089| <<bg_migration_vm_start_bh>> qemu_bh_delete(s->vm_start_bh);
+ *   - migration/migration.c|3090| <<bg_migration_vm_start_bh>> s->vm_start_bh = NULL;
+ *   - migration/migration.c|3200| <<bg_migration_thread>> s->vm_start_bh = qemu_bh_new(bg_migration_vm_start_bh, s);
+ *   - migration/migration.c|3201| <<bg_migration_thread>> qemu_bh_schedule(s->vm_start_bh);
+ *
+ * 在以下使用bg_migration_vm_start_bh():
+ *   - migration/migration.c|3200| <<bg_migration_thread>> s->vm_start_bh = qemu_bh_new(bg_migration_vm_start_bh, s);
+ */
 static void bg_migration_vm_start_bh(void *opaque)
 {
     MigrationState *s = opaque;
@@ -3209,6 +3269,11 @@ fail:
     return NULL;
 }
 
+/*
+ * called by:
+ *   - migration/channel.c|92| <<migration_channel_connect>> migrate_fd_connect(s, error);
+ *   - migration/rdma.c|4227| <<rdma_start_outgoing_migration>> migrate_fd_connect(s, NULL);
+ */
 void migrate_fd_connect(MigrationState *s, Error *error_in)
 {
     Error *local_err = NULL;
@@ -3293,6 +3358,9 @@ void migrate_fd_connect(MigrationState *s, Error *error_in)
         return;
     }
 
+    /*
+     * 这里是multifd!!!!?????
+     */
     if (multifd_save_setup(&local_err) != 0) {
         migrate_set_error(s, local_err);
         error_report_err(local_err);
diff --git a/migration/migration.h b/migration/migration.h
index 6eea18db3..26db71d6e 100644
--- a/migration/migration.h
+++ b/migration/migration.h
@@ -244,6 +244,14 @@ struct MigrationState {
 
     /*< public >*/
     QemuThread thread;
+    /*
+     * 在以下使用MigrationState->vm_start_bh:
+     *   - migration/migration.c|1403| <<migrate_init>> s->vm_start_bh = 0;
+     *   - migration/migration.c|3089| <<bg_migration_vm_start_bh>> qemu_bh_delete(s->vm_start_bh);
+     *   - migration/migration.c|3090| <<bg_migration_vm_start_bh>> s->vm_start_bh = NULL;
+     *   - migration/migration.c|3200| <<bg_migration_thread>> s->vm_start_bh = qemu_bh_new(bg_migration_vm_start_bh, s);
+     *   - migration/migration.c|3201| <<bg_migration_thread>> qemu_bh_schedule(s->vm_start_bh);
+     */
     QEMUBH *vm_start_bh;
     QEMUBH *cleanup_bh;
     /* Protected by qemu_file_lock */
@@ -285,6 +293,16 @@ struct MigrationState {
      * this threshold; it's calculated from the requested downtime and
      * measured bandwidth
      */
+    /*
+     * 在以下使用MigrationState->threshold_size:
+     *   - migration/migration.c|1426| <<migrate_init>> s->threshold_size = 0;
+     *   - migration/migration.c|2702| <<migration_update_counters>> s->threshold_size = bandwidth * migrate_downtime_limit();
+     *   - migration/migration.c|2727| <<migration_update_counters>> bandwidth, s->threshold_size);
+     *   - migration/migration.c|2767| <<migration_iteration_run>> if (must_precopy <= s->threshold_size) {
+     *   - migration/migration.c|2773| <<migration_iteration_run>> if ((!pending_size || pending_size < s->threshold_size) && can_switchover) {
+     *   - migration/migration.c|2780| <<migration_iteration_run>> if (!in_postcopy && must_precopy <= s->threshold_size && can_switchover &&
+     *   - migration/ram.c|3302| <<ram_state_pending_exact>> if (!migration_in_postcopy() && remaining_size < s->threshold_size) {
+     */
     int64_t threshold_size;
 
     /* params from 'migrate-set-parameters' */
diff --git a/migration/multifd.c b/migration/multifd.c
index 0f6b20387..a595a29f9 100644
--- a/migration/multifd.c
+++ b/migration/multifd.c
@@ -392,6 +392,11 @@ struct {
  * false.
  */
 
+/*
+ * called by:
+ *   - migration/multifd.c|461| <<multifd_queue_page>> if (multifd_send_pages(f) < 0) {
+ *   - migration/multifd.c|592| <<multifd_send_sync_main>> if (multifd_send_pages(f) < 0) {
+ */
 static int multifd_send_pages(QEMUFile *f)
 {
     int i;
@@ -580,6 +585,13 @@ static int multifd_zero_copy_flush(QIOChannel *c)
     return ret;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1404| <<find_dirty_block>> int ret = multifd_send_sync_main(f);
+ *   - migration/ram.c|3074| <<ram_save_setup>> ret = multifd_send_sync_main(f);
+ *   - migration/ram.c|3192| <<ram_save_iterate>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+ *   - migration/ram.c|3263| <<ram_save_complete>> ret = multifd_send_sync_main(rs->pss[RAM_CHANNEL_PRECOPY].pss_channel);
+ */
 int multifd_send_sync_main(QEMUFile *f)
 {
     int i;
@@ -643,6 +655,10 @@ int multifd_send_sync_main(QEMUFile *f)
     return 0;
 }
 
+/*
+ * 在以下使用multifd_send_thread():
+ *   - migration/multifd.c|883| <<multifd_channel_connect>> qemu_thread_create(&p->thread, p->name, multifd_send_thread, p, QEMU_THREAD_JOINABLE);
+ */
 static void *multifd_send_thread(void *opaque)
 {
     MultiFDSendParams *p = opaque;
@@ -836,6 +852,11 @@ static void multifd_tls_channel_connect(MultiFDSendParams *p,
                        QEMU_THREAD_JOINABLE);
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|793| <<multifd_tls_outgoing_handshake>> if (!multifd_channel_connect(p, ioc, err)) {
+ *   - migration/multifd.c|900| <<multifd_new_send_channel_async>> if (multifd_channel_connect(p, sioc, local_err)) {
+ */
 static bool multifd_channel_connect(MultiFDSendParams *p,
                                     QIOChannel *ioc,
                                     Error *error)
@@ -886,6 +907,10 @@ static void multifd_new_send_channel_cleanup(MultiFDSendParams *p,
      error_free(err);
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|971| <<multifd_save_setup>> socket_send_channel_create(multifd_new_send_channel_async, p);
+ */
 static void multifd_new_send_channel_async(QIOTask *task, gpointer opaque)
 {
     MultiFDSendParams *p = opaque;
@@ -905,6 +930,10 @@ static void multifd_new_send_channel_async(QIOTask *task, gpointer opaque)
     multifd_new_send_channel_cleanup(p, sioc, local_err);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3361| <<migrate_fd_connect>> if (multifd_save_setup(&local_err) != 0) {
+ */
 int multifd_save_setup(Error **errp)
 {
     int thread_count;
@@ -1095,6 +1124,10 @@ void multifd_recv_sync_main(void)
     trace_multifd_recv_sync_main(multifd_recv_state->packet_num);
 }
 
+/*
+ * called by:
+ *   - migration/multifd.c|1266| <<multifd_recv_new_channel>> qemu_thread_create(&p->thread, p->name, multifd_recv_thread, p, QEMU_THREAD_JOINABLE);
+ */
 static void *multifd_recv_thread(void *opaque)
 {
     MultiFDRecvParams *p = opaque;
@@ -1232,6 +1265,10 @@ bool multifd_recv_all_channels_created(void)
  * Try to receive all multifd channels to get ready for the migration.
  * Sets @errp when failing to receive the current channel.
  */
+/*
+ * called by:
+ *   - migration/migration.c|722| <<migration_ioc_process_incoming>> multifd_recv_new_channel(ioc, &local_err);
+ */
 void multifd_recv_new_channel(QIOChannel *ioc, Error **errp)
 {
     MultiFDRecvParams *p;
diff --git a/migration/options.c b/migration/options.c
index 1d1e1321b..3b7abb7e1 100644
--- a/migration/options.c
+++ b/migration/options.c
@@ -271,6 +271,24 @@ bool migrate_late_block_activate(void)
     return s->capabilities[MIGRATION_CAPABILITY_LATE_BLOCK_ACTIVATE];
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|104| <<migration_needs_multiple_sockets>> return migrate_multifd() || migrate_postcopy_preempt();
+ *   - migration/migration.c|655| <<migration_should_start_incoming>> if (migrate_multifd()) {
+ *   - migration/migration.c|686| <<migration_ioc_process_incoming>> if (migrate_multifd() && !migrate_postcopy_ram() &&
+ *   - migration/migration.c|725| <<migration_ioc_process_incoming>> if (migrate_multifd()) {
+ *   - migration/migration.c|761| <<migration_has_all_channels>> if (migrate_multifd()) {
+ *   - migration/multifd.c|522| <<multifd_save_cleanup>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|593| <<multifd_send_sync_main>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|924| <<multifd_save_setup>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1029| <<multifd_load_shutdown>> if (migrate_multifd()) {
+ *   - migration/multifd.c|1038| <<multifd_load_cleanup>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1085| <<multifd_recv_sync_main>> if (!migrate_multifd()) {
+ *   - migration/multifd.c|1187| <<multifd_load_setup>> if (multifd_recv_state || !migrate_multifd()) {
+ *   - migration/multifd.c|1233| <<multifd_recv_all_channels_created>> if (!migrate_multifd()) {
+ *   - migration/ram.c|2163| <<ram_save_target_page_legacy>> if (migrate_multifd() && !migration_in_postcopy()) {
+ *   - migration/socket.c|186| <<socket_start_incoming_migration_internal>> if (migrate_multifd()) {
+ */
 bool migrate_multifd(void)
 {
     MigrationState *s = migrate_get_current();
diff --git a/migration/savevm.c b/migration/savevm.c
index a2cb8855e..57c8f3af9 100644
--- a/migration/savevm.c
+++ b/migration/savevm.c
@@ -1310,6 +1310,12 @@ int qemu_savevm_state_resume_prepare(MigrationState *s)
  *   0 : We haven't finished, caller have to go again
  *   1 : We have finished, we can go to complete phase
  */
+/*
+ * called by:
+ *   - migration/migration.c|2825| <<migration_iteration_run>> qemu_savevm_state_iterate(s->to_dst_file, in_postcopy);
+ *   - migration/migration.c|2907| <<bg_migration_iteration_run>> res = qemu_savevm_state_iterate(s->to_dst_file, false);
+ *   - migration/savevm.c|1634| <<qemu_savevm_state>> if (qemu_savevm_state_iterate(f, false) > 0) {
+ */
 int qemu_savevm_state_iterate(QEMUFile *f, bool postcopy)
 {
     SaveStateEntry *se;
diff --git a/migration/socket.c b/migration/socket.c
index 1b6f5baef..c0822d0a7 100644
--- a/migration/socket.c
+++ b/migration/socket.c
@@ -84,6 +84,10 @@ static void socket_connect_data_free(void *opaque)
     g_free(data);
 }
 
+/*
+ * called by:
+ *   - migration/socket.c|132| <<socket_start_outgoing_migration_internal>> qio_channel_socket_connect_async(sioc, saddr, socket_outgoing_migration, data, socket_connect_data_free, NULL);
+ */
 static void socket_outgoing_migration(QIOTask *task,
                                       gpointer opaque)
 {
@@ -108,6 +112,10 @@ out:
     object_unref(OBJECT(sioc));
 }
 
+/*
+ * called by:
+ *   - migration/socket.c|145| <<socket_start_outgoing_migration>> socket_start_outgoing_migration_internal(s, saddr, &err);
+ */
 static void
 socket_start_outgoing_migration_internal(MigrationState *s,
                                          SocketAddress *saddr,
@@ -135,6 +143,10 @@ socket_start_outgoing_migration_internal(MigrationState *s,
                                      NULL);
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|1692| <<qmp_migrate>> socket_start_outgoing_migration(s, p ? p : uri, &local_err);
+ */
 void socket_start_outgoing_migration(MigrationState *s,
                                      const char *str,
                                      Error **errp)
diff --git a/monitor/fds.c b/monitor/fds.c
index d86c2c674..953bf342b 100644
--- a/monitor/fds.c
+++ b/monitor/fds.c
@@ -494,6 +494,21 @@ void monitor_fdset_dup_fd_remove(int dup_fd)
     monitor_fdset_dup_fd_find_remove(dup_fd, true);
 }
 
+/*
+ * called by:
+ *   - hw/remote/proxy.c|90| <<pci_proxy_dev_realize>> fd = monitor_fd_param(monitor_cur(), dev->fd, errp);
+ *   - hw/remote/remote-obj.c|55| <<remote_object_set_fd>> fd = monitor_fd_param(monitor_cur(), str, errp);
+ *   - hw/scsi/vhost-scsi.c|223| <<vhost_scsi_realize>> vhostfd = monitor_fd_param(monitor_cur(), vs->conf.vhostfd, errp);
+ *   - hw/virtio/vhost-vsock.c|142| <<vhost_vsock_device_realize>> vhostfd = monitor_fd_param(monitor_cur(), vsock->conf.vhostfd, errp);
+ *   - migration/fd.c|58| <<fd_start_incoming_migration>> int fd = monitor_fd_param(monitor_cur(), fdname, errp);
+ *   - net/dgram.c|330| <<net_dgram_mcast_init>> fd = monitor_fd_param(monitor_cur(), local->u.fd.str, errp);
+ *   - net/dgram.c|561| <<net_init_dgram>> fd = monitor_fd_param(monitor_cur(), local->u.fd.str, errp);
+ *   - net/socket.c|711| <<net_init_socket>> fd = monitor_fd_param(monitor_cur(), sock->fd, errp);
+ *   - net/tap.c|731| <<net_init_tap_one>> vhostfd = monitor_fd_param(monitor_cur(), vhostfdname, &err);
+ *   - net/tap.c|848| <<net_init_tap>> fd = monitor_fd_param(monitor_cur(), tap->fd, errp);
+ *   - net/tap.c|903| <<net_init_tap>> fd = monitor_fd_param(monitor_cur(), fds[i], errp);
+ *   - net/vhost-vdpa.c|1491| <<net_init_vhost_vdpa>> vdpa_device_fd = monitor_fd_param(monitor_cur(), opts->vhostfd, errp);
+ */
 int monitor_fd_param(Monitor *mon, const char *fdname, Error **errp)
 {
     int fd;
diff --git a/softmmu/balloon.c b/softmmu/balloon.c
index e0e8969a4..b889ef84b 100644
--- a/softmmu/balloon.c
+++ b/softmmu/balloon.c
@@ -52,6 +52,10 @@ static bool have_balloon(Error **errp)
     return true;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio-balloon.c|863| <<virtio_balloon_device_realize>> ret = qemu_add_balloon_handler(virtio_balloon_to_target, virtio_balloon_stat, s);
+ */
 int qemu_add_balloon_handler(QEMUBalloonEvent *event_func,
                              QEMUBalloonStatus *stat_func, void *opaque)
 {
diff --git a/softmmu/dma-helpers.c b/softmmu/dma-helpers.c
index 246396480..370819362 100644
--- a/softmmu/dma-helpers.c
+++ b/softmmu/dma-helpers.c
@@ -219,6 +219,15 @@ static const AIOCBInfo dma_aiocb_info = {
     .get_aio_context    = dma_get_aio_context,
 };
 
+/*
+ * called by:
+ *   - hw/ide/core.c|959| <<ide_dma_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk),
+ *   - hw/ide/macio.c|193| <<pmac_ide_transfer_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk), &s->sg,
+ *   - hw/scsi/scsi-disk.c|429| <<scsi_do_read>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *   - hw/scsi/scsi-disk.c|591| <<scsi_write_data>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *   - softmmu/dma-helpers.c|262| <<dma_blk_read>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ *   - softmmu/dma-helpers.c|280| <<dma_blk_write>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ */
 BlockAIOCB *dma_blk_io(AioContext *ctx,
     QEMUSGList *sg, uint64_t offset, uint32_t align,
     DMAIOFunc *io_func, void *io_func_opaque,
diff --git a/softmmu/vl.c b/softmmu/vl.c
index b0b96f67f..fa9055284 100644
--- a/softmmu/vl.c
+++ b/softmmu/vl.c
@@ -2630,6 +2630,34 @@ static void qemu_machine_creation_done(void)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  0x0000555555863ee0 in cpu_list_add (cpu=0x555556f6f8e0) at /home/opc/ext4/qemu/include/qemu/thread.h:122
+ * #1  0x0000555555beba88 in cpu_exec_realizefn (cpu=cpu@entry=0x555556f6f8e0, errp=errp@entry=0x7fffffffd720) at ../cpu-target.c:143
+ * #2  0x0000555555b4376f in x86_cpu_realizefn (dev=0x555556f6f8e0, errp=0x7fffffffd780) at ../target/i386/cpu.c:7308
+ * #3  0x0000555555c5ac2e in device_set_realized (obj=<optimized out>, value=<optimized out>, errp=0x7fffffffd800) at ../hw/core/qdev.c:510
+ * #4  0x0000555555c5e976 in property_set_bool (obj=0x555556f6f8e0, v=<optimized out>, name=<optimized out>, opaque=0x555556cf40c0, errp=0x7fffffffd800) at ../qom/object.c:2305
+ * #5  0x0000555555c61ae4 in object_property_set (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", v=v@entry=0x555556f78020, errp=0x7fffffffd800, errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/object.c:1435
+ * #6  0x0000555555c64f60 in object_property_set_qobject (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=0x555556f505a0, errp=errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/qom-qobject.c:28
+ * #7  0x0000555555c62105 in object_property_set_bool (obj=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=true, errp=errp@entry=0x555556c66b58 <error_fatal>)
+ *     at ../qom/object.c:1504 
+ * #8  0x0000555555c5b5ae in qdev_realize (dev=<optimized out>, bus=bus@entry=0x0, errp=errp@entry=0x555556c66b58 <error_fatal>) at ../hw/core/qdev.c:292
+ * #9  0x0000555555b07ce1 in x86_cpu_new (x86ms=<optimized out>, apic_id=0, errp=0x555556c66b58 <error_fatal>) at /home/opc/ext4/qemu/include/hw/qdev-core.h:77
+ * #10 0x0000555555b07dc6 in x86_cpus_init (x86ms=x86ms@entry=0x555556f33c00, default_cpu_version=<optimized out>) at ../hw/i386/x86.c:148
+ * #11 0x0000555555b0e7db in pc_init1 (machine=0x555556f33c00, pci_type=0x555555f0cbf0 "i440FX", host_type=0x555555f0cc0f "i440FX-pcihost") at ../hw/i386/pc_piix.c:191
+ * #12 0x00005555558e8281 in machine_run_board_init (machine=0x555556f33c00, mem_path=<optimized out>, errp=<optimized out>) at ../hw/core/machine.c:1508
+ * #13 0x0000555555a80d85 in qmp_x_exit_preconfig () at ../system/vl.c:2610
+ * #14 0x0000555555a80d85 in qmp_x_exit_preconfig (errp=<optimized out>) at ../system/vl.c:2701
+ * #15 0x0000555555a8498b in qemu_init (errp=<optimized out>) at ../system/vl.c:3750
+ * #16 0x0000555555a8498b in qemu_init (argc=<optimized out>, argv=<optimized out>) at ../system/vl.c:3750
+ * #17 0x000055555585d559 in main (argc=<optimized out>, argv=<optimized out>) at ../system/main.c:47
+ *
+ * called by:
+ *   - monitor/hmp-cmds.c|150| <<hmp_exit_preconfig>> qmp_x_exit_preconfig(&err);
+ *   - softmmu/vl.c|3653| <<qemu_init>> qmp_x_exit_preconfig(&error_fatal);
+ */
 void qmp_x_exit_preconfig(Error **errp)
 {
     if (phase_check(PHASE_MACHINE_INITIALIZED)) {
@@ -2662,6 +2690,11 @@ void qmp_x_exit_preconfig(Error **errp)
     }
 }
 
+/*
+ * called by:
+ *   - softmmu/main.c|47| <<main>> qemu_init(argc, argv);
+ *   - tests/qtest/fuzz/fuzz.c|227| <<LLVMFuzzerInitialize>> qemu_init(result.we_wordc, result.we_wordv);
+ */
 void qemu_init(int argc, char **argv)
 {
     QemuOpts *opts;
@@ -3644,6 +3677,9 @@ void qemu_init(int argc, char **argv)
         exit(0);
     }
 
+    /*
+     * 似乎这里是初始化vCPU
+     */
     if (!preconfig_requested) {
         qmp_x_exit_preconfig(&error_fatal);
     }
diff --git a/target/arm/kvm64.c b/target/arm/kvm64.c
index 94bbd9661..91f676373 100644
--- a/target/arm/kvm64.c
+++ b/target/arm/kvm64.c
@@ -1007,6 +1007,10 @@ static int kvm_arch_get_sve(CPUState *cs)
     return 0;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2851| <<do_kvm_cpu_synchronize_state>> kvm_arch_get_registers(cpu);
+ */
 int kvm_arch_get_registers(CPUState *cs)
 {
     struct kvm_one_reg reg;
diff --git a/target/i386/cpu.c b/target/i386/cpu.c
index 97ad229d8..658f0f129 100644
--- a/target/i386/cpu.c
+++ b/target/i386/cpu.c
@@ -6981,6 +6981,28 @@ static void x86_cpu_enable_xsave_components(X86CPU *cpu)
  *   any CPUID data based on host capabilities.
  */
 
+/*
+ * (gdb) bt
+ * #0  0x0000555555b43505 in x86_cpu_realizefn (dev=0x555556f6f8e0, errp=0x7fffffffd780) at ../target/i386/cpu.c:7249
+ * #1  0x0000555555c5ac7e in device_set_realized (obj=<optimized out>, value=<optimized out>, errp=0x7fffffffd800) at ../hw/core/qdev.c:510
+ * #2  0x0000555555c5e9c6 in property_set_bool (obj=0x555556f6f8e0, v=<optimized out>, name=<optimized out>, opaque=0x555556cf40c0, errp=0x7fffffffd800) at ../qom/object.c:2305
+ * #3  0x0000555555c61b34 in object_property_set (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", v=v@entry=0x555556f78040, errp=0x7fffffffd800, errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/object.c:1435
+ * #4  0x0000555555c64fb0 in object_property_set_qobject (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=0x555556f505f0, errp=errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/qom-qobject.c:28
+ * #5  0x0000555555c62155 in object_property_set_bool (obj=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=true, errp=errp@entry=0x555556c66b58 <error_fatal>)
+ *     at ../qom/object.c:1504\
+ * #6  0x0000555555c5b5fe in qdev_realize (dev=<optimized out>, bus=bus@entry=0x0, errp=errp@entry=0x555556c66b58 <error_fatal>) at ../hw/core/qdev.c:292
+ * #7  0x0000555555b07ce1 in x86_cpu_new (x86ms=<optimized out>, apic_id=0, errp=0x555556c66b58 <error_fatal>) at /home/opc/ext4/qemu/include/hw/qdev-core.h:77
+ * #8  0x0000555555b07dc6 in x86_cpus_init (x86ms=x86ms@entry=0x555556f33c00, default_cpu_version=<optimized out>) at ../hw/i386/x86.c:148
+ * #9  0x0000555555b0e7db in pc_init1 (machine=0x555556f33c00, pci_type=0x555555f0cbf0 "i440FX", host_type=0x555555f0cc0f "i440FX-pcihost") at ../hw/i386/pc_piix.c:191
+ * #10 0x00005555558e8281 in machine_run_board_init (machine=0x555556f33c00, mem_path=<optimized out>, errp=<optimized out>) at ../hw/core/machine.c:1508
+ * #11 0x0000555555a80d85 in qmp_x_exit_preconfig () at ../system/vl.c:2610
+ * #12 0x0000555555a80d85 in qmp_x_exit_preconfig (errp=<optimized out>) at ../system/vl.c:2701
+ * #13 0x0000555555a8498b in qemu_init (errp=<optimized out>) at ../system/vl.c:3750
+ * #14 0x0000555555a8498b in qemu_init (argc=<optimized out>, argv=<optimized out>) at ../system/vl.c:3750
+ * #15 0x000055555585d559 in main (argc=<optimized out>, argv=<optimized out>) at ../system/main.c:47
+ */
 /* Expand CPU configuration data, based on configured features
  * and host/accelerator capabilities when appropriate.
  */
diff --git a/target/i386/cpu.h b/target/i386/cpu.h
index e0771a104..98b6ed2aa 100644
--- a/target/i386/cpu.h
+++ b/target/i386/cpu.h
@@ -1954,6 +1954,19 @@ struct ArchCPU {
     /* Features that were filtered out because of missing host capabilities */
     FeatureWordArray filtered_features;
 
+    /*
+     * 在以下使用i386的ArchCPU->enable_pmu:
+     *   - target/i386/cpu.c|7794| <<global>> DEFINE_PROP_BOOL("pmu", X86CPU, enable_pmu, false),
+     *   - target/i386/cpu.c|6056| <<cpu_x86_cpuid>> if (!cpu->enable_pmu) {
+     *   - target/i386/cpu.c|6206| <<cpu_x86_cpuid>> if (accel_uses_host_cpuid() && cpu->enable_pmu) {
+     *   - target/i386/cpu.c|6246| <<cpu_x86_cpuid>> if (accel_uses_host_cpuid() && cpu->enable_pmu &&
+     *   - target/i386/cpu.c|6314| <<cpu_x86_cpuid>> if (kvm_enabled() && cpu->enable_pmu &&
+     *   - target/i386/cpu.c|6322| <<cpu_x86_cpuid>> accel_uses_host_cpuid() && cpu->enable_pmu &&
+     *   - target/i386/cpu.c|7264| <<x86_cpu_realizefn>> if (!cpu->enable_pmu) {
+     *   - target/i386/kvm/kvm.c|3323| <<kvm_init_msrs>> if (has_msr_perf_capabs && cpu->enable_pmu) {
+     *   - target/i386/kvm/kvm.c|3617| <<kvm_put_msrs>> if (kvm_enabled() && cpu->enable_pmu &&
+     *   - target/i386/kvm/kvm.c|4048| <<kvm_get_msrs>> if (kvm_enabled() && cpu->enable_pmu &&
+     */
     /* Enable PMU CPUID bits. This can't be enabled by default yet because
      * it doesn't have ABI stability guarantees, as it passes all PMU CPUID
      * bits returned by GET_SUPPORTED_CPUID (that depend on host CPU and kernel
diff --git a/target/i386/kvm/kvm.c b/target/i386/kvm/kvm.c
index ebfaf3d24..e34406789 100644
--- a/target/i386/kvm/kvm.c
+++ b/target/i386/kvm/kvm.c
@@ -2556,6 +2556,41 @@ static void register_smram_listener(Notifier *n, void *unused)
                                  &smram_address_space, 1, "kvm-smram");
 }
 
+/*
+ * (gdb) bt
+ * #0  0x0000555555863ee0 in cpu_list_add (cpu=0x555556f6f8e0) at /home/opc/ext4/qemu/include/qemu/thread.h:122
+ * #1  0x0000555555beba88 in cpu_exec_realizefn (cpu=cpu@entry=0x555556f6f8e0, errp=errp@entry=0x7fffffffd720) at ../cpu-target.c:143
+ * #2  0x0000555555b4376f in x86_cpu_realizefn (dev=0x555556f6f8e0, errp=0x7fffffffd780) at ../target/i386/cpu.c:7308
+ * #3  0x0000555555c5ac2e in device_set_realized (obj=<optimized out>, value=<optimized out>, errp=0x7fffffffd800) at ../hw/core/qdev.c:510
+ * #4  0x0000555555c5e976 in property_set_bool (obj=0x555556f6f8e0, v=<optimized out>, name=<optimized out>, opaque=0x555556cf40c0, errp=0x7fffffffd800) at ../qom/object.c:2305
+ * #5  0x0000555555c61ae4 in object_property_set (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", v=v@entry=0x555556f78020, errp=0x7fffffffd800, errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/object.c:1435
+ * #6  0x0000555555c64f60 in object_property_set_qobject (obj=obj@entry=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=0x555556f505a0, errp=errp@entry=0x555556c66b58
+ *     <error_fatal>) at ../qom/qom-qobject.c:28
+ * #7  0x0000555555c62105 in object_property_set_bool (obj=0x555556f6f8e0, name=name@entry=0x555555f3c542 "realized", value=value@entry=true, errp=errp@entry=0x555556c66b58 <error_fatal>)
+ *     at ../qom/object.c:1504
+ * #8  0x0000555555c5b5ae in qdev_realize (dev=<optimized out>, bus=bus@entry=0x0, errp=errp@entry=0x555556c66b58 <error_fatal>) at ../hw/core/qdev.c:292
+ * #9  0x0000555555b07ce1 in x86_cpu_new (x86ms=<optimized out>, apic_id=0, errp=0x555556c66b58 <error_fatal>) at /home/opc/ext4/qemu/include/hw/qdev-core.h:77
+ * #10 0x0000555555b07dc6 in x86_cpus_init (x86ms=x86ms@entry=0x555556f33c00, default_cpu_version=<optimized out>) at ../hw/i386/x86.c:148
+ * #11 0x0000555555b0e7db in pc_init1 (machine=0x555556f33c00, pci_type=0x555555f0cbf0 "i440FX", host_type=0x555555f0cc0f "i440FX-pcihost") at ../hw/i386/pc_piix.c:191
+ * #12 0x00005555558e8281 in machine_run_board_init (machine=0x555556f33c00, mem_path=<optimized out>, errp=<optimized out>) at ../hw/core/machine.c:1508
+ * #13 0x0000555555a80d85 in qmp_x_exit_preconfig () at ../system/vl.c:2610
+ * #14 0x0000555555a80d85 in qmp_x_exit_preconfig (errp=<optimized out>) at ../system/vl.c:2701
+ * #15 0x0000555555a8498b in qemu_init (errp=<optimized out>) at ../system/vl.c:3750
+ * #16 0x0000555555a8498b in qemu_init (argc=<optimized out>, argv=<optimized out>) at ../system/vl.c:3750
+ * #17 0x000055555585d559 in main (argc=<optimized out>, argv=<optimized out>) at ../system/main.c:47
+ *
+ * (gdb) bt
+ * #0  0x0000555555af95d0 in kvm_arch_init (ms=ms@entry=0x555556f33c00, s=s@entry=0x555556f4a6f0) at ../target/i386/kvm/kvm.c:2518
+ * #1  0x0000555555c4cf01 in kvm_init (ms=0x555556f33c00) at ../accel/kvm/kvm-all.c:2564
+ * #2  0x0000555555adea82 in accel_init_machine (accel=accel@entry=0x555556f4a6f0, ms=0x555556f33c00) at ../accel/accel-system.c:39
+ * #3  0x0000555555a7f0ba in do_configure_accelerator (opaque=0x7fffffffdb30, opts=0x555556ceb9d0, errp=<optimized out>) at ../system/vl.c:2321
+ * #4  0x0000555555ddb042 in qemu_opts_foreach (list=<optimized out>, func=func@entry=0x555555a7efd0 <do_configure_accelerator>, opaque=opaque@entry=0x7fffffffdb30, errp=0x555556c66b58 <error_fatal>)
+ *     at ../util/qemu-option.c:1135
+ * #5  0x0000555555a83ea5 in qemu_init (progname=<optimized out>) at ../system/vl.c:2390
+ * #6  0x0000555555a83ea5 in qemu_init (argc=<optimized out>, argv=<optimized out>) at ../system/vl.c:3694
+ * #7  0x000055555585d559 in main (argc=<optimized out>, argv=<optimized out>) at ../system/main.c:47
+ */
 int kvm_arch_init(MachineState *ms, KVMState *s)
 {
     uint64_t identity_base = 0xfffbc000;
@@ -3268,6 +3303,10 @@ static void kvm_msr_entry_add_vmx(X86CPU *cpu, FeatureWordArray f)
     }
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|3316| <<kvm_init_msrs>> kvm_msr_entry_add_perf(cpu, env->features);
+ */
 static void kvm_msr_entry_add_perf(X86CPU *cpu, FeatureWordArray f)
 {
     uint64_t kvm_perf_cap =
@@ -3297,6 +3336,10 @@ static int kvm_buf_set_msrs(X86CPU *cpu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|2267| <<kvm_arch_init_vcpu>> kvm_init_msrs(cpu);
+ */
 static void kvm_init_msrs(X86CPU *cpu)
 {
     CPUX86State *env = &cpu->env;
@@ -3331,6 +3374,10 @@ static void kvm_init_msrs(X86CPU *cpu)
     assert(kvm_buf_set_msrs(cpu) == 0);
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|4788| <<kvm_arch_put_registers>> ret = kvm_put_msrs(x86_cpu, level);
+ */
 static int kvm_put_msrs(X86CPU *cpu, int level)
 {
     CPUX86State *env = &cpu->env;
@@ -4419,6 +4466,10 @@ static int kvm_get_apic(X86CPU *cpu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|4788| <<kvm_arch_put_registers>> ret = kvm_put_vcpu_events(x86_cpu, level);
+ */
 static int kvm_put_vcpu_events(X86CPU *cpu, int level)
 {
     CPUState *cs = CPU(cpu);
@@ -4714,6 +4765,12 @@ static int kvm_get_nested_state(X86CPU *cpu)
     return ret;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|2876| <<do_kvm_cpu_synchronize_post_reset>> kvm_arch_put_registers(cpu, KVM_PUT_RESET_STATE);
+ *   - accel/kvm/kvm-all.c|2887| <<do_kvm_cpu_synchronize_post_init>> kvm_arch_put_registers(cpu, KVM_PUT_FULL_STATE);
+ *   - accel/kvm/kvm-all.c|2980| <<kvm_cpu_exec>> kvm_arch_put_registers(cpu, KVM_PUT_RUNTIME_STATE);
+ */
 int kvm_arch_put_registers(CPUState *cpu, int level)
 {
     X86CPU *x86_cpu = X86_CPU(cpu);
diff --git a/util/aio-wait.c b/util/aio-wait.c
index b5336cf5f..180fca2f2 100644
--- a/util/aio-wait.c
+++ b/util/aio-wait.c
@@ -33,6 +33,26 @@ static void dummy_bh_cb(void *opaque)
     /* The point is to make AIO_WAIT_WHILE()'s aio_poll() return */
 }
 
+/*
+ * called by:
+ *   - block/block-backend.c|1533| <<blk_dec_in_flight>> aio_wait_kick();
+ *   - block/export/fuse.c|308| <<read_from_fuse_export>> aio_wait_kick();
+ *   - block/export/vduse-blk.c|55| <<vduse_blk_inflight_dec>> aio_wait_kick();
+ *   - block/graph-lock.c|236| <<bdrv_graph_co_rdlock>> aio_wait_kick();
+ *   - block/graph-lock.c|259| <<bdrv_graph_co_rdunlock>> aio_wait_kick();
+ *   - block/io.c|771| <<bdrv_wakeup>> aio_wait_kick();
+ *   - block/nvme.c|513| <<nvme_admin_cmd_sync_cb>> aio_wait_kick();
+ *   - block/qcow2.c|1926| <<qcow2_open_entry>> aio_wait_kick();
+ *   - block/throttle-groups.c|424| <<throttle_group_restart_queue_entry>> aio_wait_kick();
+ *   - blockjob.c|268| <<block_job_on_idle_locked>> aio_wait_kick();
+ *   - nbd/server.c|1529| <<nbd_request_put>> aio_wait_kick();
+ *   - nbd/server.c|2667| <<nbd_trip>> aio_wait_kick();
+ *   - net/colo-compare.c|789| <<_compare_chr_send>> aio_wait_kick();
+ *   - net/filter-mirror.c|103| <<filter_send_co>> aio_wait_kick();
+ *   - tests/unit/test-bdrv-drain.c|497| <<test_iothread_drain_co_entry>> aio_wait_kick();
+ *   - util/aio-wait.c|72| <<aio_wait_bh>> aio_wait_kick();
+ *   - util/vhost-user-server.c|224| <<vu_client_trip>> aio_wait_kick();
+ */
 void aio_wait_kick(void)
 {
     /*
diff --git a/util/iov.c b/util/iov.c
index 866fb577f..f89039cf3 100644
--- a/util/iov.c
+++ b/util/iov.c
@@ -40,6 +40,11 @@ size_t iov_from_buf_full(const struct iovec *iov, unsigned int iov_cnt,
     return done;
 }
 
+/*
+ * called by:
+ *   - block/mirror.c|1536| <<bdrv_mirror_top_pwritev>> iov_to_buf_full(qiov->iov, qiov->niov, 0, bounce_buf, bytes);
+ *   - include/qemu/iov.h|62| <<iov_to_buf>> return iov_to_buf_full(iov, iov_cnt, offset, buf, bytes);
+ */
 size_t iov_to_buf_full(const struct iovec *iov, const unsigned int iov_cnt,
                        size_t offset, void *buf, size_t bytes)
 {
@@ -47,6 +52,13 @@ size_t iov_to_buf_full(const struct iovec *iov, const unsigned int iov_cnt,
     unsigned int i;
     for (i = 0, done = 0; (offset || done < bytes) && i < iov_cnt; i++) {
         if (offset < iov[i].iov_len) {
+            /*
+	     * 如果所有的iov_len加起来不等于bytes???
+	     * bytes比较大??
+	     *
+	     * 如果函数的参数offset=0
+	     * 应该一直进入这个if吧
+	     */
             size_t len = MIN(iov[i].iov_len - offset, bytes - done);
             memcpy(buf + done, iov[i].iov_base + offset, len);
             done += len;
@@ -316,6 +328,16 @@ void qemu_iovec_add(QEMUIOVector *qiov, void *base, size_t len)
  * of src".
  * Only vector pointers are processed, not the actual data buffers.
  */
+/*
+ * called by:
+ *   - block/io.c|1654| <<bdrv_create_padded_qiov>> qemu_iovec_concat_iov(&pad->pre_collapse_qiov, iov, collapse_count, iov_offset, SIZE_MAX);
+ *   - block/io.c|1676| <<bdrv_create_padded_qiov>> qemu_iovec_concat_iov(&pad->local_qiov, iov, niov, iov_offset, bytes);
+ *   - block/vhdx.c|1412| <<vhdx_co_writev>> qemu_iovec_concat_iov(&hd_qiov, &iov1, 1, 0, iov1.iov_len);
+ *   - block/vhdx.c|1428| <<vhdx_co_writev>> qemu_iovec_concat_iov(&hd_qiov, &iov2, 1, 0, iov2.iov_len);
+ *   - hw/scsi/virtio-scsi.c|182| <<virtio_scsi_parse_req>> if (qemu_iovec_concat_iov(&req->resp_iov, req->elem.in_sg, req->elem.in_num, 0, resp_size) < resp_size) {
+ *   - util/iov.c|358| <<qemu_iovec_concat>> qemu_iovec_concat_iov(dst, src->iov, src->niov, soffset, sbytes);
+ *   - util/iov.c|463| <<qemu_iovec_init_slice>> qemu_iovec_concat_iov(qiov, slice_iov, slice_niov, slice_head, len);
+ */
 size_t qemu_iovec_concat_iov(QEMUIOVector *dst,
                              struct iovec *src_iov, unsigned int src_cnt,
                              size_t soffset, size_t sbytes)
@@ -329,6 +351,9 @@ size_t qemu_iovec_concat_iov(QEMUIOVector *dst,
     assert(dst->nalloc != -1);
     for (i = 0, done = 0; done < sbytes && i < src_cnt; i++) {
         if (soffset < src_iov[i].iov_len) {
+            /*
+	     * 如果soffset是0, 会一直进入这个if吧
+	     */
             size_t len = MIN(src_iov[i].iov_len - soffset, sbytes - done);
             qemu_iovec_add(dst, src_iov[i].iov_base + soffset, len);
             done += len;
diff --git a/util/lockcnt.c b/util/lockcnt.c
index 5da36946b..17037b2a3 100644
--- a/util/lockcnt.c
+++ b/util/lockcnt.c
@@ -30,6 +30,14 @@
 
 void qemu_lockcnt_init(QemuLockCnt *lockcnt)
 {
+    /*
+     * struct QemuLockCnt {
+     * #ifndef CONFIG_LINUX
+     *     QemuMutex mutex;
+     * #endif
+     *     unsigned count;
+     * };
+     */
     lockcnt->count = 0;
 }
 
@@ -53,6 +61,13 @@ void qemu_lockcnt_destroy(QemuLockCnt *lockcnt)
  * is set the caller has effectively acquired the lock.  If it returns
  * with the lock not taken, it must wake another futex waiter.
  */
+/*
+ * called by:
+ *   - util/lockcnt.c|139| <<qemu_lockcnt_inc>> if (qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, QEMU_LOCKCNT_COUNT_STEP,
+ *   - util/lockcnt.c|184| <<qemu_lockcnt_dec_and_lock>> if (qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, locked_state, &waited)) {
+ *   - util/lockcnt.c|225| <<qemu_lockcnt_dec_if_lock>> if (qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, locked_state, &waited)) {
+ *   - util/lockcnt.c|259| <<qemu_lockcnt_lock>> while (!qemu_lockcnt_cmpxchg_or_wait(lockcnt, &val, val + step, &waited)) {
+ */
 static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
                                          int new_if_free, bool *waited)
 {
@@ -61,6 +76,11 @@ static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
         int expected = *val;
 
         trace_lockcnt_fast_path_attempt(lockcnt, expected, new_if_free);
+        /*
+	 * 将old和ptr指向的内容比较,
+	 * 如果相等,则将new写入到ptr中,返回old,
+	 * 如果不相等,则返回ptr指向的内容.
+         */
         *val = qatomic_cmpxchg(&lockcnt->count, expected, new_if_free);
         if (*val == expected) {
             trace_lockcnt_fast_path_success(lockcnt, expected, new_if_free);
@@ -91,6 +111,16 @@ static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
         if ((*val & QEMU_LOCKCNT_STATE_MASK) == QEMU_LOCKCNT_STATE_WAITING) {
             *waited = true;
             trace_lockcnt_futex_wait(lockcnt, *val);
+            /*
+	     * QemuLockCnt *lockcnt
+	     *
+	     * struct QemuLockCnt {
+	     * #ifndef CONFIG_LINUX
+	     *     QemuMutex mutex;
+	     * #endif
+	     *     unsigned count;
+	     * };
+	     */
             qemu_futex_wait(&lockcnt->count, *val);
             *val = qatomic_read(&lockcnt->count);
             trace_lockcnt_futex_wait_resume(lockcnt, *val);
@@ -102,6 +132,14 @@ static bool qemu_lockcnt_cmpxchg_or_wait(QemuLockCnt *lockcnt, int *val,
     return false;
 }
 
+/*
+ * called by:
+ *   - util/lockcnt.c|153| <<qemu_lockcnt_inc>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|204| <<qemu_lockcnt_dec_and_lock>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|244| <<qemu_lockcnt_dec_if_lock>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|283| <<qemu_lockcnt_inc_and_unlock>> lockcnt_wake(lockcnt);
+ *   - util/lockcnt.c|301| <<qemu_lockcnt_unlock>> lockcnt_wake(lockcnt);
+ */
 static void lockcnt_wake(QemuLockCnt *lockcnt)
 {
     trace_lockcnt_futex_wake(lockcnt);
@@ -289,6 +327,20 @@ void qemu_lockcnt_unlock(QemuLockCnt *lockcnt)
     }
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|144| <<accel_has_to_wait>> if (qemu_lockcnt_count(&cpu->in_ioctl_lock)) {
+ *   - accel/accel-blocker.c|151| <<accel_has_to_wait>> return needs_to_wait || qemu_lockcnt_count(&accel_in_ioctl_lock);
+ *   - util/aio-posix.c|87| <<aio_remove_fd_handler>> if (qemu_lockcnt_count(&ctx->list_lock)) {
+ *   - util/aio-posix.c|528| <<run_poll_handlers>> assert(qemu_lockcnt_count(&ctx->list_lock) > 0);
+ *
+ * qemu_lockcnt_count: query a LockCnt's count.
+ * @lockcnt: the lockcnt to query.
+ *
+ * Note that the count can change at any time.  Still, while the
+ * lockcnt is locked, one can usefully check whether the count
+ * is non-zero.
+ */
 unsigned qemu_lockcnt_count(QemuLockCnt *lockcnt)
 {
     return qatomic_read(&lockcnt->count) >> QEMU_LOCKCNT_COUNT_SHIFT;
diff --git a/util/qemu-thread-posix.c b/util/qemu-thread-posix.c
index b2e26e212..27fb4e665 100644
--- a/util/qemu-thread-posix.c
+++ b/util/qemu-thread-posix.c
@@ -361,6 +361,18 @@ static inline void qemu_futex_wait(QemuEvent *ev, unsigned val)
 #define EV_FREE        1
 #define EV_BUSY       -1
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|63| <<accel_blocker_init>> qemu_event_init(&accel_in_ioctl_event, false);
+ *   - migration/colo.c|648| <<migrate_start_colo_process>> qemu_event_init(&s->colo_checkpoint_event, false);
+ *   - migration/migration.c|149| <<migration_object_init>> qemu_event_init(&current_incoming->main_thread_load_event, false);
+ *   - tests/unit/test-aio-multithread.c|75| <<create_aio_contexts>> qemu_event_init(&done_event, false);
+ *   - tests/unit/test-bdrv-drain.c|2056| <<main>> qemu_event_init(&done_event, false);
+ *   - util/qemu-timer.c|104| <<timerlist_new>> qemu_event_init(&timer_list->timers_done_ev, true);
+ *   - util/rcu.c|343| <<drain_call_rcu>> qemu_event_init(&rcu_drain.drain_complete_event, false);
+ *   - util/rcu.c|408| <<rcu_init_complete>> qemu_event_init(&rcu_gp_event, true);
+ *   - util/rcu.c|410| <<rcu_init_complete>> qemu_event_init(&rcu_call_ready_event, false);
+ */
 void qemu_event_init(QemuEvent *ev, bool init)
 {
 #ifndef __linux__
@@ -368,10 +380,26 @@ void qemu_event_init(QemuEvent *ev, bool init)
     pthread_cond_init(&ev->cond, NULL);
 #endif
 
+    /*
+     * struct QemuEvent {
+     * #ifndef __linux__
+     *     pthread_mutex_t lock;
+     *     pthread_cond_t cond;
+     * #endif
+     *     unsigned value;
+     *     bool initialized;
+     * };
+     */
     ev->value = (init ? EV_SET : EV_FREE);
     ev->initialized = true;
 }
 
+/*
+ * called by:
+ *   - migration/colo.c|632| <<colo_process_checkpoint>> qemu_event_destroy(&s->colo_checkpoint_event);
+ *   - tests/unit/test-aio-multithread.c|96| <<join_aio_contexts>> qemu_event_destroy(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|2118| <<main>> qemu_event_destroy(&done_event);
+ */
 void qemu_event_destroy(QemuEvent *ev)
 {
     assert(ev->initialized);
@@ -382,6 +410,35 @@ void qemu_event_destroy(QemuEvent *ev)
 #endif
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|91| <<accel_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - accel/accel-blocker.c|120| <<accel_cpu_ioctl_end>> qemu_event_set(&accel_in_ioctl_event);
+ *   - include/qemu/rcu.h|117| <<rcu_read_unlock>> qemu_event_set(&rcu_gp_event);
+ *   - migration/colo.c|71| <<colo_checkpoint_notify>> qemu_event_set(&s->colo_checkpoint_event);
+ *   - migration/colo.c|808| <<colo_shutdown>> qemu_event_set(&s->colo_checkpoint_event);
+ *   - migration/savevm.c|2832| <<qemu_loadvm_state>> qemu_event_set(&mis->main_thread_load_event);
+ *   - tests/unit/test-aio-multithread.c|42| <<ctx_run_bh_cb>> qemu_event_set(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|504| <<test_iothread_aio_cb>> qemu_event_set(&done_event);
+ *   - util/qemu-timer.c|584| <<timerlist_run_timers>> qemu_event_set(&timer_list->timers_done_ev);
+ *   - util/rcu.c|313| <<call_rcu1>> qemu_event_set(&rcu_call_ready_event);
+ *   - util/rcu.c|325| <<drain_rcu_callback>> qemu_event_set(&event->drain_complete_event);
+ *
+ * struct QemuEvent {
+ * #ifndef __linux__
+ *     pthread_mutex_t lock;
+ *     pthread_cond_t cond;
+ * #endif
+ *     unsigned value;
+ *     bool initialized;
+ * };
+ *
+ * 核心思想:
+ * 如果已经是EV_SET了, 目的达到了, 什么也不用做.
+ * 如果不是EV_SET, 用xchg换成EV_SET:
+ *   - 如果旧的是EV_FREE, 什么也不做
+ *   - 如果旧的是EV_BUSY, wake up!
+ */
 void qemu_event_set(QemuEvent *ev)
 {
     assert(ev->initialized);
@@ -393,18 +450,53 @@ void qemu_event_set(QemuEvent *ev)
      * ev->value we need a full memory barrier here.
      */
     smp_mb();
+    /*
+     * #define EV_SET         0
+     * #define EV_FREE        1
+     * #define EV_BUSY       -1
+     */
     if (qatomic_read(&ev->value) != EV_SET) {
         int old = qatomic_xchg(&ev->value, EV_SET);
 
         /* Pairs with memory barrier in kernel futex_wait system call.  */
         smp_mb__after_rmw();
+        /*
+	 * #define EV_SET         0
+	 * #define EV_FREE        1
+	 * #define EV_BUSY       -1
+	 */
         if (old == EV_BUSY) {
+            /*
+	     * FUTEX_WAIT: 如果futex word中仍然保存着参数val给定的值,
+	     * 那么当前线程则进入睡眠,等待FUTEX_WAKE的操作唤醒它.
+	     *
+	     * FUTEX_WAKE: 最多唤醒val个等待在futex word上的线程.
+	     * Val或者等于1(唤醒1个等待线程)或者等于INT_MAX(唤醒全部等待线程)
+	     *
+	     * called by:
+	     *   - tests/unit/test-aio-multithread.c|331| <<mcs_mutex_unlock>> qemu_futex_wake(&nodes[next].locked, 1);
+	     *   - util/lockcnt.c|116| <<lockcnt_wake>> qemu_futex_wake(&lockcnt->count, 1);
+	     *   - util/qemu-thread-posix.c|413| <<qemu_event_set>> qemu_futex_wake(ev, INT_MAX);
+	     */
             /* There were waiters, wake them up.  */
             qemu_futex_wake(ev, INT_MAX);
         }
     }
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|178| <<accel_ioctl_inhibit_begin>> qemu_event_reset(&accel_in_ioctl_event);
+ *   - migration/colo.c|498| <<colo_do_checkpoint_transaction>> qemu_event_reset(&s->colo_checkpoint_event);
+ *   - migration/migration.c|257| <<migration_incoming_state_destroy>> qemu_event_reset(&mis->main_thread_load_event);
+ *   - tests/unit/test-aio-multithread.c|52| <<ctx_run>> qemu_event_reset(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|562| <<test_iothread_common>> qemu_event_reset(&done_event);
+ *   - util/qemu-thread-win32.c|295| <<qemu_event_reset>> void qemu_event_reset(QemuEvent *ev)
+ *   - util/qemu-timer.c|513| <<timerlist_run_timers>> qemu_event_reset(&timer_list->timers_done_ev);
+ *   - util/rcu.c|84| <<wait_for_readers>> qemu_event_reset(&rcu_gp_event);
+ *   - util/rcu.c|272| <<call_rcu_thread>> qemu_event_reset(&rcu_call_ready_event);
+ *   - util/rcu.c|291| <<call_rcu_thread>> qemu_event_reset(&rcu_call_ready_event);
+ */
 void qemu_event_reset(QemuEvent *ev)
 {
     assert(ev->initialized);
@@ -422,6 +514,28 @@ void qemu_event_reset(QemuEvent *ev)
     smp_mb__after_rmw();
 }
 
+/*
+ * called by:
+ *   - accel/accel-blocker.c|196| <<accel_ioctl_inhibit_begin>> qemu_event_wait(&accel_in_ioctl_event);
+ *   - migration/colo.c|585| <<colo_process_checkpoint>> qemu_event_wait(&s->colo_checkpoint_event);
+ *   - migration/savevm.c|1956| <<postcopy_ram_listen_thread>> qemu_event_wait(&mis->main_thread_load_event);
+ *   - tests/unit/test-aio-multithread.c|54| <<ctx_run>> qemu_event_wait(&done_event);
+ *   - tests/unit/test-bdrv-drain.c|599| <<test_iothread_common>> qemu_event_wait(&done_event);
+ *   - util/qemu-timer.c|167| <<qemu_clock_enable>> qemu_event_wait(&tl->timers_done_ev);
+ *   - util/rcu.c|136| <<wait_for_readers>> qemu_event_wait(&rcu_gp_event);
+ *   - util/rcu.c|278| <<call_rcu_thread>> qemu_event_wait(&rcu_call_ready_event);
+ *   - util/rcu.c|294| <<call_rcu_thread>> qemu_event_wait(&rcu_call_ready_event);
+ *   - util/rcu.c|364| <<drain_call_rcu>> qemu_event_wait(&rcu_drain.drain_complete_event);
+ *
+ * struct QemuEvent {
+ * #ifndef __linux__
+ *     pthread_mutex_t lock;
+ *     pthread_cond_t cond;
+ * #endif
+ *     unsigned value;
+ *     bool initialized;
+ * };
+ */
 void qemu_event_wait(QemuEvent *ev)
 {
     unsigned value;
@@ -438,6 +552,11 @@ void qemu_event_wait(QemuEvent *ev)
      * qemu_futex_wait() will ensure the check is done correctly.
      */
     value = qatomic_load_acquire(&ev->value);
+    /*
+     * #define EV_SET         0
+     * #define EV_FREE        1
+     * #define EV_BUSY       -1
+     */
     if (value != EV_SET) {
         if (value == EV_FREE) {
             /*
@@ -452,10 +571,20 @@ void qemu_event_wait(QemuEvent *ev)
              * like the load above.
              */
             if (qatomic_cmpxchg(&ev->value, EV_FREE, EV_BUSY) == EV_SET) {
+                /*
+		 * 这里多名ev->value又被设置成EV_SET了, 什么也不用做了
+		 */
                 return;
             }
         }
 
+	/*
+	 * FUTEX_WAIT: 如果futex word中仍然保存着参数val给定的值,
+	 * 那么当前线程则进入睡眠,等待FUTEX_WAKE的操作唤醒它.
+	 *
+	 * FUTEX_WAKE: 最多唤醒val个等待在futex word上的线程.
+	 * Val或者等于1(唤醒1个等待线程)或者等于INT_MAX(唤醒全部等待线程)
+	 */
         /*
          * This is the final check for a concurrent set, so it does need
          * a smp_mb() pairing with the second barrier of qemu_event_set().
-- 
2.34.1

