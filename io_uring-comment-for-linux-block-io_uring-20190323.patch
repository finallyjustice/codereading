From 501b26b7c1f7a28bb6b5cba05bad90bff299841c Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 3 Apr 2019 01:49:03 +0800
Subject: [PATCH 1/1] io_uring comment for linux-block:io_uring-20190323

This is for linux-block: io_uring-20190323

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 fs/io_uring.c                 | 213 ++++++++++++++++++++++++++++++++++++++++++
 include/uapi/linux/io_uring.h |  63 +++++++++++++
 2 files changed, 276 insertions(+)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index c592a09..736830e 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -112,12 +112,36 @@ struct io_ring_ctx {
 	struct {
 		unsigned int		flags;
 		bool			compat;
+		/*
+		 * used by:
+		 *   - fs/io_uring.c|2436| <<io_sqe_buffer_unregister>> if (ctx->account_mem)
+		 *   - fs/io_uring.c|2528| <<io_sqe_buffer_register>> if (ctx->account_mem) {
+		 *   - fs/io_uring.c|2545| <<io_sqe_buffer_register>> if (ctx->account_mem)
+		 *   - fs/io_uring.c|2556| <<io_sqe_buffer_register>> if (ctx->account_mem)
+		 *   - fs/io_uring.c|2589| <<io_sqe_buffer_register>> if (ctx->account_mem)
+		 *   - fs/io_uring.c|2647| <<io_ring_ctx_free>> if (ctx->account_mem)
+		 *   - fs/io_uring.c|2979| <<io_uring_create>> ctx->account_mem = account_mem;
+		 */
 		bool			account_mem;
 
 		/* SQ ring */
 		struct io_sq_ring	*sq_ring;
+		/*
+		 * used by:
+		 *   - fs/io_uring.c|1714| <<io_commit_sqring>> if (ctx->cached_sq_head != READ_ONCE(ring->r.head)) {
+		 *   - fs/io_uring.c|1720| <<io_commit_sqring>> smp_store_release(&ring->r.head, ctx->cached_sq_head);
+		 *   - fs/io_uring.c|1735| <<io_drop_sqring>> ctx->cached_sq_head--;
+		 *   - fs/io_uring.c|1759| <<io_get_sqring>> head = ctx->cached_sq_head;
+		 *   - fs/io_uring.c|1769| <<io_get_sqring>> ctx->cached_sq_head++;
+		 *   - fs/io_uring.c|1774| <<io_get_sqring>> ctx->cached_sq_head++;
+		 *   - fs/io_uring.c|2606| <<io_uring_poll>> if (READ_ONCE(ctx->sq_ring->r.tail) + 1 != ctx->cached_sq_head)
+		 */
 		unsigned		cached_sq_head;
 		unsigned		sq_entries;
+		/*
+		 * 修改的地方:
+		 *   - fs/io_uring.c|2792| <<io_allocate_scq_urings>> ctx->sq_mask = sq_ring->ring_mask;
+		 */
 		unsigned		sq_mask;
 		unsigned		sq_thread_idle;
 		struct io_uring_sqe	*sq_sqes;
@@ -127,7 +151,23 @@ struct io_ring_ctx {
 	struct workqueue_struct	*sqo_wq;
 	struct task_struct	*sqo_thread;	/* if using sq thread polling */
 	struct mm_struct	*sqo_mm;
+	/*
+	 * used by:
+	 *   - fs/io_uring.c|390| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->sqo_wait))
+	 *   - fs/io_uring.c|391| <<io_cqring_ev_posted>> wake_up(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|1866| <<io_sq_thread>> prepare_to_wait(&ctx->sqo_wait, &wait,
+	 *   - fs/io_uring.c|1875| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|1881| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|1887| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|2228| <<io_sq_offload_start>> init_waitqueue_head(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|2693| <<SYSCALL_DEFINE6(io_uring_enter)>> wake_up(&ctx->sqo_wait);
+	 */
 	wait_queue_head_t	sqo_wait;
+	/*
+	 * used by:
+	 *   - fs/io_uring.c|1824| <<io_sq_thread>> while (!kthread_should_stop() && !ctx->sqo_stop) {
+	 *   - fs/io_uring.c|2060| <<io_sq_thread_stop>> ctx->sqo_stop = 1;
+	 */
 	unsigned		sqo_stop;
 
 	struct {
@@ -135,6 +175,10 @@ struct io_ring_ctx {
 		struct io_cq_ring	*cq_ring;
 		unsigned		cached_cq_tail;
 		unsigned		cq_entries;
+		/*
+		 * 修改的地方:
+		 *   - fs/io_uring.c|2815| <<io_allocate_scq_urings>> ctx->cq_mask = cq_ring->ring_mask;
+		 */
 		unsigned		cq_mask;
 		struct wait_queue_head	cq_wait;
 		struct fasync_struct	*cq_fasync;
@@ -255,6 +299,16 @@ struct io_submit_state {
 	unsigned int		ios_left;
 };
 
+/*
+ * called by:
+ *   - fs/io_uring.c|409| <<io_get_req>> req = kmem_cache_alloc(req_cachep, gfp);
+ *   - fs/io_uring.c|417| <<io_get_req>> ret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);
+ *   - fs/io_uring.c|424| <<io_get_req>> state->reqs[0] = kmem_cache_alloc(req_cachep, gfp);
+ *   - fs/io_uring.c|451| <<io_free_req_many>> kmem_cache_free_bulk(req_cachep, *nr, reqs);
+ *   - fs/io_uring.c|462| <<io_free_req>> kmem_cache_free(req_cachep, req);
+ *   - fs/io_uring.c|1662| <<io_submit_state_end>> kmem_cache_free_bulk(req_cachep, state->free_reqs,
+ *   - fs/io_uring.c|2961| <<io_uring_init>> req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);
+ */
 static struct kmem_cache *req_cachep;
 
 static const struct file_operations io_uring_fops;
@@ -279,6 +333,12 @@ static void io_ring_ctx_ref_free(struct percpu_ref *ref)
 	complete(&ctx->ctx_done);
 }
 
+/*
+ * 分配和简单初始化io_ring_ctx
+ *
+ * called by only:
+ *   - fs/io_uring.c|2875| <<io_uring_create>> ctx = io_ring_ctx_alloc(p);
+ */
 static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 {
 	struct io_ring_ctx *ctx;
@@ -345,6 +405,12 @@ static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
 	return &ring->cqes[tail & ctx->cq_mask];
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|434| <<io_cqring_add_event>> io_cqring_fill_event(ctx, user_data, res, ev_flags);
+ *   - fs/io_uring.c|536| <<io_iopoll_complete>> io_cqring_fill_event(ctx, req->user_data, req->error, 0);
+ *   - fs/io_uring.c|1223| <<io_poll_complete>> io_cqring_fill_event(ctx, req->user_data, mangle_poll(mask), 0);
+ */
 static void io_cqring_fill_event(struct io_ring_ctx *ctx, u64 ki_user_data,
 				 long res, unsigned ev_flags)
 {
@@ -375,6 +441,15 @@ static void io_cqring_ev_posted(struct io_ring_ctx *ctx)
 		wake_up(&ctx->sqo_wait);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|684| <<io_complete_rw>> io_cqring_add_event(req->ctx, req->user_data, res, 0);
+ *   - fs/io_uring.c|1107| <<io_nop>> io_cqring_add_event(ctx, user_data, err, 0);
+ *   - fs/io_uring.c|1156| <<io_fsync>> io_cqring_add_event(req->ctx, sqe->user_data, ret, 0);
+ *   - fs/io_uring.c|1214| <<io_poll_remove>> io_cqring_add_event(req->ctx, sqe->user_data, ret, 0);
+ *   - fs/io_uring.c|1514| <<io_sq_wq_submit_work>> io_cqring_add_event(ctx, sqe->user_data, ret, 0);
+ *   - fs/io_uring.c|1837| <<io_submit_sqes>> io_cqring_add_event(ctx, sqes[i].sqe->user_data, ret, 0);
+ */
 static void io_cqring_add_event(struct io_ring_ctx *ctx, u64 user_data,
 				long res, unsigned ev_flags)
 {
@@ -396,6 +471,10 @@ static void io_ring_drop_ctx_refs(struct io_ring_ctx *ctx, unsigned refs)
 		wake_up(&ctx->wait);
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|1656| <<io_submit_sqe>> req = io_get_req(ctx, state);
+ */
 static struct io_kiocb *io_get_req(struct io_ring_ctx *ctx,
 				   struct io_submit_state *state)
 {
@@ -1593,6 +1672,11 @@ static int io_req_set_file(struct io_ring_ctx *ctx, const struct sqe_submit *s,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1830| <<io_submit_sqes>> ret = io_submit_sqe(ctx, &sqes[i], statep);
+ *   - fs/io_uring.c|2000| <<io_ring_submit>> ret = io_submit_sqe(ctx, &s, statep);
+ */
 static int io_submit_sqe(struct io_ring_ctx *ctx, struct sqe_submit *s,
 			 struct io_submit_state *state)
 {
@@ -1711,6 +1795,13 @@ static void io_drop_sqring(struct io_ring_ctx *ctx)
  * used, it's important that those reads are done through READ_ONCE() to
  * prevent a re-load down the line.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|1864| <<io_sq_thread>> if (!io_get_sqring(ctx, &sqes[0])) {
+ *   - fs/io_uring.c|1894| <<io_sq_thread>> if (!io_get_sqring(ctx, &sqes[0])) {
+ *   - fs/io_uring.c|1923| <<io_sq_thread>> } while (io_get_sqring(ctx, &sqes[i]));
+ *   - fs/io_uring.c|1962| <<io_ring_submit>> if (!io_get_sqring(ctx, &s))
+ */
 static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
 {
 	struct io_sq_ring *ring = ctx->sq_ring;
@@ -1727,9 +1818,15 @@ static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
 	head = ctx->cached_sq_head;
 	/* See comment at the top of this file */
 	smp_rmb();
+	/*
+	 * r的类型: struct io_uring r;
+	 */
 	if (head == READ_ONCE(ring->r.tail))
 		return false;
 
+	/*
+	 * array是最后的位置: u32 array[]
+	 */
 	head = READ_ONCE(ring->array[head & ctx->sq_mask]);
 	if (head < ctx->sq_entries) {
 		s->index = head;
@@ -1746,6 +1843,10 @@ static bool io_get_sqring(struct io_ring_ctx *ctx, struct sqe_submit *s)
 	return false;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|1965| <<io_sq_thread>> inflight += io_submit_sqes(ctx, sqes, i, cur_mm != NULL,
+ */
 static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 			  unsigned int nr, bool has_user, bool mm_fault)
 {
@@ -1780,6 +1881,11 @@ static int io_submit_sqes(struct io_ring_ctx *ctx, struct sqe_submit *sqes,
 	return submitted;
 }
 
+/*
+ * used by:
+ *   - fs/io_uring.c|2256| <<io_sq_offload_start>> ctx->sqo_thread = kthread_create_on_cpu(io_sq_thread,
+ *   - fs/io_uring.c|2260| <<io_sq_offload_start>> ctx->sqo_thread = kthread_create(io_sq_thread, ctx,
+ */
 static int io_sq_thread(void *data)
 {
 	struct sqe_submit sqes[IO_IOPOLL_BATCH];
@@ -2027,6 +2133,11 @@ static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2069| <<io_finish_async>> io_sq_thread_stop(ctx);
+ *   - fs/io_uring.c|2285| <<io_sq_offload_start>> io_sq_thread_stop(ctx);
+ */
 static void io_sq_thread_stop(struct io_ring_ctx *ctx)
 {
 	if (ctx->sqo_thread) {
@@ -2204,6 +2315,10 @@ static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2992| <<io_uring_create>> ret = io_sq_offload_start(ctx, p);
+ */
 static int io_sq_offload_start(struct io_ring_ctx *ctx,
 			       struct io_uring_params *p)
 {
@@ -2300,12 +2415,22 @@ static void *io_mem_alloc(size_t size)
 	return (void *) __get_free_pages(gfp_flags, get_order(size));
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2649| <<io_ring_ctx_free>> ring_pages(ctx->sq_entries, ctx->cq_entries));
+ *   - fs/io_uring.c|2960| <<io_uring_create>> ring_pages(p->sq_entries, p->cq_entries));
+ *   - fs/io_uring.c|2973| <<io_uring_create>> io_unaccount_mem(user, ring_pages(p->sq_entries,
+ */
 static unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)
 {
 	struct io_sq_ring *sq_ring;
 	struct io_cq_ring *cq_ring;
 	size_t bytes;
 
+	/*
+	 * array在io_sq_ring的类型: u32 array[] --> 在struct的最后
+	 * 分配sq_entries个u32(作为array)和sq_ring放在一起
+	 */
 	bytes = struct_size(sq_ring, array, sq_entries);
 	bytes += array_size(sizeof(struct io_uring_sqe), sq_entries);
 	bytes += struct_size(cq_ring, cqes, cq_entries);
@@ -2313,6 +2438,12 @@ static unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)
 	return (bytes + PAGE_SIZE - 1) / PAGE_SIZE;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2514| <<io_sqe_buffer_register>> io_sqe_buffer_unregister(ctx);
+ *   - fs/io_uring.c|2529| <<io_ring_ctx_free>> io_sqe_buffer_unregister(ctx);
+ *   - fs/io_uring.c|2963| <<__io_uring_register>> ret = io_sqe_buffer_unregister(ctx);
+ */
 static int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)
 {
 	int i, j;
@@ -2338,6 +2469,10 @@ static int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)
 	return 0;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2401| <<io_sqe_buffer_register>> ret = io_copy_iov(ctx, &iov, arg, i);
+ */
 static int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,
 		       void __user *arg, unsigned index)
 {
@@ -2363,6 +2498,10 @@ static int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2957| <<__io_uring_register>> ret = io_sqe_buffer_register(ctx, arg, nr_args);
+ */
 static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 				  unsigned nr_args)
 {
@@ -2505,6 +2644,10 @@ static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2577| <<io_ring_ctx_wait_and_kill>> io_ring_ctx_free(ctx);
+ */
 static void io_ring_ctx_free(struct io_ring_ctx *ctx)
 {
 	io_finish_async(ctx);
@@ -2532,6 +2675,9 @@ static void io_ring_ctx_free(struct io_ring_ctx *ctx)
 	kfree(ctx);
 }
 
+/*
+ * struct file_operations io_uring_fops.poll = io_uring_poll()
+ */
 static __poll_t io_uring_poll(struct file *file, poll_table *wait)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -2548,6 +2694,9 @@ static __poll_t io_uring_poll(struct file *file, poll_table *wait)
 	return mask;
 }
 
+/*
+ * struct file_operations io_uring_fops.fasync = io_uring_fasync()
+ */
 static int io_uring_fasync(int fd, struct file *file, int on)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -2555,6 +2704,11 @@ static int io_uring_fasync(int fd, struct file *file, int on)
 	return fasync_helper(fd, file, on, &ctx->cq_fasync);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2585| <<io_uring_release>> io_ring_ctx_wait_and_kill(ctx);
+ *   - fs/io_uring.c|2864| <<io_uring_create>> io_ring_ctx_wait_and_kill(ctx);
+ */
 static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 {
 	mutex_lock(&ctx->uring_lock);
@@ -2567,6 +2721,9 @@ static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 	io_ring_ctx_free(ctx);
 }
 
+/*
+ * struct file_operations io_uring_fops.release = io_uring_release()
+ */
 static int io_uring_release(struct inode *inode, struct file *file)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -2576,6 +2733,9 @@ static int io_uring_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * struct file_operations io_uring_fops.mmap = io_uring_mmap()
+ */
 static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	loff_t offset = (loff_t) vma->vm_pgoff << PAGE_SHIFT;
@@ -2692,6 +2852,12 @@ static const struct file_operations io_uring_fops = {
 	.fasync		= io_uring_fasync,
 };
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2834| <<io_uring_create>> ret = io_allocate_scq_urings(ctx, p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()-->io_allocate_scq_urings()
+ */
 static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 				  struct io_uring_params *p)
 {
@@ -2699,6 +2865,10 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 	struct io_cq_ring *cq_ring;
 	size_t size;
 
+	/*
+	 * array在io_sq_ring的类型: u32 array[] --> 在struct的最后
+	 * 分配sq_entries个u32(作为array)和sq_ring放在一起
+	 */
 	sq_ring = io_mem_alloc(struct_size(sq_ring, array, p->sq_entries));
 	if (!sq_ring)
 		return -ENOMEM;
@@ -2740,6 +2910,12 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
  * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,
  * we have to tie this fd to a socket for file garbage collection purposes.
  */
+/*
+ * called by only:
+ *   - fs/io_uring.c|2842| <<io_uring_create>> ret = io_uring_get_fd(ctx);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()-->io_uring_get_fd()
+ */
 static int io_uring_get_fd(struct io_ring_ctx *ctx)
 {
 	struct file *file;
@@ -2778,6 +2954,12 @@ static int io_uring_get_fd(struct io_ring_ctx *ctx)
 	return ret;
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2890| <<io_uring_setup>> ret = io_uring_create(entries, &p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)-->io_uring_setup()-->io_uring_create()
+ */
 static int io_uring_create(unsigned entries, struct io_uring_params *p)
 {
 	struct user_struct *user = NULL;
@@ -2794,6 +2976,7 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 	 * since the sqes are only used at submission time. This allows for
 	 * some flexibility in overcommitting a bit.
 	 */
+	/* round the given value up to nearest power of two */
 	p->sq_entries = roundup_pow_of_two(entries);
 	p->cq_entries = 2 * p->sq_entries;
 
@@ -2809,6 +2992,9 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 		}
 	}
 
+	/*
+	 * 分配和简单初始化io_ring_ctx
+	 */
 	ctx = io_ring_ctx_alloc(p);
 	if (!ctx) {
 		if (account_mem)
@@ -2821,14 +3007,23 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
 	ctx->account_mem = account_mem;
 	ctx->user = user;
 
+	/*
+	 * 分配sq和cq的ring buffer
+	 */
 	ret = io_allocate_scq_urings(ctx, p);
 	if (ret)
 		goto err;
 
+	/*
+	 * 核心思想是根据相关的情况创建kthread和workqueue
+	 */
 	ret = io_sq_offload_start(ctx, p);
 	if (ret)
 		goto err;
 
+	/*
+	 * 获取一个&io_uring_fops的fd
+	 */
 	ret = io_uring_get_fd(ctx);
 	if (ret < 0)
 		goto err;
@@ -2860,6 +3055,10 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p)
  * ring size, we return the actual sq/cq ring sizes (among other things) in the
  * params structure passed in.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|2903| <<SYSCALL_DEFINE2(io_uring_setup)>> return io_uring_setup(entries, params);
+ */
 static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 {
 	struct io_uring_params p;
@@ -2877,6 +3076,9 @@ static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 			IORING_SETUP_SQ_AFF))
 		return -EINVAL;
 
+	/*
+	 * 只看这一个函数就可以了
+	 */
 	ret = io_uring_create(entries, &p);
 	if (ret < 0)
 		return ret;
@@ -2893,6 +3095,10 @@ SYSCALL_DEFINE2(io_uring_setup, u32, entries,
 	return io_uring_setup(entries, params);
 }
 
+/*
+ * called by only:
+ *   - fs/io_uring.c|2952| <<SYSCALL_DEFINE4(io_uring_register)>> ret = __io_uring_register(ctx, opcode, arg, nr_args);
+ */
 static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 			       void __user *arg, unsigned nr_args)
 {
@@ -2962,3 +3168,10 @@ static int __init io_uring_init(void)
 	return 0;
 };
 __initcall(io_uring_init);
+
+/*
+ * 一共3个系统调用:
+ *   - io_uring_enter
+ *   - io_uring_setup
+ *   - io_uring_register
+ */
diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e234086..b35df02 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -37,16 +37,51 @@ struct io_uring_sqe {
 /*
  * sqe->flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|1606| <<io_req_set_file>> if (flags & IOSQE_FIXED_FILE) {
+ *   - fs/io_uring.c|1630| <<io_submit_sqe>> if (unlikely(s->sqe->flags & ~IOSQE_FIXED_FILE))
+ */
 #define IOSQE_FIXED_FILE	(1U << 0)	/* use fixed fileset */
 
 /*
  * io_uring_setup() flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|608| <<io_iopoll_reap_events>> if (!(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|810| <<io_prep_rw>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|1081| <<io_nop>> if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1099| <<io_prep_fsync>> if (unlikely(ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1175| <<io_poll_remove>> if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1296| <<io_poll_add>> if (unlikely(req->ctx->flags & IORING_SETUP_IOPOLL))
+ *   - fs/io_uring.c|1401| <<__io_submit_sqe>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|1831| <<io_sq_thread>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|2734| <<SYSCALL_DEFINE6(io_uring_enter)>> if (ctx->flags & IORING_SETUP_IOPOLL) {
+ *   - fs/io_uring.c|2973| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ */
 #define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
+/*
+ * used by:
+ *   - fs/io_uring.c|2251| <<io_sq_offload_start>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|2702| <<SYSCALL_DEFINE6(io_uring_enter)>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|2973| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ */
 #define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
+/*
+ * used by:
+ *   - fs/io_uring.c|2252| <<io_sq_offload_start>> if (p->flags & IORING_SETUP_SQ_AFF) {
+ *   - fs/io_uring.c|2269| <<io_sq_offload_start>> } else if (p->flags & IORING_SETUP_SQ_AFF) {
+ *   - fs/io_uring.c|2974| <<io_uring_setup>> IORING_SETUP_SQ_AFF))
+ */
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 
 #define IORING_OP_NOP		0
+/*
+ * used by:
+ *   - fs/io_uring.c|1368| <<__io_submit_sqe>> case IORING_OP_READV:
+ *   - fs/io_uring.c|1420| <<io_async_list_from_sqe>> case IORING_OP_READV:
+ */
 #define IORING_OP_READV		1
 #define IORING_OP_WRITEV	2
 #define IORING_OP_FSYNC		3
@@ -72,8 +107,20 @@ struct io_uring_cqe {
 /*
  * Magic offsets for the application to mmap the data it needs
  */
+/*
+ * used by only:
+ *   - fs/io_uring.c|2651| <<io_uring_mmap>> case IORING_OFF_SQ_RING:
+ */
 #define IORING_OFF_SQ_RING		0ULL
+/*
+ * used by only:
+ *   - fs/io_uring.c|2657| <<io_uring_mmap>> case IORING_OFF_CQ_RING:
+ */
 #define IORING_OFF_CQ_RING		0x8000000ULL
+/*
+ * used by only:
+ *   - fs/io_uring.c|2654| <<io_uring_mmap>> case IORING_OFF_SQES:
+ */
 #define IORING_OFF_SQES			0x10000000ULL
 
 /*
@@ -94,6 +141,12 @@ struct io_sqring_offsets {
 /*
  * sq_ring->flags
  */
+/*
+ * used by:
+ *   - fs/io_uring.c|1881| <<io_sq_thread>> ctx->sq_ring->flags |= IORING_SQ_NEED_WAKEUP;
+ *   - fs/io_uring.c|1894| <<io_sq_thread>> ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
+ *   - fs/io_uring.c|1900| <<io_sq_thread>> ctx->sq_ring->flags &= ~IORING_SQ_NEED_WAKEUP;
+ */
 #define IORING_SQ_NEED_WAKEUP	(1U << 0) /* needs io_uring_enter wakeup */
 
 struct io_cqring_offsets {
@@ -109,7 +162,17 @@ struct io_cqring_offsets {
 /*
  * io_uring_enter(2) flags
  */
+/*
+ * used by (主要是第二个):
+ *   - fs/io_uring.c|2681| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))
+ *   - fs/io_uring.c|2720| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & IORING_ENTER_GETEVENTS) {
+ */
 #define IORING_ENTER_GETEVENTS	(1U << 0)
+/*
+ * used by (主要是第二个):
+ *   - fs/io_uring.c|2681| <<SYSCALL_DEFINE6>> if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))
+ *   - fs/io_uring.c|2703| <<SYSCALL_DEFINE6>> if (flags & IORING_ENTER_SQ_WAKEUP)
+ */
 #define IORING_ENTER_SQ_WAKEUP	(1U << 1)
 
 /*
-- 
2.7.4

