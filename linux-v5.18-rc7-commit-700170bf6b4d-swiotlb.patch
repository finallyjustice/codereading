From 28b112883bae4b3b2d30adc59065e36c534068c4 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 26 Jun 2022 10:12:59 -0700
Subject: [PATCH 1/1] linux v5.18-rc7 commit 700170bf6b4d swiotlb

linux v5.18-rc7 commit 700170bf6b4d swiotlb

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/kernel/pci-dma.c |  65 ++++++++++++++
 drivers/xen/swiotlb-xen.c |  71 +++++++++++++++
 include/linux/swiotlb.h   |  66 ++++++++++++++
 kernel/dma/direct.c       |   4 +
 kernel/dma/direct.h       |   5 ++
 kernel/dma/swiotlb.c      | 185 ++++++++++++++++++++++++++++++++++++++
 6 files changed, 396 insertions(+)

diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c
index 30bbe4abb5d6..1cd196c59114 100644
--- a/arch/x86/kernel/pci-dma.c
+++ b/arch/x86/kernel/pci-dma.c
@@ -25,6 +25,13 @@ EXPORT_SYMBOL(dma_ops);
 
 #ifdef CONFIG_IOMMU_DEBUG
 int panic_on_overflow __read_mostly = 1;
+/*
+ * 在以下使用force_iommu:
+ *   - arch/x86/kernel/pci-dma.c|170| <<iommu_setup>> force_iommu = 1;
+ *   - arch/x86/kernel/pci-dma.c|173| <<iommu_setup>> force_iommu = 0;
+ *   - arch/x86/kernel/pci-dma.c|178| <<iommu_setup>> force_iommu = 1;
+ *   - arch/x86/kernel/pci-dma.c|186| <<iommu_setup>> force_iommu = 1;
+ */
 int force_iommu __read_mostly = 1;
 #else
 int panic_on_overflow __read_mostly = 0;
@@ -33,12 +40,37 @@ int force_iommu __read_mostly = 0;
 
 int iommu_merge __read_mostly = 0;
 
+/*
+ * 在以下设置no_iommu:
+ *   - arch/x86/kernel/pci-dma.c|167| <<iommu_setup>> no_iommu = 1;
+ *   - arch/x86/kernel/tboot.c|529| <<tboot_force_iommu>> no_iommu = 0;
+ *   - drivers/iommu/intel/iommu.c|4001| <<platform_optin_force_iommu>> no_iommu = 0;
+ */
 int no_iommu __read_mostly;
 /* Set this to 1 if there is a HW IOMMU in the system */
 int iommu_detected __read_mostly = 0;
 
 #ifdef CONFIG_SWIOTLB
+/*
+ * 在以下使用x86_swiotlb_enable:
+ *   - arch/x86/kernel/amd_gart_64.c|809| <<gart_iommu_init>> x86_swiotlb_enable = false;
+ *   - arch/x86/kernel/pci-dma.c|55| <<pci_swiotlb_detect>> x86_swiotlb_enable = true;
+ *   - arch/x86/kernel/pci-dma.c|62| <<pci_swiotlb_detect>> x86_swiotlb_enable = true;
+ *   - arch/x86/kernel/pci-dma.c|70| <<pci_swiotlb_detect>> x86_swiotlb_enable = true;
+ *   - arch/x86/kernel/pci-dma.c|84| <<pci_xen_swiotlb_init>> if (!xen_initial_domain() && !x86_swiotlb_enable)
+ *   - arch/x86/kernel/pci-dma.c|86| <<pci_xen_swiotlb_init>> x86_swiotlb_enable = true;
+ *   - arch/x86/kernel/pci-dma.c|134| <<pci_iommu_alloc>> swiotlb_init(x86_swiotlb_enable, x86_swiotlb_flags);
+ *   - arch/x86/kernel/pci-dma.c|185| <<iommu_setup>> x86_swiotlb_enable = true;
+ *   - arch/x86/kernel/pci-dma.c|208| <<pci_iommu_init>> if (x86_swiotlb_enable) {
+ */
 bool x86_swiotlb_enable;
+/*
+ * 在以下使用x86_swiotlb_flags:
+ *   - arch/x86/kernel/pci-dma.c|64| <<pci_swiotlb_detect>> x86_swiotlb_flags |= SWIOTLB_FORCE;
+ *   - arch/x86/kernel/pci-dma.c|80| <<pci_xen_swiotlb_init>> x86_swiotlb_flags |= SWIOTLB_ANY;
+ *   - arch/x86/kernel/pci-dma.c|81| <<pci_xen_swiotlb_init>> swiotlb_init_remap(true, x86_swiotlb_flags, xen_swiotlb_fixup);
+ *   - arch/x86/kernel/pci-dma.c|123| <<pci_iommu_alloc>> swiotlb_init(x86_swiotlb_enable, x86_swiotlb_flags);
+ */
 static unsigned int x86_swiotlb_flags;
 
 static void __init pci_swiotlb_detect(void)
@@ -74,16 +106,37 @@ static inline void __init pci_swiotlb_detect(void)
 #ifdef CONFIG_SWIOTLB_XEN
 static void __init pci_xen_swiotlb_init(void)
 {
+	/*
+	 * 在以下使用x86_swiotlb_enable:
+	 *   - arch/x86/kernel/amd_gart_64.c|809| <<gart_iommu_init>> x86_swiotlb_enable = false;
+	 *   - arch/x86/kernel/pci-dma.c|55| <<pci_swiotlb_detect>> x86_swiotlb_enable = true;
+	 *   - arch/x86/kernel/pci-dma.c|62| <<pci_swiotlb_detect>> x86_swiotlb_enable = true;
+	 *   - arch/x86/kernel/pci-dma.c|70| <<pci_swiotlb_detect>> x86_swiotlb_enable = true;
+	 *   - arch/x86/kernel/pci-dma.c|84| <<pci_xen_swiotlb_init>> if (!xen_initial_domain() && !x86_swiotlb_enable)
+	 *   - arch/x86/kernel/pci-dma.c|86| <<pci_xen_swiotlb_init>> x86_swiotlb_enable = true;
+	 *   - arch/x86/kernel/pci-dma.c|134| <<pci_iommu_alloc>> swiotlb_init(x86_swiotlb_enable, x86_swiotlb_flags);
+	 *   - arch/x86/kernel/pci-dma.c|185| <<iommu_setup>> x86_swiotlb_enable = true;
+	 *   - arch/x86/kernel/pci-dma.c|208| <<pci_iommu_init>> if (x86_swiotlb_enable) {
+	 */
 	if (!xen_initial_domain() && !x86_swiotlb_enable)
 		return;
 	x86_swiotlb_enable = true;
 	x86_swiotlb_flags |= SWIOTLB_ANY;
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/pci-dma.c|81| <<pci_xen_swiotlb_init>> swiotlb_init_remap(true, x86_swiotlb_flags, xen_swiotlb_fixup);
+	 *   - kernel/dma/swiotlb.c|287| <<swiotlb_init>> return swiotlb_init_remap(addressing_limit, flags, NULL);
+	 */
 	swiotlb_init_remap(true, x86_swiotlb_flags, xen_swiotlb_fixup);
 	dma_ops = &xen_swiotlb_dma_ops;
 	if (IS_ENABLED(CONFIG_PCI))
 		pci_request_acs();
 }
 
+/*
+ * called by:
+ *   - drivers/pci/xen-pcifront.c|685| <<pcifront_connect_and_init_dma>> err = pci_xen_swiotlb_init_late();
+ */
 int pci_xen_swiotlb_init_late(void)
 {
 	if (dma_ops == &xen_swiotlb_dma_ops)
@@ -110,6 +163,11 @@ static inline void __init pci_xen_swiotlb_init(void)
 }
 #endif /* CONFIG_SWIOTLB_XEN */
 
+/*
+ * called by:
+ *   - arch/x86/mm/init_32.c|732| <<mem_init>> pci_iommu_alloc();
+ *   - arch/x86/mm/init_64.c|1332| <<mem_init>> pci_iommu_alloc();
+ */
 void __init pci_iommu_alloc(void)
 {
 	if (xen_pv_domain()) {
@@ -190,6 +248,13 @@ early_param("iommu", iommu_setup);
 
 static int __init pci_iommu_init(void)
 {
+	/*
+	 * 在以下设置:
+	 *   - arch/x86/kernel/x86_init.c|100| <<global>> .iommu_init = iommu_init_noop,
+	 *   - arch/x86/kernel/aperture_64.c|432| <<gart_iommu_hole_init>> x86_init.iommu.iommu_init = gart_iommu_init;
+	 *   - drivers/iommu/amd/init.c|3110| <<amd_iommu_detect>> x86_init.iommu.iommu_init = amd_iommu_init;
+	 *   - drivers/iommu/intel/dmar.c|936| <<detect_intel_iommu>> x86_init.iommu.iommu_init = intel_iommu_init;
+	 */
 	x86_init.iommu.iommu_init();
 
 #ifdef CONFIG_SWIOTLB
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 67aa74d20162..a67231102c50 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -44,6 +44,10 @@
  * Quick lookup value of the bus address of the IOTLB.
  */
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|58| <<xen_phys_to_dma>> return phys_to_dma(dev, xen_phys_to_bus(dev, paddr));
+ */
 static inline phys_addr_t xen_phys_to_bus(struct device *dev, phys_addr_t paddr)
 {
 	unsigned long bfn = pfn_to_bfn(XEN_PFN_DOWN(paddr));
@@ -53,11 +57,22 @@ static inline phys_addr_t xen_phys_to_bus(struct device *dev, phys_addr_t paddr)
 	return baddr;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|153| <<xen_swiotlb_alloc_coherent>> *dma_handle = xen_phys_to_dma(dev, phys);
+ *   - drivers/xen/swiotlb-xen.c|209| <<xen_swiotlb_map_page>> dma_addr_t dev_addr = xen_phys_to_dma(dev, phys);
+ *   - drivers/xen/swiotlb-xen.c|233| <<xen_swiotlb_map_page>> dev_addr = xen_phys_to_dma(dev, map);
+ *   - drivers/xen/swiotlb-xen.c|417| <<xen_swiotlb_dma_supported>> return xen_phys_to_dma(hwdev, io_tlb_default_mem.end - 1) <= mask;
+ */
 static inline dma_addr_t xen_phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
 	return phys_to_dma(dev, xen_phys_to_bus(dev, paddr));
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|74| <<xen_dma_to_phys>> return xen_bus_to_phys(dev, dma_to_phys(dev, dma_addr));
+ */
 static inline phys_addr_t xen_bus_to_phys(struct device *dev,
 					  phys_addr_t baddr)
 {
@@ -68,12 +83,24 @@ static inline phys_addr_t xen_bus_to_phys(struct device *dev,
 	return paddr;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|268| <<xen_swiotlb_unmap_page>> phys_addr_t paddr = xen_dma_to_phys(hwdev, dev_addr);
+ *   - drivers/xen/swiotlb-xen.c|291| <<xen_swiotlb_sync_single_for_cpu>> phys_addr_t paddr = xen_dma_to_phys(dev, dma_addr);
+ *   - drivers/xen/swiotlb-xen.c|311| <<xen_swiotlb_sync_single_for_device>> phys_addr_t paddr = xen_dma_to_phys(dev, dma_addr);
+ */
 static inline phys_addr_t xen_dma_to_phys(struct device *dev,
 					  dma_addr_t dma_addr)
 {
 	return xen_bus_to_phys(dev, dma_to_phys(dev, dma_addr));
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|155| <<xen_swiotlb_alloc_coherent>> range_straddles_page_boundary(phys, size)) {
+ *   - drivers/xen/swiotlb-xen.c|184| <<xen_swiotlb_free_coherent>> WARN_ON_ONCE(range_straddles_page_boundary(phys, size)))
+ *   - drivers/xen/swiotlb-xen.c|218| <<xen_swiotlb_map_page>> !range_straddles_page_boundary(phys, size) &&
+ */
 static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 {
 	unsigned long next_bfn, xen_pfn = XEN_PFN_DOWN(p);
@@ -88,6 +115,12 @@ static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|280| <<xen_swiotlb_unmap_page>> if (is_xen_swiotlb_buffer(hwdev, dev_addr))
+ *   - drivers/xen/swiotlb-xen.c|300| <<xen_swiotlb_sync_single_for_cpu>> if (is_xen_swiotlb_buffer(dev, dma_addr))
+ *   - drivers/xen/swiotlb-xen.c|313| <<xen_swiotlb_sync_single_for_device>> if (is_xen_swiotlb_buffer(dev, dma_addr))
+ */
 static int is_xen_swiotlb_buffer(struct device *dev, dma_addr_t dma_addr)
 {
 	unsigned long bfn = XEN_PFN_DOWN(dma_to_phys(dev, dma_addr));
@@ -104,6 +137,11 @@ static int is_xen_swiotlb_buffer(struct device *dev, dma_addr_t dma_addr)
 }
 
 #ifdef CONFIG_X86
+/*
+ * 在以下使用xen_swiotlb_fixup():
+ *   - arch/x86/kernel/pci-dma.c|118| <<pci_xen_swiotlb_init>> swiotlb_init_remap(true, x86_swiotlb_flags, xen_swiotlb_fixup);
+ *   - arch/x86/kernel/pci-dma.c|136| <<pci_xen_swiotlb_init_late>> GFP_KERNEL, xen_swiotlb_fixup);
+ */
 int xen_swiotlb_fixup(void *buf, unsigned long nslabs)
 {
 	int rc;
@@ -130,6 +168,9 @@ int xen_swiotlb_fixup(void *buf, unsigned long nslabs)
 	return 0;
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.alloc = xen_swiotlb_alloc_coherent()
+ */
 static void *
 xen_swiotlb_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t flags, unsigned long attrs)
@@ -164,6 +205,9 @@ xen_swiotlb_alloc_coherent(struct device *dev, size_t size,
 	return NULL;
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.free = xen_swiotlb_free_coherent()
+ */
 static void
 xen_swiotlb_free_coherent(struct device *dev, size_t size, void *vaddr,
 		dma_addr_t dma_handle, unsigned long attrs)
@@ -191,6 +235,9 @@ xen_swiotlb_free_coherent(struct device *dev, size_t size, void *vaddr,
  * Once the device is given the dma address, the device owns this memory until
  * either xen_swiotlb_unmap_page or xen_swiotlb_dma_sync_single is performed.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.map_page = xen_swiotlb_map_page()
+ */
 static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 				unsigned long offset, size_t size,
 				enum dma_data_direction dir,
@@ -250,6 +297,9 @@ static dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
  * After this call, reads by the cpu to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.unmap_page = xen_swiotlb_unmap_page()
+ */
 static void xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 		size_t size, enum dma_data_direction dir, unsigned long attrs)
 {
@@ -269,6 +319,9 @@ static void xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 		swiotlb_tbl_unmap_single(hwdev, paddr, size, dir, attrs);
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu()
+ */
 static void
 xen_swiotlb_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr,
 		size_t size, enum dma_data_direction dir)
@@ -286,6 +339,9 @@ xen_swiotlb_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr,
 		swiotlb_sync_single_for_cpu(dev, paddr, size, dir);
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_device = xen_swiotlb_sync_single_for_device()
+ */
 static void
 xen_swiotlb_sync_single_for_device(struct device *dev, dma_addr_t dma_addr,
 		size_t size, enum dma_data_direction dir)
@@ -307,6 +363,9 @@ xen_swiotlb_sync_single_for_device(struct device *dev, dma_addr_t dma_addr,
  * Unmap a set of streaming mode DMA translations.  Again, cpu read rules
  * concerning calls here are the same as for swiotlb_unmap_page() above.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.unmap_sg = xen_swiotlb_unmap_sg()
+ */
 static void
 xen_swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -322,6 +381,9 @@ xen_swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.map_sg = xen_swiotlb_map_sg()
+ */
 static int
 xen_swiotlb_map_sg(struct device *dev, struct scatterlist *sgl, int nelems,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -346,6 +408,9 @@ xen_swiotlb_map_sg(struct device *dev, struct scatterlist *sgl, int nelems,
 	return -EIO;
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_sg_for_cpu = xen_swiotlb_sync_sg_for_cpu()
+ */
 static void
 xen_swiotlb_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,
 			    int nelems, enum dma_data_direction dir)
@@ -359,6 +424,9 @@ xen_swiotlb_sync_sg_for_cpu(struct device *dev, struct scatterlist *sgl,
 	}
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.sync_sg_for_device = xen_swiotlb_sync_sg_for_device()
+ */
 static void
 xen_swiotlb_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,
 			       int nelems, enum dma_data_direction dir)
@@ -378,6 +446,9 @@ xen_swiotlb_sync_sg_for_device(struct device *dev, struct scatterlist *sgl,
  * during bus mastering, then you would pass 0x00ffffff as the mask to
  * this function.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.dma_supported = xen_swiotlb_dma_supported()
+ */
 static int
 xen_swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
diff --git a/include/linux/swiotlb.h b/include/linux/swiotlb.h
index 7ed35dd3de6e..cb2edb340483 100644
--- a/include/linux/swiotlb.h
+++ b/include/linux/swiotlb.h
@@ -13,8 +13,33 @@ struct device;
 struct page;
 struct scatterlist;
 
+/*
+ * 在以下使用SWIOTLB_VERBOSE:
+ *   - arch/arm/mm/init.c|274| <<mem_init>> swiotlb_init(max_pfn > arm_dma_pfn_limit, SWIOTLB_VERBOSE);
+ *   - arch/arm64/mm/init.c|454| <<mem_init>> swiotlb_init(max_pfn > PFN_DOWN(arm64_dma_phys_limit), SWIOTLB_VERBOSE);
+ *   - arch/ia64/mm/init.c|440| <<mem_init>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/mips/cavium-octeon/dma-octeon.c|238| <<plat_swiotlb_setup>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/riscv/mm/init.c|123| <<mem_init>> swiotlb_init(max_pfn > PFN_DOWN(dma32_phys_limit), SWIOTLB_VERBOSE);
+ *   - arch/s390/mm/init.c|188| <<pv_init>> swiotlb_init(true, SWIOTLB_FORCE | SWIOTLB_VERBOSE);
+ *   - kernel/dma/swiotlb.c|346| <<swiotlb_init_remap>> if (flags & SWIOTLB_VERBOSE)
+ */
 #define SWIOTLB_VERBOSE	(1 << 0) /* verbose initialization */
+/*
+ * 在以下使用SWIOTLB_FORCE:
+ *   - arch/powerpc/platforms/pseries/svm.c|31| <<init_svm>> ppc_swiotlb_flags |= SWIOTLB_ANY | SWIOTLB_FORCE;
+ *   - arch/s390/mm/init.c|188| <<pv_init>> swiotlb_init(true, SWIOTLB_FORCE | SWIOTLB_VERBOSE);
+ *   - arch/x86/kernel/pci-dma.c|64| <<pci_swiotlb_detect>> x86_swiotlb_flags |= SWIOTLB_FORCE;
+ *   - kernel/dma/swiotlb.c|344| <<swiotlb_init_remap>> mem->force_bounce = flags & SWIOTLB_FORCE;
+ */
 #define SWIOTLB_FORCE	(1 << 1) /* force bounce buffering */
+/*
+ * 在以下使用SWIOTLB_ANY:
+ *   - arch/powerpc/platforms/pseries/svm.c|31| <<init_svm>> ppc_swiotlb_flags |= SWIOTLB_ANY | SWIOTLB_FORCE;
+ *   - arch/x86/kernel/pci-dma.c|80| <<pci_xen_swiotlb_init>> x86_swiotlb_flags |= SWIOTLB_ANY;
+ *   - kernel/dma/swiotlb.c|318| <<swiotlb_init_remap>> if (flags & SWIOTLB_ANY)
+ */
 #define SWIOTLB_ANY	(1 << 2) /* allow any memory for the buffer */
 
 /*
@@ -60,6 +85,9 @@ dma_addr_t swiotlb_map(struct device *dev, phys_addr_t phys,
 		size_t size, enum dma_data_direction dir, unsigned long attrs);
 
 #ifdef CONFIG_SWIOTLB
+/*
+ * 删除了! :)
+ */
 extern enum swiotlb_force swiotlb_force;
 
 /**
@@ -99,11 +127,42 @@ struct io_tlb_mem {
 	unsigned int index;
 	spinlock_t lock;
 	struct dentry *debugfs;
+	/*
+	 * 在以下使用io_tlb_mem->late_alloc:
+	 *   - kernel/dma/swiotlb.c|183| <<swiotlb_update_mem_attributes>> if (!mem->nslabs || mem->late_alloc)
+	 *   - kernel/dma/swiotlb.c|204| <<swiotlb_init_io_tlb_mem>> mem->late_alloc = late_alloc;
+	 *   - kernel/dma/swiotlb.c|375| <<swiotlb_exit>> if (mem->late_alloc) {
+	 */
 	bool late_alloc;
+	/*
+	 * 在以下使用io_tlb_mem->force_bounce:
+	 *   - include/linux/swiotlb.h|124| <<is_swiotlb_force_bounce>> return mem && mem->force_bounce;
+	 *   - kernel/dma/swiotlb.c|207| <<swiotlb_init_io_tlb_mem>> mem->force_bounce = true;
+	 *   - kernel/dma/swiotlb.c|279| <<swiotlb_init_remap>> mem->force_bounce = flags & SWIOTLB_FORCE;
+	 *   - kernel/dma/swiotlb.c|839| <<rmem_swiotlb_device_init>> mem->force_bounce = true;
+	 *
+	 * %true if swiotlb bouncing is forced
+	 */
 	bool force_bounce;
+	/*
+	 * 在以下使用io_tlb_mem->for_alloc:
+	 *   - include/linux/swiotlb.h|175| <<is_swiotlb_for_alloc>> return dev->dma_io_tlb_mem->for_alloc;
+	 *   - kernel/dma/swiotlb.c|840| <<rmem_swiotlb_device_init>> mem->for_alloc = true;
+	 *
+	 * 似乎只rmem用
+	 * %true if the pool is used for memory allocation
+	 */
 	bool for_alloc;
 	struct io_tlb_slot {
 		phys_addr_t orig_addr;
+		/*
+		 * 在以下使用alloc_size:
+		 *   - kernel/dma/swiotlb.c|302| <<swiotlb_init_io_tlb_mem>> mem->slots[i].alloc_size = 0;
+		 *   - kernel/dma/swiotlb.c|553| <<swiotlb_bounce>> size_t alloc_size = mem->slots[index].alloc_size;
+		 *   - kernel/dma/swiotlb.c|703| <<swiotlb_find_slots>> mem->slots[i].alloc_size =
+		 *   - kernel/dma/swiotlb.c|788| <<swiotlb_release_slots>> int nslots = nr_slots(mem->slots[index].alloc_size + offset);
+		 *   - kernel/dma/swiotlb.c|810| <<swiotlb_release_slots>> mem->slots[i].alloc_size = 0;
+		 */
 		size_t alloc_size;
 		unsigned int list;
 	} *slots;
@@ -117,6 +176,13 @@ static inline bool is_swiotlb_buffer(struct device *dev, phys_addr_t paddr)
 	return mem && paddr >= mem->start && paddr < mem->end;
 }
 
+/*
+ * called by:
+ *   - drivers/xen/swiotlb-xen.c|211| <<xen_swiotlb_map_page>> !is_swiotlb_force_bounce(dev))
+ *   - include/trace/events/swiotlb.h|27| <<__field>> __entry->force = is_swiotlb_force_bounce(dev);
+ *   - kernel/dma/direct.c|576| <<dma_direct_max_mapping_size>> (dma_addressing_limited(dev) || is_swiotlb_force_bounce(dev)))
+ *   - kernel/dma/direct.h|90| <<dma_direct_map_page>> if (is_swiotlb_force_bounce(dev))
+ */
 static inline bool is_swiotlb_force_bounce(struct device *dev)
 {
 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
diff --git a/kernel/dma/direct.c b/kernel/dma/direct.c
index e978f36e6be8..435a8ff0f9c6 100644
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@ -466,6 +466,10 @@ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
 }
 #endif
 
+/*
+ * called by:
+ *   - kernel/dma/mapping.c|193| <<__dma_map_sg_attrs>> ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
+ */
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs)
 {
diff --git a/kernel/dma/direct.h b/kernel/dma/direct.h
index a78c0ba70645..952835ea3e91 100644
--- a/kernel/dma/direct.h
+++ b/kernel/dma/direct.h
@@ -80,6 +80,11 @@ static inline void dma_direct_sync_single_for_cpu(struct device *dev,
 		arch_dma_mark_clean(paddr, size);
 }
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|476| <<dma_direct_map_sg>> sg->dma_address = dma_direct_map_page(dev, sg_page(sg),
+ *   - kernel/dma/mapping.c|156| <<dma_map_page_attrs>> addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
+ */
 static inline dma_addr_t dma_direct_map_page(struct device *dev,
 		struct page *page, unsigned long offset, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
diff --git a/kernel/dma/swiotlb.c b/kernel/dma/swiotlb.c
index dfa1de89dc94..e0a439535066 100644
--- a/kernel/dma/swiotlb.c
+++ b/kernel/dma/swiotlb.c
@@ -58,17 +58,53 @@
  * 64bit capable cards will only lightly use the swiotlb.  If we can't
  * allocate a contiguous 1MB, we're probably in trouble anyway.
  */
+/*
+ * 在以下使用IO_TLB_MIN_SLABS:
+ *   - kernel/dma/swiotlb.c|338| <<swiotlb_init_remap>> if (nslabs < IO_TLB_MIN_SLABS)
+ *   - kernel/dma/swiotlb.c|408| <<swiotlb_init_late>> while ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {
+ *   - kernel/dma/swiotlb.c|427| <<swiotlb_init_late>> if (nslabs < IO_TLB_MIN_SLABS)
+ */
 #define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)
 
 #define INVALID_PHYS_ADDR (~(phys_addr_t)0)
 
+/*
+ * 在以下使用swiotlb_force_bounce:
+ *   - kernel/dma/swiotlb.c|85| <<setup_io_tlb_npages>> swiotlb_force_bounce = true;
+ *   - kernel/dma/swiotlb.c|206| <<swiotlb_init_io_tlb_mem>> if (swiotlb_force_bounce)
+ *   - kernel/dma/swiotlb.c|241| <<swiotlb_init_remap>> if (!addressing_limit && !swiotlb_force_bounce)
+ *   - kernel/dma/swiotlb.c|363| <<swiotlb_exit>> if (swiotlb_force_bounce)
+ */
 static bool swiotlb_force_bounce;
+/*
+ * 在以下使用swiotlb_force_disable:
+ *   - kernel/dma/swiotlb.c|87| <<setup_io_tlb_npages>> swiotlb_force_disable = true;
+ *   - kernel/dma/swiotlb.c|243| <<swiotlb_init_remap>> if (swiotlb_force_disable)
+ *   - kernel/dma/swiotlb.c|305| <<swiotlb_init_late>> if (swiotlb_force_disable)
+ */
 static bool swiotlb_force_disable;
 
 struct io_tlb_mem io_tlb_default_mem;
 
+/*
+ * 在以下使用swiotlb_unencrypted_base:
+ *   - arch/x86/kernel/cpu/mshyperv.c|337| <<ms_hyperv_init_platform>> swiotlb_unencrypted_base = ms_hyperv.shared_gpa_boundary;
+ *   - kernel/dma/swiotlb.c|153| <<swiotlb_mem_remap>> if (swiotlb_unencrypted_base) {
+ *   - kernel/dma/swiotlb.c|154| <<swiotlb_mem_remap>> phys_addr_t paddr = mem->start + swiotlb_unencrypted_base;
+ *   - kernel/dma/swiotlb.c|220| <<swiotlb_init_io_tlb_mem>> if (swiotlb_unencrypted_base)
+ *
+ * 这个一个就可以了, 为每一个start加上就行
+ */
 phys_addr_t swiotlb_unencrypted_base;
 
+/*
+ * 在以下使用default_nslabs:
+ *   - kernel/dma/swiotlb.c|99| <<setup_io_tlb_npages>> default_nslabs =
+ *   - kernel/dma/swiotlb.c|134| <<swiotlb_size_or_default>> return default_nslabs << IO_TLB_SHIFT;
+ *   - kernel/dma/swiotlb.c|149| <<swiotlb_adjust_size>> if (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)
+ *   - kernel/dma/swiotlb.c|152| <<swiotlb_adjust_size>> default_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);
+ *   - kernel/dma/swiotlb.c|308| <<swiotlb_init_remap>> unsigned long nslabs = default_nslabs;
+ */
 static unsigned long default_nslabs = IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT;
 
 static int __init
@@ -90,6 +126,13 @@ setup_io_tlb_npages(char *str)
 }
 early_param("swiotlb", setup_io_tlb_npages);
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gem/i915_gem_internal.c|49| <<i915_gem_object_get_pages_internal>> max_segment = swiotlb_max_segment();
+ *   - drivers/gpu/drm/i915/i915_scatterlist.h|132| <<i915_sg_segment_size>> unsigned int size = swiotlb_max_segment();
+ *
+ * 这个应该还是不变
+ */
 unsigned int swiotlb_max_segment(void)
 {
 	if (!io_tlb_default_mem.nslabs)
@@ -98,11 +141,22 @@ unsigned int swiotlb_max_segment(void)
 }
 EXPORT_SYMBOL_GPL(swiotlb_max_segment);
 
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|129| <<xen_mm_init>> rc = swiotlb_init_late(swiotlb_size_or_default(),
+ *   - arch/x86/kernel/pci-dma.c|94| <<pci_xen_swiotlb_init_late>> int rc = swiotlb_init_late(swiotlb_size_or_default(),
+ *   - arch/x86/kernel/setup.c|459| <<reserve_crashkernel_low>> low_size = max(swiotlb_size_or_default() + (8UL << 20), 256UL << 20);
+ */
 unsigned long swiotlb_size_or_default(void)
 {
 	return default_nslabs << IO_TLB_SHIFT;
 }
 
+/*
+ * called by:
+ *   - arch/mips/cavium-octeon/dma-octeon.c|237| <<plat_swiotlb_setup>> swiotlb_adjust_size(swiotlbsize);
+ *   - arch/x86/mm/mem_encrypt_amd.c|244| <<sev_setup_arch>> swiotlb_adjust_size(size);
+ */
 void __init swiotlb_adjust_size(unsigned long size)
 {
 	/*
@@ -117,6 +171,15 @@ void __init swiotlb_adjust_size(unsigned long size)
 	pr_info("SWIOTLB bounce buffer size adjusted to %luMB", size >> 20);
 }
 
+/*
+ * called by:
+ *   - arch/powerpc/kernel/dma-swiotlb.c|24| <<check_swiotlb_enabled>> swiotlb_print_info();
+ *   - arch/x86/kernel/pci-dma.c|206| <<pci_iommu_init>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|347| <<swiotlb_init_remap>> swiotlb_print_info();
+ *   - kernel/dma/swiotlb.c|431| <<swiotlb_init_late>> swiotlb_print_info();
+ *
+ * 应该加参数, print low or high
+ */
 void swiotlb_print_info(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -146,11 +209,22 @@ static inline unsigned long nr_slots(u64 val)
  * Isolation VMs).
  */
 #ifdef CONFIG_HAS_IOMEM
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|189| <<swiotlb_update_mem_attributes>> mem->vaddr = swiotlb_mem_remap(mem, bytes);
+ */
 static void *swiotlb_mem_remap(struct io_tlb_mem *mem, unsigned long bytes)
 {
 	void *vaddr = NULL;
 
 	if (swiotlb_unencrypted_base) {
+		/*
+		 * 在以下使用swiotlb_unencrypted_base:
+		 *   - arch/x86/kernel/cpu/mshyperv.c|337| <<ms_hyperv_init_platform>> swiotlb_unencrypted_base = ms_hyperv.shared_gpa_boundary;
+		 *   - kernel/dma/swiotlb.c|153| <<swiotlb_mem_remap>> if (swiotlb_unencrypted_base) {
+		 *   - kernel/dma/swiotlb.c|154| <<swiotlb_mem_remap>> phys_addr_t paddr = mem->start + swiotlb_unencrypted_base;
+		 *   - kernel/dma/swiotlb.c|220| <<swiotlb_init_io_tlb_mem>> if (swiotlb_unencrypted_base)
+		 */
 		phys_addr_t paddr = mem->start + swiotlb_unencrypted_base;
 
 		vaddr = memremap(paddr, bytes, MEMREMAP_WB);
@@ -174,6 +248,15 @@ static void *swiotlb_mem_remap(struct io_tlb_mem *mem, unsigned long bytes)
  * call SWIOTLB when the operations are possible.  It needs to be called
  * before the SWIOTLB memory is used.
  */
+/*
+ * called by:
+ *   - arch/powerpc/platforms/pseries/svm.c|34| <<init_svm>> swiotlb_update_mem_attributes();
+ *   - arch/s390/mm/init.c|189| <<pv_init>> swiotlb_update_mem_attributes();
+ *   - arch/x86/hyperv/hv_init.c|510| <<hyperv_init>> swiotlb_update_mem_attributes();
+ *   - arch/x86/mm/mem_encrypt.c|86| <<mem_encrypt_init>> swiotlb_update_mem_attributes();
+ *
+ * 应该修改, 为low和high都做操作
+ */
 void __init swiotlb_update_mem_attributes(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
@@ -191,6 +274,12 @@ void __init swiotlb_update_mem_attributes(void)
 		mem->vaddr = vaddr;
 }
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|278| <<swiotlb_init_remap>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+ *   - kernel/dma/swiotlb.c|351| <<swiotlb_init_late>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(vstart), nslabs, true);
+ *   - kernel/dma/swiotlb.c|838| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+ */
 static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
 				    unsigned long nslabs, bool late_alloc)
 {
@@ -229,6 +318,11 @@ static void swiotlb_init_io_tlb_mem(struct io_tlb_mem *mem, phys_addr_t start,
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/pci-dma.c|81| <<pci_xen_swiotlb_init>> swiotlb_init_remap(true, x86_swiotlb_flags, xen_swiotlb_fixup);
+ *   - kernel/dma/swiotlb.c|287| <<swiotlb_init>> return swiotlb_init_remap(addressing_limit, flags, NULL);
+ */
 void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 		int (*remap)(void *tlb, unsigned long nslabs))
 {
@@ -238,8 +332,21 @@ void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 	size_t bytes;
 	void *tlb;
 
+	/*
+	 * 在以下使用swiotlb_force_bounce:
+	 *   - kernel/dma/swiotlb.c|85| <<setup_io_tlb_npages>> swiotlb_force_bounce = true;
+	 *   - kernel/dma/swiotlb.c|206| <<swiotlb_init_io_tlb_mem>> if (swiotlb_force_bounce)
+	 *   - kernel/dma/swiotlb.c|241| <<swiotlb_init_remap>> if (!addressing_limit && !swiotlb_force_bounce)
+	 *   - kernel/dma/swiotlb.c|363| <<swiotlb_exit>> if (swiotlb_force_bounce)
+	 */
 	if (!addressing_limit && !swiotlb_force_bounce)
 		return;
+	/*
+	 * 在以下使用swiotlb_force_disable:
+	 *   - kernel/dma/swiotlb.c|87| <<setup_io_tlb_npages>> swiotlb_force_disable = true;
+	 *   - kernel/dma/swiotlb.c|243| <<swiotlb_init_remap>> if (swiotlb_force_disable)
+	 *   - kernel/dma/swiotlb.c|305| <<swiotlb_init_late>> if (swiotlb_force_disable)
+	 */
 	if (swiotlb_force_disable)
 		return;
 
@@ -282,8 +389,26 @@ void __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,
 		swiotlb_print_info();
 }
 
+/*
+ * called by:
+ *   - arch/arm/mm/init.c|274| <<mem_init>> swiotlb_init(max_pfn > arm_dma_pfn_limit, SWIOTLB_VERBOSE);
+ *   - arch/arm64/mm/init.c|454| <<mem_init>> swiotlb_init(max_pfn > PFN_DOWN(arm64_dma_phys_limit), SWIOTLB_VERBOSE);
+ *   - arch/ia64/mm/init.c|440| <<mem_init>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/mips/cavium-octeon/dma-octeon.c|238| <<plat_swiotlb_setup>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/mips/loongson64/dma.c|27| <<plat_swiotlb_setup>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/mips/sibyte/common/dma.c|13| <<plat_swiotlb_setup>> swiotlb_init(true, SWIOTLB_VERBOSE);
+ *   - arch/powerpc/mm/mem.c|254| <<mem_init>> swiotlb_init(ppc_swiotlb_enable, ppc_swiotlb_flags);
+ *   - arch/riscv/mm/init.c|123| <<mem_init>> swiotlb_init(max_pfn > PFN_DOWN(dma32_phys_limit), SWIOTLB_VERBOSE);
+ *   - arch/s390/mm/init.c|188| <<pv_init>> swiotlb_init(true, SWIOTLB_FORCE | SWIOTLB_VERBOSE);
+ *   - arch/x86/kernel/pci-dma.c|123| <<pci_iommu_alloc>> swiotlb_init(x86_swiotlb_enable, x86_swiotlb_flags);
+ */
 void __init swiotlb_init(bool addressing_limit, unsigned int flags)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kernel/pci-dma.c|81| <<pci_xen_swiotlb_init>> swiotlb_init_remap(true, x86_swiotlb_flags, xen_swiotlb_fixup);
+	 *   - kernel/dma/swiotlb.c|287| <<swiotlb_init>> return swiotlb_init_remap(addressing_limit, flags, NULL);
+	 */
 	return swiotlb_init_remap(addressing_limit, flags, NULL);
 }
 
@@ -292,6 +417,12 @@ void __init swiotlb_init(bool addressing_limit, unsigned int flags)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * called by:
+ *   - arch/arm/xen/mm.c|129| <<xen_mm_init>> rc = swiotlb_init_late(swiotlb_size_or_default(),
+ *   - arch/x86/kernel/pci-dma.c|101| <<pci_xen_swiotlb_init_late>> int rc = swiotlb_init_late(swiotlb_size_or_default(),
+ *   - arch/x86/pci/sta2x11-fixup.c|60| <<sta2x11_new_instance>> if (swiotlb_init_late(size, GFP_DMA, NULL))
+ */
 int swiotlb_init_late(size_t size, gfp_t gfp_mask,
 		int (*remap)(void *tlb, unsigned long nslabs))
 {
@@ -348,18 +479,37 @@ int swiotlb_init_late(size_t size, gfp_t gfp_mask,
 
 	set_memory_decrypted((unsigned long)vstart,
 			     (nslabs << IO_TLB_SHIFT) >> PAGE_SHIFT);
+	/*
+	 * called by:
+	 *   - kernel/dma/swiotlb.c|278| <<swiotlb_init_remap>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+	 *   - kernel/dma/swiotlb.c|351| <<swiotlb_init_late>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(vstart), nslabs, true);
+	 *   - kernel/dma/swiotlb.c|838| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+	 */
 	swiotlb_init_io_tlb_mem(mem, virt_to_phys(vstart), nslabs, true);
 
 	swiotlb_print_info();
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/ia64/hp/common/sba_iommu.c|2106| <<sba_init>> swiotlb_exit();
+ *   - arch/powerpc/kernel/dma-swiotlb.c|26| <<check_swiotlb_enabled>> swiotlb_exit();
+ *   - arch/x86/kernel/pci-dma.c|212| <<pci_iommu_init>> swiotlb_exit();
+ */
 void __init swiotlb_exit(void)
 {
 	struct io_tlb_mem *mem = &io_tlb_default_mem;
 	unsigned long tbl_vaddr;
 	size_t tbl_size, slots_size;
 
+	/*
+	 * 在以下使用swiotlb_force_bounce:
+	 *   - kernel/dma/swiotlb.c|85| <<setup_io_tlb_npages>> swiotlb_force_bounce = true;
+	 *   - kernel/dma/swiotlb.c|206| <<swiotlb_init_io_tlb_mem>> if (swiotlb_force_bounce)
+	 *   - kernel/dma/swiotlb.c|241| <<swiotlb_init_remap>> if (!addressing_limit && !swiotlb_force_bounce)
+	 *   - kernel/dma/swiotlb.c|363| <<swiotlb_exit>> if (swiotlb_force_bounce)
+	 */
 	if (swiotlb_force_bounce)
 		return;
 
@@ -571,6 +721,12 @@ static int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,
 	return index;
 }
 
+/*
+ * called by:
+ *   - drivers/iommu/dma-iommu.c|992| <<iommu_dma_map_page>> phys = swiotlb_tbl_map_single(dev, phys, size, aligned_size,
+ *   - drivers/xen/swiotlb-xen.c|219| <<xen_swiotlb_map_page>> map = swiotlb_tbl_map_single(dev, phys, size, size, 0, dir, attrs);
+ *   - kernel/dma/swiotlb.c|781| <<swiotlb_map>> swiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, 0, dir,
+ */
 phys_addr_t swiotlb_tbl_map_single(struct device *dev, phys_addr_t orig_addr,
 		size_t mapping_size, size_t alloc_size,
 		unsigned int alloc_align_mask, enum dma_data_direction dir,
@@ -705,6 +861,11 @@ void swiotlb_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,
  * Create a swiotlb mapping for the buffer at @paddr, and in case of DMAing
  * to the device copy the data into it as well.
  */
+/*
+ * called by:
+ *   - kernel/dma/direct.h|91| <<dma_direct_map_page>> return swiotlb_map(dev, phys, size, dir, attrs);
+ *   - kernel/dma/direct.h|95| <<dma_direct_map_page>> return swiotlb_map(dev, phys, size, dir, attrs);
+ */
 dma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,
 		enum dma_data_direction dir, unsigned long attrs)
 {
@@ -758,6 +919,11 @@ bool is_swiotlb_active(struct device *dev)
 }
 EXPORT_SYMBOL_GPL(is_swiotlb_active);
 
+/*
+ * called by:
+ *   - kernel/dma/swiotlb.c|887| <<swiotlb_create_default_debugfs>> swiotlb_create_debugfs_files(&io_tlb_default_mem, "swiotlb");
+ *   - kernel/dma/swiotlb.c|961| <<rmem_swiotlb_device_init>> swiotlb_create_debugfs_files(mem, rmem->name);
+ */
 static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem,
 					 const char *dirname)
 {
@@ -769,6 +935,10 @@ static void swiotlb_create_debugfs_files(struct io_tlb_mem *mem,
 	debugfs_create_ulong("io_tlb_used", 0400, mem->debugfs, &mem->used);
 }
 
+/*
+ * 在以下使用swiotlb_create_default_debugfs():
+ *   - kernel/dma/swiotlb.c|885| <<swiotlb_create_default_debugfs>> static int __init __maybe_unused swiotlb_create_default_debugfs(void )
+ */
 static int __init __maybe_unused swiotlb_create_default_debugfs(void)
 {
 	swiotlb_create_debugfs_files(&io_tlb_default_mem, "swiotlb");
@@ -781,6 +951,10 @@ late_initcall(swiotlb_create_default_debugfs);
 
 #ifdef CONFIG_DMA_RESTRICTED_POOL
 
+/*
+ * called by:
+ *   - kernel/dma/direct.c|107| <<dma_direct_alloc_swiotlb>> struct page *page = swiotlb_alloc(dev, size);
+ */
 struct page *swiotlb_alloc(struct device *dev, size_t size)
 {
 	struct io_tlb_mem *mem = dev->dma_io_tlb_mem;
@@ -835,12 +1009,23 @@ static int rmem_swiotlb_device_init(struct reserved_mem *rmem,
 
 		set_memory_decrypted((unsigned long)phys_to_virt(rmem->base),
 				     rmem->size >> PAGE_SHIFT);
+		/*
+		 * called by:
+		 *   - kernel/dma/swiotlb.c|278| <<swiotlb_init_remap>> swiotlb_init_io_tlb_mem(mem, __pa(tlb), nslabs, false);
+		 *   - kernel/dma/swiotlb.c|351| <<swiotlb_init_late>> swiotlb_init_io_tlb_mem(mem, virt_to_phys(vstart), nslabs, true);
+		 *   - kernel/dma/swiotlb.c|838| <<rmem_swiotlb_device_init>> swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
+		 */
 		swiotlb_init_io_tlb_mem(mem, rmem->base, nslabs, false);
 		mem->force_bounce = true;
 		mem->for_alloc = true;
 
 		rmem->priv = mem;
 
+		/*
+		 * called by:
+		 *   - kernel/dma/swiotlb.c|887| <<swiotlb_create_default_debugfs>> swiotlb_create_debugfs_files(&io_tlb_default_mem, "swiotlb");
+		 *   - kernel/dma/swiotlb.c|961| <<rmem_swiotlb_device_init>> swiotlb_create_debugfs_files(mem, rmem->name);
+		 */
 		swiotlb_create_debugfs_files(mem, rmem->name);
 	}
 
-- 
2.17.1

