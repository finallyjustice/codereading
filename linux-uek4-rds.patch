From 1173152d02dd6051e1bff90e9b86e9216f467f85 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Mon, 30 Dec 2019 14:31:29 -0800
Subject: [PATCH 1/1] linux uek4 rds

v4.1.12-124.24.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 drivers/infiniband/core/cma.c           |  23 ++++++
 drivers/infiniband/core/core_priv.h     |   5 ++
 drivers/infiniband/core/cq.c            |   4 +
 drivers/infiniband/core/device.c        |  20 +++++
 drivers/infiniband/core/mad.c           |  11 +++
 drivers/infiniband/core/verbs.c         |  29 +++++++
 drivers/infiniband/hw/mlx4/cq.c         |  13 +++
 drivers/infiniband/hw/mlx4/qp.c         |  64 +++++++++++++++
 drivers/net/ethernet/mellanox/mlx4/cq.c |  43 ++++++++++
 drivers/net/ethernet/mellanox/mlx4/eq.c |  10 +++
 include/rdma/ib_verbs.h                 |  18 +++++
 net/rds/ib.h                            |   6 ++
 net/rds/ib_cm.c                         |  25 ++++++
 net/rds/ib_ring.c                       |   7 ++
 net/rds/ib_send.c                       |  12 +++
 net/rds/message.c                       |   7 ++
 net/rds/rds.h                           |  56 +++++++++++++
 net/rds/send.c                          | 136 ++++++++++++++++++++++++++++++++
 net/rds/threads.c                       |  16 ++++
 19 files changed, 505 insertions(+)

diff --git a/drivers/infiniband/core/cma.c b/drivers/infiniband/core/cma.c
index 008ae72..e525e16 100644
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -819,6 +819,15 @@ static void cma_deref_id(struct rdma_id_private *id_priv)
 		complete(&id_priv->comp);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/cma.c|1972| <<cma_new_conn_id>> id = __rdma_create_id(listen_id->route.addr.dev_addr.net,
+ *   - drivers/infiniband/core/cma.c|2034| <<cma_new_udp_id>> id = __rdma_create_id(net, listen_id->event_handler, listen_id->context,
+ *   - drivers/infiniband/core/cma.c|2303| <<iw_conn_req_handler>> new_cm_id = __rdma_create_id(listen_id->id.route.addr.dev_addr.net,
+ *   - drivers/infiniband/core/cma.c|2432| <<cma_listen_on_dev>> id = __rdma_create_id(net, cma_listen_handler, id_priv, id_priv->id.ps,
+ *   - drivers/infiniband/core/ucma.c|480| <<ucma_create_id>> cm_id = __rdma_create_id(current->nsproxy->net_ns,
+ *   - include/rdma/rdma_cm.h|178| <<rdma_create_id>> __rdma_create_id((net), (event_handler), (context), (ps), (qp_type), \
+ */
 struct rdma_cm_id *__rdma_create_id(struct net *net,
 				    rdma_cm_event_handler event_handler,
 				    void *context, enum rdma_port_space ps,
@@ -896,6 +905,20 @@ static int cma_init_conn_qp(struct rdma_id_private *id_priv, struct ib_qp *qp)
 	return ib_modify_qp(qp, &qp_attr, qp_attr_mask);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iser_verbs.c|478| <<iser_create_ib_conn_res>> ret = rdma_create_qp(ib_conn->cma_id, device->pd, &init_attr);
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|146| <<isert_create_qp>> ret = rdma_create_qp(cma_id, device->pd, &attr);
+ *   - drivers/nvme/host/rdma.c|265| <<nvme_rdma_create_qp>> ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
+ *   - drivers/nvme/target/rdma.c|979| <<nvmet_rdma_create_queue_ib>> ret = rdma_create_qp(queue->cm_id, ndev->pd, &qp_attr);
+ *   - drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c|770| <<kiblnd_create_conn>> rc = rdma_create_qp(cmid, conn->ibc_hdev->ibh_pd, init_qp_attr);
+ *   - net/9p/trans_rdma.c|726| <<rdma_create_trans>> err = rdma_create_qp(rdma->cm_id, rdma->pd, &qp_attr);
+ *   - net/rds/ib_cm.c|910| <<rds_ib_setup_qp>> ret = rdma_create_qp(ic->i_cm_id, ic->i_pd, &qp_attr);
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|816| <<svc_rdma_accept>> ret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|693| <<rpcrdma_ep_recreate_xprt>> err = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|742| <<rpcrdma_ep_reconnect>> err = rdma_create_qp(id, ia->ri_pd, &ep->rep_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|776| <<rpcrdma_ep_connect>> rc = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);
+ */
 int rdma_create_qp(struct rdma_cm_id *id, struct ib_pd *pd,
 		   struct ib_qp_init_attr *qp_init_attr)
 {
diff --git a/drivers/infiniband/core/core_priv.h b/drivers/infiniband/core/core_priv.h
index 022faae..c88f16f 100644
--- a/drivers/infiniband/core/core_priv.h
+++ b/drivers/infiniband/core/core_priv.h
@@ -324,6 +324,11 @@ struct ib_device *ib_device_get_by_index(u32 ifindex);
 void nldev_init(void);
 void nldev_exit(void);
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/uverbs_cmd.c|2200| <<_ib_create_qp>> qp = _ib_create_qp(device, pd, &attr, uhw,
+ *   - drivers/infiniband/core/verbs.c|881| <<ib_create_qp>> qp = _ib_create_qp(device, pd, qp_init_attr, NULL, NULL);
+ */
 static inline struct ib_qp *_ib_create_qp(struct ib_device *dev,
 					  struct ib_pd *pd,
 					  struct ib_qp_init_attr *attr,
diff --git a/drivers/infiniband/core/cq.c b/drivers/infiniband/core/cq.c
index af5ad6a..8af4c4e 100644
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@ -134,6 +134,10 @@ static void ib_cq_completion_workqueue(struct ib_cq *cq, void *private)
  * specified context. The ULP must use wr->wr_cqe instead of wr->wr_id
  * to use this CQ abstraction.
  */
+/*
+ * called by:
+ *  - include/rdma/ib_verbs.h|3478| <<ib_alloc_cq>> __ib_alloc_cq((device), (priv), (nr_cqe), (comp_vect), (poll_ctx), KBUILD_MODNAME)
+ */
 struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
 			    int nr_cqe, int comp_vector,
 			    enum ib_poll_context poll_ctx, const char *caller)
diff --git a/drivers/infiniband/core/device.c b/drivers/infiniband/core/device.c
index 3661662..8852228 100644
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -639,6 +639,26 @@ EXPORT_SYMBOL(ib_unregister_device);
  * ib_register_client() is called, the client will receive an add
  * callback for all devices already registered.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/core/cm.c|4752| <<ib_cm_init>> ret = ib_register_client(&cm_client);
+ *   - drivers/infiniband/core/cma.c|4880| <<cma_init>> ret = ib_register_client(&cma_client);
+ *   - drivers/infiniband/core/mad.c|3372| <<ib_mad_init>> if (ib_register_client(&mad_client)) {
+ *   - drivers/infiniband/core/multicast.c|881| <<mcast_init>> ret = ib_register_client(&mcast_client);
+ *   - drivers/infiniband/core/rdmaip.c|2984| <<rdmaip_init>> ret = ib_register_client(&rdmaip_client);
+ *   - drivers/infiniband/core/sa_query.c|2480| <<ib_sa_init>> ret = ib_register_client(&sa_client);
+ *   - drivers/infiniband/core/ucm.c|1350| <<ib_ucm_init>> ret = ib_register_client(&ucm_client);
+ *   - drivers/infiniband/core/user_mad.c|1390| <<ib_umad_init>> ret = ib_register_client(&umad_client);
+ *   - drivers/infiniband/core/uverbs_main.c|1355| <<ib_uverbs_init>> ret = ib_register_client(&uverbs_client);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_main.c|2671| <<ipoib_init_module>> ret = ib_register_client(&ipoib_client);
+ *   - drivers/infiniband/ulp/opa_vnic/opa_vnic_vema.c|1059| <<opa_vnic_init>> rc = ib_register_client(&opa_vnic_client);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|3778| <<srp_init_module>> ret = ib_register_client(&srp_client);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3134| <<srpt_init_module>> ret = ib_register_client(&srpt_client);
+ *   - drivers/nvme/host/rdma.c|2058| <<nvme_rdma_init_module>> ret = ib_register_client(&nvme_rdma_ib_client);
+ *   - drivers/nvme/target/rdma.c|1619| <<nvmet_rdma_init>> ret = ib_register_client(&nvmet_rdma_ib_client);
+ *   - net/rds/ib.c|699| <<rds_ib_init>> ret = ib_register_client(&rds_ib_client);
+ *   - net/smc/smc_ib.c|533| <<smc_ib_register_client>> return ib_register_client(&smc_ib_client);
+ */
 int ib_register_client(struct ib_client *client)
 {
 	struct ib_device *device;
diff --git a/drivers/infiniband/core/mad.c b/drivers/infiniband/core/mad.c
index 8eeb8a6..94f120d 100644
--- a/drivers/infiniband/core/mad.c
+++ b/drivers/infiniband/core/mad.c
@@ -3164,6 +3164,10 @@ static void destroy_mad_qp(struct ib_mad_qp_info *qp_info)
  * Open the port
  * Create the QP, PD, MR, and CQ if needed
  */
+/*
+ * called by:
+ *   - drivers/infiniband/core/mad.c|3308| <<ib_mad_init_device>> if (ib_mad_port_open(device, i)) {
+ */
 static int ib_mad_port_open(struct ib_device *device,
 			    int port_num)
 {
@@ -3295,6 +3299,13 @@ static int ib_mad_port_close(struct ib_device *device, int port_num)
 	return 0;
 }
 
+/*
+ * struct ib_client mad_client.add = ib_mad_init_device()
+ *
+ * 在以下调用:
+ *   - drivers/infiniband/core/device.c|558| <<ib_register_device>> client->add(device);
+ *   - drivers/infiniband/core/device.c|670| <<ib_register_client>> client->add(device);
+ */
 static void ib_mad_init_device(struct ib_device *device)
 {
 	int start, i;
diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c
index b75a783..e709e00 100644
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@ -856,6 +856,23 @@ static struct ib_qp *ib_create_xrc_qp(struct ib_qp *qp,
 	return qp;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/cma.c|911| <<rdma_create_qp>> qp = ib_create_qp(pd, qp_init_attr);
+ *   - drivers/infiniband/core/mad.c|3137| <<create_mad_qp>> qp_info->qp = ib_create_qp(qp_info->port_priv->pd, &qp_init_attr);
+ *   - drivers/infiniband/core/uverbs_cmd.c|2198| <<create_mad_qp>> qp = ib_create_qp(pd, &attr);
+ *   - drivers/infiniband/hw/mlx4/mad.c|1837| <<create_pv_sqp>> tun_qp->qp = ib_create_qp(ctx->pd, &qp_init_attr.init_attr);
+ *   - drivers/infiniband/hw/mlx4/qp.c|1577| <<mlx4_ib_create_qp>> sqp->roce_v2_gsi = ib_create_qp(pd, init_attr);
+ *   - drivers/infiniband/hw/mlx5/gsi.c|187| <<mlx5_ib_gsi_create_qp>> gsi->rx_qp = ib_create_qp(pd, &hw_init_attr);
+ *   - drivers/infiniband/hw/mlx5/gsi.c|269| <<create_gsi_ud_qp>> return ib_create_qp(pd, &init_attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_cm.c|273| <<ipoib_cm_create_rx_qp>> return ib_create_qp(priv->pd, &attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_cm.c|1104| <<ipoib_cm_create_tx_qp>> tx_qp = ib_create_qp(priv->pd, &attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_verbs.c|205| <<ipoib_transport_dev_init>> priv->qp = ib_create_qp(priv->pd, &init_attr);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|527| <<srp_create_ch_ib>> qp = ib_create_qp(dev->pd, init_attr);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1661| <<srpt_create_ch_ib>> ch->qp = ib_create_qp(sdev->pd, qp_init);
+ *   - net/rds/ib_cm.c|1853| <<rds_ib_setup_fastreg>> rds_ibdev->fastreg_qp = ib_create_qp(rds_ibdev->pd, &qp_init_attr);
+ *   - net/smc/smc_ib.c|251| <<smc_ib_create_queue_pair>> lnk->roce_qp = ib_create_qp(lnk->roce_pd, &qp_attr);
+ */
 struct ib_qp *ib_create_qp(struct ib_pd *pd,
 			   struct ib_qp_init_attr *qp_init_attr)
 {
@@ -1540,6 +1557,18 @@ EXPORT_SYMBOL(ib_destroy_qp);
 
 /* Completion queues */
 
+/*
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/mad.c|2015| <<create_pv_resources>> ctx->cq = ib_create_cq(ctx->ib_dev, mlx4_ib_tunnel_comp_handler,
+ *   - drivers/infiniband/hw/mlx4/main.c|1413| <<mlx4_ib_alloc_xrcd>> xrcd->cq = ib_create_cq(ibdev, NULL, NULL, xrcd, &cq_attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_verbs.c|175| <<ipoib_transport_dev_init>> priv->recv_cq = ib_create_cq(priv->ca, ipoib_ib_completion, NULL,
+ *   - drivers/infiniband/ulp/ipoib/ipoib_verbs.c|183| <<ipoib_transport_dev_init>> priv->send_cq = ib_create_cq(priv->ca, ipoib_send_comp_handler, NULL,
+ *   - drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c|740| <<kiblnd_create_conn>> cq = ib_create_cq(cmid->device,
+ *   - net/rds/ib_cm.c|729| <<rds_ib_check_cq>> *cqp = ib_create_cq(dev, comp_handler, event_handler, ctx, &cq_attr);
+ *   - net/rds/ib_cm.c|1833| <<rds_ib_setup_fastreg>> rds_ibdev->fastreg_cq = ib_create_cq(rds_ibdev->dev,
+ *   - net/smc/smc_ib.c|449| <<smc_ib_setup_per_ibdev>> smcibdev->roce_cq_send = ib_create_cq(smcibdev->ibdev,
+ *   - net/smc/smc_ib.c|457| <<smc_ib_setup_per_ibdev>> smcibdev->roce_cq_recv = ib_create_cq(smcibdev->ibdev,
+ */
 struct ib_cq *ib_create_cq(struct ib_device *device,
 			   ib_comp_handler comp_handler,
 			   void (*event_handler)(struct ib_event *, void *),
diff --git a/drivers/infiniband/hw/mlx4/cq.c b/drivers/infiniband/hw/mlx4/cq.c
index 0f94287..823b905 100644
--- a/drivers/infiniband/hw/mlx4/cq.c
+++ b/drivers/infiniband/hw/mlx4/cq.c
@@ -171,6 +171,19 @@ err_buf:
 }
 
 #define CQ_CREATE_FLAGS_SUPPORTED IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION
+/*
+ * used by:
+ *   - drivers/infiniband/hw/mlx4/main.c|2916| <<mlx4_ib_add>> ibdev->ib_dev.create_cq = mlx4_ib_create_cq
+ *
+ * called by:
+ *   - drivers/infiniband/core/cq.c|148| <<__ib_alloc_cq>> cq = dev->create_cq(dev, &cq_attr, NULL, NULL);
+ *   - drivers/infiniband/core/uverbs_cmd.c|1704| <<__ib_alloc_cq>> cq = ib_dev->create_cq(ib_dev, &attr, obj->uobject.context, uhw);
+ *   - drivers/infiniband/core/uverbs_cmd.c|1791| <<ib_uverbs_create_cq>> obj = create_cq(file, &ucore, &uhw, &cmd_ex,
+ *   - drivers/infiniband/core/uverbs_cmd.c|1839| <<ib_uverbs_ex_create_cq>> obj = create_cq(file, ucore, uhw, &cmd,
+ *   - drivers/infiniband/core/uverbs_std_types_cq.c|117| <<UVERBS_HANDLER>> cq = ib_dev->create_cq(ib_dev, &attr, obj->uobject.context, &uhw);
+ *   - drivers/infiniband/core/verbs.c|1551| <<UVERBS_HANDLER>> cq = device->create_cq(device, cq_attr, NULL, NULL);
+ *   - drivers/infiniband/hw/cxgb4/cq.c|960| <<c4iw_create_cq>> ret = create_cq(&rhp->rdev, &chp->cq,
+ */
 struct ib_cq *mlx4_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
 				struct ib_ucontext *context,
diff --git a/drivers/infiniband/hw/mlx4/qp.c b/drivers/infiniband/hw/mlx4/qp.c
index 8d346cb..5bfd359 100644
--- a/drivers/infiniband/hw/mlx4/qp.c
+++ b/drivers/infiniband/hw/mlx4/qp.c
@@ -841,6 +841,25 @@ static void mlx4_ib_release_wqn(struct mlx4_ib_ucontext *context,
 	mutex_unlock(&context->wqn_ranges_mutex);
 }
 
+/*
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_initiate_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_work_handler [rdma_cm]  
+ * [0] process_one_work            
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/qp.c|1490| <<_mlx4_ib_create_qp>> err = create_qp_common(to_mdev(pd->device), pd, MLX4_IB_QP_SRC,
+ *   - drivers/infiniband/hw/mlx4/qp.c|1521| <<_mlx4_ib_create_qp>> err = create_qp_common(to_mdev(pd->device), pd, MLX4_IB_QP_SRC,
+ *   - drivers/infiniband/hw/mlx4/qp.c|4115| <<mlx4_ib_create_wq>> err = create_qp_common(dev, pd, MLX4_IB_RWQ_SRC, &ib_qp_init_attr,
+ */
 static int create_qp_common(struct mlx4_ib_dev *dev, struct ib_pd *pd,
 			    enum mlx4_ib_source_type src,
 			    struct ib_qp_init_attr *init_attr,
@@ -1421,6 +1440,10 @@ static u32 get_sqp_num(struct mlx4_ib_dev *dev, struct ib_qp_init_attr *attr)
 		return dev->dev->caps.spec_qps[attr->port_num - 1].qp1_proxy;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/qp.c|1546| <<mlx4_ib_create_qp>> ibqp = _mlx4_ib_create_qp(pd, init_attr, udata);
+ */
 static struct ib_qp *_mlx4_ib_create_qp(struct ib_pd *pd,
 					struct ib_qp_init_attr *init_attr,
 					struct ib_udata *udata)
@@ -1536,6 +1559,47 @@ static struct ib_qp *_mlx4_ib_create_qp(struct ib_pd *pd,
 	return &qp->ibqp;
 }
 
+/*
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_initiate_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 其他创建qp的例子:
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_handle_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_listen_handler [rdma_cm]
+ * [0] cma_req_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_req_handler [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * used by:
+ *   - drivers/infiniband/hw/mlx4/main.c|2910| <<mlx4_ib_add>> ibdev->ib_dev.create_qp = mlx4_ib_create_qp;
+ *
+ * called by:
+ *   - drivers/infiniband/core/core_priv.h|338| <<_ib_create_qp>> qp = dev->create_qp(pd, attr, udata);
+ *   - drivers/infiniband/core/uverbs_cmd.c|2345| <<ib_uverbs_create_qp>> err = create_qp(file, &ucore, &uhw, &cmd_ex,
+ *   - drivers/infiniband/core/uverbs_cmd.c|2392| <<ib_uverbs_ex_create_qp>> err = create_qp(file, ucore, uhw, &cmd,
+ *   - drivers/infiniband/hw/cxgb4/qp.c|1895| <<c4iw_create_qp>> ret = create_qp(&rhp->rdev, &qhp->wq, &schp->cq, &rchp->cq,
+ */
 struct ib_qp *mlx4_ib_create_qp(struct ib_pd *pd,
 				struct ib_qp_init_attr *init_attr,
 				struct ib_udata *udata) {
diff --git a/drivers/net/ethernet/mellanox/mlx4/cq.c b/drivers/net/ethernet/mellanox/mlx4/cq.c
index b80214e..acfb6e9 100644
--- a/drivers/net/ethernet/mellanox/mlx4/cq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/cq.c
@@ -101,11 +101,28 @@ static void mlx4_add_cq_to_tasklet(struct mlx4_cq *cq)
 	spin_unlock_irqrestore(&tasklet_ctx->lock, flags);
 }
 
+/*
+ * [0] mlx4_ib_cq_comp [mlx4_ib]
+ * [0] mlx4_cq_completion [mlx4_core]
+ * [0] mlx4_eq_int [mlx4_core]
+ * [0] mlx4_msi_x_interrupt [mlx4_core]
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ */
 void mlx4_cq_completion(struct mlx4_dev *dev, u32 cqn)
 {
 	struct mlx4_cq *cq;
 
 	rcu_read_lock();
+	/*
+	 * 插入的地方:
+	 *   - drivers/net/ethernet/mellanox/mlx4/cq.c|333| <<mlx4_cq_alloc>> err = radix_tree_insert(&cq_table->tree, cq->cqn, cq);
+	 */
 	cq = radix_tree_lookup(&mlx4_priv(dev)->cq_table.tree,
 			       cqn & (dev->caps.num_cqs - 1));
 	rcu_read_unlock();
@@ -120,6 +137,9 @@ void mlx4_cq_completion(struct mlx4_dev *dev, u32 cqn)
 	 */
 	++cq->arm_sn;
 
+	/*
+	 * 对于ib的mlx4_ib_cq_comp [mlx4_ib]
+	 */
 	cq->comp(cq);
 }
 
@@ -287,6 +307,29 @@ static void mlx4_cq_free_icm(struct mlx4_dev *dev, int cqn)
 		__mlx4_cq_free_icm(dev, cqn);
 }
 
+/*
+ * [0] mlx4_cq_alloc [mlx4_core]
+ * [0] mlx4_ib_create_cq [mlx4_ib]
+ * [0] ib_create_cq [ib_core]
+ * [0] rds_ib_check_cq.constprop.17 [rds_rdma]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_handle_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_listen_handler [rdma_cm]
+ * [0] cma_req_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_req_handler [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/cq.c|253| <<mlx4_ib_create_cq>> err = mlx4_cq_alloc(dev->dev, entries, &cq->buf.mtt, uar,
+ *   - drivers/net/ethernet/mellanox/mlx4/en_cq.c|144| <<mlx4_en_activate_cq>> err = mlx4_cq_alloc(mdev->dev, cq->size, &cq->wqres.mtt,
+ */
 int mlx4_cq_alloc(struct mlx4_dev *dev, int nent,
 		  struct mlx4_mtt *mtt, struct mlx4_uar *uar, u64 db_rec,
 		  struct mlx4_cq *cq, unsigned vector, int collapsed,
diff --git a/drivers/net/ethernet/mellanox/mlx4/eq.c b/drivers/net/ethernet/mellanox/mlx4/eq.c
index ac40f26..1cffcfd 100644
--- a/drivers/net/ethernet/mellanox/mlx4/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/eq.c
@@ -490,6 +490,11 @@ void mlx4_master_handle_slave_flr(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx4/eq.c|856| <<mlx4_interrupt>> work |= mlx4_eq_int(dev, &priv->eq_table.eq[i]);
+ *   - drivers/net/ethernet/mellanox/mlx4/eq.c|866| <<mlx4_msi_x_interrupt>> mlx4_eq_int(dev, eq);
+ */
 static int mlx4_eq_int(struct mlx4_dev *dev, struct mlx4_eq *eq)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -1449,6 +1454,11 @@ struct cpu_rmap *mlx4_get_cpu_rmap(struct mlx4_dev *dev, int port)
 }
 EXPORT_SYMBOL(mlx4_get_cpu_rmap);
 
+/*
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/main.c|2582| <<mlx4_ib_alloc_eqs>> if (!mlx4_assign_eq(dev, i,
+ *   - drivers/net/ethernet/mellanox/mlx4/en_cq.c|112| <<mlx4_en_activate_cq>> err = mlx4_assign_eq(mdev->dev, priv->port,
+ */
 int mlx4_assign_eq(struct mlx4_dev *dev, u8 port, int *vector)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2d8f287..5a8b7d8 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3474,6 +3474,24 @@ static inline int ib_post_recv(struct ib_qp *qp,
 struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
 			    int nr_cqe, int comp_vector,
 			    enum ib_poll_context poll_ctx, const char *caller);
+/*
+ * called by:
+ *   - drivers/infiniband/core/mad.c|3199| <<ib_mad_port_open>> port_priv->cq = ib_alloc_cq(port_priv->device, port_priv, cq_size, 0,
+ *   - drivers/infiniband/hw/mlx5/gsi.c|171| <<mlx5_ib_gsi_create_qp>> gsi->cq = ib_alloc_cq(pd->device, gsi, init_attr->cap.max_send_wr, 0,
+ *   - drivers/infiniband/hw/mlx5/main.c|3241| <<create_umr_res>> cq = ib_alloc_cq(&dev->ib_dev, NULL, 128, 0, IB_POLL_SOFTIRQ);
+ *   - drivers/infiniband/ulp/iser/iser_verbs.c|99| <<iser_create_device_ib_res>> comp->cq = ib_alloc_cq(ib_dev, comp, max_cqe, i,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|279| <<isert_alloc_comps>> comp->cq = ib_alloc_cq(device->ib_device, comp, max_cqe, i,
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|503| <<srp_create_ch_ib>> recv_cq = ib_alloc_cq(dev->dev, ch, target->queue_size + 1,
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|510| <<srp_create_ch_ib>> send_cq = ib_alloc_cq(dev->dev, ch, m * target->queue_size,
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1632| <<srpt_create_ch_ib>> ch->cq = ib_alloc_cq(sdev->device, ch, ch->rq_size + srp_sq_size,
+ *   - drivers/nvme/host/rdma.c|448| <<nvme_rdma_create_queue_ib>> queue->ib_cq = ib_alloc_cq(ibdev, queue,
+ *   - drivers/nvme/target/rdma.c|948| <<nvmet_rdma_create_queue_ib>> queue->cq = ib_alloc_cq(ndev->device, queue,
+ *   - net/9p/trans_rdma.c|703| <<rdma_create_trans>> rdma->cq = ib_alloc_cq(rdma->cm_id->device, client,
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|783| <<svc_rdma_accept>> newxprt->sc_sq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_sq_depth,
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|789| <<svc_rdma_accept>> newxprt->sc_rq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_rq_depth,
+ *   - net/sunrpc/xprtrdma/verbs.c|577| <<rpcrdma_ep_create>> sendcq = ib_alloc_cq(ia->ri_device, NULL,
+ *   - net/sunrpc/xprtrdma/verbs.c|587| <<rpcrdma_ep_create>> recvcq = ib_alloc_cq(ia->ri_device, NULL,
+ */
 #define ib_alloc_cq(device, priv, nr_cqe, comp_vect, poll_ctx) \
 	__ib_alloc_cq((device), (priv), (nr_cqe), (comp_vect), (poll_ctx), KBUILD_MODNAME)
 
diff --git a/net/rds/ib.h b/net/rds/ib.h
index 80228d1..9d446cc 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -452,6 +452,12 @@ static inline struct rds_connection *rds_ib_map_conn(struct rds_connection *conn
 	return (struct rds_connection *)(unsigned long)id;
 }
 
+/*
+ * called by:
+ *   - net/rds/ib.c|413| <<rds_ib_laddr_check_cm>> cm_id = rds_ib_rdma_create_id(net, rds_rdma_cm_event_handler, &dummy_ic,
+ *   - net/rds/ib_cm.c|1459| <<rds_ib_conn_path_connect>> ic->i_cm_id = rds_ib_rdma_create_id(rds_conn_net(conn),
+ *   - net/rds/rdma_transport.c|386| <<rds_rdma_listen_init_common>> cm_id = rds_ib_rdma_create_id(&init_net, handler, dummy_ic, NULL, RDMA_PS_TCP, IB_QPT_RC);
+ */
 static inline struct rdma_cm_id *rds_ib_rdma_create_id(struct net *net,
 						       rdma_cm_event_handler event_handler,
 						       struct rds_ib_connection *ic,
diff --git a/net/rds/ib_cm.c b/net/rds/ib_cm.c
index f801268..4a31080 100644
--- a/net/rds/ib_cm.c
+++ b/net/rds/ib_cm.c
@@ -712,6 +712,11 @@ static inline void ibdev_put_vector(struct rds_ib_device *rds_ibdev, int index)
 	mutex_unlock(&rds_ibdev->vector_load_lock);
 }
 
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|857| <<rds_ib_setup_qp>> rds_ib_check_cq(dev, rds_ibdev, &ic->i_scq_vector, &ic->i_scq,
+ *   - net/rds/ib_cm.c|868| <<rds_ib_setup_qp>> rds_ib_check_cq(dev, rds_ibdev, &ic->i_rcq_vector, &ic->i_rcq,
+ */
 static void rds_ib_check_cq(struct ib_device *dev, struct rds_ib_device *rds_ibdev,
 			    int *vector, struct ib_cq **cqp, ib_comp_handler comp_handler,
 			    void (*event_handler)(struct ib_event *, void *),
@@ -808,6 +813,11 @@ void rds_dma_hdrs_free(struct dma_pool *pool, struct rds_header **hdrs,
  * This needs to be very careful to not leave IS_ERR pointers around for
  * cleanup to trip over.
  */
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|1299| <<rds_ib_cm_handle_connect>> err = rds_ib_setup_qp(conn);
+ *   - net/rds/ib_cm.c|1404| <<rds_ib_cm_initiate_connect>> ret = rds_ib_setup_qp(conn);
+ */
 static int rds_ib_setup_qp(struct rds_connection *conn)
 {
 	struct rds_ib_connection *ic = conn->c_transport_data;
@@ -1355,6 +1365,12 @@ void rds_ib_conn_destroy_init(struct rds_connection *conn)
 	queue_delayed_work(rds_aux_wq, &work->work, 0);
 }
 
+/*
+ * called by (处理RDMA_CM_EVENT_ROUTE_RESOLVED):
+ *   - net/rds/rdma_transport.c|212| <<rds_rdma_cm_event_handler_cmn>> ret = trans->cm_initiate_connect(cm_id, isv6);
+ *
+ * struct rds_transport rds_ib_transport.cm_initiate_connect = rds_ib_cm_initiate_connect()
+ */
 int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id, bool isv6)
 {
 	struct rds_connection *conn = rds_ib_get_conn(cm_id);
@@ -1435,6 +1451,12 @@ out:
 	return ret;
 }
 
+/*
+ * struct rds_transport rds_ib_transport.conn_path_connect = rds_ib_conn_path_connect()
+ *
+ * called by:
+ *   - net/rds/threads.c|245| <<rds_connect_worker>> ret = conn->c_trans->conn_path_connect(cp);
+ */
 int rds_ib_conn_path_connect(struct rds_conn_path *cp)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -1673,6 +1695,9 @@ void rds_ib_conn_path_shutdown(struct rds_conn_path *cp)
 	ic->i_active_side = 0;
 }
 
+/*
+ * struct rds_transport rds_ib_transport.conn_alloc = rds_ib_conn_alloc()
+ */
 int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp)
 {
 	struct rds_ib_connection *ic;
diff --git a/net/rds/ib_ring.c b/net/rds/ib_ring.c
index b66cd42..b61d27f 100644
--- a/net/rds/ib_ring.c
+++ b/net/rds/ib_ring.c
@@ -63,6 +63,13 @@
  */
 DECLARE_WAIT_QUEUE_HEAD(rds_ib_ring_empty_wait);
 
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|1655| <<rds_ib_conn_path_shutdown>> rds_ib_ring_init(&ic->i_send_ring, rds_ib_sysctl_max_send_wr);
+ *   - net/rds/ib_cm.c|1656| <<rds_ib_conn_path_shutdown>> rds_ib_ring_init(&ic->i_recv_ring, rds_ib_sysctl_max_recv_wr);
+ *   - net/rds/ib_cm.c|1707| <<rds_ib_conn_alloc>> rds_ib_ring_init(&ic->i_send_ring, rds_ib_sysctl_max_send_wr);
+ *   - net/rds/ib_cm.c|1708| <<rds_ib_conn_alloc>> rds_ib_ring_init(&ic->i_recv_ring, rds_ib_sysctl_max_recv_wr);
+ */
 void rds_ib_ring_init(struct rds_ib_work_ring *ring, u32 nr)
 {
 	memset(ring, 0, sizeof(*ring));
diff --git a/net/rds/ib_send.c b/net/rds/ib_send.c
index 475235f..b2bf1b6 100644
--- a/net/rds/ib_send.c
+++ b/net/rds/ib_send.c
@@ -564,6 +564,11 @@ static inline int rds_ib_set_wr_signal_state(struct rds_ib_connection *ic,
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off)
 {
+	/*
+	 * 在以下修改c_transport_data:
+	 *   - net/rds/ib_cm.c|1718| <<rds_ib_conn_alloc>> conn->c_transport_data = ic;
+	 *   - net/rds/loop.c|134| <<rds_loop_conn_alloc>> conn->c_transport_data = lc;
+	 */
 	struct rds_ib_connection *ic = conn->c_transport_data;
 	struct ib_device *dev = ic->i_cm_id->device;
 	struct rds_ib_send_work *send = NULL;
@@ -730,6 +735,7 @@ int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 	do {
 		unsigned int len = 0;
 
+		/* send是struct rds_ib_send_work */
 		/* Set up the header */
 		send->s_wr.send_flags = send_flags;
 		send->s_wr.opcode = IB_WR_SEND;
@@ -836,6 +842,12 @@ int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 
 	/* XXX need to worry about failed_wr and partial sends. */
 	failed_wr = &first->s_wr;
+	/*
+	 * conn是struct rds_connection
+	 * ic是struct rds_ib_connection *ic = conn->c_transport_data;
+	 *
+	 * qp似乎在rds_cond_queue_reconnect_work()创建, 调用rds_connect_worker()
+	 */
 	ret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);
 	rdsdebug("ic %p first %p (wr %p) ret %d wr %p\n", ic,
 		 first, &first->s_wr, ret, failed_wr);
diff --git a/net/rds/message.c b/net/rds/message.c
index c305f88..a0e0bc9 100644
--- a/net/rds/message.c
+++ b/net/rds/message.c
@@ -247,6 +247,13 @@ EXPORT_SYMBOL_GPL(rds_message_add_rdma_dest_extension);
  * rds ops will need. This is to minimize memory allocation count. Then, each rds op
  * can grab SGs when initializing its part of the rds_message.
  */
+/*
+ * called by:
+ *   - net/rds/cong.c|187| <<rds_cong_map_pages>> rm = rds_message_alloc(extra_bytes, GFP_NOWAIT);
+ *   - net/rds/send.c|1474| <<rds_sendmsg>> rm = rds_message_alloc(ret, GFP_KERNEL);
+ *   - net/rds/send.c|1706| <<rds_send_internal>> rm = rds_message_alloc(ret, gfp);
+ *   - net/rds/send.c|1825| <<rds_send_probe>> rm = rds_message_alloc(0, GFP_ATOMIC);
+ */
 struct rds_message *rds_message_alloc(unsigned int extra_len, gfp_t gfp)
 {
 	struct rds_message *rm;
diff --git a/net/rds/rds.h b/net/rds/rds.h
index 9ae01c7..d29acac 100644
--- a/net/rds/rds.h
+++ b/net/rds/rds.h
@@ -157,6 +157,24 @@ enum {
 	RDS_CONN_DOWN = 0,
 	RDS_CONN_CONNECTING,
 	RDS_CONN_DISCONNECTING,
+	/*
+	 * 使用RDS_CONN_UP的地方:
+	 *   - net/rds/connection.c|471| <<rds_conn_shutdown>> if (!rds_conn_path_transition(cp, RDS_CONN_UP,
+	 *   - net/rds/connection.c|888| <<rds_conn_info_visitor>> atomic_read(&cp->cp_state) == RDS_CONN_UP,
+	 *   - net/rds/connection.c|917| <<rds6_conn_info_visitor>> atomic_read(&cp->cp_state) == RDS_CONN_UP,
+	 *   - net/rds/connection.c|1058| <<rds_conn_path_drop>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 *   - net/rds/ib.c|272| <<rds_ib_conn_info_visitor>> if (rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/ib.c|320| <<rds6_ib_conn_info_visitor>> if (rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/ib_cm.c|1203| <<rds_ib_cm_handle_connect>> rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/ib_cm.c|1218| <<rds_ib_cm_handle_connect>> if (rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/rds.h|172| <<conn_state_mnem>> CASE_RET(RDS_CONN_UP);
+	 *   - net/rds/rds.h|1174| <<rds_conn_path_up>> return atomic_read(&cp->cp_state) == RDS_CONN_UP;
+	 *   - net/rds/tcp_listen.c|210| <<rds_tcp_accept_one>> BUG_ON(conn_state == RDS_CONN_UP);
+	 *   - net/rds/threads.c|108| <<rds_connect_path_complete>> if (!rds_conn_path_transition(cp, curr, RDS_CONN_UP)) {
+	 *   - net/rds/threads.c|289| <<rds_send_worker>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 *   - net/rds/threads.c|320| <<rds_recv_worker>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 *   - net/rds/threads.c|353| <<rds_hb_worker>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 */
 	RDS_CONN_UP,
 	RDS_CONN_RESETTING,
 	RDS_CONN_ERROR,
@@ -180,6 +198,11 @@ static inline const char *conn_state_mnem(int state)
 
 /* Bits for c_flags */
 #define RDS_LL_SEND_FULL	0
+/*
+ * 在以下使用RDS_RECONNECT_PENDING:
+ *   - net/rds/rds.h|1069| <<rds_cond_queue_reconnect_work>> if (!test_and_set_bit(RDS_RECONNECT_PENDING, &cp->cp_flags))
+ *   - net/rds/rds.h|1077| <<rds_clear_reconnect_pending_work_bit>> clear_bit(RDS_RECONNECT_PENDING, &cp->cp_flags);
+ */
 #define RDS_RECONNECT_PENDING	1
 #define RDS_IN_XMIT		2
 #define RDS_RECV_REFILL		3
@@ -271,6 +294,20 @@ struct rds_conn_path {
 
 	spinlock_t		cp_lock;		/* protect msg queues */
 	u64			cp_next_tx_seq;
+	/*
+	 * 在以下使用cp_send_queue:
+	 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+	 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+	 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+	 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+	 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+	 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+	 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+	 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+	 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+	 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 */
 	struct list_head	cp_send_queue;
 	struct list_head	cp_retrans;
 
@@ -842,6 +879,11 @@ struct rds_sock {
 	 * newline
 	 */
 	spinlock_t		rs_snd_lock;
+	/*
+	 * 在以下使用rs_send_queue:
+	 *   - net/rds/send.c|917| <<rds_send_drop_to>> list_for_each_entry_safe(rm, tmp, &rs->rs_send_queue, m_sock_item) {
+	 *   - net/rds/send.c|1062| <<rds_send_queue_rm>> list_add_tail(&rm->m_sock_item, &rs->rs_send_queue);
+	 */
 	struct list_head	rs_send_queue;
 	u32			rs_snd_bytes; /* Total bytes to all peers */
 	u32			rs_buf_info_dest_cnt;
@@ -1059,12 +1101,26 @@ static inline void rds_clear_shutdown_pending_work_bit(struct rds_conn_path *cp)
 	smp_mb__after_atomic();
 }
 
+/*
+ * called by:
+ *   - net/rds/send.c|1529| <<rds_sendmsg>> rds_cond_queue_reconnect_work(&conn->c_path[0],
+ *   - net/rds/threads.c|219| <<rds_queue_reconnect>> rds_cond_queue_reconnect_work(cp, delay);
+ */
 static inline void rds_cond_queue_reconnect_work(struct rds_conn_path *cp, unsigned long delay)
 {
+	/*
+	 * cp->cp_conn_w的使用的地方:
+	 *   - net/rds/connection.c|209| <<__rds_conn_path_init>> INIT_DELAYED_WORK(&cp->cp_conn_w, rds_connect_worker);
+	 *   - net/rds/connection.c|521| <<rds_conn_shutdown>> cancel_delayed_work_sync(&cp->cp_conn_w);
+	 *   - net/rds/rds.h|1070| <<rds_cond_queue_reconnect_work>> queue_delayed_work(cp->cp_wq, &cp->cp_conn_w, delay);
+	 */
 	if (!test_and_set_bit(RDS_RECONNECT_PENDING, &cp->cp_flags))
 		queue_delayed_work(cp->cp_wq, &cp->cp_conn_w, delay);
 }
 
+/*
+ * 清空rds_conn_path->cp_flags中的RDS_RECONNECT_PENDING
+ */
 static inline void rds_clear_reconnect_pending_work_bit(struct rds_conn_path *cp)
 {
 	/* clear_bit() does not imply a memory barrier */
diff --git a/net/rds/send.c b/net/rds/send.c
index 4648bce..3f90e98 100644
--- a/net/rds/send.c
+++ b/net/rds/send.c
@@ -319,6 +319,20 @@ restart:
 
 			spin_lock_irqsave(&cp->cp_lock, flags);
 
+			/*
+			 * 在以下使用cp_send_queue:
+			 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+			 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+			 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+			 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+			 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+			 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+			 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+			 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+			 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+			 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+			 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+			 */
 			if (!list_empty(&cp->cp_send_queue)) {
 				rm = list_entry(cp->cp_send_queue.next,
 						struct rds_message,
@@ -433,6 +447,9 @@ restart:
 
 		if (rm->data.op_active && !cp->cp_xmit_data_sent) {
 			rm->m_final_op = &rm->data;
+			/*
+			 * 一个例子是rds_ib_xmit()
+			 */
 			ret = conn->c_trans->xmit(conn, rm,
 						  cp->cp_xmit_hdr_off,
 						  cp->cp_xmit_sg,
@@ -902,6 +919,11 @@ void rds_send_drop_acked(struct rds_connection *conn, u64 ack,
 }
 EXPORT_SYMBOL_GPL(rds_send_drop_acked);
 
+/*
+ * called by:
+ *   - net/rds/af_rds.c|122| <<rds_release>> rds_send_drop_to(rs, NULL);
+ *   - net/rds/af_rds.c|399| <<rds_cancel_sent_to>> rds_send_drop_to(rs, &sin6);
+ */
 void rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in6 *dest)
 {
 	struct rds_message *rm, *tmp;
@@ -1019,6 +1041,12 @@ void rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in6 *dest)
  * possible that another thread can race with us and remove the
  * message from the flow with RDS_CANCEL_SENT_TO.
  */
+/*
+ * called by:
+ *   - net/rds/send.c|1609| <<rds_sendmsg>> while (!rds_send_queue_rm(rs, conn, cpath, rm, rs->rs_bound_port,
+ *   - net/rds/send.c|1623| <<rds_sendmsg>> rds_send_queue_rm(rs, conn, cpath, rm,
+ *   - net/rds/send.c|1772| <<rds_send_internal>> if (!rds_send_queue_rm(rs, conn, &conn->c_path[0], rm,
+ */
 static int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,
 			     struct rds_conn_path *cp,
 			     struct rds_message *rm, __be16 sport,
@@ -1078,6 +1106,20 @@ static int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,
 			goto out;
 		}
 		rm->m_inc.i_hdr.h_sequence = cpu_to_be64(cp->cp_next_tx_seq++);
+		/*
+		 * 在以下使用cp_send_queue:
+		 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+		 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+		 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+		 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+		 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+		 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+		 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+		 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+		 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+		 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+		 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+		 */
 		list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
 		set_bit(RDS_MSG_ON_CONN, &rm->m_flags);
 
@@ -1286,6 +1328,60 @@ static int rds_send_mprds_hash(struct rds_sock *rs, struct rds_connection *conn)
 	return hash;
 }
 
+/*
+ * 创建qp的例子:
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_initiate_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_work_handler [rdma_cm]  
+ * [0] process_one_work            
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 其他创建qp的例子:
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_handle_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_listen_handler [rdma_cm]
+ * [0] cma_req_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_req_handler [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 一个创建cq的例子:
+ * [0] mlx4_cq_alloc [mlx4_core]
+ * [0] mlx4_ib_create_cq [mlx4_ib]
+ * [0] ib_create_cq [ib_core]
+ * [0] rds_ib_check_cq.constprop.17 [rds_rdma]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_handle_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_listen_handler [rdma_cm]
+ * [0] cma_req_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_req_handler [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 一个qp有一个cq, 多个cq对应一个eq
+ */
 int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 {
 	struct sock *sk = sock->sk;
@@ -1458,6 +1554,9 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 	 */
 	if (nonblock) {
 		spin_lock_irqsave(&rs->rs_snd_lock, flags);
+		/*
+		 * rs来自struct rds_sock *rs = rds_sk_to_rs(sk);
+		 */
 		no_space = bufi->rsbi_snd_bytes >= rds_sk_sndbuf(rs);
 		spin_unlock_irqrestore(&rs->rs_snd_lock, flags);
 		if (no_space) {
@@ -1500,8 +1599,17 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 
 	/* rds_conn_create has a spinlock that runs with IRQ off.
 	 * Caching the conn in the socket helps a lot. */
+	/*
+	 * rs来自struct rds_sock *rs = rds_sk_to_rs(sk);
+	 */
 	if (rs->rs_conn && ipv6_addr_equal(&rs->rs_conn->c_faddr, &daddr) &&
 	    rs->rs_tos == rs->rs_conn->c_tos) {
+		/*
+		 * struct rds_sock *rs = rds_sk_to_rs(sk)
+		 * conn是struct rds_connection
+		 *
+		 * rs->rs_conn的值是在下面的else部分设置的
+		 */
 		conn = rs->rs_conn;
 		cpath = rs->rs_conn_path;
 	} else {
@@ -1514,6 +1622,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 			ret = PTR_ERR(conn);
 			goto out;
 		}
+		/*
+		 * struct rds_connection conn
+		 *  -> int c_npaths;
+		 *  -> struct rds_transport *c_trans;
+		 *      -> unsigned int t_mp_capable:1;
+		 *  -> struct rds_conn_path *c_path;
+		 */
 		if (conn->c_trans->t_mp_capable) {
 			/* c_npaths == 0 if we have not talked to this peer
 			 * before.  Initiate a connection request to the
@@ -1631,6 +1746,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 	if (!dport)
 		rds_stats_inc(s_send_ping);
 
+	/*
+	 * struct rds_connection conn
+	 *  -> int c_npaths;
+	 *  -> struct rds_transport *c_trans;
+	 *      -> unsigned int t_mp_capable:1;
+	 *  -> struct rds_conn_path *c_path;
+	 */
 	ret = rds_send_xmit(cpath);
 	if (ret == -ENOMEM || ret == -EAGAIN)
 		rds_cond_queue_send_work(cpath, 1);
@@ -1826,6 +1948,20 @@ static int rds_send_probe(struct rds_conn_path *cp, __be16 sport,
 		goto out;
 
 	spin_lock_irqsave(&cp->cp_lock, flags);
+	/*
+	 * 在以下使用cp_send_queue:
+	 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+	 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+	 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+	 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+	 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+	 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+	 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+	 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+	 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+	 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 */
 	list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
 	set_bit(RDS_MSG_ON_CONN, &rm->m_flags);
 	rds_message_addref(rm);
diff --git a/net/rds/threads.c b/net/rds/threads.c
index 53d548c..85c987c 100644
--- a/net/rds/threads.c
+++ b/net/rds/threads.c
@@ -101,6 +101,12 @@ static inline void rds_update_avg_connect_time(struct rds_conn_path *cp)
 	atomic64_set(&cp->cp_conn->c_trans->rds_avg_conn_jf, new_avg_jf);
 }
 
+/*
+ * called by:
+ *   - net/rds/tcp_connect.c|69| <<rds_tcp_state_change>> rds_connect_path_complete(cp, RDS_CONN_CONNECTING);
+ *   - net/rds/tcp_listen.c|219| <<rds_tcp_accept_one>> rds_connect_path_complete(cp, RDS_CONN_RESETTING);
+ *   - net/rds/tcp_listen.c|222| <<rds_tcp_accept_one>> rds_connect_path_complete(cp, RDS_CONN_CONNECTING);
+ */
 void rds_connect_path_complete(struct rds_conn_path *cp, int curr)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -221,6 +227,10 @@ void rds_queue_reconnect(struct rds_conn_path *cp)
 				       rds_sysctl_reconnect_max_jiffies);
 }
 
+/*
+ * used by:
+ *   - net/rds/connection.c|209| <<__rds_conn_path_init>> INIT_DELAYED_WORK(&cp->cp_conn_w, rds_connect_worker);
+ */
 void rds_connect_worker(struct work_struct *work)
 {
 	struct rds_conn_path *cp = container_of(work,
@@ -242,6 +252,9 @@ void rds_connect_worker(struct work_struct *work)
 		cp->cp_connection_start = get_seconds();
 		cp->cp_drop_source = DR_DEFAULT;
 
+		/*
+		 * 一个例子是rds_ib_conn_path_connect()
+		 */
 		ret = conn->c_trans->conn_path_connect(cp);
 		rds_rtd_ptr(RDS_RTD_CM_EXT,
 			    "conn %p for <%pI6c,%pI6c,%d> dispatched, ret %d\n",
@@ -265,6 +278,9 @@ void rds_connect_worker(struct work_struct *work)
 			conn, conn_state_mnem(atomic_read(&cp->cp_state)));
 	}
 out:
+	/*
+	 * 清空rds_conn_path->cp_flags中的RDS_RECONNECT_PENDING
+	 */
 	rds_clear_reconnect_pending_work_bit(cp);
 }
 
-- 
2.7.4

