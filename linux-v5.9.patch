From d6daeb5f64790e2bf8f52c8f43e7883ae979ae4c Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 14 Dec 2020 10:57:37 -0800
Subject: [PATCH 1/1] linux v5.9

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/bfq-iosched.c                          |  14 +
 block/bio.c                                  |   4 +
 block/blk-cgroup-rwstat.c                    |  11 +
 block/blk-cgroup.c                           |  87 ++++
 block/blk-core.c                             | 300 ++++++++++++
 block/blk-crypto-internal.h                  |   5 +
 block/blk-iocost.c                           |  47 ++
 block/blk-iolatency.c                        |  59 +++
 block/blk-lib.c                              |  61 +++
 block/blk-mq-cpumap.c                        |  25 +
 block/blk-mq-debugfs.c                       |   3 +
 block/blk-mq-sched.c                         |  34 ++
 block/blk-mq-sched.h                         |   8 +
 block/blk-mq-tag.c                           | 235 ++++++++++
 block/blk-mq-tag.h                           |  40 ++
 block/blk-mq-virtio.c                        |   5 +
 block/blk-mq.c                               | 459 +++++++++++++++++++
 block/blk-rq-qos.h                           |   4 +
 block/blk-settings.c                         |  33 ++
 block/blk-throttle.c                         |  94 ++++
 block/bounce.c                               |  10 +
 block/elevator.c                             |   6 +
 drivers/md/dm-linear.c                       |  11 +
 drivers/md/dm-table.c                        |   5 +
 drivers/md/dm.c                              |   6 +
 drivers/net/tap.c                            |   5 +
 drivers/net/tun.c                            |   4 +
 drivers/net/virtio_net.c                     |  48 ++
 drivers/net/xen-netback/rx.c                 |   5 +
 drivers/scsi/hosts.c                         |  25 +
 drivers/scsi/scsi.c                          |  66 +++
 drivers/scsi/scsi_common.c                   |  32 ++
 drivers/scsi/scsi_debug.c                    |  17 +
 drivers/scsi/scsi_debugfs.c                  |   7 +
 drivers/scsi/scsi_devinfo.c                  |   7 +
 drivers/scsi/scsi_dh.c                       |  35 ++
 drivers/scsi/scsi_error.c                    |   6 +
 drivers/scsi/scsi_ioctl.c                    |  11 +
 drivers/scsi/scsi_lib.c                      | 104 +++++
 drivers/scsi/scsi_lib_dma.c                  |  10 +
 drivers/scsi/scsi_logging.c                  |   5 +
 drivers/scsi/scsi_netlink.c                  |   4 +
 drivers/scsi/scsi_pm.c                       |  14 +
 drivers/scsi/scsi_scan.c                     | 222 +++++++++
 drivers/scsi/scsi_sysfs.c                    |  15 +
 drivers/scsi/scsi_trace.c                    |  14 +
 drivers/scsi/scsicam.c                       |   5 +
 drivers/scsi/sd.c                            |  38 ++
 drivers/scsi/virtio_scsi.c                   | 400 ++++++++++++++++
 drivers/target/loopback/tcm_loop.c           |  33 ++
 drivers/target/target_core_configfs.c        |  28 ++
 drivers/target/target_core_fabric_configfs.c |   3 +
 drivers/target/target_core_file.c            |  39 ++
 drivers/target/target_core_hba.c             |  25 +-
 drivers/target/target_core_sbc.c             |   6 +
 drivers/target/target_core_transport.c       |  48 ++
 drivers/vhost/net.c                          |  62 +++
 drivers/vhost/scsi.c                         | 129 ++++++
 drivers/vhost/vhost.c                        | 264 +++++++++++
 drivers/vhost/vhost.h                        |  38 ++
 fs/eventfd.c                                 |  11 +
 include/linux/blk-cgroup.h                   |   8 +
 include/linux/blk-mq.h                       |  39 ++
 include/linux/blkdev.h                       |  16 +
 include/linux/kernel.h                       |  10 +
 include/linux/kvm_host.h                     |  11 +
 include/linux/overflow.h                     |   9 +
 include/linux/poll.h                         |   7 +
 include/linux/sched.h                        |  17 +
 include/scsi/scsi_device.h                   |  49 ++
 include/scsi/scsi_host.h                     |  42 ++
 include/uapi/linux/virtio_scsi.h             |   7 +
 kernel/locking/mutex.c                       |  13 +
 kernel/panic.c                               |   9 +
 kernel/sched/core.c                          |  16 +
 kernel/sched/cputime.c                       |   8 +
 kernel/sched/sched.h                         |   8 +
 mm/slub.c                                    |  10 +
 net/core/dev.c                               |  17 +
 net/ipv4/devinet.c                           |  13 +
 net/packet/af_packet.c                       |  91 ++++
 virt/kvm/kvm_main.c                          |  50 ++
 82 files changed, 3800 insertions(+), 1 deletion(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index fa98470df3f0..750660c538ee 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -132,6 +132,13 @@
 #include "bfq-iosched.h"
 #include "blk-wbt.h"
 
+/*
+ * 从block/bfq-iosched.c又分裂出了如下三个文件:
+ * create mode 100644 block/bfq-cgroup.c
+ * create mode 100644 block/bfq-iosched.h
+ * create mode 100644 block/bfq-wf2q.c
+ */
+
 #define BFQ_BFQQ_FNS(name)						\
 void bfq_mark_bfqq_##name(struct bfq_queue *bfqq)			\
 {									\
@@ -6809,6 +6816,13 @@ static int __init bfq_init(void)
 	int ret;
 
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	ret = blkcg_policy_register(&blkcg_policy_bfq);
 	if (ret)
 		return ret;
diff --git a/block/bio.c b/block/bio.c
index e865ea55b9f9..8ccd44620237 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -335,6 +335,10 @@ static void bio_chain_endio(struct bio *bio)
  *
  * The caller must not set bi_private or bi_end_io in @bio.
  */
+/*
+ * 核心思想是把bio->bi_private = parent
+ * bio->bi_end_io  = bio_chain_endio
+ */
 void bio_chain(struct bio *bio, struct bio *parent)
 {
 	BUG_ON(bio->bi_private || bio->bi_end_io);
diff --git a/block/blk-cgroup-rwstat.c b/block/blk-cgroup-rwstat.c
index 85d5790ac49b..8468dacc5982 100644
--- a/block/blk-cgroup-rwstat.c
+++ b/block/blk-cgroup-rwstat.c
@@ -5,6 +5,17 @@
  */
 #include "blk-cgroup-rwstat.h"
 
+/*
+ * called by:
+ *   - block/bfq-cgroup.c|464| <<bfqg_stats_init>> if (blkg_rwstat_init(&stats->bytes, gfp) ||
+ *   - block/bfq-cgroup.c|465| <<bfqg_stats_init>> blkg_rwstat_init(&stats->ios, gfp))
+ *   - block/bfq-cgroup.c|469| <<bfqg_stats_init>> if (blkg_rwstat_init(&stats->merged, gfp) ||
+ *   - block/bfq-cgroup.c|470| <<bfqg_stats_init>> blkg_rwstat_init(&stats->service_time, gfp) ||
+ *   - block/bfq-cgroup.c|471| <<bfqg_stats_init>> blkg_rwstat_init(&stats->wait_time, gfp) ||
+ *   - block/bfq-cgroup.c|472| <<bfqg_stats_init>> blkg_rwstat_init(&stats->queued, gfp) ||
+ *   - block/blk-throttle.c|496| <<throtl_pd_alloc>> if (blkg_rwstat_init(&tg->stat_bytes, gfp))
+ *   - block/blk-throttle.c|499| <<throtl_pd_alloc>> if (blkg_rwstat_init(&tg->stat_ios, gfp))
+ */
 int blkg_rwstat_init(struct blkg_rwstat *rwstat, gfp_t gfp)
 {
 	int i, ret;
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index c195365c9817..ed73f92b45a9 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -34,6 +34,14 @@
 
 #define MAX_KEY_LEN 100
 
+/*
+ * 在以下调用blkcg_policy_register():
+ *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+ *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+ *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+ */
+
 /*
  * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.
  * blkcg_pol_register_mutex nests outside of it and synchronizes entire
@@ -219,6 +227,12 @@ EXPORT_SYMBOL_GPL(blkg_lookup_slowpath);
  * If @new_blkg is %NULL, this function tries to allocate a new one as
  * necessary using %GFP_NOWAIT.  @new_blkg is always consumed on return.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|354| <<blkg_lookup_create>> blkg = blkg_create(pos, q, NULL);
+ *   - block/blk-cgroup.c|663| <<__acquires>> blkg = blkg_create(pos, q, new_blkg);
+ *   - block/blk-cgroup.c|1145| <<blkcg_init_queue>> blkg = blkg_create(&blkcg_root, q, new_blkg);
+ */
 static struct blkcg_gq *blkg_create(struct blkcg *blkcg,
 				    struct request_queue *q,
 				    struct blkcg_gq *new_blkg)
@@ -935,6 +949,9 @@ static struct cftype blkcg_files[] = {
 	{ }	/* terminate */
 };
 
+/*
+ * struct cgroup_subsys io_cgrp_subsys.legacy_cftypes = blkcg_legacy_files[]
+ */
 static struct cftype blkcg_legacy_files[] = {
 	{
 		.name = "reset_stats",
@@ -1127,6 +1144,10 @@ static int blkcg_css_online(struct cgroup_subsys_state *css)
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|574| <<blk_alloc_queue>> if (blkcg_init_queue(q))
+ */
 int blkcg_init_queue(struct request_queue *q)
 {
 	struct blkcg_gq *new_blkg, *blkg;
@@ -1142,6 +1163,12 @@ int blkcg_init_queue(struct request_queue *q)
 	/* Make sure the root blkg exists. */
 	rcu_read_lock();
 	spin_lock_irq(&q->queue_lock);
+	/*
+	 * called by:
+	 *   - block/blk-cgroup.c|354| <<blkg_lookup_create>> blkg = blkg_create(pos, q, NULL);
+	 *   - block/blk-cgroup.c|663| <<__acquires>> blkg = blkg_create(pos, q, new_blkg);
+	 *   - block/blk-cgroup.c|1145| <<blkcg_init_queue>> blkg = blkg_create(&blkcg_root, q, new_blkg);
+	 */
 	blkg = blkg_create(&blkcg_root, q, new_blkg);
 	if (IS_ERR(blkg))
 		goto err_unlock;
@@ -1239,6 +1266,23 @@ static void blkcg_exit(struct task_struct *tsk)
 	tsk->throttle_queue = NULL;
 }
 
+/*
+ * 在以下使用io_cgrp_subsys:
+ *   - block/bfq-cgroup.c|512| <<bfq_cpd_init>> d->weight = cgroup_subsys_on_dfl(io_cgrp_subsys) ?
+ *   - block/bfq-iosched.c|5499| <<bfq_insert_request>> if (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)
+ *   - block/blk-cgroup.c|1472| <<blkcg_policy_register>> WARN_ON(cgroup_add_dfl_cftypes(&io_cgrp_subsys,
+ *   - block/blk-cgroup.c|1475| <<blkcg_policy_register>> WARN_ON(cgroup_add_legacy_cftypes(&io_cgrp_subsys,
+ *   - block/blk-cgroup.c|1919| <<blk_cgroup_bio_start>> if (cgroup_subsys_on_dfl(io_cgrp_subsys))
+ *   - block/blk-throttle.c|302| <<tg_bps_limit>> if (cgroup_subsys_on_dfl(io_cgrp_subsys) && !blkg->parent)
+ *   - block/blk-throttle.c|332| <<tg_iops_limit>> if (cgroup_subsys_on_dfl(io_cgrp_subsys) && !blkg->parent)
+ *   - block/blk-throttle.c|555| <<throtl_pd_init>> if (cgroup_subsys_on_dfl(io_cgrp_subsys) && blkg->parent)
+ *   - block/blk-throttle.c|1409| <<tg_conf_updated>> if (!cgroup_subsys_on_dfl(io_cgrp_subsys) || !blkg->parent ||
+ *   - block/blk-throttle.c|2178| <<blk_throtl_bio>> if (!cgroup_subsys_on_dfl(io_cgrp_subsys)) {
+ *   - include/linux/backing-dev.h|248| <<inode_cgwb_enabled>> cgroup_subsys_on_dfl(io_cgrp_subsys) &&
+ *   - mm/backing-dev.c|442| <<cgwb_create>> blkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);
+ *   - mm/backing-dev.c|561| <<wb_get_lookup>> blkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);
+ *   - mm/page_io.c|289| <<bio_associate_blkg_from_page>> css = cgroup_e_css(page->mem_cgroup->css.cgroup, &io_cgrp_subsys);
+ */
 struct cgroup_subsys io_cgrp_subsys = {
 	.css_alloc = blkcg_css_alloc,
 	.css_online = blkcg_css_online,
@@ -1415,6 +1459,13 @@ EXPORT_SYMBOL_GPL(blkcg_deactivate_policy);
  * Register @pol with blkcg core.  Might sleep and @pol may be modified on
  * successful registration.  Returns 0 on success and -errno on failure.
  */
+/*
+ * 在以下调用blkcg_policy_register():
+ *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+ *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+ *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+ */
 int blkcg_policy_register(struct blkcg_policy *pol)
 {
 	struct blkcg *blkcg;
@@ -1540,6 +1591,14 @@ bool __blkcg_punt_bio_submit(struct bio *bio)
 		return false;
 
 	spin_lock_bh(&blkg->async_bio_lock);
+	/*
+	 * 在以下使用blkcg_gq->async_bios:
+	 *   - block/blk-cgroup.c|100| <<__blkg_release>> WARN_ON(!bio_list_empty(&blkg->async_bios));
+	 *   - block/blk-cgroup.c|133| <<blkg_async_bio_workfn>> bio_list_merge(&bios, &blkg->async_bios);
+	 *   - block/blk-cgroup.c|134| <<blkg_async_bio_workfn>> bio_list_init(&blkg->async_bios);
+	 *   - block/blk-cgroup.c|170| <<blkg_alloc>> bio_list_init(&blkg->async_bios);
+	 *   - block/blk-cgroup.c|1594| <<__blkcg_punt_bio_submit>> bio_list_add(&blkg->async_bios, bio);
+	 */
 	bio_list_add(&blkg->async_bios, bio);
 	spin_unlock_bh(&blkg->async_bio_lock);
 
@@ -1862,6 +1921,34 @@ static int blk_cgroup_io_type(struct bio *bio)
 	return BLKG_IOSTAT_READ;
 }
 
+/*
+ * [0] blk_cgroup_bio_start
+ * [0] submit_bio_checks
+ * [0] submit_bio_noacct
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra.57
+ * [0] ll_rw_block
+ * [0] jread
+ * [0] do_one_pass
+ * [0] jbd2_journal_recover
+ * [0] jbd2_journal_load
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] path_mount
+ * [0] init_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-core.c|1059| <<submit_bio_checks>> blk_cgroup_bio_start(bio);
+ */
 void blk_cgroup_bio_start(struct bio *bio)
 {
 	int rwd = blk_cgroup_io_type(bio), cpu;
diff --git a/block/blk-core.c b/block/blk-core.c
index 10c08ac50697..6b1e9c768128 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -64,11 +64,25 @@ DEFINE_IDA(blk_queue_ida);
 /*
  * For queue allocation
  */
+/*
+ * 在以下使用blk_requestq_cachep:
+ *   - block/blk-core.c|564| <<blk_alloc_queue>> q = kmem_cache_alloc_node(blk_requestq_cachep,
+ *   - block/blk-core.c|640| <<blk_alloc_queue>> kmem_cache_free(blk_requestq_cachep, q);
+ *   - block/blk-core.c|1963| <<blk_dev_init>> blk_requestq_cachep = kmem_cache_create("request_queue",
+ *   - block/blk-sysfs.c|872| <<blk_free_queue_rcu>> kmem_cache_free(blk_requestq_cachep, q);
+ */
 struct kmem_cache *blk_requestq_cachep;
 
 /*
  * Controlling structure to kblockd
  */
+/*
+ * 在以下使用kblockd_workqueue:
+ *   - block/blk-core.c|1816| <<kblockd_schedule_work>> return queue_work(kblockd_workqueue, work);
+ *   - block/blk-core.c|1823| <<kblockd_mod_delayed_work_on>> return mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);
+ *   - block/blk-core.c|1965| <<blk_dev_init>> kblockd_workqueue = alloc_workqueue("kblockd",
+ *   - block/blk-core.c|1967| <<blk_dev_init>> if (!kblockd_workqueue)
+ */
 static struct workqueue_struct *kblockd_workqueue;
 
 /**
@@ -107,6 +121,11 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_queue_flag_test_and_set);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|298| <<blk_kick_flush>> blk_rq_init(q, flush_rq);
+ *   - drivers/scsi/scsi_error.c|2353| <<scsi_ioctl_reset>> blk_rq_init(NULL, rq);
+ */
 void blk_rq_init(struct request_queue *q, struct request *rq)
 {
 	memset(rq, 0, sizeof(*rq));
@@ -155,6 +174,13 @@ static const char *const blk_op_name[] = {
  * string format. Useful in the debugging and tracing bio or request. For
  * invalid REQ_OP_XXX it returns string "UNKNOWN".
  */
+/*
+ * called by:
+ *   - block/blk-core.c|236| <<print_req_error>> blk_rq_pos(req), req_op(req), blk_op_str(req_op(req)),
+ *   - block/blk-mq-debugfs.c|332| <<__blk_mq_debugfs_rq_show>> const char *op_str = blk_op_str(op);
+ *   - drivers/block/null_blk_trace.h|51| <<__field>> blk_op_str(__entry->op),
+ *   - include/trace/events/f2fs.h|81| <<show_bio_op>> #define show_bio_op(op) blk_op_str(op)
+ */
 inline const char *blk_op_str(unsigned int op)
 {
 	const char *op_str = "UNKNOWN";
@@ -213,6 +239,10 @@ int blk_status_to_errno(blk_status_t status)
 }
 EXPORT_SYMBOL_GPL(blk_status_to_errno);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1616| <<blk_update_request>> print_req_error(req, error, __func__);
+ */
 static void print_req_error(struct request *req, blk_status_t status,
 		const char *caller)
 {
@@ -259,6 +289,15 @@ static void req_bio_endio(struct request *rq, struct bio *bio,
 		bio_endio(bio);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1670| <<blk_update_request>> blk_dump_rq_flags(req, "request botched");
+ *   - drivers/block/ps3disk.c|190| <<ps3disk_do_request>> blk_dump_rq_flags(req, DEVICE_NAME " bad request");
+ *   - drivers/ide/ide-cd.c|741| <<cdrom_newpc_intr>> blk_dump_rq_flags(rq, "cdrom_newpc_intr");
+ *   - drivers/ide/ide-cd.c|887| <<ide_cd_do_request>> blk_dump_rq_flags(rq, "ide_cd_do_request");
+ *   - drivers/ide/ide-floppy.c|240| <<ide_floppy_do_request>> blk_dump_rq_flags(rq, (rq->rq_disk
+ *   - drivers/scsi/sr.c|452| <<sr_init_command>> blk_dump_rq_flags(rq, "Unknown sr command");
+ */
 void blk_dump_rq_flags(struct request *rq, char *msg)
 {
 	printk(KERN_INFO "%s: dev %s: flags=%llx\n", msg,
@@ -291,6 +330,13 @@ EXPORT_SYMBOL(blk_dump_rq_flags);
  *     and blkcg_exit_queue() to be called with queue lock initialized.
  *
  */
+/*
+ * called by:
+ *   - block/blk-core.c|413| <<blk_cleanup_queue>> blk_sync_queue(q);
+ *   - drivers/md/md.c|6278| <<mddev_detach>> blk_sync_queue(mddev->queue);
+ *   - drivers/nvme/host/core.c|4651| <<nvme_sync_queues>> blk_sync_queue(ns->queue);
+ *   - drivers/nvme/host/core.c|4655| <<nvme_sync_queues>> blk_sync_queue(ctrl->admin_q);
+ */
 void blk_sync_queue(struct request_queue *q)
 {
 	del_timer_sync(&q->timeout);
@@ -302,12 +348,25 @@ EXPORT_SYMBOL(blk_sync_queue);
  * blk_set_pm_only - increment pm_only counter
  * @q: request queue pointer
  */
+/*
+ * called by:
+ *   - block/blk-pm.c|76| <<blk_pre_runtime_suspend>> blk_set_pm_only(q);
+ *   - drivers/scsi/scsi_lib.c|2520| <<scsi_device_quiesce>> blk_set_pm_only(q);
+ */
 void blk_set_pm_only(struct request_queue *q)
 {
 	atomic_inc(&q->pm_only);
 }
 EXPORT_SYMBOL_GPL(blk_set_pm_only);
 
+/*
+ * called by:
+ *   - block/blk-pm.c|100| <<blk_pre_runtime_suspend>> blk_clear_pm_only(q);
+ *   - block/blk-pm.c|134| <<blk_post_runtime_suspend>> blk_clear_pm_only(q);
+ *   - block/blk-pm.c|219| <<blk_set_runtime_active>> blk_clear_pm_only(q);
+ *   - drivers/scsi/scsi_lib.c|2537| <<scsi_device_quiesce>> blk_clear_pm_only(q);
+ *   - drivers/scsi/scsi_lib.c|2562| <<scsi_device_resume>> blk_clear_pm_only(sdev->request_queue);
+ */
 void blk_clear_pm_only(struct request_queue *q)
 {
 	int pm_only;
@@ -335,6 +394,15 @@ void blk_put_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_put_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|436| <<blk_cleanup_queue>> blk_set_queue_dying(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|4217| <<mtip_pci_remove>> blk_set_queue_dying(dd->queue);
+ *   - drivers/block/rbd.c|7268| <<do_rbd_remove>> blk_set_queue_dying(rbd_dev->disk->queue);
+ *   - drivers/md/dm.c|2348| <<__dm_destroy>> blk_set_queue_dying(md->queue);
+ *   - drivers/nvme/host/core.c|105| <<nvme_set_queue_dying>> blk_set_queue_dying(ns->queue);
+ *   - drivers/nvme/host/multipath.c|691| <<nvme_mpath_remove_disk>> blk_set_queue_dying(head->disk->queue);
+ */
 void blk_set_queue_dying(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_DYING, q);
@@ -350,6 +418,17 @@ void blk_set_queue_dying(struct request_queue *q)
 		blk_mq_wake_waiters(q);
 
 	/* Make blk_queue_enter() reexamine the DYING flag. */
+	/*
+	 * 在以下使用request_queue->mq_freeze_wq:
+	 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wake_up_all(&q->mq_freeze_wq);
 }
 EXPORT_SYMBOL_GPL(blk_set_queue_dying);
@@ -363,6 +442,65 @@ EXPORT_SYMBOL_GPL(blk_set_queue_dying);
  *
  * Context: can sleep
  */
+/*
+ * 调用的例子:
+ *   - block/blk-mq.c|3262| <<blk_mq_init_queue_data>> blk_cleanup_queue(uninit_q);
+ *   - block/bsg-lib.c|331| <<bsg_remove_queue>> blk_cleanup_queue(q);
+ *   - block/bsg-lib.c|408| <<bsg_setup_queue>> blk_cleanup_queue(q);
+ *   - drivers/block/loop.c|2179| <<loop_add>> blk_cleanup_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|2193| <<loop_remove>> blk_cleanup_queue(lo->lo_queue);
+ *   - drivers/block/nbd.c|228| <<nbd_dev_remove>> blk_cleanup_queue(q);
+ *   - drivers/block/null_blk_main.c|1557| <<null_del_dev>> blk_cleanup_queue(nullb->q);
+ *   - drivers/block/null_blk_main.c|1866| <<null_add_dev>> blk_cleanup_queue(nullb->q);
+ *   - drivers/block/virtio_blk.c|914| <<virtblk_remove>> blk_cleanup_queue(vblk->disk->queue);
+ *   - drivers/block/xen-blkfront.c|1219| <<xlvbd_release_gendisk>> blk_cleanup_queue(info->rq);
+ *   - drivers/md/dm.c|1878| <<cleanup_mapped_device>> blk_cleanup_queue(md->queue);
+ *   - drivers/md/md.c|5599| <<md_free>> blk_cleanup_queue(mddev->queue);
+ *   - drivers/md/md.c|5716| <<md_alloc>> blk_cleanup_queue(mddev->queue);
+ *   - drivers/nvdimm/blk.c|234| <<nd_blk_release_queue>> blk_cleanup_queue(q);
+ *   - drivers/nvdimm/btt.c|1531| <<btt_blk_init>> blk_cleanup_queue(btt->btt_queue);
+ *   - drivers/nvdimm/btt.c|1554| <<btt_blk_init>> blk_cleanup_queue(btt->btt_queue);
+ *   - drivers/nvdimm/btt.c|1570| <<btt_blk_cleanup>> blk_cleanup_queue(btt->btt_queue);
+ *   - drivers/nvdimm/pmem.c|341| <<pmem_pagemap_cleanup>> blk_cleanup_queue(q);
+ *   - drivers/nvme/host/core.c|4009| <<nvme_alloc_ns>> blk_cleanup_queue(ns->queue);
+ *   - drivers/nvme/host/core.c|4033| <<nvme_ns_remove>> blk_cleanup_queue(ns->queue);
+ *   - drivers/nvme/host/fc.c|2359| <<nvme_fc_ctrl_free>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/fc.c|2369| <<nvme_fc_ctrl_free>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|2370| <<nvme_fc_ctrl_free>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/fc.c|2831| <<nvme_fc_create_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/fc.c|3568| <<nvme_fc_init_ctrl>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/fc.c|3570| <<nvme_fc_init_ctrl>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/multipath.c|402| <<nvme_mpath_alloc_disk>> blk_cleanup_queue(q);
+ *   - drivers/nvme/host/multipath.c|695| <<nvme_mpath_remove_disk>> blk_cleanup_queue(head->disk->queue);
+ *   - drivers/nvme/host/pci.c|1583| <<nvme_dev_remove_admin>> blk_cleanup_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|833| <<nvme_rdma_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|834| <<nvme_rdma_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/rdma.c|924| <<nvme_rdma_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/host/rdma.c|927| <<nvme_rdma_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/host/rdma.c|946| <<nvme_rdma_destroy_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/rdma.c|1001| <<nvme_rdma_configure_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/host/tcp.c|1751| <<nvme_tcp_destroy_io_queues>> blk_cleanup_queue(ctrl->connect_q);
+ *   - drivers/nvme/host/tcp.c|1806| <<nvme_tcp_configure_io_queues>> blk_cleanup_queue(ctrl->connect_q);
+ *   - drivers/nvme/host/tcp.c|1819| <<nvme_tcp_destroy_admin_queue>> blk_cleanup_queue(ctrl->admin_q);
+ *   - drivers/nvme/host/tcp.c|1820| <<nvme_tcp_destroy_admin_queue>> blk_cleanup_queue(ctrl->fabrics_q);
+ *   - drivers/nvme/host/tcp.c|1874| <<nvme_tcp_configure_admin_queue>> blk_cleanup_queue(ctrl->admin_q);
+ *   - drivers/nvme/host/tcp.c|1877| <<nvme_tcp_configure_admin_queue>> blk_cleanup_queue(ctrl->fabrics_q);
+ *   - drivers/nvme/target/loop.c|256| <<nvme_loop_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/target/loop.c|257| <<nvme_loop_destroy_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/target/loop.c|273| <<nvme_loop_free_ctrl>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/nvme/target/loop.c|396| <<nvme_loop_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.admin_q);
+ *   - drivers/nvme/target/loop.c|398| <<nvme_loop_configure_admin_queue>> blk_cleanup_queue(ctrl->ctrl.fabrics_q);
+ *   - drivers/nvme/target/loop.c|540| <<nvme_loop_create_io_queues>> blk_cleanup_queue(ctrl->ctrl.connect_q);
+ *   - drivers/scsi/scsi_sysfs.c|1441| <<__scsi_remove_device>> blk_cleanup_queue(sdev->request_queue);
+ *
+ * blk_cleanup_queue - shutdown a request queue
+ * @q: request queue to shutdown
+ *
+ * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
+ * put it.  All future requests will be failed immediately with -ENODEV.
+ *
+ * Context: can sleep
+ */
 void blk_cleanup_queue(struct request_queue *q)
 {
 	/* cannot be called from atomic context */
@@ -392,6 +530,21 @@ void blk_cleanup_queue(struct request_queue *q)
 
 	/* @q won't process any more request, flush async actions */
 	del_timer_sync(&q->backing_dev_info->laptop_mode_wb_timer);
+	/*
+	 * cancel any pending callbacks on a queue
+	 *
+	 * The block layer may perform asynchronous callback activity
+	 * on a queue, such as calling the unplug function after a timeout.
+	 * A block device may call blk_sync_queue to ensure that any
+	 * such activity is cancelled, thus allowing it to release resources
+	 * that the callbacks might use. The caller must already have made sure
+	 * that its ->submit_bio will not re-add plugging prior to calling
+	 * this function.
+	 *
+	 * This function does not cancel any asynchronous activity arising
+	 * out of elevator or throttling code. That would require elevator_exit()
+	 * and blkcg_exit_queue() to be called with queue lock initialized.
+	 */
 	blk_sync_queue(q);
 
 	if (queue_is_mq(q))
@@ -422,6 +575,14 @@ EXPORT_SYMBOL(blk_cleanup_queue);
  * @q: request queue pointer
  * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT
  */
+/*
+ * called by:
+ *   - block/blk-core.c|551| <<bio_queue_enter>> ret = blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0);
+ *   - block/blk-mq.c|554| <<blk_mq_alloc_request>> ret = blk_queue_enter(q, flags);
+ *   - block/blk-mq.c|604| <<blk_mq_alloc_request_hctx>> ret = blk_queue_enter(q, flags);
+ *   - fs/block_dev.c|696| <<bdev_read_page>> result = blk_queue_enter(bdev->bd_disk->queue, 0);
+ *   - fs/block_dev.c|732| <<bdev_write_page>> result = blk_queue_enter(bdev->bd_disk->queue, 0);
+ */
 int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 {
 	const bool pm = flags & BLK_MQ_REQ_PREEMPT;
@@ -459,6 +620,17 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 		 */
 		smp_rmb();
 
+		/*
+		 * 在以下使用request_queue->mq_freeze_wq:
+		 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+		 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+		 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+		 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+		 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+		 */
 		wait_event(q->mq_freeze_wq,
 			   (!q->mq_freeze_depth &&
 			    (pm || (blk_pm_request_resume(q),
@@ -469,6 +641,11 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1223| <<__submit_bio_noacct>> if (unlikely(bio_queue_enter(bio) != 0))
+ *   - block/blk-core.c|1268| <<__submit_bio_noacct_mq>> if (unlikely(bio_queue_enter(bio) != 0))
+ */
 static inline int bio_queue_enter(struct bio *bio)
 {
 	struct request_queue *q = bio->bi_disk->queue;
@@ -486,6 +663,19 @@ static inline int bio_queue_enter(struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1089| <<__submit_bio>> blk_queue_exit(disk->queue);
+ *   - block/blk-core.c|1175| <<__submit_bio_noacct_mq>> blk_queue_exit(disk->queue);
+ *   - block/blk-mq-tag.c|453| <<blk_mq_queue_tag_busy_iter>> blk_queue_exit(q);
+ *   - block/blk-mq.c|422| <<blk_mq_alloc_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|481| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|501| <<__blk_mq_free_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|1012| <<blk_mq_timeout_work>> blk_queue_exit(q);
+ *   - block/blk-mq.c|2264| <<blk_mq_submit_bio>> blk_queue_exit(q);
+ *   - fs/block_dev.c|701| <<bdev_read_page>> blk_queue_exit(bdev->bd_disk->queue);
+ *   - fs/block_dev.c|745| <<bdev_write_page>> blk_queue_exit(bdev->bd_disk->queue);
+ */
 void blk_queue_exit(struct request_queue *q)
 {
 	percpu_ref_put(&q->q_usage_counter);
@@ -496,6 +686,17 @@ static void blk_queue_usage_counter_release(struct percpu_ref *ref)
 	struct request_queue *q =
 		container_of(ref, struct request_queue, q_usage_counter);
 
+	/*
+	 * 在以下使用request_queue->mq_freeze_wq:
+	 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wake_up_all(&q->mq_freeze_wq);
 }
 
@@ -510,6 +711,30 @@ static void blk_timeout_work(struct work_struct *work)
 {
 }
 
+/*
+ * called by:
+ *   - arch/m68k/emu/nfblock.c|122| <<nfhd_init_one>> dev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - arch/xtensa/platforms/iss/simdisk.c|269| <<simdisk_setup>> dev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - block/blk-mq.c|3251| <<blk_mq_init_queue_data>> uninit_q = blk_alloc_queue(set->numa_node);
+ *   - drivers/block/brd.c|385| <<brd_alloc>> brd->brd_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/drbd/drbd_main.c|2749| <<drbd_create_device>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/null_blk_main.c|1812| <<null_add_dev>> nullb->q = blk_alloc_queue(dev->home_node);
+ *   - drivers/block/pktcdvd.c|2753| <<pkt_setup_dev>> disk->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/ps3vram.c|739| <<ps3vram_probe>> queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/rsxx/dev.c|239| <<rsxx_setup_dev>> card->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/umem.c|890| <<mm_pci_probe>> card->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/block/zram/zram_drv.c|1895| <<zram_add>> queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/lightnvm/core.c|379| <<nvm_create_tgt>> tqueue = blk_alloc_queue(dev->q->node);
+ *   - drivers/md/bcache/super.c|943| <<bcache_device_init>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/md/dm.c|1944| <<alloc_dev>> md->queue = blk_alloc_queue(numa_node_id);
+ *   - drivers/md/md.c|5708| <<md_alloc>> mddev->queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/blk.c|254| <<nsblk_attach_disk>> q = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/btt.c|1525| <<btt_blk_init>> btt->btt_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/nvdimm/pmem.c|425| <<pmem_attach_disk>> q = blk_alloc_queue(dev_to_node(dev));
+ *   - drivers/nvme/host/multipath.c|377| <<nvme_mpath_alloc_disk>> q = blk_alloc_queue(ctrl->numa_node);
+ *   - drivers/s390/block/dcssblk.c|654| <<dcssblk_add_store>> dev_info->dcssblk_queue = blk_alloc_queue(NUMA_NO_NODE);
+ *   - drivers/s390/block/xpram.c|347| <<xpram_setup_blkdev>> xpram_queues[i] = blk_alloc_queue(NUMA_NO_NODE);
+ */
 struct request_queue *blk_alloc_queue(int node_id)
 {
 	struct request_queue *q;
@@ -750,6 +975,10 @@ bool bio_attempt_discard_merge(struct request_queue *q, struct request *req,
  *
  * Caller must ensure !blk_queue_nomerges(q) beforehand.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2375| <<blk_mq_submit_bio>> blk_attempt_plug_merge(q, bio, nr_segs, &same_queue_rq))
+ */
 bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
 		unsigned int nr_segs, struct request **same_queue_rq)
 {
@@ -820,6 +1049,12 @@ static int __init setup_fail_make_request(char *str)
 }
 __setup("fail_make_request=", setup_fail_make_request);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1095| <<should_fail_bio>> if (should_fail_request(&bio->bi_disk->part0, bio->bi_iter.bi_size))
+ *   - block/blk-core.c|1135| <<blk_partition_remap>> if (unlikely(should_fail_request(p, bio->bi_iter.bi_size)))
+ *   - block/blk-core.c|1580| <<blk_insert_cloned_request>> should_fail_request(&rq->rq_disk->part0, blk_rq_bytes(rq)))
+ */
 static bool should_fail_request(struct hd_struct *part, unsigned int bytes)
 {
 	return part->make_it_fail && should_fail(&fail_make_request, bytes);
@@ -894,6 +1129,10 @@ static inline int bio_check_eod(struct bio *bio, sector_t maxsector)
 /*
  * Remap block n of partition p to block n+start(p) of the disk.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1208| <<submit_bio_checks>> if (unlikely(blk_partition_remap(bio)))
+ */
 static inline int blk_partition_remap(struct bio *bio)
 {
 	struct hd_struct *p;
@@ -957,6 +1196,10 @@ static inline blk_status_t blk_check_zone_append(struct request_queue *q,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1423| <<submit_bio_noacct>> if (!submit_bio_checks(bio))
+ */
 static noinline_for_stack bool submit_bio_checks(struct bio *bio)
 {
 	struct request_queue *q = bio->bi_disk->queue;
@@ -1076,6 +1319,10 @@ static noinline_for_stack bool submit_bio_checks(struct bio *bio)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1368| <<__submit_bio_noacct>> ret = __submit_bio(bio);
+ */
 static blk_qc_t __submit_bio(struct bio *bio)
 {
 	struct gendisk *disk = bio->bi_disk;
@@ -1109,6 +1356,10 @@ static blk_qc_t __submit_bio(struct bio *bio)
  * bio_list_on_stack[1] contains bios that were submitted before the current
  *	->submit_bio_bio, but that haven't been processed yet.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1447| <<submit_bio_noacct>> return __submit_bio_noacct(bio);
+ */
 static blk_qc_t __submit_bio_noacct(struct bio *bio)
 {
 	struct bio_list bio_list_on_stack[2];
@@ -1268,6 +1519,12 @@ blk_qc_t submit_bio(struct bio *bio)
 	 * the submitting cgroup IO-throttled, submission can be a significant
 	 * part of overall IO time.
 	 */
+	/*
+	 * 在以下使用BIO_WORKINGSET:
+	 *   - block/bio.c|920| <<__bio_add_page>> if (!bio_flagged(bio, BIO_WORKINGSET) && unlikely(PageWorkingset(page)))
+	 *   - block/bio.c|921| <<__bio_add_page>> bio_set_flag(bio, BIO_WORKINGSET);
+	 *   - block/blk-core.c|1505| <<submit_bio>> bio_flagged(bio, BIO_WORKINGSET))) {
+	 */
 	if (unlikely(bio_op(bio) == REQ_OP_READ &&
 	    bio_flagged(bio, BIO_WORKINGSET))) {
 		unsigned long pflags;
@@ -1499,6 +1756,10 @@ EXPORT_SYMBOL(disk_end_io_acct);
  * Steal bios from a request and add them to a bio list.
  * The request must not have been partially completed before.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/multipath.c|87| <<nvme_failover_req>> blk_steal_bios(&ns->head->requeue_list, req);
+ */
 void blk_steal_bios(struct bio_list *list, struct request *rq)
 {
 	if (rq->bio) {
@@ -1542,6 +1803,13 @@ EXPORT_SYMBOL_GPL(blk_steal_bios);
  *     %false - this request doesn't have any more data
  *     %true  - this request has more data
  **/
+/*
+ * 调用的例子:
+ *   - block/blk-mq.c|710| <<blk_mq_end_request>> if (blk_update_request(rq, error, blk_rq_bytes(rq)))
+ *   - drivers/block/loop.c|486| <<lo_complete_rq>> blk_update_request(rq, BLK_STS_OK, cmd->ret);
+ *   - drivers/md/dm-rq.c|120| <<end_clone_bio>> blk_update_request(tio->orig, BLK_STS_OK, tio->completed);
+ *   - drivers/scsi/scsi_lib.c|566| <<scsi_end_request>> if (blk_update_request(req, error, bytes))
+ */
 bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
@@ -1758,12 +2026,32 @@ int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 }
 EXPORT_SYMBOL_GPL(blk_rq_prep_clone);
 
+/*
+ * called by:
+ *   - block/blk-core.c|559| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+ *   - block/blk-timeout.c|87| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+ *   - drivers/ide/ide-io.c|907| <<ide_insert_request_head>> kblockd_schedule_work(&drive->rq_work);
+ *   - drivers/ide/ide-probe.c|1183| <<drive_rq_insert_work>> kblockd_schedule_work(&drive->rq_work);
+ *   - drivers/nvme/host/multipath.c|91| <<nvme_failover_req>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|101| <<nvme_kick_requeue_lists>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|142| <<nvme_mpath_clear_ctrl_paths>> kblockd_schedule_work(&ns->head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|430| <<nvme_mpath_set_live>> kblockd_schedule_work(&head->requeue_work);
+ *   - drivers/nvme/host/multipath.c|693| <<nvme_mpath_remove_disk>> kblockd_schedule_work(&head->requeue_work);
+ *   - drivers/nvme/host/nvme.h|673| <<nvme_mpath_check_last_path>> kblockd_schedule_work(&head->requeue_work);
+ *   - drivers/scsi/scsi_lib.c|553| <<scsi_run_queue_async>> kblockd_schedule_work(&sdev->requeue_work);
+ */
 int kblockd_schedule_work(struct work_struct *work)
 {
 	return queue_work(kblockd_workqueue, work);
 }
 EXPORT_SYMBOL(kblockd_schedule_work);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1051| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+ *   - block/blk-mq.c|1058| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+ *   - block/blk-mq.c|1839| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ */
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork,
 				unsigned long delay)
 {
@@ -1888,6 +2176,14 @@ void blk_finish_plug(struct blk_plug *plug)
 }
 EXPORT_SYMBOL(blk_finish_plug);
 
+/*
+ * called by:
+ *   - fs/block_dev.c|247| <<__blkdev_direct_IO_simple>> blk_io_schedule();
+ *   - fs/block_dev.c|441| <<__blkdev_direct_IO>> blk_io_schedule();
+ *   - fs/direct-io.c|503| <<dio_await_one>> blk_io_schedule();
+ *   - fs/iomap/direct-io.c|572| <<iomap_dio_rw>> blk_io_schedule();
+ *   - mm/page_io.c|444| <<swap_readpage>> blk_io_schedule();
+ */
 void blk_io_schedule(void)
 {
 	/* Prevent hang_check timer from firing at us during very long I/O */
@@ -1900,6 +2196,10 @@ void blk_io_schedule(void)
 }
 EXPORT_SYMBOL_GPL(blk_io_schedule);
 
+/*
+ * called by:
+ *   - block/genhd.c|1229| <<genhd_device_init>> blk_dev_init();
+ */
 int __init blk_dev_init(void)
 {
 	BUILD_BUG_ON(REQ_OP_LAST >= (1 << REQ_OP_BITS));
diff --git a/block/blk-crypto-internal.h b/block/blk-crypto-internal.h
index d2b0f565d83c..146b6120d1af 100644
--- a/block/blk-crypto-internal.h
+++ b/block/blk-crypto-internal.h
@@ -128,6 +128,11 @@ static inline bool blk_crypto_bio_prep(struct bio **bio_ptr)
 }
 
 blk_status_t __blk_crypto_init_request(struct request *rq);
+/*
+ * called by:
+ *   - block/blk-crypto-internal.h|165| <<blk_crypto_insert_cloned_request>> return blk_crypto_init_request(rq);
+ *   - block/blk-mq.c|2400| <<blk_mq_submit_bio>> ret = blk_crypto_init_request(rq);
+ */
 static inline blk_status_t blk_crypto_init_request(struct request *rq)
 {
 	if (blk_crypto_rq_is_encrypted(rq))
diff --git a/block/blk-iocost.c b/block/blk-iocost.c
index d37b55db2409..298e72f08506 100644
--- a/block/blk-iocost.c
+++ b/block/blk-iocost.c
@@ -1718,6 +1718,9 @@ static u64 calc_size_vtime_cost(struct request *rq, struct ioc *ioc)
 	return cost;
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.throttle = ioc_rqos_throttle()
+ */
 static void ioc_rqos_throttle(struct rq_qos *rqos, struct bio *bio)
 {
 	struct blkcg_gq *blkg = bio->bi_blkg;
@@ -1844,6 +1847,9 @@ static void ioc_rqos_throttle(struct rq_qos *rqos, struct bio *bio)
 	finish_wait(&iocg->waitq, &wait.wait);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.merge = ioc_rqos_merge()
+ */
 static void ioc_rqos_merge(struct rq_qos *rqos, struct request *rq,
 			   struct bio *bio)
 {
@@ -1897,6 +1903,9 @@ static void ioc_rqos_merge(struct rq_qos *rqos, struct request *rq,
 	spin_unlock_irqrestore(&iocg->waitq.lock, flags);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.done_bio = ioc_rqos_done_bio()
+ */
 static void ioc_rqos_done_bio(struct rq_qos *rqos, struct bio *bio)
 {
 	struct ioc_gq *iocg = blkg_to_iocg(bio->bi_blkg);
@@ -1905,6 +1914,9 @@ static void ioc_rqos_done_bio(struct rq_qos *rqos, struct bio *bio)
 		atomic64_add(bio->bi_iocost_cost, &iocg->done_vtime);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.done = ioc_rqos_done()
+ */
 static void ioc_rqos_done(struct rq_qos *rqos, struct request *rq)
 {
 	struct ioc *ioc = rqos_to_ioc(rqos);
@@ -1940,6 +1952,9 @@ static void ioc_rqos_done(struct rq_qos *rqos, struct request *rq)
 	this_cpu_add(ioc->pcpu_stat->rq_wait_ns, rq_wait_ns);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.queue_depth_changed = ioc_rqos_queue_depth_changed()
+ */
 static void ioc_rqos_queue_depth_changed(struct rq_qos *rqos)
 {
 	struct ioc *ioc = rqos_to_ioc(rqos);
@@ -1949,6 +1964,9 @@ static void ioc_rqos_queue_depth_changed(struct rq_qos *rqos)
 	spin_unlock_irq(&ioc->lock);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.exit = ioc_rqos_exit()
+ */
 static void ioc_rqos_exit(struct rq_qos *rqos)
 {
 	struct ioc *ioc = rqos_to_ioc(rqos);
@@ -1964,6 +1982,10 @@ static void ioc_rqos_exit(struct rq_qos *rqos)
 	kfree(ioc);
 }
 
+/*
+ * 在以下使用ioc_rqos_ops:
+ *   - block/blk-iocost.c|1994| <<blk_iocost_init>> rqos->ops = &ioc_rqos_ops;
+ */
 static struct rq_qos_ops ioc_rqos_ops = {
 	.throttle = ioc_rqos_throttle,
 	.merge = ioc_rqos_merge,
@@ -1973,6 +1995,11 @@ static struct rq_qos_ops ioc_rqos_ops = {
 	.exit = ioc_rqos_exit,
 };
 
+/*
+ * called by:
+ *   - block/blk-iocost.c|2262| <<ioc_qos_write>> ret = blk_iocost_init(disk->queue);
+ *   - block/blk-iocost.c|2429| <<ioc_cost_model_write>> ret = blk_iocost_init(disk->queue);
+ */
 static int blk_iocost_init(struct request_queue *q)
 {
 	struct ioc *ioc;
@@ -2514,6 +2541,19 @@ static struct cftype ioc_files[] = {
 	{}
 };
 
+/*
+ * 在以下使用blkcg_policy_iocost:
+ *   - block/blk-iocost.c|639| <<blkg_to_iocg>> return pd_to_iocg(blkg_to_pd(blkg, &blkcg_policy_iocost));
+ *   - block/blk-iocost.c|649| <<blkcg_to_iocc>> return container_of(blkcg_to_cpd(blkcg, &blkcg_policy_iocost),
+ *   - block/blk-iocost.c|1956| <<ioc_rqos_exit>> blkcg_deactivate_policy(rqos->q, &blkcg_policy_iocost);
+ *   - block/blk-iocost.c|2014| <<blk_iocost_init>> ret = blkcg_activate_policy(q, &blkcg_policy_iocost);
+ *   - block/blk-iocost.c|2130| <<ioc_weight_show>> &blkcg_policy_iocost, seq_cft(sf)->private, false);
+ *   - block/blk-iocost.c|2169| <<ioc_weight_write>> ret = blkg_conf_prep(blkcg, &blkcg_policy_iocost, buf, &ctx);
+ *   - block/blk-iocost.c|2226| <<ioc_qos_show>> &blkcg_policy_iocost, seq_cft(sf)->private, false);
+ *   - block/blk-iocost.c|2393| <<ioc_cost_model_show>> &blkcg_policy_iocost, seq_cft(sf)->private, false);
+ *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+ *   - block/blk-iocost.c|2533| <<ioc_exit>> return blkcg_policy_unregister(&blkcg_policy_iocost);
+ */
 static struct blkcg_policy blkcg_policy_iocost = {
 	.dfl_cftypes	= ioc_files,
 	.cpd_alloc_fn	= ioc_cpd_alloc,
@@ -2525,6 +2565,13 @@ static struct blkcg_policy blkcg_policy_iocost = {
 
 static int __init ioc_init(void)
 {
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	return blkcg_policy_register(&blkcg_policy_iocost);
 }
 
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index f90429cf4edf..3092e9a6d024 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -715,6 +715,10 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1173| <<blkcg_init_queue>> ret = blk_iolatency_init(q);
+ */
 int blk_iolatency_init(struct request_queue *q)
 {
 	struct blk_iolatency *blkiolat;
@@ -935,6 +939,39 @@ static size_t iolatency_pd_stat(struct blkg_policy_data *pd, char *buf,
 }
 
 
+/*
+ * [0] iolatency_pd_alloc
+ * [0] blkg_alloc
+ * [0] blkg_conf_prep
+ * [0] tg_set_conf.constprop.31
+ * [0] cgroup_file_write
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] iolatency_pd_alloc
+ * [0] blkcg_activate_policy
+ * [0] blk_iolatency_init
+ * [0] blkcg_init_queue
+ * [0] blk_alloc_queue
+ * [0] blk_mq_init_queue_data
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp,
 						   struct request_queue *q,
 						   struct blkcg *blkcg)
@@ -950,6 +987,10 @@ static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp,
 		kfree(iolat);
 		return NULL;
 	}
+	/*
+	 * struct iolatency_grp:
+	 *  -> struct blkg_policy_data pd;
+	 */
 	return &iolat->pd;
 }
 
@@ -1030,6 +1071,17 @@ static struct cftype iolatency_files[] = {
 	{}
 };
 
+/*
+ * 在以下使用blkcg_policy_iolatency:
+ *   - block/blk-iolatency.c|184| <<blkg_to_lat>> return pd_to_lat(blkg_to_pd(blkg, &blkcg_policy_iolatency));
+ *   - block/blk-iolatency.c|647| <<blkcg_iolatency_exit>> blkcg_deactivate_policy(rqos->q, &blkcg_policy_iolatency);
+ *   - block/blk-iolatency.c|735| <<blk_iolatency_init>> ret = blkcg_activate_policy(q, &blkcg_policy_iolatency);
+ *   - block/blk-iolatency.c|801| <<iolatency_set_limit>> ret = blkg_conf_prep(blkcg, &blkcg_policy_iolatency, buf, &ctx);
+ *   - block/blk-iolatency.c|885| <<iolatency_print_limit>> &blkcg_policy_iolatency, seq_cft(sf)->private, false);
+ *   - block/blk-iolatency.c|990| <<iolatency_pd_init>> if (blkg->parent && blkg_to_pd(blkg->parent, &blkcg_policy_iolatency)) {
+ *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-iolatency.c|1049| <<iolatency_exit>> return blkcg_policy_unregister(&blkcg_policy_iolatency);
+ */
 static struct blkcg_policy blkcg_policy_iolatency = {
 	.dfl_cftypes	= iolatency_files,
 	.pd_alloc_fn	= iolatency_pd_alloc,
@@ -1041,6 +1093,13 @@ static struct blkcg_policy blkcg_policy_iolatency = {
 
 static int __init iolatency_init(void)
 {
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	return blkcg_policy_register(&blkcg_policy_iolatency);
 }
 
diff --git a/block/blk-lib.c b/block/blk-lib.c
index 0d1811e57ac7..7f2a808f8958 100644
--- a/block/blk-lib.c
+++ b/block/blk-lib.c
@@ -10,11 +10,21 @@
 
 #include "blk.h"
 
+/*
+ * 核心思想是分配一个新的bio new
+ * bio->bi_private = new
+ * bio->bi_end_io  = bio_chain_endio
+ * 然后返回new
+ */
 struct bio *blk_next_bio(struct bio *bio, unsigned int nr_pages, gfp_t gfp)
 {
 	struct bio *new = bio_alloc(gfp, nr_pages);
 
 	if (bio) {
+		/*
+		 * 核心思想是把bio->bi_private = new
+		 * bio->bi_end_io  = bio_chain_endio
+		 */
 		bio_chain(bio, new);
 		submit_bio(bio);
 	}
@@ -22,6 +32,24 @@ struct bio *blk_next_bio(struct bio *bio, unsigned int nr_pages, gfp_t gfp)
 	return new;
 }
 
+/*
+ * blkdiscard is used to discard device sectors.  This is useful for
+ * solid-state drivers (SSDs) and thinly-provisioned storage.  Unlike
+ * fstrim(8), this command is used directly on the block device.
+ *
+ * By default, blkdiscard will discard all blocks on the device.
+ * Options may be used to modify this behavior based on range or size,
+ * as explained below.
+ *
+ * called by:
+ *   - block/blk-lib.c|139| <<blkdev_issue_discard>> ret = __blkdev_issue_discard(bdev, sector, nr_sects, gfp_mask, flags,
+ *   - drivers/md/dm-thin.c|403| <<issue_discard>> return __blkdev_issue_discard(tc->pool_dev->bdev, s, len,
+ *   - drivers/md/raid0.c|536| <<raid0_handle_discard>> if (__blkdev_issue_discard(rdev->bdev,
+ *   - drivers/nvme/target/io-cmd-bdev.c|347| <<nvmet_bdev_discard_range>> ret = __blkdev_issue_discard(ns->bdev,
+ *   - fs/ext4/mballoc.c|3034| <<ext4_issue_discard>> return __blkdev_issue_discard(sb->s_bdev,
+ *   - fs/f2fs/segment.c|1193| <<__submit_discard_cmd>> err = __blkdev_issue_discard(bdev,
+ *   - fs/xfs/xfs_log_cil.c|545| <<xlog_discard_busy_extents>> error = __blkdev_issue_discard(mp->m_ddev_targp->bt_bdev,
+ */
 int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, int flags,
 		struct bio **biop)
@@ -94,6 +122,12 @@ int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 
 		WARN_ON_ONCE((req_sects << 9) > UINT_MAX);
 
+		/*
+		 * 核心思想是分配一个新的bio new
+		 * bio->bi_private = new
+		 * bio->bi_end_io  = bio_chain_endio
+		 * 然后返回new
+		 */
 		bio = blk_next_bio(bio, 0, gfp_mask);
 		bio->bi_iter.bi_sector = sector;
 		bio_set_dev(bio, bdev);
@@ -101,6 +135,7 @@ int __blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 
 		bio->bi_iter.bi_size = req_sects << 9;
 		sector += req_sects;
+		/* nr_sects用在while循环 */
 		nr_sects -= req_sects;
 
 		/*
@@ -128,6 +163,32 @@ EXPORT_SYMBOL(__blkdev_issue_discard);
  * Description:
  *    Issue a discard request for the sectors in question.
  */
+/*
+ * called by:
+ *   - block/ioctl.c|138| <<blk_ioctl_discard>> return blkdev_issue_discard(bdev, start >> 9, len >> 9,
+ *   - drivers/block/drbd/drbd_receiver.c|1552| <<drbd_issue_discard_or_zero_out>> err |= blkdev_issue_discard(bdev, start, max_discard_sectors, GFP_NOIO, 0);
+ *   - drivers/block/drbd/drbd_receiver.c|1564| <<drbd_issue_discard_or_zero_out>> err |= blkdev_issue_discard(bdev, start, nr, GFP_NOIO, 0);
+ *   - drivers/block/xen-blkback/blkback.c|1036| <<dispatch_discard_io>> err = blkdev_issue_discard(bdev, req->u.discard.sector_number,
+ *   - drivers/md/bcache/alloc.c|338| <<bch_allocator_thread>> blkdev_issue_discard(ca->bdev,
+ *   - drivers/md/raid5-cache.c|1345| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev,
+ *   - drivers/md/raid5-cache.c|1349| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev,
+ *   - drivers/md/raid5-cache.c|1353| <<r5l_write_super_and_discard_space>> blkdev_issue_discard(bdev, log->rdev->data_offset, end,
+ *   - drivers/target/target_core_file.c|565| <<fd_execute_unmap>> ret = blkdev_issue_discard(bdev,
+ *   - drivers/target/target_core_iblock.c|398| <<iblock_execute_unmap>> ret = blkdev_issue_discard(bdev,
+ *   - fs/block_dev.c|2014| <<blkdev_fallocate>> error = blkdev_issue_discard(bdev, start >> 9, len >> 9,
+ *   - fs/btrfs/extent-tree.c|1260| <<btrfs_issue_discard>> ret = blkdev_issue_discard(bdev, start >> 9, size >> 9,
+ *   - fs/btrfs/extent-tree.c|1277| <<btrfs_issue_discard>> ret = blkdev_issue_discard(bdev, start >> 9, bytes_left >> 9,
+ *   - fs/f2fs/file.c|3775| <<f2fs_secure_erase>> ret = blkdev_issue_discard(bdev, sector, nr_sects, GFP_NOFS,
+ *   - fs/nilfs2/sufile.c|1100| <<nilfs_sufile_trim_fs>> ret = blkdev_issue_discard(nilfs->ns_bdev,
+ *   - fs/nilfs2/sufile.c|1134| <<nilfs_sufile_trim_fs>> ret = blkdev_issue_discard(nilfs->ns_bdev,
+ *   - fs/nilfs2/the_nilfs.c|672| <<nilfs_discard_segments>> ret = blkdev_issue_discard(nilfs->ns_bdev,
+ *   - fs/nilfs2/the_nilfs.c|682| <<nilfs_discard_segments>> ret = blkdev_issue_discard(nilfs->ns_bdev,
+ *   - fs/xfs/xfs_discard.c|117| <<xfs_trim_extents>> error = blkdev_issue_discard(bdev, dbno, dlen, GFP_NOFS, 0);
+ *   - include/linux/blkdev.h|1335| <<sb_issue_discard>> return blkdev_issue_discard(sb->s_bdev,
+ *   - mm/swapfile.c|182| <<discard_swap>> err = blkdev_issue_discard(si->bdev, start_block,
+ *   - mm/swapfile.c|193| <<discard_swap>> err = blkdev_issue_discard(si->bdev, start_block,
+ *   - mm/swapfile.c|244| <<discard_swap_cluster>> if (blkdev_issue_discard(si->bdev, start_block,
+ */
 int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
 		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags)
 {
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 0157f2b3485a..c9c57dc4784a 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -18,6 +18,11 @@
 static int queue_index(struct blk_mq_queue_map *qmap,
 		       unsigned int nr_queues, const int q)
 {
+	/*
+	 * 比如nvme的poll queue没有中断
+	 * The poll queue(s) doesn't have an IRQ (and hence IRQ
+	 * affinity), so use the regular blk-mq cpu mapping
+	 */
 	return qmap->queue_offset + (q % nr_queues);
 }
 
@@ -32,6 +37,20 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3350| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3649| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|450| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|2186| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2348| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2349| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/nvme/host/tcp.c|2358| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7751| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1766| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
@@ -83,6 +102,12 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2323| <<blk_mq_alloc_rq_map>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|2379| <<blk_mq_alloc_rqs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], hctx_idx);
+ *   - block/blk-mq.c|3136| <<blk_mq_realloc_hw_ctxs>> node = blk_mq_hw_queue_to_node(&set->map[HCTX_TYPE_DEFAULT], i);
+ */
 int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index 3f09bcb8a6fd..e9d3a7722a32 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -414,6 +414,9 @@ static int hctx_busy_show(void *data, struct seq_file *m)
 	struct blk_mq_hw_ctx *hctx = data;
 	struct show_busy_params params = { .m = m, .hctx = hctx };
 
+	/*
+	 * iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+	 */
 	blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
 				&params);
 
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index d2790e5b06d1..92eba1c4305e 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -286,6 +286,11 @@ static int blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|356| <<blk_mq_sched_dispatch_requests>> if (__blk_mq_sched_dispatch_requests(hctx) == -EAGAIN) {
+ *   - block/blk-mq-sched.c|357| <<blk_mq_sched_dispatch_requests>> if (__blk_mq_sched_dispatch_requests(hctx) == -EAGAIN)
+ */
 static int __blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -339,6 +344,10 @@ static int __blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1754| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -493,6 +502,10 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|536| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq)) {
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -517,6 +530,15 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|64| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|1002| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|1008| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2239| <<__blk_mq_try_issue_directly>> blk_mq_sched_insert_request(rq, false, run_queue, false);
+ *   - block/blk-mq.c|2439| <<blk_mq_submit_bio>> blk_mq_sched_insert_request(rq, false, true, true);
+ *   - block/blk-mq.c|2472| <<blk_mq_submit_bio>> blk_mq_sched_insert_request(rq, false, true, true);
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -716,6 +738,13 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
  * called in either blk_queue_cleanup or elevator_switch, tagset
  * is required for freeing requests
  */
+/*
+ * called by:
+ *   - block/blk-core.c|472| <<blk_cleanup_queue>> blk_mq_sched_free_requests(q);
+ *   - block/blk-mq-sched.c|697| <<blk_mq_init_sched>> blk_mq_sched_free_requests(q);
+ *   - block/blk-mq-sched.c|709| <<blk_mq_init_sched>> blk_mq_sched_free_requests(q);
+ *   - block/blk.h|203| <<elevator_exit>> blk_mq_sched_free_requests(q);
+ */
 void blk_mq_sched_free_requests(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -727,6 +756,11 @@ void blk_mq_sched_free_requests(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|698| <<blk_mq_init_sched>> blk_mq_exit_sched(q, eq);
+ *   - block/elevator.c|195| <<__elevator_exit>> blk_mq_exit_sched(q, e);
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index e81ca1bf6e10..64eff5cecfcb 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -31,6 +31,10 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e);
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e);
 void blk_mq_sched_free_requests(struct request_queue *q);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2378| <<blk_mq_submit_bio>> if (blk_mq_sched_bio_merge(q, bio, nr_segs))
+ */
 static inline bool
 blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio,
 		unsigned int nr_segs)
@@ -70,6 +74,10 @@ static inline void blk_mq_sched_requeue_request(struct request *rq)
 		e->type->ops.requeue_request(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|82| <<blk_mq_hctx_has_pending>> blk_mq_sched_has_work(hctx);
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 32d82e23b095..e2394d2ffbc3 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,6 +21,10 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|62| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
@@ -33,6 +37,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|263| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -44,6 +53,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|70| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -56,6 +69,12 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|92| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|114| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|120| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
@@ -68,6 +87,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|383| <<__blk_mq_alloc_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|475| <<blk_mq_alloc_request_hctx>> tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
@@ -160,6 +184,13 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|157| <<blk_mq_get_tag>> blk_mq_put_tag(tags, data->ctx, tag + tag_offset);
+ *   - block/blk-mq.c|497| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|499| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|199| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,
 		    unsigned int tag)
 {
@@ -181,6 +212,16 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * 在以下使用bt_iter():
+ *   - block/blk-mq-tag.c|229| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ *
+ * blk_mq_queue_tag_busy_iter()
+ *  -> bt_for_each() --> 两处
+ *     -> bt_iter()
+ *
+ * 使用的是tags->rqs[bitnr] 不是static
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_iter_data *iter_data = data;
@@ -216,6 +257,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * 在以下使用bt_for_each():
+ *   - block/blk-mq-tag.c|419| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|420| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -226,6 +272,13 @@ static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 		.reserved = reserved,
 	};
 
+	/*
+	 * blk_mq_queue_tag_busy_iter()
+	 *  -> bt_for_each() --> 两处
+	 *     -> bt_iter()
+	 *
+	 * bt_iter(): 使用的是tags->rqs[bitnr] 不是static
+	 */
 	sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
 }
 
@@ -236,10 +289,36 @@ struct bt_tags_iter_data {
 	unsigned int flags;
 };
 
+/*
+ * 在以下使用BT_TAG_ITER_RESERVED:
+ *   - block/blk-mq-tag.c|301| <<bt_tags_iter>> bool reserved = iter_data->flags & BT_TAG_ITER_RESERVED;
+ *   - block/blk-mq-tag.c|369| <<__blk_mq_all_tag_iter>> WARN_ON_ONCE(flags & BT_TAG_ITER_RESERVED);
+ *   - block/blk-mq-tag.c|373| <<__blk_mq_all_tag_iter>> flags | BT_TAG_ITER_RESERVED);
+ */
 #define BT_TAG_ITER_RESERVED		(1 << 0)
+/*
+ * 在以下使用BT_TAG_ITER_STARTED:
+ *   - block/blk-mq-tag.c|263| <<bt_tags_iter>> if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
+ *   - block/blk-mq-tag.c|342| <<blk_mq_tagset_busy_iter>> BT_TAG_ITER_STARTED);
+ */
 #define BT_TAG_ITER_STARTED		(1 << 1)
+/*
+ * 在以下使用BT_TAG_ITER_STATIC_RQS:
+ *   - block/blk-mq-tag.c|257| <<bt_tags_iter>> if (iter_data->flags & BT_TAG_ITER_STATIC_RQS)
+ *   - block/blk-mq-tag.c|321| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+ */
 #define BT_TAG_ITER_STATIC_RQS		(1 << 2)
 
+/*
+ * 在以下使用bt_tags_iter():
+ *   - block/blk-mq-tag.c|292| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ *
+ * __blk_mq_all_tag_iter()
+ * -> bt_tags_for_each() --> 两处调用
+ *    -> bt_tags_iter()
+ *
+ * 根据BT_TAG_ITER_STATIC_RQS的情况决定用static还是rqs
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -254,12 +333,24 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	 * We can hit rq == NULL here, because the tagging functions
 	 * test and set the bit before assigning ->rqs[].
 	 */
+	/*
+	 * 在以下使用BT_TAG_ITER_STATIC_RQS:
+	 *   - block/blk-mq-tag.c|257| <<bt_tags_iter>> if (iter_data->flags & BT_TAG_ITER_STATIC_RQS)
+	 *   - block/blk-mq-tag.c|321| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+	 */
 	if (iter_data->flags & BT_TAG_ITER_STATIC_RQS)
 		rq = tags->static_rqs[bitnr];
 	else
 		rq = tags->rqs[bitnr];
 	if (!rq)
 		return true;
+	/*
+	 * 在以下使用BT_TAG_ITER_STARTED:
+	 *   - block/blk-mq-tag.c|263| <<bt_tags_iter>> if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
+	 *   - block/blk-mq-tag.c|342| <<blk_mq_tagset_busy_iter>> BT_TAG_ITER_STARTED);
+	 *
+	 * 这整个函数Return true to continue iterating tags, false to stop.
+	 */
 	if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
 	    !blk_mq_request_started(rq))
 		return true;
@@ -278,6 +369,11 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @data:	Will be passed as second argument to @fn.
  * @flags:	BT_TAG_ITER_*
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|301| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, flags | BT_TAG_ITER_RESERVED);
+ *   - block/blk-mq-tag.c|303| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, flags);
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, unsigned int flags)
 {
@@ -288,10 +384,24 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 		.flags = flags,
 	};
 
+	/*
+	 * sbitmap_for_each_set() - Iterate over each set bit in a &struct sbitmap.
+	 * @sb: Bitmap to iterate over.
+	 * @fn: Callback. Should return true to continue or false to break early.
+	 * @data: Pointer to pass to callback.
+	 */
 	if (tags->rqs)
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|352| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+ *   - block/blk-mq-tag.c|372| <<blk_mq_tagset_busy_iter>> __blk_mq_all_tag_iter(tagset->tags[i], fn, priv,
+ *
+ * blk_mq_all_tag_iter(): iterate over all requests in a tag map (所以用了BT_TAG_ITER_STATIC_RQS)
+ * blk_mq_tagset_busy_iter(): iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+ */
 static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv, unsigned int flags)
 {
@@ -315,6 +425,10 @@ static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
  *
  * Caller has to pass the tag map from which requests are allocated.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2472| <<blk_mq_hctx_has_requests>> blk_mq_all_tag_iter(tags, blk_mq_has_request, &data);
+ */
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv)
 {
@@ -331,12 +445,45 @@ void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|417| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - block/blk-mq-tag.c|436| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset,
+ *   - drivers/block/mtip32xx/mtip32xx.c|2685| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags, mtip_queue_cmd, dd);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2690| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags,
+ *   - drivers/block/mtip32xx/mtip32xx.c|3804| <<mtip_block_remove>> blk_mq_tagset_busy_iter(&dd->tags, mtip_no_dev_cleanup, dd);
+ *   - drivers/block/nbd.c|825| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/block/skd_main.c|396| <<skd_in_flight>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_inc_in_flight, &count);
+ *   - drivers/block/skd_main.c|1920| <<skd_recover_requests>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_recover_request, skdev);
+ *   - drivers/nvme/host/fc.c|3127| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|3150| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|2434| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2435| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1017| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|1036| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1893| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1914| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|410| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|420| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/scsi/hosts.c|580| <<scsi_host_busy>> blk_mq_tagset_busy_iter(&shost->tag_set,
+ *   - drivers/scsi/hosts.c|679| <<scsi_host_complete_all_commands>> blk_mq_tagset_busy_iter(&shost->tag_set, complete_all_cmds_iter,
+ *   - drivers/scsi/hosts.c|716| <<bool>> blk_mq_tagset_busy_iter(&shost->tag_set, __scsi_host_busy_iter_fn,
+ *   - drivers/scsi/ufs/ufshcd.c|1309| <<ufshcd_any_tag_in_use>> blk_mq_tagset_busy_iter(q->tag_set, ufshcd_is_busy, &busy);
+ *   - drivers/scsi/ufs/ufshcd.c|5899| <<ufshcd_tmc_handler>> blk_mq_tagset_busy_iter(q->tag_set, ufshcd_compl_tm, &ci);
+ *
+ * iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
 	int i;
 
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
+		/*
+		 * 在以下使用BT_TAG_ITER_STARTED:
+		 *   - block/blk-mq-tag.c|263| <<bt_tags_iter>> if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
+		 *   - block/blk-mq-tag.c|342| <<blk_mq_tagset_busy_iter>> BT_TAG_ITER_STARTED);
+		 */
 		if (tagset->tags && tagset->tags[i])
 			__blk_mq_all_tag_iter(tagset->tags[i], fn, priv,
 					      BT_TAG_ITER_STARTED);
@@ -344,6 +491,10 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 }
 EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
 
+/*
+ * 在以下使用blk_mq_tagset_count_completed_rqs():
+ *   - block/blk-mq-tag.c|472| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_count_completed_rqs, &count);
+ */
 static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
 		void *data, bool reserved)
 {
@@ -361,11 +512,29 @@ static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
  *
  * Note: This function has to be run after all IO queues are shutdown
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3129| <<nvme_fc_delete_association>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3152| <<nvme_fc_delete_association>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|2436| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->tagset);
+ *   - drivers/nvme/host/pci.c|2437| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1019| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1038| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/tcp.c|1895| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1916| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->tagset);
+ *   - drivers/nvme/target/loop.c|412| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/target/loop.c|422| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *
+ * 调用blk_mq_tagset_busy_iter()的时候用上了BT_TAG_ITER_STARTED
+ */
 void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)
 {
 	while (true) {
 		unsigned count = 0;
 
+		/*
+		 * iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+		 */
 		blk_mq_tagset_busy_iter(tagset,
 				blk_mq_tagset_count_completed_rqs, &count);
 		if (!count)
@@ -389,6 +558,13 @@ EXPORT_SYMBOL(blk_mq_tagset_wait_completed_request);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|118| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|128| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|890| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|995| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -417,11 +593,26 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 
 		if (tags->nr_reserved_tags)
 			bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+		/*
+		 * 在以下使用bt_for_each():
+		 *   - block/blk-mq-tag.c|419| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+		 *   - block/blk-mq-tag.c|420| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+		 *
+		 * fn()是:
+		 *   - blk_mq_check_inflight()
+		 *   - blk_mq_rq_inflight()
+		 *   - blk_mq_check_expired()
+		 */
 		bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
 	}
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|469| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|471| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, round_robin,
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -429,6 +620,14 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|501| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ *
+ * blk_mq_alloc_rq_map()
+ * -> blk_mq_init_tags()
+ *    -> blk_mq_init_bitmap_tags()
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -449,6 +648,10 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2321| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node, BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -470,6 +673,12 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2306| <<blk_mq_free_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2330| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2339| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ */
 void blk_mq_free_tags(struct blk_mq_tags *tags)
 {
 	sbitmap_queue_free(&tags->bitmap_tags);
@@ -477,6 +686,15 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3498| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3501| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ *
+ * queue_requests_store()
+ * -> blk_mq_update_nr_requests() --> 两处调用
+ *    -> blk_mq_tag_update_depth()
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -542,6 +760,23 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * called by:
+ *   - drivers/block/nbd.c|178| <<nbd_cmd_handle>> u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/block/skd_main.c|486| <<skd_mq_queue_rq>> const u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/block/skd_main.c|607| <<skd_timed_out>> blk_mq_unique_tag(req));
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2186| <<srp_queuecommand>> tag = blk_mq_unique_tag(scmnd->request);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2817| <<srp_abort>> tag = blk_mq_unique_tag(scmnd->request);
+ *   - drivers/nvme/host/nvme.h|181| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/cxlflash/main.c|436| <<cmd_to_target_hwq>> tag = blk_mq_unique_tag(scp->request);
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|645| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|843| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|4704| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|4717| <<get_tag>> return blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|7189| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5298| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|543| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index b1acac518c4e..d58804145b98 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -11,6 +11,13 @@ struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用blk_mq_tags->active_queues:
+	 *   - block/blk-mq-debugfs.c|452| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|67| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.h|93| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -37,6 +44,12 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|123| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|170| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1189| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -54,19 +67,36 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|376| <<__blk_mq_alloc_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|472| <<blk_mq_alloc_request_hctx>> blk_mq_tag_busy(data.hctx);
+ *   - block/blk-mq.c|1103| <<__blk_mq_get_driver_tag>> blk_mq_tag_busy(rq->mq_hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return false;
 
+	/*
+	 * 只在这里被调用
+	 */
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1009| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2581| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return;
 
+	/*
+	 * 只在这里被调用
+	 */
 	__blk_mq_tag_idle(hctx);
 }
 
@@ -74,6 +104,11 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|81| <<__blk_mq_get_tag>> if (!data->q->elevator && !hctx_may_queue(data->hctx, bt))
+ *   - block/blk-mq.c|1110| <<__blk_mq_get_driver_tag>> if (!hctx_may_queue(rq->mq_hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -101,6 +136,11 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|197| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1105| <<__blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 7b8a42c35102..332f74f1f79b 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,6 +21,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|685| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|716| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index cdced4aca2e8..43a64cae77d4 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -41,11 +41,24 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 在以下使用blk_cpu_done:
+ *   - block/blk-mq.c|577| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-mq.c|596| <<blk_mq_trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+ *   - block/blk-mq.c|616| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+ *   - block/blk-mq.c|617| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+ *   - block/blk-mq.c|3898| <<blk_mq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+ */
 static DEFINE_PER_CPU(struct list_head, blk_cpu_done);
 
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * 在以下使用blk_mq_poll_stats_bkt():
+ *   - block/blk-mq.c|3246| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3786| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, sectors, bucket;
@@ -69,6 +82,22 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用blk_mq_hw_ctx->ctx_map, Bitmap for each software queue. If bit is on, there is a
+	 * pending request in that software queue:
+	 *   - block/blk-mq-debugfs.c|445| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|253| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq-sysfs.c|44| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|81| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|93| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|94| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|1077| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1115| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2747| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2767| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2930| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
 			blk_mq_sched_has_work(hctx);
@@ -130,6 +159,16 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	inflight[1] = mi.inflight[1];
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|347| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|183| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3802| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|354| <<pmem_pagemap_kill>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|4617| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ *   - drivers/nvme/host/multipath.c|43| <<nvme_mpath_start_freeze>> blk_freeze_queue_start(h->disk->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	mutex_lock(&q->mq_freeze_lock);
@@ -146,6 +185,17 @@ EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
+	/*
+	 * 在以下使用request_queue->mq_freeze_wq:
+	 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
@@ -153,6 +203,17 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout)
 {
+	/*
+	 * 在以下使用request_queue->mq_freeze_wq:
+	 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	return wait_event_timeout(q->mq_freeze_wq,
 					percpu_ref_is_zero(&q->q_usage_counter),
 					timeout);
@@ -163,6 +224,11 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);
  * Guarantee no request is in use, so we can change any data structure of
  * the queue afterward.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|395| <<blk_cleanup_queue>> blk_freeze_queue(q);
+ *   - block/blk-mq.c|246| <<blk_mq_freeze_queue>> blk_freeze_queue(q);
+ */
 void blk_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -176,6 +242,34 @@ void blk_freeze_queue(struct request_queue *q)
 	blk_mq_freeze_queue_wait(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1336| <<blkcg_activate_policy>> blk_mq_freeze_queue(q);
+ *   - block/blk-cgroup.c|1433| <<blkcg_deactivate_policy>> blk_mq_freeze_queue(q);
+ *   - block/blk-iolatency.c|855| <<iolatency_set_limit>> blk_mq_freeze_queue(blkg->q);
+ *   - block/blk-mq.c|3029| <<blk_mq_update_tag_set_depth>> blk_mq_freeze_queue(q);
+ *   - block/blk-mq.c|3622| <<blk_mq_update_nr_requests>> blk_mq_freeze_queue(q);
+ *   - block/blk-mq.c|3742| <<__blk_mq_update_nr_hw_queues>> blk_mq_freeze_queue(q);
+ *   - block/blk-sysfs.c|499| <<queue_wb_lat_store>> blk_mq_freeze_queue(q);
+ *   - block/blk-zoned.c|518| <<blk_revalidate_disk_zones>> blk_mq_freeze_queue(q);
+ *   - block/elevator.c|694| <<elevator_init_mq>> blk_mq_freeze_queue(q);
+ *   - block/elevator.c|722| <<elevator_switch>> blk_mq_freeze_queue(q);
+ *   - drivers/block/aoe/aoedev.c|229| <<aoedev_downdev>> blk_mq_freeze_queue(d->blkq);
+ *   - drivers/block/ataflop.c|732| <<do_format>> blk_mq_freeze_queue(q);
+ *   - drivers/block/loop.c|218| <<__loop_update_dio>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|740| <<loop_change_fd>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1226| <<__loop_clr_fd>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1380| <<loop_set_status>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1625| <<loop_set_block_size>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1933| <<lo_release>> blk_mq_freeze_queue(lo->lo_queue);
+ *   - drivers/block/rbd.c|7267| <<do_rbd_remove>> blk_mq_freeze_queue(rbd_dev->disk->queue);
+ *   - drivers/block/sunvdc.c|1136| <<vdc_queue_drain>> blk_mq_freeze_queue(q);
+ *   - drivers/block/swim3.c|834| <<release_drive>> blk_mq_freeze_queue(q);
+ *   - drivers/mtd/mtd_blkdevs.c|500| <<del_mtd_blktrans_dev>> blk_mq_freeze_queue(old->rq);
+ *   - drivers/nvme/host/core.c|1960| <<nvme_update_disk_info>> blk_mq_freeze_queue(disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2522| <<scsi_device_quiesce>> blk_mq_freeze_queue(q);
+ *   - drivers/scsi/sd.c|3520| <<scsi_disk_release>> blk_mq_freeze_queue(q);
+ */
 void blk_mq_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -186,6 +280,36 @@ void blk_mq_freeze_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue);
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1394| <<blkcg_activate_policy>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-cgroup.c|1451| <<blkcg_deactivate_policy>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-iolatency.c|864| <<iolatency_set_limit>> blk_mq_unfreeze_queue(blkg->q);
+ *   - block/blk-mq.c|3031| <<blk_mq_update_tag_set_depth>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-mq.c|3650| <<blk_mq_update_nr_requests>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-mq.c|3788| <<__blk_mq_update_nr_hw_queues>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-pm.c|90| <<blk_pre_runtime_suspend>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-sysfs.c|505| <<queue_wb_lat_store>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-zoned.c|531| <<blk_revalidate_disk_zones>> blk_mq_unfreeze_queue(q);
+ *   - block/elevator.c|700| <<elevator_init_mq>> blk_mq_unfreeze_queue(q);
+ *   - block/elevator.c|728| <<elevator_switch>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/aoe/aoedev.c|232| <<aoedev_downdev>> blk_mq_unfreeze_queue(d->blkq);
+ *   - drivers/block/ataflop.c|794| <<do_format>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/loop.c|228| <<__loop_update_dio>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|747| <<loop_change_fd>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1261| <<__loop_clr_fd>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1416| <<loop_set_status>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1641| <<loop_set_block_size>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1934| <<lo_release>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/sunvdc.c|1142| <<vdc_queue_drain>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/swim3.c|837| <<release_drive>> blk_mq_unfreeze_queue(q);
+ *   - drivers/mtd/mtd_blkdevs.c|503| <<del_mtd_blktrans_dev>> blk_mq_unfreeze_queue(old->rq);
+ *   - drivers/nvme/host/core.c|2026| <<nvme_update_disk_info>> blk_mq_unfreeze_queue(disk->queue);
+ *   - drivers/nvme/host/core.c|4580| <<nvme_unfreeze>> blk_mq_unfreeze_queue(ns->queue);
+ *   - drivers/nvme/host/multipath.c|23| <<nvme_mpath_unfreeze>> blk_mq_unfreeze_queue(h->disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2530| <<scsi_device_quiesce>> blk_mq_unfreeze_queue(q);
+ *   - drivers/scsi/sd.c|3521| <<scsi_disk_release>> blk_mq_unfreeze_queue(q);
+ */
 void blk_mq_unfreeze_queue(struct request_queue *q)
 {
 	mutex_lock(&q->mq_freeze_lock);
@@ -193,6 +317,17 @@ void blk_mq_unfreeze_queue(struct request_queue *q)
 	WARN_ON_ONCE(q->mq_freeze_depth < 0);
 	if (!q->mq_freeze_depth) {
 		percpu_ref_resurrect(&q->q_usage_counter);
+		/*
+		 * 在以下使用request_queue->mq_freeze_wq:
+		 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+		 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+		 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+		 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+		 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+		 */
 		wake_up_all(&q->mq_freeze_wq);
 	}
 	mutex_unlock(&q->mq_freeze_lock);
@@ -253,6 +388,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|350| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -267,11 +406,21 @@ void blk_mq_wake_waiters(struct request_queue *q)
  * Only need start/end time stamping if we have iostat or
  * blk stats enabled, or using an IO scheduler.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|446| <<blk_mq_rq_ctx_init>> if (blk_mq_need_time_stamp(rq))
+ *   - block/blk-mq.c|678| <<__blk_mq_end_request>> if (blk_mq_need_time_stamp(rq))
+ */
 static inline bool blk_mq_need_time_stamp(struct request *rq)
 {
 	return (rq->rq_flags & (RQF_IO_STAT | RQF_STATS)) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|540| <<__blk_mq_alloc_request>> return blk_mq_rq_ctx_init(data, tag, alloc_time_ns);
+ *   - block/blk-mq.c|626| <<blk_mq_alloc_request_hctx>> return blk_mq_rq_ctx_init(&data, tag, alloc_time_ns);
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, u64 alloc_time_ns)
 {
@@ -343,6 +492,11 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|558| <<blk_mq_alloc_request>> rq = __blk_mq_alloc_request(&data);
+ *   - block/blk-mq.c|2384| <<blk_mq_submit_bio>> rq = __blk_mq_alloc_request(&data);
+ */
 static struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data)
 {
 	struct request_queue *q = data->q;
@@ -424,6 +578,10 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|514| <<nvme_alloc_request>> req = blk_mq_alloc_request_hctx(q, op, flags,
+ */
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
 {
@@ -557,6 +715,26 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 调用blk_mq_end_request()的例子:
+ *   - block/blk-flush.c|205| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|394| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1629| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|2295| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - block/blk-mq.c|2334| <<blk_mq_try_issue_list_directly>> blk_mq_end_request(rq, ret);
+ *   - block/bsg-lib.c|158| <<bsg_teardown_job>> blk_mq_end_request(rq, BLK_STS_OK);
+ *   - drivers/block/loop.c|500| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/nbd.c|340| <<nbd_complete_rq>> blk_mq_end_request(req, cmd->status);
+ *   - drivers/block/null_blk_main.c|674| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|171| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|928| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2115| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/md/dm-rq.c|167| <<dm_end_request>> blk_mq_end_request(rq, error);
+ *   - drivers/md/dm-rq.c|264| <<dm_softirq_done>> blk_mq_end_request(rq, tio->error);
+ *   - drivers/nvme/host/core.c|298| <<nvme_end_req>> blk_mq_end_request(req, status);
+ *   - drivers/nvme/host/multipath.c|90| <<nvme_failover_req>> blk_mq_end_request(req, 0);
+ *   - drivers/scsi/scsi_transport_fc.c|3581| <<fc_bsg_job_timeout>> blk_mq_end_request(req, BLK_STS_IOERR);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -574,6 +752,14 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 	struct list_head *cpu_list, local_list;
 
 	local_irq_disable();
+	/*
+	 * 在以下使用blk_cpu_done:
+	 *   - block/blk-mq.c|577| <<blk_done_softirq>> cpu_list = this_cpu_ptr(&blk_cpu_done);
+	 *   - block/blk-mq.c|596| <<blk_mq_trigger_softirq>> list = this_cpu_ptr(&blk_cpu_done);
+	 *   - block/blk-mq.c|616| <<blk_softirq_cpu_dead>> list_splice_init(&per_cpu(blk_cpu_done, cpu),
+	 *   - block/blk-mq.c|617| <<blk_softirq_cpu_dead>> this_cpu_ptr(&blk_cpu_done));
+	 *   - block/blk-mq.c|3898| <<blk_mq_init>> INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
+	 */
 	cpu_list = this_cpu_ptr(&blk_cpu_done);
 	list_replace_init(cpu_list, &local_list);
 	local_irq_enable();
@@ -582,11 +768,23 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 		struct request *rq;
 
 		rq = list_entry(local_list.next, struct request, ipi_list);
+		/*
+		 * struct request:
+		 *     union {
+		 *         struct hlist_node hash; // merge hash
+		 *         struct list_head ipi_list;
+		 *     };
+		 */
 		list_del_init(&rq->ipi_list);
 		rq->q->mq_ops->complete(rq);
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|639| <<__blk_mq_complete_request_remote>> blk_mq_trigger_softirq(rq);
+ *   - block/blk-mq.c|681| <<blk_mq_complete_request_remote>> blk_mq_trigger_softirq(rq);
+ */
 static void blk_mq_trigger_softirq(struct request *rq)
 {
 	struct list_head *list;
@@ -606,6 +804,10 @@ static void blk_mq_trigger_softirq(struct request *rq)
 	local_irq_restore(flags);
 }
 
+/*
+ * 正如注释说的, If a CPU goes away, splice its entries to
+ * the current CPU and trigger a run of the softirq
+ */
 static int blk_softirq_cpu_dead(unsigned int cpu)
 {
 	/*
@@ -641,6 +843,10 @@ static void __blk_mq_complete_request_remote(void *data)
 		rq->q->mq_ops->complete(rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|845| <<blk_mq_complete_request_remote>> if (blk_mq_complete_need_ipi(rq)) {
+ */
 static inline bool blk_mq_complete_need_ipi(struct request *rq)
 {
 	int cpu = raw_smp_processor_id();
@@ -659,6 +865,11 @@ static inline bool blk_mq_complete_need_ipi(struct request *rq)
 	return cpu_online(rq->mq_ctx->cpu);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|869| <<blk_mq_complete_request>> if (!blk_mq_complete_request_remote(rq))
+ *   - drivers/nvme/host/nvme.h|560| <<nvme_try_complete_req>> return blk_mq_complete_request_remote(req);
+ */
 bool blk_mq_complete_request_remote(struct request *rq)
 {
 	WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
@@ -708,6 +919,13 @@ static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 		srcu_read_unlock(hctx->srcu, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1702| <<__blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|1827| <<blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|2222| <<blk_mq_try_issue_directly>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|2240| <<blk_mq_request_issue_directly>> hctx_lock(hctx, &srcu_idx);
+ */
 static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
 	__acquires(hctx->srcu)
 {
@@ -752,6 +970,13 @@ void blk_mq_start_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_start_request);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|944| <<blk_mq_requeue_request>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1444| <<blk_mq_handle_dev_resource>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|1457| <<blk_mq_handle_zone_resource>> __blk_mq_requeue_request(rq);
+ *   - block/blk-mq.c|2150| <<__blk_mq_issue_directly>> __blk_mq_requeue_request(rq);
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -866,6 +1091,10 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 }
 EXPORT_SYMBOL(blk_mq_tag_to_rq);
 
+/*
+ * 在以下使用blk_mq_rq_inflight():
+ *   - block/blk-mq.c|1062| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ */
 static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			       void *priv, bool reserved)
 {
@@ -883,6 +1112,10 @@ static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|2445| <<dm_wait_for_completion>> if (!blk_mq_queue_inflight(md->queue))
+ */
 bool blk_mq_queue_inflight(struct request_queue *q)
 {
 	bool busy = false;
@@ -1017,6 +1250,10 @@ struct flush_busy_ctx_data {
 	struct list_head *list;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|335| <<__blk_mq_sched_dispatch_requests>> blk_mq_flush_busy_ctxs(hctx, &rq_list);
+ */
 static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
 {
 	struct flush_busy_ctx_data *flush_data = data;
@@ -1100,6 +1337,12 @@ static bool __blk_mq_get_driver_tag(struct request *rq)
 	unsigned int tag_offset = rq->mq_hctx->tags->nr_reserved_tags;
 	int tag;
 
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|376| <<__blk_mq_alloc_request>> blk_mq_tag_busy(data->hctx);
+	 *   - block/blk-mq.c|472| <<blk_mq_alloc_request_hctx>> blk_mq_tag_busy(data.hctx);
+	 *   - block/blk-mq.c|1103| <<__blk_mq_get_driver_tag>> blk_mq_tag_busy(rq->mq_hctx);
+	 */
 	blk_mq_tag_busy(rq->mq_hctx);
 
 	if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
@@ -1490,6 +1733,11 @@ bool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list,
  *
  * Send pending requests to the hardware.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1831| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|2032| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1527,6 +1775,13 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
+	/*
+	 * 在以下调用hctx_lock():
+	 *   - block/blk-mq.c|1702| <<__blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|1827| <<blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|2222| <<blk_mq_try_issue_directly>> hctx_lock(hctx, &srcu_idx);
+	 *   - block/blk-mq.c|2240| <<blk_mq_request_issue_directly>> hctx_lock(hctx, &srcu_idx);
+	 */
 	hctx_lock(hctx, &srcu_idx);
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
@@ -1613,6 +1868,19 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * 在以下调用kblockd_mod_delayed_work_on():
+	 *   - block/blk-mq.c|1051| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+	 *   - block/blk-mq.c|1058| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+	 *   - block/blk-mq.c|1839| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *
+	 * 在以下使用blk_mq_hw_ctx->blk_mq_run_work_fn:
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1839| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1957| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq.c|2024| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 *   - block/blk-mq.c|2948| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
@@ -1639,6 +1907,24 @@ EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
  * pending requests to be sent. If this is true, run the queue to send requests
  * to hardware.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|90| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|358| <<blk_mq_sched_dispatch_requests>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|589| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|623| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|132| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1376| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1697| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1902| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1986| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2006| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|2097| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2449| <<blk_mq_submit_bio>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2825| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|698| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - drivers/block/rnbd/rnbd-clt.c|177| <<rnbd_clt_dev_requeue>> blk_mq_run_hw_queue(q->hctx, true);
+ */
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1794,6 +2080,14 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * 在以下使用blk_mq_hw_ctx->blk_mq_run_work_fn:
+ *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+ *   - block/blk-mq.c|1839| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+ *   - block/blk-mq.c|1957| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+ *   - block/blk-mq.c|2024| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+ *   - block/blk-mq.c|2948| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1826,6 +2120,10 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|570| <<blk_mq_sched_insert_request>> __blk_mq_insert_request(hctx, rq, at_head);
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1846,6 +2144,14 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|407| <<blk_insert_flush>> blk_mq_request_bypass_insert(rq, false, false);
+ *   - block/blk-mq-sched.c|559| <<blk_mq_sched_insert_request>> blk_mq_request_bypass_insert(rq, at_head, false);
+ *   - block/blk-mq.c|1000| <<blk_mq_requeue_work>> blk_mq_request_bypass_insert(rq, false, false);
+ *   - block/blk-mq.c|2267| <<blk_mq_try_issue_directly>> blk_mq_request_bypass_insert(rq, false, true);
+ *   - block/blk-mq.c|2304| <<blk_mq_try_issue_list_directly>> blk_mq_request_bypass_insert(rq, false,
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool at_head,
 				  bool run_queue)
 {
@@ -1933,6 +2239,10 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	} while(!list_empty(&list));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2398| <<blk_mq_submit_bio>> blk_mq_bio_to_request(rq, bio, nr_segs);
+ */
 static void blk_mq_bio_to_request(struct request *rq, struct bio *bio,
 		unsigned int nr_segs)
 {
@@ -2137,9 +2447,20 @@ static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
  *
  * Returns: Request queue cookie.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1319| <<__submit_bio>> return blk_mq_submit_bio(bio);
+ *   - block/blk-core.c|1413| <<__submit_bio_noacct_mq>> ret = blk_mq_submit_bio(bio);
+ *   - drivers/md/dm.c|1780| <<dm_submit_bio>> return blk_mq_submit_bio(bio);
+ */
 blk_qc_t blk_mq_submit_bio(struct bio *bio)
 {
 	struct request_queue *q = bio->bi_disk->queue;
+	/*
+	 * Reads are always treated as synchronous, as are requests with the FUA or
+	 * PREFLUSH flag.  Other operations may be marked as synchronous using the
+	 * REQ_SYNC flag.
+	 */
 	const int is_sync = op_is_sync(bio->bi_opf);
 	const int is_flush_fua = op_is_flush(bio->bi_opf);
 	struct blk_mq_alloc_data data = {
@@ -2306,6 +2627,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|633| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|550| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2750| <<__blk_mq_alloc_map_and_request>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2455,6 +2782,10 @@ static bool blk_mq_has_request(struct request *rq, void *data, bool reserved)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2511| <<blk_mq_hctx_notify_offline>> while (blk_mq_hctx_has_requests(hctx))
+ */
 static bool blk_mq_hctx_has_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->sched_tags ?
@@ -2477,6 +2808,12 @@ static inline bool blk_mq_last_cpu_in_hctx(unsigned int cpu,
 	return true;
 }
 
+/*
+ * offline一个cpu的时候, 如果是某个hctx的最后一个cpu就设置BLK_MQ_S_INACTIVE到hctx->state
+ * 然后Try to grab a reference to the queue and wait for any outstanding
+ * requests.  If we could not grab a reference the queue has been
+ * frozen and there are no requests.
+ */
 static int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,
@@ -2510,6 +2847,9 @@ static int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)
 	return 0;
 }
 
+/*
+ * online一个cpu的时候把cpu对应的hctx->state清除BLK_MQ_S_INACTIVE
+ */
 static int blk_mq_hctx_notify_online(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,
@@ -2621,9 +2961,21 @@ static int blk_mq_init_hctx(struct request_queue *q,
 {
 	hctx->queue_num = hctx_idx;
 
+	/*
+	 * 在以下使用CPUHP_AP_BLK_MQ_ONLINE:
+	 *   - block/blk-mq.c|2605| <<blk_mq_remove_cpuhp>> cpuhp_state_remove_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,
+	 *   - block/blk-mq.c|2667| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,
+	 *   - block/blk-mq.c|3943| <<blk_mq_init>> cpuhp_setup_state_multi(CPUHP_AP_BLK_MQ_ONLINE, "block/mq:online",
+	 */
 	if (!(hctx->flags & BLK_MQ_F_STACKING))
 		cpuhp_state_add_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,
 				&hctx->cpuhp_online);
+	/*
+	 * 在以下使用CPUHP_BLK_MQ_DEAD:
+	 *   - block/blk-mq.c|2607| <<blk_mq_remove_cpuhp>> cpuhp_state_remove_instance_nocalls(CPUHP_BLK_MQ_DEAD,
+	 *   - block/blk-mq.c|2669| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
+	 *   - block/blk-mq.c|3941| <<blk_mq_init>> cpuhp_setup_state_multi(CPUHP_BLK_MQ_DEAD, "block/mq:dead", NULL,
+	 */
 	cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
 
 	hctx->tags = set->tags[hctx_idx];
@@ -3021,6 +3373,30 @@ struct request_queue *blk_mq_init_queue_data(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL_GPL(blk_mq_init_queue_data);
 
+/*
+ * 部分调用blk_mq_init_queue()的例子:
+ *   - block/blk-mq.c|3406| <<blk_mq_init_sq_queue>> q = blk_mq_init_queue(set);
+ *   - block/bsg-lib.c|390| <<bsg_setup_queue>> q = blk_mq_init_queue(set);
+ *   - drivers/block/loop.c|2122| <<loop_add>> lo->lo_queue = blk_mq_init_queue(&lo->tag_set);
+ *   - drivers/block/nbd.c|1723| <<nbd_dev_add>> q = blk_mq_init_queue(&nbd->tag_set);
+ *   - drivers/block/virtio_blk.c|782| <<virtblk_probe>> q = blk_mq_init_queue(&vblk->tag_set);
+ *   - drivers/block/xen-blkfront.c|996| <<xlvbd_init_blk_queue>> rq = blk_mq_init_queue(&info->tag_set);
+ *   - drivers/nvme/host/core.c|3927| <<nvme_alloc_ns>> ns->queue = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/fc.c|2810| <<nvme_fc_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3491| <<nvme_fc_init_ctrl>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/fc.c|3497| <<nvme_fc_init_ctrl>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|1605| <<nvme_alloc_admin_tags>> dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|884| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|890| <<nvme_rdma_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/rdma.c|967| <<nvme_rdma_configure_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/nvme/host/tcp.c|1772| <<nvme_tcp_configure_io_queues>> ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+ *   - drivers/nvme/host/tcp.c|1841| <<nvme_tcp_configure_admin_queue>> ctrl->fabrics_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1847| <<nvme_tcp_configure_admin_queue>> ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+ *   - drivers/nvme/target/loop.c|362| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.fabrics_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|368| <<nvme_loop_configure_admin_queue>> ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+ *   - drivers/nvme/target/loop.c|527| <<nvme_loop_create_io_queues>> ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+ *   - drivers/scsi/scsi_lib.c|1884| <<scsi_mq_alloc_queue>> sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	return blk_mq_init_queue_data(set, NULL);
@@ -3244,6 +3620,10 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 EXPORT_SYMBOL(blk_mq_init_allocated_queue);
 
 /* tags can _not_ be used after returning from blk_mq_exit_queue */
+/*
+ * called by:
+ *   - block/blk-core.c|460| <<blk_cleanup_queue>> blk_mq_exit_queue(q);
+ */
 void blk_mq_exit_queue(struct request_queue *q)
 {
 	struct blk_mq_tag_set	*set = q->tag_set;
@@ -3304,6 +3684,11 @@ static int blk_mq_alloc_map_and_requests(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3496| <<blk_mq_alloc_tag_set>> ret = blk_mq_update_queue_map(set);
+ *   - block/blk-mq.c|3695| <<__blk_mq_update_nr_hw_queues>> blk_mq_update_queue_map(set);
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	/*
@@ -3471,6 +3856,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|82| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3652,6 +4041,17 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1268| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/null_blk_main.c|327| <<nullb_apply_submit_queues>> blk_mq_update_nr_hw_queues(set, submit_queues);
+ *   - drivers/block/xen-blkfront.c|2123| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2883| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2285| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|989| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1794| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|470| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -3661,6 +4061,10 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called by:
+ *   - block/blk-mq.c|3774| <<blk_mq_poll_nsecs>> if (!blk_poll_stats_enable(q))
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
@@ -3670,6 +4074,10 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|551| <<__blk_mq_end_request>> blk_mq_poll_stats_start(rq->q);
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3683,6 +4091,10 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3245| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3694,6 +4106,10 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3816| <<blk_mq_poll_hybrid_sleep>> nsecs = blk_mq_poll_nsecs(q, rq);
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct request *rq)
 {
@@ -3726,6 +4142,10 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3875| <<blk_mq_poll_hybrid>> return blk_mq_poll_hybrid_sleep(q, rq);
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct request *rq)
 {
@@ -3779,6 +4199,10 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3886| <<blk_poll>> if (blk_mq_poll_hybrid(q, hctx, cookie))
+ */
 static bool blk_mq_poll_hybrid(struct request_queue *q,
 			       struct blk_mq_hw_ctx *hctx, blk_qc_t cookie)
 {
@@ -3816,6 +4240,17 @@ static bool blk_mq_poll_hybrid(struct request_queue *q,
  *    looping until at least one completion is found, unless the task is
  *    otherwise marked running (or we need to reschedule).
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|867| <<nvme_execute_rq_polled>> blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+ *   - fs/block_dev.c|246| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/block_dev.c|284| <<blkdev_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
+ *   - fs/block_dev.c|440| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc, true))
+ *   - fs/direct-io.c|502| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie, true))
+ *   - fs/iomap/direct-io.c|58| <<iomap_dio_iopoll>> return blk_poll(q, READ_ONCE(kiocb->ki_cookie), spin);
+ *   - fs/iomap/direct-io.c|570| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|443| <<swap_readpage>> if (!blk_poll(disk->queue, qc, true))
+ */
 int blk_poll(struct request_queue *q, blk_qc_t cookie, bool spin)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -3872,6 +4307,13 @@ EXPORT_SYMBOL_GPL(blk_poll);
 
 unsigned int blk_mq_rq_cpu(struct request *rq)
 {
+	/*
+	 * request:
+	 *  -> struct request_queue *q;
+	 *  -> struct blk_mq_ctx *mq_ctx;
+	 *      -> unsigned int cpu;
+	 *  -> struct blk_mq_hw_ctx *mq_hctx;
+	 */
 	return rq->mq_ctx->cpu;
 }
 EXPORT_SYMBOL(blk_mq_rq_cpu);
@@ -3884,11 +4326,28 @@ static int __init blk_mq_init(void)
 		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 
+	/*
+	 * 正如blk_softirq_cpu_dead()注释说的,
+	 * If a CPU goes away, splice its entries to the current CPU and
+	 * trigger a run of the softirq
+	 */
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
 				  "block/softirq:dead", NULL,
 				  blk_softirq_cpu_dead);
+	/*
+	 * 在以下使用CPUHP_BLK_MQ_DEAD:
+	 *   - block/blk-mq.c|2607| <<blk_mq_remove_cpuhp>> cpuhp_state_remove_instance_nocalls(CPUHP_BLK_MQ_DEAD,
+	 *   - block/blk-mq.c|2669| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
+	 *   - block/blk-mq.c|3941| <<blk_mq_init>> cpuhp_setup_state_multi(CPUHP_BLK_MQ_DEAD, "block/mq:dead", NULL,
+	 */
 	cpuhp_setup_state_multi(CPUHP_BLK_MQ_DEAD, "block/mq:dead", NULL,
 				blk_mq_hctx_notify_dead);
+	/*
+	 * 在以下使用CPUHP_AP_BLK_MQ_ONLINE:
+	 *   - block/blk-mq.c|2605| <<blk_mq_remove_cpuhp>> cpuhp_state_remove_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,
+	 *   - block/blk-mq.c|2667| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,
+	 *   - block/blk-mq.c|3943| <<blk_mq_init>> cpuhp_setup_state_multi(CPUHP_AP_BLK_MQ_ONLINE, "block/mq:online",
+	 */
 	cpuhp_setup_state_multi(CPUHP_AP_BLK_MQ_ONLINE, "block/mq:online",
 				blk_mq_hctx_notify_online,
 				blk_mq_hctx_notify_offline);
diff --git a/block/blk-rq-qos.h b/block/blk-rq-qos.h
index 2bc43e94f4c4..c67c90f0022e 100644
--- a/block/blk-rq-qos.h
+++ b/block/blk-rq-qos.h
@@ -171,6 +171,10 @@ static inline void rq_qos_done_bio(struct request_queue *q, struct bio *bio)
 		__rq_qos_done_bio(q->rq_qos, bio);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2381| <<blk_mq_submit_bio>> rq_qos_throttle(q, bio);
+ */
 static inline void rq_qos_throttle(struct request_queue *q, struct bio *bio)
 {
 	/*
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 34b721a2743a..db4c8f2d5c08 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -22,6 +22,39 @@ EXPORT_SYMBOL(blk_max_low_pfn);
 
 unsigned long blk_max_pfn;
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3446| <<blk_mq_init_allocated_queue>> blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);
+ *   - block/blk-sysfs.c|453| <<queue_io_timeout_store>> blk_queue_rq_timeout(q, msecs_to_jiffies(val)); 
+ *   - block/bsg-lib.c|397| <<bsg_setup_queue>> blk_queue_rq_timeout(q, BLK_DEFAULT_SG_TIMEOUT);
+ *   - drivers/block/nbd.c|1365| <<nbd_set_cmd_timeout>> blk_queue_rq_timeout(nbd->disk->queue, timeout * HZ); 
+ *   - drivers/block/nbd.c|1367| <<nbd_set_cmd_timeout>> blk_queue_rq_timeout(nbd->disk->queue, 30 * HZ);
+ *   - drivers/block/skd_main.c|2872| <<skd_cons_disk>> blk_queue_rq_timeout(q, 8 * HZ);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2888| <<srp_slave_configure>> blk_queue_rq_timeout(q, timeout);
+ *   - drivers/mmc/core/queue.c|479| <<mmc_init_queue>> blk_queue_rq_timeout(mq->queue, 60 * HZ);
+ *   - drivers/s390/block/dasd_devmap.c|1349| <<dasd_timeout_store>> blk_queue_rq_timeout(q, device->blk_timeout * HZ);
+ *   - drivers/scsi/3w-9xxx.c|1982| <<twa_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, 60 * HZ);
+ *   - drivers/scsi/3w-sas.c|1535| <<twl_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, 60 * HZ);
+ *   - drivers/scsi/3w-xxxx.c|2230| <<tw_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, 60 * HZ);
+ *   - drivers/scsi/aacraid/linit.c|494| <<aac_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, timeout * HZ);
+ *   - drivers/scsi/hpsa.c|2153| <<hpsa_slave_configure>> blk_queue_rq_timeout(sdev->request_queue,
+ *   - drivers/scsi/hpsa.c|2158| <<hpsa_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, CTLR_TIMEOUT);
+ *   - drivers/scsi/ibmvscsi/ibmvscsi.c|1861| <<ibmvscsi_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, 120 * HZ);
+ *   - drivers/scsi/ipr.c|4963| <<ipr_slave_configure>> blk_queue_rq_timeout(sdev->request_queue,
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|2026| <<megasas_set_static_target_properties>> blk_queue_rq_timeout(sdev->request_queue, scmd_timeout * HZ);
+ *   - drivers/scsi/pmcraid.c|232| <<pmcraid_slave_configure>> blk_queue_rq_timeout(scsi_dev->request_queue,
+ *   - drivers/scsi/scsi_sysfs.c|694| <<sdev_store_timeout>> blk_queue_rq_timeout(sdev->request_queue, timeout * HZ);
+ *   - drivers/scsi/scsi_transport_fc.c|3804| <<fc_bsg_hostadd>> blk_queue_rq_timeout(q, FC_DEFAULT_BSG_TIMEOUT);
+ *   - drivers/scsi/scsi_transport_fc.c|3833| <<fc_bsg_rportadd>> blk_queue_rq_timeout(q, BLK_DEFAULT_SG_TIMEOUT);
+ *   - drivers/scsi/sd.c|3376| <<sd_probe>> blk_queue_rq_timeout(sdp->request_queue, SD_TIMEOUT);
+ *   - drivers/scsi/sd.c|3378| <<sd_probe>> blk_queue_rq_timeout(sdp->request_queue,
+ *   - drivers/scsi/snic/snic_main.c|93| <<snic_slave_configure>> blk_queue_rq_timeout(sdev->request_queue, tmo);
+ *   - drivers/scsi/sr.c|765| <<sr_probe>> blk_queue_rq_timeout(sdev->request_queue, SR_TIMEOUT);
+ *   - drivers/scsi/st.c|2378| <<st_set_options>> blk_queue_rq_timeout(STp->device->request_queue,
+ *   - drivers/scsi/st.c|4360| <<st_probe>> blk_queue_rq_timeout(tpnt->device->request_queue, ST_TIMEOUT);
+ *   - drivers/scsi/stex.c|591| <<stex_slave_config>> blk_queue_rq_timeout(sdev->request_queue, 60 * HZ);
+ *   - drivers/scsi/storvsc_drv.c|1518| <<storvsc_device_configure>> blk_queue_rq_timeout(sdevice->request_queue, (storvsc_timeout * HZ));
+ */
 void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
 {
 	q->rq_timeout = timeout;
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index fee3325edf27..a91b3ff07e34 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -14,6 +14,20 @@
 #include "blk.h"
 #include "blk-cgroup-rwstat.h"
 
+/*
+ * 似乎是cgroup-blkio有多种限流的算法 (block/Kconfig也有一些解释).
+ * - throttle
+ * - iolatency
+ * - iocost
+ * - bfq
+ *
+ * # mkdir /sys/fs/cgroup/blkio/test
+ *
+ * # echo "253:0 1" > /sys/fs/cgroup/blkio/test/blkio.throttle.write_iops_device
+ *
+ * # cgexec -g blkio:test dd if=/dev/zero of=/dev/vda bs=1M count=10 oflag=direct
+ */
+
 /* Max dispatch from a group in 1 round */
 static int throtl_grp_quantum = 8;
 
@@ -323,6 +337,15 @@ static uint64_t tg_bps_limit(struct throtl_grp *tg, int rw)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|575| <<tg_update_has_rules>> tg_iops_limit(tg, rw) != UINT_MAX));
+ *   - block/blk-throttle.c|872| <<throtl_trim_slice>> io_trim = (tg_iops_limit(tg, rw) * tg->td->throtl_slice * nr_slices) /
+ *   - block/blk-throttle.c|916| <<tg_with_in_iops_limit>> tmp = (u64)tg_iops_limit(tg, rw) * jiffy_elapsed_rnd;
+ *   - block/blk-throttle.c|1002| <<tg_may_dispatch>> tg_iops_limit(tg, rw) == UINT_MAX) {
+ *   - block/blk-throttle.c|1393| <<tg_conf_updated>> tg_iops_limit(tg, READ), tg_iops_limit(tg, WRITE));
+ *   - block/blk-throttle.c|2252| <<blk_throtl_bio>> tg->io_disp[rw], tg_iops_limit(tg, rw),
+ */
 static unsigned int tg_iops_limit(struct throtl_grp *tg, int rw)
 {
 	struct blkcg_gq *blkg = tg_to_blkg(tg);
@@ -482,6 +505,39 @@ static void throtl_service_queue_init(struct throtl_service_queue *sq)
 	timer_setup(&sq->pending_timer, throtl_pending_timer_fn, 0);
 }
 
+/*
+ * [0] throtl_pd_alloc
+ * [0] blkg_alloc
+ * [0] blkg_conf_prep
+ * [0] tg_set_conf.constprop.31
+ * [0] cgroup_file_write
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] throtl_pd_alloc
+ * [0] blkcg_activate_policy
+ * [0] blk_throtl_init
+ * [0] blkcg_init_queue
+ * [0] blk_alloc_queue
+ * [0] blk_mq_init_queue_data
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static struct blkg_policy_data *throtl_pd_alloc(gfp_t gfp,
 						struct request_queue *q,
 						struct blkcg *blkcg)
@@ -575,6 +631,18 @@ static void tg_update_has_rules(struct throtl_grp *tg)
 			  tg_iops_limit(tg, rw) != UINT_MAX));
 }
 
+/*
+ * [0] throtl_pd_online
+ * [0] blkg_create
+ * [0] blkg_conf_prep
+ * [0] tg_set_conf.constprop.31
+ * [0] cgroup_file_write
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void throtl_pd_online(struct blkg_policy_data *pd)
 {
 	struct throtl_grp *tg = pd_to_tg(pd);
@@ -1508,6 +1576,9 @@ static int tg_print_rwstat_recursive(struct seq_file *sf, void *v)
 	return 0;
 }
 
+/*
+ * struct blkcg_policy blkcg_policy_throtl.legacy_cftypes = throtl_legacy_files[]
+ */
 static struct cftype throtl_legacy_files[] = {
 	{
 		.name = "throttle.read_bps_device",
@@ -2158,6 +2229,10 @@ static inline void throtl_update_latency_buckets(struct throtl_data *td)
 }
 #endif
 
+/*
+ * called by:
+ *   - block/blk-core.c|1054| <<submit_bio_checks>> if (blk_throtl_bio(bio)) {
+ */
 bool blk_throtl_bio(struct bio *bio)
 {
 	struct request_queue *q = bio->bi_disk->queue;
@@ -2307,6 +2382,10 @@ void blk_throtl_stat_add(struct request *rq, u64 time_ns)
 			     time_ns >> 10);
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1445| <<bio_endio>> blk_throtl_bio_endio(bio);
+ */
 void blk_throtl_bio_endio(struct bio *bio)
 {
 	struct blkcg_gq *blkg;
@@ -2360,6 +2439,10 @@ void blk_throtl_bio_endio(struct bio *bio)
 }
 #endif
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1169| <<blkcg_init_queue>> ret = blk_throtl_init(q);
+ */
 int blk_throtl_init(struct request_queue *q)
 {
 	struct throtl_data *td;
@@ -2413,6 +2496,10 @@ void blk_throtl_exit(struct request_queue *q)
 	kfree(q->td);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|1049| <<blk_register_queue>> blk_throtl_register_queue(q);
+ */
 void blk_throtl_register_queue(struct request_queue *q)
 {
 	struct throtl_data *td;
@@ -2474,6 +2561,13 @@ static int __init throtl_init(void)
 	if (!kthrotld_workqueue)
 		panic("Failed to create kthrotld\n");
 
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	return blkcg_policy_register(&blkcg_policy_throtl);
 }
 
diff --git a/block/bounce.c b/block/bounce.c
index 431be88a0240..88f8e4863b49 100644
--- a/block/bounce.c
+++ b/block/bounce.c
@@ -31,6 +31,11 @@
 static struct bio_set bounce_bio_set, bounce_bio_split;
 static mempool_t page_pool, isa_page_pool;
 
+/*
+ * called by:
+ *   - block/bounce.c|65| <<init_emergency_pool>> init_bounce_bioset();
+ *   - block/bounce.c|122| <<init_emergency_isa_pool>> init_bounce_bioset();
+ */
 static void init_bounce_bioset(void)
 {
 	static bool bounce_bs_setup;
@@ -359,6 +364,11 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 	*bio_orig = bio;
 }
 
+/*
+ * called by:
+ *   - block/blk-map.c|534| <<blk_rq_append_bio>> blk_queue_bounce(rq->q, bio);
+ *   - block/blk-mq.c|2368| <<blk_mq_submit_bio>> blk_queue_bounce(q, &bio);
+ */
 void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 {
 	mempool_t *pool;
diff --git a/block/elevator.c b/block/elevator.c
index 90ed7a28c21d..9b78a5d154ae 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -527,6 +527,12 @@ void elv_unregister_queue(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|6844| <<bfq_init>> ret = elv_register(&iosched_bfq_mq);
+ *   - block/kyber-iosched.c|1037| <<kyber_init>> return elv_register(&kyber_sched);
+ *   - block/mq-deadline.c|804| <<deadline_init>> return elv_register(&mq_deadline);
+ */
 int elv_register(struct elevator_type *e)
 {
 	/* create icq_cache if requested */
diff --git a/drivers/md/dm-linear.c b/drivers/md/dm-linear.c
index e1db43446327..1c820c23fd83 100644
--- a/drivers/md/dm-linear.c
+++ b/drivers/md/dm-linear.c
@@ -26,6 +26,13 @@ struct linear_c {
 /*
  * Construct a linear mapping: <dev_path> <offset>
  */
+/*
+ * called by:
+ *   - drivers/md/dm-crypt.c|2877| <<crypt_ctr_cipher>> ret = cc->iv_gen_ops->ctr(cc, ti, ivopts);
+ *   - drivers/md/dm-exception-store.c|236| <<dm_exception_store_create>> r = type->ctr(tmp_store, (strlen(argv[0]) > 1 ? &argv[0][1] : NULL));
+ *   - drivers/md/dm-log.c|167| <<dm_dirty_log_create>> if (type->ctr(log, ti, argc, argv)) {
+ *   - drivers/md/dm-table.c|763| <<dm_table_add_target>> r = tgt->type->ctr(tgt, argc, argv);
+ */
 static int linear_ctr(struct dm_target *ti, unsigned int argc, char **argv)
 {
 	struct linear_c *lc;
@@ -95,6 +102,10 @@ static void linear_map_bio(struct dm_target *ti, struct bio *bio)
 			linear_map_sector(ti, bio->bi_iter.bi_sector);
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm.c|1291| <<__map_bio>> r = ti->type->map(ti, clone);
+ */
 static int linear_map(struct dm_target *ti, struct bio *bio)
 {
 	linear_map_bio(ti, bio);
diff --git a/drivers/md/dm-table.c b/drivers/md/dm-table.c
index 229f461e7def..a3591f88b9d8 100644
--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@ -683,6 +683,11 @@ static int validate_hardware_logical_block_alignment(struct dm_table *table,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/md/dm-ioctl.c|1305| <<populate_table>> r = dm_table_add_target(table, spec->target_type,
+ *   - drivers/md/dm-ioctl.c|2102| <<dm_early_create>> r = dm_table_add_target(t, spec_array[i]->target_type,
+ */
 int dm_table_add_target(struct dm_table *t, const char *type,
 			sector_t start, sector_t len, char *params)
 {
diff --git a/drivers/md/dm.c b/drivers/md/dm.c
index 6ed05ca65a0f..92644b9997df 100644
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -1269,6 +1269,12 @@ void dm_accept_partial_bio(struct bio *bio, unsigned n_sectors)
 }
 EXPORT_SYMBOL_GPL(dm_accept_partial_bio);
 
+/*
+ * called by:
+ *   - drivers/md/dm-mpath.c|639| <<__multipath_map_bio>> struct pgpath *pgpath = __map_bio(m, bio);
+ *   - drivers/md/dm.c|1411| <<__clone_and_map_simple_bio>> return __map_bio(tio);
+ *   - drivers/md/dm.c|1463| <<__clone_and_map_data_bio>> (void ) __map_bio(tio);
+ */
 static blk_qc_t __map_bio(struct dm_target_io *tio)
 {
 	int r;
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 1f4bdd94407a..ed52dda0c067 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -315,6 +315,11 @@ void tap_del_queues(struct tap_dev *tap)
 }
 EXPORT_SYMBOL_GPL(tap_del_queues);
 
+/*
+ * 在以下使用tap_handle_frame():
+ *   - drivers/net/ipvlan/ipvtap.c|94| <<ipvtap_newlink>> err = netdev_rx_handler_register(dev, tap_handle_frame, &vlantap->tap);
+ *   - drivers/net/macvtap.c|102| <<macvtap_newlink>> err = netdev_rx_handler_register(dev, tap_handle_frame, &vlantap->tap);
+ */
 rx_handler_result_t tap_handle_frame(struct sk_buff **pskb)
 {
 	struct sk_buff *skb = *pskb;
diff --git a/drivers/net/tun.c b/drivers/net/tun.c
index 7959b5c2d11f..10c8399a26c9 100644
--- a/drivers/net/tun.c
+++ b/drivers/net/tun.c
@@ -2659,6 +2659,10 @@ static const struct attribute_group tun_attr_group = {
 	.attrs = tun_dev_attrs
 };
 
+/*
+ * 处理cmd=TUNSETIFF:
+ *   - drivers/net/tun.c|3041| <<__tun_chr_ioctl>> ret = tun_set_iff(net, file, &ifr);
+ */
 static int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)
 {
 	struct tun_struct *tun;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 668685c09e65..a8faee130759 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -1442,6 +1442,17 @@ static void virtnet_poll_cleantx(struct receive_queue *rq)
 		netif_tx_wake_queue(txq);
 }
 
+/*
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ */
 static int virtnet_poll(struct napi_struct *napi, int budget)
 {
 	struct receive_queue *rq =
@@ -1503,6 +1514,43 @@ static int virtnet_open(struct net_device *dev)
 	return 0;
 }
 
+/*
+ * commit b92f1e6751a6aaaf0b1e05128661ce0f23ed3695
+ * Author: Willem de Bruijn <willemb@google.com>
+ * Date:   Mon Apr 24 13:49:27 2017 -0400
+ *
+ * virtio-net: transmit napi
+ *
+ * Convert virtio-net to a standard napi tx completion path. This enables
+ * better TCP pacing using TCP small queues and increases single stream
+ * throughput.
+ *
+ * The virtio-net driver currently cleans tx descriptors on transmission
+ * of new packets in ndo_start_xmit. Latency depends on new traffic, so
+ * is unbounded. To avoid deadlock when a socket reaches its snd limit,
+ * packets are orphaned on tranmission. This breaks socket backpressure,
+ * including TSQ.
+ *
+ * Napi increases the number of interrupts generated compared to the
+ * current model, which keeps interrupts disabled as long as the ring
+ * has enough free descriptors. Keep tx napi optional and disabled for
+ * now. Follow-on patches will reduce the interrupt cost.
+ *
+ * Signed-off-by: Willem de Bruijn <willemb@google.com>
+ * Signed-off-by: Jason Wang <jasowang@redhat.com>
+ * Signed-off-by: David S. Miller <davem@davemloft.net>
+ *
+ *
+ * [0] virtnet_poll_tx
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ */
 static int virtnet_poll_tx(struct napi_struct *napi, int budget)
 {
 	struct send_queue *sq = container_of(napi, struct send_queue, napi);
diff --git a/drivers/net/xen-netback/rx.c b/drivers/net/xen-netback/rx.c
index ac034f69a170..81e222a8dd50 100644
--- a/drivers/net/xen-netback/rx.c
+++ b/drivers/net/xen-netback/rx.c
@@ -571,6 +571,11 @@ static void xenvif_queue_carrier_off(struct xenvif_queue *queue)
 	spin_unlock(&vif->lock);
 }
 
+/*
+ * called by:
+ *   - drivers/net/xen-netback/rx.c|596| <<xenvif_kthread_guest_rx>> xenvif_queue_carrier_on(queue);
+ *   - drivers/net/xen-netback/rx.c|627| <<xenvif_kthread_guest_rx>> xenvif_queue_carrier_on(queue);
+ */
 static void xenvif_queue_carrier_on(struct xenvif_queue *queue)
 {
 	struct xenvif *vif = queue->vif;
diff --git a/drivers/scsi/hosts.c b/drivers/scsi/hosts.c
index 37d1c5565d90..ad5d93f0a2e9 100644
--- a/drivers/scsi/hosts.c
+++ b/drivers/scsi/hosts.c
@@ -205,6 +205,16 @@ EXPORT_SYMBOL(scsi_remove_host);
  * Return value: 
  * 	0 on success / != 0 for error
  **/
+/*
+ * called by:
+ *   - drivers/ata/libata-scsi.c|4270| <<ata_scsi_add_hosts>> rc = scsi_add_host_with_dma(shost, &ap->tdev, ap->host->dev);
+ *   - drivers/firewire/sbp2.c|1150| <<sbp2_probe>> if (scsi_add_host_with_dma(shost, &unit->device,
+ *   - drivers/scsi/bfa/bfad_im.c|571| <<bfad_im_scsi_host_alloc>> error = scsi_add_host_with_dma(im_port->shost, dev, &bfad->pcidev->dev);
+ *   - drivers/scsi/csiostor/csio_init.c|644| <<csio_shost_init>> if (scsi_add_host_with_dma(shost, dev, &hw->pdev->dev))
+ *   - drivers/scsi/lpfc/lpfc_init.c|4456| <<lpfc_create_port>> error = scsi_add_host_with_dma(shost, dev, &phba->pcidev->dev);
+ *   - drivers/scsi/qla2xxx/qla_attr.c|2977| <<qla24xx_vport_create>> if (scsi_add_host_with_dma(vha->host, &fc_vport->dev,
+ *   - include/scsi/scsi_host.h|749| <<scsi_add_host>> return scsi_add_host_with_dma(host, dev, dev);
+ */
 int scsi_add_host_with_dma(struct Scsi_Host *shost, struct device *dev,
 			   struct device *dma_dev)
 {
@@ -368,6 +378,17 @@ static struct device_type scsi_host_type = {
  * Return value:
  * 	Pointer to a new Scsi_Host
  **/
+/*
+ * 部分调用的例子:
+ *   - drivers/scsi/megaraid.c|4228| <<megaraid_probe_one>> host = scsi_host_alloc(&megaraid_template, sizeof(adapter_t));
+ *   - drivers/scsi/megaraid/megaraid_mbox.c|619| <<megaraid_io_attach>> host = scsi_host_alloc(&megaraid_template_g, 8);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|7347| <<megasas_probe_one>> host = scsi_host_alloc(&megasas_template,
+ *   - drivers/scsi/qla2xxx/qla_os.c|4811| <<qla2x00_create_host>> host = scsi_host_alloc(sht, sizeof(scsi_qla_host_t));
+ *   - drivers/scsi/virtio_scsi.c|1119| <<virtscsi_probe>> shost = scsi_host_alloc(&virtscsi_host_template,
+ *   - drivers/scsi/vmw_pvscsi.c|1438| <<pvscsi_probe>> host = scsi_host_alloc(&pvscsi_template, sizeof(struct pvscsi_adapter));
+ *   - drivers/target/loopback/tcm_loop.c|326| <<tcm_loop_driver_probe>> sh = scsi_host_alloc(&tcm_loop_driver_template,
+ *   - include/scsi/libfc.h|865| <<libfc_host_alloc>> shost = scsi_host_alloc(sht, sizeof(*lport) + priv_size);
+ */
 struct Scsi_Host *scsi_host_alloc(struct scsi_host_template *sht, int privsize)
 {
 	struct Scsi_Host *shost;
@@ -593,6 +614,10 @@ void scsi_host_put(struct Scsi_Host *shost)
 }
 EXPORT_SYMBOL(scsi_host_put);
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi.c|763| <<init_scsi>> error = scsi_init_hosts();
+ */
 int scsi_init_hosts(void)
 {
 	return class_register(&shost_class);
diff --git a/drivers/scsi/scsi.c b/drivers/scsi/scsi.c
index 24619c3bebd5..5bcf5bf4fff3 100644
--- a/drivers/scsi/scsi.c
+++ b/drivers/scsi/scsi.c
@@ -81,6 +81,17 @@
  * Note - the initial logging level can be set here to log events at boot time.
  * After the system is up, you may enable logging via the /proc interface.
  */
+/*
+ * 在以下使用scsi_logging_level:
+ *   - drivers/scsi/scsi.c|750| <<global>> module_param(scsi_logging_level, int , S_IRUGO|S_IWUSR);
+ *   - drivers/scsi/scsi.c|751| <<global>> MODULE_PARM_DESC(scsi_logging_level, "a bit mask of logging levels");
+ *   - drivers/scsi/scsi_sysctl.c|17| <<global>> .data = &scsi_logging_level,
+ *   - drivers/scsi/scsi_sysctl.c|18| <<global>> .maxlen = sizeof(scsi_logging_level),
+ *   - drivers/scsi/scsi.c|113| <<scsi_log_send>> if (unlikely(scsi_logging_level)) {
+ *   - drivers/scsi/scsi.c|140| <<scsi_log_completion>> if (unlikely(scsi_logging_level)) {
+ *   - drivers/scsi/scsi_lib.c|780| <<scsi_io_completion_action>> if (unlikely(scsi_logging_level))
+ *   - drivers/scsi/scsi_logging.h|45| <<SCSI_LOG_LEVEL>> ((scsi_logging_level >> (SHIFT)) & ((1 << (BITS)) - 1))
+ */
 unsigned int scsi_logging_level;
 #if defined(CONFIG_SCSI_LOGGING)
 EXPORT_SYMBOL(scsi_logging_level);
@@ -164,6 +175,12 @@ void scsi_log_completion(struct scsi_cmnd *cmd, int disposition)
  *              request, waking processes that are waiting on results,
  *              etc.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|164| <<scmd_eh_abort_handler>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_error.c|2113| <<scsi_eh_flush_done_q>> scsi_finish_command(scmd);
+ *   - drivers/scsi/scsi_lib.c|1472| <<scsi_softirq_done>> scsi_finish_command(cmd);
+ */
 void scsi_finish_command(struct scsi_cmnd *cmd)
 {
 	struct scsi_device *sdev = cmd->device;
@@ -292,6 +309,12 @@ EXPORT_SYMBOL(scsi_track_queue_full);
  *
  * Returns size of the vpd page on success or a negative error number.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi.c|350| <<scsi_get_vpd_page>> result = scsi_vpd_inquiry(sdev, buf, 0, buf_len);
+ *   - drivers/scsi/scsi.c|369| <<scsi_get_vpd_page>> result = scsi_vpd_inquiry(sdev, buf, page, buf_len);
+ *   - drivers/scsi/scsi.c|397| <<scsi_get_vpd_buf>> result = scsi_vpd_inquiry(sdev, vpd_buf->data, page, vpd_len);
+ */
 static int scsi_vpd_inquiry(struct scsi_device *sdev, unsigned char *buffer,
 							u8 page, unsigned len)
 {
@@ -437,6 +460,11 @@ static void scsi_update_vpd_page(struct scsi_device *sdev, u8 page,
  * structure. This information can be used to identify the device
  * uniquely.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|979| <<scsi_add_lun>> scsi_attach_vpd(sdev);
+ *   - drivers/scsi/scsi_scan.c|1514| <<scsi_rescan_device>> scsi_attach_vpd(sdev);
+ */
 void scsi_attach_vpd(struct scsi_device *sdev)
 {
 	int i;
@@ -642,11 +670,20 @@ EXPORT_SYMBOL(__starget_for_each_device);
  * they need to access the device list in irq context.  Otherwise you
  * really want to use scsi_device_lookup_by_target instead.
  **/
+/*
+ * called by:
+ *   - drivers/scsi/esp_scsi.c|1202| <<esp_reconnect>> dev = __scsi_device_lookup_by_target(tp->starget, lun);
+ *   - drivers/scsi/scsi.c|678| <<scsi_device_lookup_by_target>> sdev = __scsi_device_lookup_by_target(starget, lun);
+ */
 struct scsi_device *__scsi_device_lookup_by_target(struct scsi_target *starget,
 						   u64 lun)
 {
 	struct scsi_device *sdev;
 
+	/*
+	 * struct scsi_target:
+	 *  -> struct list_head devices;
+	 */
 	list_for_each_entry(sdev, &starget->devices, same_target_siblings) {
 		if (sdev->sdev_state == SDEV_DEL)
 			continue;
@@ -667,6 +704,14 @@ EXPORT_SYMBOL(__scsi_device_lookup_by_target);
  * @starget.  The returned scsi_device has an additional reference that
  * needs to be released with scsi_device_put once you're done with it.
  **/
+/*
+ * called by:
+ *   - drivers/message/fusion/mptspi.c|908| <<mptspi_write_spi_device_pg1>> sdev = scsi_device_lookup_by_target(starget, i);
+ *   - drivers/scsi/aic7xxx/aic79xx_proc.c|182| <<ahd_dump_target_state>> dev = scsi_device_lookup_by_target(starget, lun);
+ *   - drivers/scsi/aic7xxx/aic7xxx_proc.c|162| <<ahc_dump_target_state>> sdev = scsi_device_lookup_by_target(starget, lun);
+ *   - drivers/scsi/scsi_scan.c|1058| <<scsi_probe_and_add_lun>> sdev = scsi_device_lookup_by_target(starget, lun);
+ *   - drivers/scsi/scsi_scan.c|1322| <<scsi_report_lun_scan>> if (!(sdev = scsi_device_lookup_by_target(starget, 0))) {
+ */
 struct scsi_device *scsi_device_lookup_by_target(struct scsi_target *starget,
 						 u64 lun)
 {
@@ -700,6 +745,11 @@ EXPORT_SYMBOL(scsi_device_lookup_by_target);
  * they need to access the device list in irq context.  Otherwise you
  * really want to use scsi_device_lookup instead.
  **/
+/*
+ * called by:
+ *   - drivers/scsi/53c700.c|1087| <<process_script_interrupt>> SDp = __scsi_device_lookup(host, 0, reselection_id, lun);
+ *   - drivers/scsi/scsi.c|738| <<scsi_device_lookup>> sdev = __scsi_device_lookup(shost, channel, id, lun);
+ */
 struct scsi_device *__scsi_device_lookup(struct Scsi_Host *shost,
 		uint channel, uint id, u64 lun)
 {
@@ -708,6 +758,12 @@ struct scsi_device *__scsi_device_lookup(struct Scsi_Host *shost,
 	list_for_each_entry(sdev, &shost->__devices, siblings) {
 		if (sdev->sdev_state == SDEV_DEL)
 			continue;
+		/*
+		 * struct scsi_device *sdev:
+		 *  -> unsigned int channel;
+		 *  -> unsigned int id;
+		 *  -> u64 lun;
+		 */
 		if (sdev->channel == channel && sdev->id == id &&
 				sdev->lun ==lun)
 			return sdev;
@@ -728,6 +784,16 @@ EXPORT_SYMBOL(__scsi_device_lookup);
  * for a given host.  The returned scsi_device has an additional reference that
  * needs to be released with scsi_device_put once you're done with it.
  **/
+/*
+ * 调用的例子:
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|8705| <<megasas_add_remove_devices>> sdev1 = scsi_device_lookup(host, channel, id, 0);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|8718| <<megasas_add_remove_devices>> sdev1 = scsi_device_lookup(host, i, j, 0);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|8737| <<megasas_add_remove_devices>> sdev1 = scsi_device_lookup(host,
+ *   - drivers/scsi/scsi_proc.c|280| <<scsi_remove_single_device>> sdev = scsi_device_lookup(shost, channel, id, lun);
+ *   - drivers/scsi/virtio_scsi.c|412| <<virtscsi_handle_transport_reset>> sdev = scsi_device_lookup(shost, 0, target, lun);
+ *   - drivers/scsi/virtio_scsi.c|440| <<virtscsi_handle_param_change>> sdev = scsi_device_lookup(shost, 0, target, lun);
+ *   - drivers/target/loopback/tcm_loop.c|649| <<tcm_loop_port_unlink>> sd = scsi_device_lookup(tl_hba->sh, 0, tl_tpg->tl_tpgt,
+ */
 struct scsi_device *scsi_device_lookup(struct Scsi_Host *shost,
 		uint channel, uint id, u64 lun)
 {
diff --git a/drivers/scsi/scsi_common.c b/drivers/scsi/scsi_common.c
index 90349498f686..cf1a9ac85552 100644
--- a/drivers/scsi/scsi_common.c
+++ b/drivers/scsi/scsi_common.c
@@ -135,6 +135,25 @@ EXPORT_SYMBOL(int_to_scsilun);
  * Return value:
  *	true if valid sense data information found, else false;
  */
+/*
+ * called by:
+ *   - drivers/cdrom/cdrom.c|2222| <<cdrom_read_cdda_bpc>> scsi_normalize_sense(req->sense, req->sense_len,
+ *   - drivers/ide/ide-cd.c|475| <<ide_cd_queue_pc>> scsi_normalize_sense(scsi_req(rq)->sense,
+ *   - drivers/scsi/cxlflash/superpipe.c|1782| <<process_sense>> rc = scsi_normalize_sense((const u8 *)&verify->sense_data,
+ *   - drivers/scsi/hpsa.c|366| <<decode_sense_data>> rc = scsi_normalize_sense(sense_data, sense_data_len, &sshdr);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|3064| <<ibmvscsis_fast_fail>> if (scsi_normalize_sense(se_cmd->sense_buffer,
+ *   - drivers/scsi/myrb.c|316| <<myrb_get_event>> scsi_normalize_sense(ev_buf->sense, 32, &sshdr);
+ *   - drivers/scsi/myrs.c|826| <<myrs_log_event>> if (!scsi_normalize_sense(ev->sense_data, 40, &sshdr)) {
+ *   - drivers/scsi/scsi_error.c|2427| <<scsi_command_normalize_sense>> return scsi_normalize_sense(cmd->sense_buffer,
+ *   - drivers/scsi/scsi_lib.c|287| <<__scsi_execute>> scsi_normalize_sense(rq->sense, rq->sense_len, sshdr);
+ *   - drivers/scsi/scsi_logging.c|348| <<scsi_log_print_sense>> if (scsi_normalize_sense(sense_buffer, sense_len, &sshdr))
+ *   - drivers/scsi/sg.c|1388| <<sg_rq_end_io>> && scsi_normalize_sense(sense, SCSI_SENSE_BUFFERSIZE, &sshdr)
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|2760| <<pqi_process_raid_io_error>> scsi_normalize_sense(error_info->data,
+ *   - drivers/scsi/st.c|331| <<st_analyze_sense>> s->have_sense = scsi_normalize_sense(SRpnt->sense,
+ *   - drivers/scsi/storvsc_drv.c|1087| <<storvsc_command_completion>> if (scsi_normalize_sense(scmnd->sense_buffer,
+ *   - drivers/usb/storage/transport.c|787| <<usb_stor_invoke_transport>> scsi_normalize_sense(srb->sense_buffer, SCSI_SENSE_BUFFERSIZE,
+ *   - drivers/xen/xen-scsiback.c|341| <<scsiback_send_response>> scsi_normalize_sense(sense_buffer, VSCSIIF_SENSE_BUFFERSIZE,
+ */
 bool scsi_normalize_sense(const u8 *sense_buffer, int sb_len,
 			  struct scsi_sense_hdr *sshdr)
 {
@@ -257,6 +276,14 @@ EXPORT_SYMBOL(scsi_build_sense_buffer);
  * Return value:
  *	0 on success or -EINVAL for invalid sense buffer length
  **/
+/*
+ * called by:
+ *   - drivers/ata/libata-scsi.c|217| <<ata_scsi_set_sense_information>> scsi_set_sense_information(cmd->sense_buffer,
+ *   - drivers/ata/libata-scsi.c|988| <<ata_gen_ata_sense>> scsi_set_sense_information(sb, SCSI_SENSE_BUFFERSIZE, block);
+ *   - drivers/scsi/libiscsi.c|820| <<iscsi_scsi_cmd_rsp>> scsi_set_sense_information(sc->sense_buffer,
+ *   - drivers/scsi/scsi_debug.c|7136| <<resp_not_ready>> scsi_set_sense_information(scp->sense_buffer, SCSI_SENSE_BUFFERSIZE,
+ *   - drivers/target/target_core_transport.c|3297| <<translate_sense_reason>> WARN_ON_ONCE(scsi_set_sense_information(buffer,
+ */
 int scsi_set_sense_information(u8 *buf, int buf_len, u64 info)
 {
 	if ((buf[0] & 0x7f) == 0x72) {
@@ -307,6 +334,11 @@ EXPORT_SYMBOL(scsi_set_sense_information);
  * Return value:
  *	0 on success or -EINVAL for invalid sense buffer length
  */
+/*
+ * called by:
+ *   - drivers/ata/libata-scsi.c|226| <<ata_scsi_set_invalid_field>> scsi_set_sense_field_pointer(cmd->sense_buffer, SCSI_SENSE_BUFFERSIZE,
+ *   - drivers/ata/libata-scsi.c|235| <<ata_scsi_set_invalid_parameter>> scsi_set_sense_field_pointer(cmd->sense_buffer, SCSI_SENSE_BUFFERSIZE,
+ */
 int scsi_set_sense_field_pointer(u8 *buf, int buf_len, u16 fp, u8 bp, bool cd)
 {
 	u8 *ucp, len;
diff --git a/drivers/scsi/scsi_debug.c b/drivers/scsi/scsi_debug.c
index 1ad7260d4758..7b3ffacad347 100644
--- a/drivers/scsi/scsi_debug.c
+++ b/drivers/scsi/scsi_debug.c
@@ -7481,3 +7481,20 @@ static struct bus_type pseudo_lld_bus = {
 	.remove = sdebug_driver_remove,
 	.drv_groups = sdebug_drv_groups,
 };
+
+/*
+ * # modprobe scsi_debug
+ *
+ * [524414.648482] scsi host3: scsi_eh_3: sleeping
+ * [524414.648632] scsi host3: scsi_debug: version 0190 [20200710]
+ *                   dev_size_mb=8, opts=0x0, submit_queues=1, statistics=0
+ * [524414.654163] scsi 3:0:0:0: Direct-Access     Linux    scsi_debug       0190 PQ: 0 ANSI: 7
+ * [524414.656539] sd 3:0:0:0: Power-on or device reset occurred
+ * [524414.656880] sd 3:0:0:0: Attached scsi generic sg1 type 0
+ * [524414.660083] sd 3:0:0:0: [sdb] 16384 512-byte logical blocks: (8.39 MB/8.00 MiB)
+ * [524414.663224] sd 3:0:0:0: [sdb] Write Protect is off
+ * [524414.664464] sd 3:0:0:0: [sdb] Mode Sense: 73 00 10 08
+ * [524414.666517] sd 3:0:0:0: [sdb] Write cache: enabled, read cache: enabled, supports DPO and FUA
+ * [524414.671891] sd 3:0:0:0: [sdb] Optimal transfer size 524288 bytes
+ * [524414.694620] sd 3:0:0:0: [sdb] Attached SCSI disk
+ */
diff --git a/drivers/scsi/scsi_debugfs.c b/drivers/scsi/scsi_debugfs.c
index c19ea7ab54cb..98edbbd68208 100644
--- a/drivers/scsi/scsi_debugfs.c
+++ b/drivers/scsi/scsi_debugfs.c
@@ -31,6 +31,13 @@ static int scsi_flags_show(struct seq_file *m, const unsigned long flags,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|349| <<__blk_mq_debugfs_rq_show>> mq_ops->show_rq(m, rq);
+ *
+ * struct blk_mq_ops scsi_mq_ops_no_commit.show_rq = scsi_show_rq()
+ * struct blk_mq_ops scsi_mq_ops.show_rq = scsi_show_rq()
+ */
 void scsi_show_rq(struct seq_file *m, struct request *rq)
 {
 	struct scsi_cmnd *cmd = container_of(scsi_req(rq), typeof(*cmd), req);
diff --git a/drivers/scsi/scsi_devinfo.c b/drivers/scsi/scsi_devinfo.c
index ba84244c1b4f..79e7872a5d35 100644
--- a/drivers/scsi/scsi_devinfo.c
+++ b/drivers/scsi/scsi_devinfo.c
@@ -841,6 +841,10 @@ EXPORT_SYMBOL(scsi_dev_info_remove_list);
  *	Add command line entries from scsi_dev_flags, then add
  *	scsi_static_device_list entries to the scsi device info list.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi.c|760| <<init_scsi>> error = scsi_init_devinfo();
+ */
 int __init scsi_init_devinfo(void)
 {
 #ifdef CONFIG_SCSI_PROC_FS
@@ -867,6 +871,9 @@ int __init scsi_init_devinfo(void)
 	}
 
 #ifdef CONFIG_SCSI_PROC_FS
+	/*
+	 * /proc/scsi/device_info
+	 */
 	p = proc_create("scsi/device_info", 0, NULL, &scsi_devinfo_proc_ops);
 	if (!p) {
 		error = -ENOMEM;
diff --git a/drivers/scsi/scsi_dh.c b/drivers/scsi/scsi_dh.c
index 6f41e4b5a2b8..1dd846a4eb0c 100644
--- a/drivers/scsi/scsi_dh.c
+++ b/drivers/scsi/scsi_dh.c
@@ -14,6 +14,11 @@
 #include "scsi_priv.h"
 
 static DEFINE_SPINLOCK(list_lock);
+/*
+ * 在以下使用scsi_dh_list:
+ *   - drivers/scsi/scsi_dh.c|93| <<__scsi_dh_lookup>> list_for_each_entry(tmp, &scsi_dh_list, list) {
+ *   - drivers/scsi/scsi_dh.c|208| <<scsi_register_device_handler>> list_add(&scsi_dh->list, &scsi_dh_list);
+ */
 static LIST_HEAD(scsi_dh_list);
 
 struct scsi_dh_blist {
@@ -85,6 +90,14 @@ scsi_dh_find_driver(struct scsi_device *sdev)
 }
 
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_dh.c|110| <<scsi_dh_lookup>> dh = __scsi_dh_lookup(name);
+ *   - drivers/scsi/scsi_dh.c|113| <<scsi_dh_lookup>> dh = __scsi_dh_lookup(name);
+ *   - drivers/scsi/scsi_dh.c|177| <<scsi_dh_add_device>> devinfo = __scsi_dh_lookup(drv);
+ *   - drivers/scsi/scsi_dh.c|201| <<scsi_register_device_handler>> if (__scsi_dh_lookup(scsi_dh->name))
+ *   - drivers/scsi/scsi_dh.c|226| <<scsi_unregister_device_handler>> if (!__scsi_dh_lookup(scsi_dh->name))
+ */
 static struct scsi_device_handler *__scsi_dh_lookup(const char *name)
 {
 	struct scsi_device_handler *tmp, *found = NULL;
@@ -100,6 +113,10 @@ static struct scsi_device_handler *__scsi_dh_lookup(const char *name)
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_dh.c|330| <<scsi_dh_attach>> scsi_dh = scsi_dh_lookup(name);
+ */
 static struct scsi_device_handler *scsi_dh_lookup(const char *name)
 {
 	struct scsi_device_handler *dh;
@@ -196,6 +213,13 @@ void scsi_dh_release_device(struct scsi_device *sdev)
  *
  * Returns 0 on success, -EBUSY if handler already registered.
  */
+/*
+ * called by:
+ *   - drivers/scsi/device_handler/scsi_dh_alua.c|1182| <<alua_init>> r = scsi_register_device_handler(&alua_dh);
+ *   - drivers/scsi/device_handler/scsi_dh_emc.c|528| <<clariion_init>> r = scsi_register_device_handler(&clariion_dh);
+ *   - drivers/scsi/device_handler/scsi_dh_hp_sw.c|248| <<hp_sw_init>> return scsi_register_device_handler(&hp_sw_dh);
+ *   - drivers/scsi/device_handler/scsi_dh_rdac.c|803| <<rdac_init>> r = scsi_register_device_handler(&rdac_dh);
+ */
 int scsi_register_device_handler(struct scsi_device_handler *scsi_dh)
 {
 	if (__scsi_dh_lookup(scsi_dh->name))
@@ -208,6 +232,12 @@ int scsi_register_device_handler(struct scsi_device_handler *scsi_dh)
 	list_add(&scsi_dh->list, &scsi_dh_list);
 	spin_unlock(&list_lock);
 
+	/*
+	 * [    1.284787] rdac: device handler registered
+	 * [    1.284844] hp_sw: device handler registered
+	 * [    1.284845] emc: device handler registered
+	 * [    1.284972] alua: device handler registered
+	 */
 	printk(KERN_INFO "%s: device handler registered\n", scsi_dh->name);
 
 	return SCSI_DH_OK;
@@ -317,6 +347,11 @@ EXPORT_SYMBOL_GPL(scsi_dh_set_params);
  *      the handler should be attached to
  * @name - name of the handler to attach
  */
+/*
+ * called by:
+ *   - drivers/md/dm-mpath.c|903| <<setup_scsi_dh>> r = scsi_dh_attach(q, m->hw_handler_name);
+ *   - drivers/scsi/scsi_sysfs.c|1088| <<sdev_store_dh_state>> err = scsi_dh_attach(sdev->request_queue, buf);
+ */
 int scsi_dh_attach(struct request_queue *q, const char *name)
 {
 	struct scsi_device *sdev;
diff --git a/drivers/scsi/scsi_error.c b/drivers/scsi/scsi_error.c
index 7d3571a2bd89..5f0463283406 100644
--- a/drivers/scsi/scsi_error.c
+++ b/drivers/scsi/scsi_error.c
@@ -784,6 +784,12 @@ static void scsi_eh_done(struct scsi_cmnd *scmd)
  * scsi_try_host_reset - ask host adapter to reset itself
  * @scmd:	SCSI cmd to send host reset.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|936| <<scsi_abort_eh_cmnd>> scsi_try_host_reset(scmd);
+ *   - drivers/scsi/scsi_error.c|1681| <<scsi_eh_host_reset>> rtn = scsi_try_host_reset(scmd);
+ *   - drivers/scsi/scsi_error.c|2391| <<scsi_ioctl_reset>> rtn = scsi_try_host_reset(scmd);
+ */
 static int scsi_try_host_reset(struct scsi_cmnd *scmd)
 {
 	unsigned long flags;
diff --git a/drivers/scsi/scsi_ioctl.c b/drivers/scsi/scsi_ioctl.c
index 14872c9dc78c..7d850d4b8f8d 100644
--- a/drivers/scsi/scsi_ioctl.c
+++ b/drivers/scsi/scsi_ioctl.c
@@ -270,6 +270,17 @@ static int scsi_ioctl_common(struct scsi_device *sdev, int cmd, void __user *arg
  * does not take a major/minor number as the dev field.  Rather, it takes
  * a pointer to a &struct scsi_device.
  */
+/*
+ * called by:
+ *   - drivers/scsi/ch.c|860| <<ch_ioctl>> return scsi_ioctl(ch->device, cmd, argp);
+ *   - drivers/scsi/sd.c|1528| <<sd_ioctl_common>> error = scsi_ioctl(sdp, cmd, p);
+ *   - drivers/scsi/sd.c|1723| <<sd_ioctl>> return scsi_ioctl(scsi_disk(bdev->bd_disk)->device, cmd, p);
+ *   - drivers/scsi/sg.c|1167| <<sg_ioctl>> return scsi_ioctl(sdp->device, cmd_in, p);
+ *   - drivers/scsi/sr.c|581| <<sr_block_ioctl>> ret = scsi_ioctl(sdev, cmd, argp);
+ *   - drivers/scsi/sr.c|589| <<sr_block_ioctl>> ret = scsi_ioctl(sdev, cmd, argp);
+ *   - drivers/scsi/st.c|3827| <<st_ioctl_common>> retval = scsi_ioctl(STp->device, cmd_in, p);
+ *   - drivers/scsi/st.c|3868| <<st_ioctl>> return scsi_ioctl(STp->device, cmd_in, p);
+ */
 int scsi_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)
 {
 	int ret = scsi_ioctl_common(sdev, cmd, arg);
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 7affaaf8b98e..286abd702338 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -237,6 +237,10 @@ void scsi_queue_insert(struct scsi_cmnd *cmd, int reason)
  * Returns the scsi_cmnd result field if a command was executed, or a negative
  * Linux error code if we didn't get that far.
  */
+/*
+ * called by:
+ *   - include/scsi/scsi_device.h|451| <<scsi_execute>> __scsi_execute(sdev, cmd, data_direction, buffer, bufflen, \
+ */
 int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
 		 int data_direction, void *buffer, unsigned bufflen,
 		 unsigned char *sense, struct scsi_sense_hdr *sshdr,
@@ -556,6 +560,12 @@ static void scsi_run_queue_async(struct scsi_device *sdev)
 }
 
 /* Returns false when no more bytes to process, true if there are more */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|800| <<scsi_io_completion_action>> if (!scsi_end_request(req, blk_stat, blk_rq_err_bytes(req)))
+ *   - drivers/scsi/scsi_lib.c|942| <<scsi_io_completion>> if (likely(!scsi_end_request(req, blk_stat, good_bytes)))
+ *   - drivers/scsi/scsi_lib.c|948| <<scsi_io_completion>> if (scsi_end_request(req, blk_stat, blk_rq_bytes(req)))
+ */
 static bool scsi_end_request(struct request *req, blk_status_t error,
 		unsigned int bytes)
 {
@@ -905,6 +915,10 @@ static int scsi_io_completion_nz_result(struct scsi_cmnd *cmd, int result,
  *   c) We can call scsi_end_request() with blk_stat other than
  *	BLK_STS_OK, to fail the remainder of the request.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi.c|231| <<scsi_finish_command>> scsi_io_completion(cmd, good_bytes);
+ */
 void scsi_io_completion(struct scsi_cmnd *cmd, unsigned int good_bytes)
 {
 	int result = cmd->result;
@@ -1142,6 +1156,10 @@ void scsi_init_command(struct scsi_device *dev, struct scsi_cmnd *cmd)
 
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1211| <<scsi_setup_cmnd>> ret = scsi_setup_scsi_cmnd(sdev, req);
+ */
 static blk_status_t scsi_setup_scsi_cmnd(struct scsi_device *sdev,
 		struct request *req)
 {
@@ -1174,6 +1192,10 @@ static blk_status_t scsi_setup_scsi_cmnd(struct scsi_device *sdev,
  * Setup a normal block command.  These are simple request from filesystems
  * that still need to be translated to SCSI CDBs from the ULD.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1221| <<scsi_setup_cmnd>> ret = scsi_setup_fs_cmnd(sdev, req);
+ */
 static blk_status_t scsi_setup_fs_cmnd(struct scsi_device *sdev,
 		struct request *req)
 {
@@ -1185,11 +1207,26 @@ static blk_status_t scsi_setup_fs_cmnd(struct scsi_device *sdev,
 			return ret;
 	}
 
+	/*
+	 * struct scsi_cmnd *cmd:
+	 * -> struct scsi_request req;
+	 *    -> unsigned char   __cmd[BLK_MAX_CDB];
+	 *    -> unsigned char   *cmd;
+	 *    -> unsigned short  cmd_len;
+	 * -> unsigned char *cmnd;
+	 */
 	cmd->cmnd = scsi_req(req)->cmd = scsi_req(req)->__cmd;
 	memset(cmd->cmnd, 0, BLK_MAX_CDB);
+	/*
+	 * 比如sd_init_command(), 其中read/write会调用sd_setup_read_write_cmnd()
+	 */
 	return scsi_cmd_to_driver(cmd)->init_command(cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1607| <<scsi_mq_prep_fn>> return scsi_setup_cmnd(sdev, req);
+ */
 static blk_status_t scsi_setup_cmnd(struct scsi_device *sdev,
 		struct request *req)
 {
@@ -1299,6 +1336,10 @@ static inline int scsi_dev_queue_ready(struct request_queue *q,
  * scsi_target_queue_ready: checks if there we can send commands to target
  * @sdev: scsi device on starget to check.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1640| <<scsi_queue_rq>> if (!scsi_target_queue_ready(shost, sdev))
+ */
 static inline int scsi_target_queue_ready(struct Scsi_Host *shost,
 					   struct scsi_device *sdev)
 {
@@ -1354,6 +1395,10 @@ static inline int scsi_target_queue_ready(struct Scsi_Host *shost,
  * return 0. We must end up running the queue again whenever 0 is
  * returned, else IO can hang.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1642| <<scsi_queue_rq>> if (!scsi_host_queue_ready(q, shost, sdev, cmd))
+ */
 static inline int scsi_host_queue_ready(struct request_queue *q,
 				   struct Scsi_Host *shost,
 				   struct scsi_device *sdev,
@@ -1482,6 +1527,10 @@ static void scsi_softirq_done(struct request *rq)
  * Return: nonzero return request was rejected and device's queue needs to be
  * plugged.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1664| <<scsi_queue_rq>> reason = scsi_dispatch_cmd(cmd);
+ */
 static int scsi_dispatch_cmd(struct scsi_cmnd *cmd)
 {
 	struct Scsi_Host *host = cmd->device->host;
@@ -1563,6 +1612,10 @@ static unsigned int scsi_mq_inline_sgl_size(struct Scsi_Host *shost)
 		sizeof(struct scatterlist);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|1666| <<scsi_queue_rq>> ret = scsi_mq_prep_fn(req);
+ */
 static blk_status_t scsi_mq_prep_fn(struct request *req)
 {
 	struct scsi_cmnd *cmd = blk_mq_rq_to_pdu(req);
@@ -1615,6 +1668,10 @@ static bool scsi_mq_get_budget(struct request_queue *q)
 	return scsi_dev_queue_ready(q, sdev);
 }
 
+/*
+ * struct blk_mq_ops scsi_mq_ops_no_commit.queue_rq = scsi_queue_rq()
+ * struct blk_mq_ops scsi_mq_ops.queue_rq = scsi_queue_rq()
+ */
 static blk_status_t scsi_queue_rq(struct blk_mq_hw_ctx *hctx,
 			 const struct blk_mq_queue_data *bd)
 {
@@ -1643,6 +1700,9 @@ static blk_status_t scsi_queue_rq(struct blk_mq_hw_ctx *hctx,
 		goto out_dec_target_busy;
 
 	if (!(req->rq_flags & RQF_DONTPREP)) {
+		/*
+		 * 只在此处调用scsi_mq_prep_fn()
+		 */
 		ret = scsi_mq_prep_fn(req);
 		if (ret != BLK_STS_OK)
 			goto out_dec_host_busy;
@@ -1836,6 +1896,11 @@ static void scsi_commit_rqs(struct blk_mq_hw_ctx *hctx)
 	shost->hostt->commit_rqs(shost, hctx->queue_num);
 }
 
+/*
+ * 在以下使用scsi_mq_ops:
+ *   - drivers/scsi/scsi_lib.c|1899| <<scsi_mq_setup_tags>> tag_set->ops = &scsi_mq_ops;
+ *   - drivers/scsi/scsi_lib.c|1931| <<scsi_device_from_queue>> q->mq_ops == &scsi_mq_ops)
+ */
 static const struct blk_mq_ops scsi_mq_ops = {
 	.get_budget	= scsi_mq_get_budget,
 	.put_budget	= scsi_mq_put_budget,
@@ -1854,8 +1919,15 @@ static const struct blk_mq_ops scsi_mq_ops = {
 	.map_queues	= scsi_map_queues,
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|313| <<scsi_alloc_sdev>> sdev->request_queue = scsi_mq_alloc_queue(sdev);
+ */
 struct request_queue *scsi_mq_alloc_queue(struct scsi_device *sdev)
 {
+	/*
+	 * 这里才是request_queue和tagset的关系
+	 */
 	sdev->request_queue = blk_mq_init_queue(&sdev->host->tag_set);
 	if (IS_ERR(sdev->request_queue))
 		return NULL;
@@ -1866,6 +1938,10 @@ struct request_queue *scsi_mq_alloc_queue(struct scsi_device *sdev)
 	return sdev->request_queue;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/hosts.c|227| <<scsi_add_host_with_dma>> error = scsi_mq_setup_tags(shost);
+ */
 int scsi_mq_setup_tags(struct Scsi_Host *shost)
 {
 	unsigned int cmd_size, sgl_size;
@@ -2659,6 +2735,11 @@ static int scsi_internal_device_block(struct scsi_device *sdev)
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|2733| <<scsi_internal_device_unblock_nowait>> scsi_start_queue(sdev);
+ *   - drivers/scsi/scsi_sysfs.c|1414| <<__scsi_remove_device>> scsi_start_queue(sdev);
+ */
 void scsi_start_queue(struct scsi_device *sdev)
 {
 	struct request_queue *q = sdev->request_queue;
@@ -2681,6 +2762,12 @@ void scsi_start_queue(struct scsi_device *sdev)
  * the offline states (which must be a legal transition) allowing the midlayer
  * to goose the queue for this device.
  */
+/*
+ * called by:
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|3388| <<_scsih_internal_device_unblock>> r = scsi_internal_device_unblock_nowait(sdev, SDEV_RUNNING);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|3407| <<_scsih_internal_device_unblock>> r = scsi_internal_device_unblock_nowait(sdev, SDEV_RUNNING);
+ *   - drivers/scsi/scsi_lib.c|2759| <<scsi_internal_device_unblock>> ret = scsi_internal_device_unblock_nowait(sdev, new_state);
+ */
 int scsi_internal_device_unblock_nowait(struct scsi_device *sdev,
 					enum scsi_device_state new_state)
 {
@@ -2734,6 +2821,11 @@ EXPORT_SYMBOL_GPL(scsi_internal_device_unblock_nowait);
  * the offline states (which must be a legal transition) allowing the midlayer
  * to goose the queue for this device.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|2799| <<device_unblock>> scsi_internal_device_unblock(sdev, *(enum scsi_device_state *)data);
+ *   - drivers/scsi/scsi_lib.c|2862| <<scsi_host_unblock>> ret = scsi_internal_device_unblock(sdev, new_state);
+ */
 static int scsi_internal_device_unblock(struct scsi_device *sdev,
 					enum scsi_device_state new_state)
 {
@@ -2803,6 +2895,13 @@ scsi_target_unblock(struct device *dev, enum scsi_device_state new_state)
 }
 EXPORT_SYMBOL_GPL(scsi_target_unblock);
 
+/*
+ * called by:
+ *   - drivers/scsi/aacraid/commsup.c|1648| <<aac_reset_adapter>> scsi_host_block(host);
+ *   - drivers/scsi/aacraid/linit.c|1920| <<aac_suspend>> scsi_host_block(shost);
+ *   - drivers/scsi/aacraid/linit.c|1972| <<aac_shutdown>> scsi_host_block(shost);
+ *   - drivers/scsi/aacraid/linit.c|2018| <<aac_pci_error_detected>> scsi_host_block(shost);
+ */
 int
 scsi_host_block(struct Scsi_Host *shost)
 {
@@ -2941,6 +3040,11 @@ EXPORT_SYMBOL(sdev_enable_disk_events);
  * If the identifier is longer than the supplied buffer the actual
  * identifier length is returned and the buffer is not zero-padded.
  */
+/*
+ * called by:
+ *   - drivers/scsi/device_handler/scsi_dh_alua.c|219| <<alua_alloc_pg>> pg->device_id_len = scsi_vpd_lun_id(sdev, pg->device_id_str,
+ *   - drivers/scsi/scsi_sysfs.c|1015| <<sdev_show_wwid>> count = scsi_vpd_lun_id(sdev, buf, PAGE_SIZE);
+ */
 int scsi_vpd_lun_id(struct scsi_device *sdev, char *id, size_t id_len)
 {
 	u8 cur_id_type = 0xff;
diff --git a/drivers/scsi/scsi_lib_dma.c b/drivers/scsi/scsi_lib_dma.c
index 5723915275ad..a759e3ec381c 100644
--- a/drivers/scsi/scsi_lib_dma.c
+++ b/drivers/scsi/scsi_lib_dma.c
@@ -20,6 +20,16 @@
  * Returns the number of sg lists actually used, zero if the sg lists
  * is NULL, or -ENOMEM if the mapping failed.
  */
+/*
+ * 调用的例子:
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|3077| <<lpfc_scsi_prep_dma_buf_s4>> nseg = scsi_dma_map(scsi_cmnd);
+ *   - drivers/scsi/megaraid.c|1731| <<mega_build_sglist>> sgcnt = scsi_dma_map(cmd);
+ *   - drivers/scsi/megaraid/megaraid_mbox.c|1347| <<megaraid_mbox_mksgl>> sgcnt = scsi_dma_map(scp);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1246| <<megasas_make_sgl32>> sge_count = scsi_dma_map(scp);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1275| <<megasas_make_sgl64>> sge_count = scsi_dma_map(scp);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|1304| <<megasas_make_sgl_skinny>> sge_count = scsi_dma_map(scp);
+ *   - drivers/scsi/megaraid/megaraid_sas_fusion.c|2315| <<megasas_make_sgl>> sge_count = scsi_dma_map(scp);
+ */
 int scsi_dma_map(struct scsi_cmnd *cmd)
 {
 	int nseg = 0;
diff --git a/drivers/scsi/scsi_logging.c b/drivers/scsi/scsi_logging.c
index 8ea44c6595ef..359090156c3a 100644
--- a/drivers/scsi/scsi_logging.c
+++ b/drivers/scsi/scsi_logging.c
@@ -378,6 +378,11 @@ void scsi_print_sense(const struct scsi_cmnd *cmd)
 }
 EXPORT_SYMBOL(scsi_print_sense);
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi.c|145| <<scsi_log_completion>> scsi_print_result(cmd, "Done", disposition);
+ *   - drivers/scsi/scsi_lib.c|790| <<scsi_io_completion_action>> scsi_print_result(cmd, NULL, FAILED);
+ */
 void scsi_print_result(const struct scsi_cmnd *cmd, const char *msg,
 		       int disposition)
 {
diff --git a/drivers/scsi/scsi_netlink.c b/drivers/scsi/scsi_netlink.c
index d7f76fd84256..fc0b94f48be6 100644
--- a/drivers/scsi/scsi_netlink.c
+++ b/drivers/scsi/scsi_netlink.c
@@ -28,6 +28,10 @@ EXPORT_SYMBOL_GPL(scsi_nl_sock);
  *
  *
  **/
+/*
+ * 在以下使用scsi_nl_rcv_msg():
+ *   - drivers/scsi/scsi_netlink.c|115| <<scsi_netlink_init>> .input = scsi_nl_rcv_msg,
+ */
 static void
 scsi_nl_rcv_msg(struct sk_buff *skb)
 {
diff --git a/drivers/scsi/scsi_pm.c b/drivers/scsi/scsi_pm.c
index 3717eea37ecb..6a18b7906809 100644
--- a/drivers/scsi/scsi_pm.c
+++ b/drivers/scsi/scsi_pm.c
@@ -324,6 +324,17 @@ void scsi_autopm_put_target(struct scsi_target *starget)
 	pm_runtime_put_sync(&starget->dev);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/hosts.c|178| <<scsi_remove_host>> scsi_autopm_get_host(shost);
+ *   - drivers/scsi/scsi_error.c|2222| <<scsi_error_handler>> if (!shost->eh_noresume && scsi_autopm_get_host(shost) != 0) {
+ *   - drivers/scsi/scsi_error.c|2351| <<scsi_ioctl_reset>> if (scsi_autopm_get_host(shost) < 0)
+ *   - drivers/scsi/scsi_priv.h|169| <<scsi_autopm_get_host>> static inline int scsi_autopm_get_host(struct Scsi_Host *h) { return 0; }
+ *   - drivers/scsi/scsi_scan.c|1527| <<__scsi_add_device>> if (scsi_host_scan_allowed(shost) && scsi_autopm_get_host(shost) == 0) {
+ *   - drivers/scsi/scsi_scan.c|1666| <<scsi_scan_target>> if (scsi_host_scan_allowed(shost) && scsi_autopm_get_host(shost) == 0) {
+ *   - drivers/scsi/scsi_scan.c|1729| <<scsi_scan_host_selected>> if (scsi_host_scan_allowed(shost) && scsi_autopm_get_host(shost) == 0) {
+ *   - drivers/scsi/scsi_scan.c|1987| <<scsi_scan_host>> if (scsi_autopm_get_host(shost) < 0)
+ */
 int scsi_autopm_get_host(struct Scsi_Host *shost)
 {
 	int	err;
@@ -341,6 +352,9 @@ void scsi_autopm_put_host(struct Scsi_Host *shost)
 	pm_runtime_put_sync(&shost->shost_gendev);
 }
 
+/*
+ * struct bus_type scsi_bus_type.pm = scsi_bus_pm_ops
+ */
 const struct dev_pm_ops scsi_bus_pm_ops = {
 	.prepare =		scsi_bus_prepare,
 	.suspend =		scsi_bus_suspend,
diff --git a/drivers/scsi/scsi_scan.c b/drivers/scsi/scsi_scan.c
index f2437a7570ce..23299c2af748 100644
--- a/drivers/scsi/scsi_scan.c
+++ b/drivers/scsi/scsi_scan.c
@@ -85,6 +85,11 @@ static const char *scsi_null_device_strs = "nullnullnullnull";
 
 #define MAX_SCSI_LUNS	512
 
+/*
+ * 在以下使用max_scsi_luns:
+ *   - drivers/scsi/scsi_scan.c|90| <<global>> module_param_named(max_luns, max_scsi_luns, ullong, S_IRUGO|S_IWUSR);
+ *   - drivers/scsi/scsi_scan.c|1207| <<scsi_sequential_lun_scan>> max_dev_lun = min(max_scsi_luns, shost->max_lun);
+ */
 static u64 max_scsi_luns = MAX_SCSI_LUNS;
 
 module_param_named(max_luns, max_scsi_luns, ullong, S_IRUGO|S_IWUSR);
@@ -97,6 +102,17 @@ MODULE_PARM_DESC(max_luns,
 #define SCSI_SCAN_TYPE_DEFAULT "sync"
 #endif
 
+/*
+ * 在以下使用scsi_scan_type:
+ *   - drivers/scsi/scsi_scan.c|107| <<global>> module_param_string(scan, scsi_scan_type, sizeof(scsi_scan_type),
+ *   - drivers/scsi/scsi_pm.c|166| <<scsi_bus_resume_common>> if (strncmp(scsi_scan_type, "async", 5) != 0)
+ *   - drivers/scsi/scsi_scan.c|1494| <<__scsi_add_device>> if (strncmp(scsi_scan_type, "none", 4) == 0)
+ *   - drivers/scsi/scsi_scan.c|1634| <<scsi_scan_target>> if (strncmp(scsi_scan_type, "none", 4) == 0)
+ *   - drivers/scsi/scsi_scan.c|1638| <<scsi_scan_target>> strncmp(scsi_scan_type, "manual", 6) == 0)
+ *   - drivers/scsi/scsi_scan.c|1752| <<scsi_prep_async_scan>> if (strncmp(scsi_scan_type, "sync", 4) == 0)
+ *   - drivers/scsi/scsi_scan.c|1881| <<scsi_scan_host>> if (strncmp(scsi_scan_type, "none", 4) == 0 ||
+ *   - drivers/scsi/scsi_scan.c|1882| <<scsi_scan_host>> strncmp(scsi_scan_type, "manual", 6) == 0)
+ */
 char scsi_scan_type[7] = SCSI_SCAN_TYPE_DEFAULT;
 
 module_param_string(scan, scsi_scan_type, sizeof(scsi_scan_type),
@@ -114,11 +130,33 @@ MODULE_PARM_DESC(inq_timeout,
 
 /* This lock protects only this list */
 static DEFINE_SPINLOCK(async_scan_lock);
+/*
+ * 在以下使用scanning_hosts:
+ *   - drivers/scsi/scsi_scan.c|138| <<scsi_complete_async_scans>> if (list_empty(&scanning_hosts))
+ *   - drivers/scsi/scsi_scan.c|154| <<scsi_complete_async_scans>> if (list_empty(&scanning_hosts))
+ *   - drivers/scsi/scsi_scan.c|156| <<scsi_complete_async_scans>> list_add_tail(&data->list, &scanning_hosts);
+ *   - drivers/scsi/scsi_scan.c|164| <<scsi_complete_async_scans>> if (!list_empty(&scanning_hosts)) {
+ *   - drivers/scsi/scsi_scan.c|165| <<scsi_complete_async_scans>> struct async_scan_data *next = list_entry(scanning_hosts.next,
+ *   - drivers/scsi/scsi_scan.c|1743| <<scsi_prep_async_scan>> if (list_empty(&scanning_hosts))
+ *   - drivers/scsi/scsi_scan.c|1745| <<scsi_prep_async_scan>> list_add_tail(&data->list, &scanning_hosts);
+ *   - drivers/scsi/scsi_scan.c|1794| <<scsi_finish_async_scan>> if (!list_empty(&scanning_hosts)) {
+ *   - drivers/scsi/scsi_scan.c|1795| <<scsi_finish_async_scan>> struct async_scan_data *next = list_entry(scanning_hosts.next,
+ */
 static LIST_HEAD(scanning_hosts);
 
 struct async_scan_data {
 	struct list_head list;
 	struct Scsi_Host *shost;
+	/*
+	 * 在以下使用async_scan_data->prev_finished:
+	 *   - drivers/scsi/scsi_scan.c|185| <<scsi_complete_async_scans>> init_completion(&data->prev_finished);
+	 *   - drivers/scsi/scsi_scan.c|195| <<scsi_complete_async_scans>> wait_for_completion(&data->prev_finished);
+	 *   - drivers/scsi/scsi_scan.c|202| <<scsi_complete_async_scans>> complete(&next->prev_finished);
+	 *   - drivers/scsi/scsi_scan.c|1803| <<scsi_prep_async_scan>> init_completion(&data->prev_finished);
+	 *   - drivers/scsi/scsi_scan.c|1836| <<scsi_prep_async_scan>> complete(&data->prev_finished);
+	 *   - drivers/scsi/scsi_scan.c|1874| <<scsi_finish_async_scan>> wait_for_completion(&data->prev_finished);
+	 *   - drivers/scsi/scsi_scan.c|1889| <<scsi_finish_async_scan>> complete(&next->prev_finished);
+	 */
 	struct completion prev_finished;
 };
 
@@ -130,6 +168,13 @@ struct async_scan_data {
  * started scanning after this function was called may or may not have
  * finished.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_pm.c|180| <<scsi_bus_prepare>> scsi_complete_async_scans();
+ *   - drivers/scsi/scsi_scan.c|1476| <<__scsi_add_device>> scsi_complete_async_scans();
+ *   - drivers/scsi/scsi_scan.c|1615| <<scsi_scan_target>> scsi_complete_async_scans();
+ *   - drivers/scsi/scsi_scan.c|1672| <<scsi_scan_host_selected>> scsi_complete_async_scans();
+ */
 int scsi_complete_async_scans(void)
 {
 	struct async_scan_data *data;
@@ -212,6 +257,12 @@ static void scsi_unlock_floptical(struct scsi_device *sdev,
  * Return value:
  *     scsi_Device pointer, or NULL on failure.
  **/
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1132| <<scsi_probe_and_add_lun>> sdev = scsi_alloc_sdev(starget, lun, hostdata);
+ *   - drivers/scsi/scsi_scan.c|1378| <<scsi_report_lun_scan>> sdev = scsi_alloc_sdev(starget, 0, NULL);
+ *   - drivers/scsi/scsi_scan.c|2086| <<scsi_get_host_dev>> sdev = scsi_alloc_sdev(starget, 0, NULL);
+ */
 static struct scsi_device *scsi_alloc_sdev(struct scsi_target *starget,
 					   u64 lun, void *hostdata)
 {
@@ -265,6 +316,9 @@ static struct scsi_device *scsi_alloc_sdev(struct scsi_target *starget,
 	 */
 	sdev->borken = 1;
 
+	/*
+	 * 只在此处调用scsi_mq_alloc_queue()
+	 */
 	sdev->request_queue = scsi_mq_alloc_queue(sdev);
 	if (!sdev->request_queue) {
 		/* release fn is set up in scsi_sysfs_device_initialise, so
@@ -406,6 +460,12 @@ static void scsi_target_reap_ref_put(struct scsi_target *starget)
  * The target is returned with an incremented reference, so the caller
  * is responsible for both reaping and doing a last put
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1518| <<__scsi_add_device>> starget = scsi_alloc_target(parent, channel, id);
+ *   - drivers/scsi/scsi_scan.c|1593| <<__scsi_scan_target>> starget = scsi_alloc_target(parent, channel, id);
+ *   - drivers/scsi/scsi_scan.c|2053| <<scsi_get_host_dev>> starget = scsi_alloc_target(&shost->shost_gendev, 0, shost->this_id);
+ */
 static struct scsi_target *scsi_alloc_target(struct device *parent,
 					     int channel, uint id)
 {
@@ -556,6 +616,28 @@ EXPORT_SYMBOL(scsi_sanitize_inquiry_string);
  *     INQUIRY data is in @inq_result; the scsi_level and INQUIRY length
  *     are copied to the scsi_device any flags value is stored in *@bflags.
  **/
+/*
+ * [0] scsi_probe_lun
+ * [0] scsi_probe_and_add_lun
+ * [0] __scsi_scan_target
+ * [0] scsi_scan_channel
+ * [0] scsi_scan_host_selected
+ * [0] scsi_scan_host
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static int scsi_probe_lun(struct scsi_device *sdev, unsigned char *inq_result,
 			  int result_len, blist_flags_t *bflags)
 {
@@ -583,6 +665,9 @@ static int scsi_probe_lun(struct scsi_device *sdev, unsigned char *inq_result,
 	for (count = 0; count < 3; ++count) {
 		int resid;
 
+		/*
+		 * unsigned char scsi_cmd[MAX_COMMAND_SIZE];
+		 */
 		memset(scsi_cmd, 0, 6);
 		scsi_cmd[0] = INQUIRY;
 		scsi_cmd[4] = (unsigned char) try_inquiry_len;
@@ -760,6 +845,10 @@ static int scsi_probe_lun(struct scsi_device *sdev, unsigned char *inq_result,
  *     SCSI_SCAN_NO_RESPONSE: could not allocate or setup a scsi_device
  *     SCSI_SCAN_LUN_PRESENT: a new scsi_device was allocated and initialized
  **/
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1156| <<scsi_probe_and_add_lun>> res = scsi_add_lun(sdev, result, &bflags, shost->async_scan);
+ */
 static int scsi_add_lun(struct scsi_device *sdev, unsigned char *inq_result,
 		blist_flags_t *bflags, int async)
 {
@@ -1039,6 +1128,14 @@ static unsigned char *scsi_inq_str(unsigned char *buf, unsigned char *inq,
  *         attached at the LUN
  *   - SCSI_SCAN_LUN_PRESENT: a new scsi_device was allocated and initialized
  **/
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1317| <<scsi_sequential_lun_scan>> if ((scsi_probe_and_add_lun(starget, lun, NULL, NULL, rescan,
+ *   - drivers/scsi/scsi_scan.c|1487| <<scsi_report_lun_scan>> res = scsi_probe_and_add_lun(starget,
+ *   - drivers/scsi/scsi_scan.c|1534| <<__scsi_add_device>> scsi_probe_and_add_lun(starget, lun, NULL, &sdev, 1, hostdata);
+ *   - drivers/scsi/scsi_scan.c|1614| <<__scsi_scan_target>> scsi_probe_and_add_lun(starget, lun, NULL, NULL, rescan, NULL);
+ *   - drivers/scsi/scsi_scan.c|1622| <<__scsi_scan_target>> res = scsi_probe_and_add_lun(starget, 0, &bflags, NULL, rescan, NULL);
+ */
 static int scsi_probe_and_add_lun(struct scsi_target *starget,
 				  u64 lun, blist_flags_t *bflagsp,
 				  struct scsi_device **sdevp,
@@ -1527,6 +1624,12 @@ void scsi_rescan_device(struct device *dev)
 }
 EXPORT_SYMBOL(scsi_rescan_device);
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1667| <<scsi_scan_target>> __scsi_scan_target(parent, channel, id, lun, rescan);
+ *   - drivers/scsi/scsi_scan.c|1698| <<scsi_scan_channel>> __scsi_scan_target(&shost->shost_gendev, channel,
+ *   - drivers/scsi/scsi_scan.c|1702| <<scsi_scan_channel>> __scsi_scan_target(&shost->shost_gendev, channel,
+ */
 static void __scsi_scan_target(struct device *parent, unsigned int channel,
 		unsigned int id, u64 lun, enum scsi_scan_mode rescan)
 {
@@ -1598,6 +1701,18 @@ static void __scsi_scan_target(struct device *parent, unsigned int channel,
  *     First try a REPORT LUN scan, if that does not scan the target, do a
  *     sequential scan of LUNs on the target id.
  **/
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|3144| <<srp_add_target>> scsi_scan_target(&target->scsi_host->shost_gendev,
+ *   - drivers/message/fusion/mptspi.c|1147| <<mpt_work_wrapper>> scsi_scan_target(&ioc->sh->shost_gendev, 1, disk, 0, SCSI_SCAN_RESCAN);
+ *   - drivers/s390/scsi/zfcp_unit.c|30| <<zfcp_unit_scsi_scan>> scsi_scan_target(&rport->dev, 0, rport->scsi_target_id, lun,
+ *   - drivers/scsi/scsi_transport_fc.c|2103| <<fc_user_scan_tgt>> scsi_scan_target(&rport->dev, channel, id, lun,
+ *   - drivers/scsi/scsi_transport_fc.c|3248| <<fc_scsi_scan_rport>> scsi_scan_target(&rport->dev, rport->channel,
+ *   - drivers/scsi/scsi_transport_iscsi.c|1834| <<iscsi_user_scan_session>> scsi_scan_target(&session->dev, 0, id,
+ *   - drivers/scsi/scsi_transport_sas.c|1544| <<sas_rphy_add>> scsi_scan_target(&rphy->dev, 0, rphy->scsi_target_id, lun,
+ *   - drivers/scsi/scsi_transport_sas.c|1670| <<sas_user_scan>> scsi_scan_target(&rphy->dev, 0, rphy->scsi_target_id,
+ *   - drivers/scsi/snic/snic_disc.c|171| <<snic_scsi_scan_tgt>> scsi_scan_target(&tgt->dev,
+ */
 void scsi_scan_target(struct device *parent, unsigned int channel,
 		      unsigned int id, u64 lun, enum scsi_scan_mode rescan)
 {
@@ -1622,6 +1737,11 @@ void scsi_scan_target(struct device *parent, unsigned int channel,
 }
 EXPORT_SYMBOL(scsi_scan_target);
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1733| <<scsi_scan_host_selected>> scsi_scan_channel(shost, channel, id, lun,
+ *   - drivers/scsi/scsi_scan.c|1736| <<scsi_scan_host_selected>> scsi_scan_channel(shost, channel, id, lun, rescan);
+ */
 static void scsi_scan_channel(struct Scsi_Host *shost, unsigned int channel,
 			      unsigned int id, u64 lun,
 			      enum scsi_scan_mode rescan)
@@ -1654,6 +1774,12 @@ static void scsi_scan_channel(struct Scsi_Host *shost, unsigned int channel,
 				id, lun, rescan);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_proc.c|255| <<scsi_add_single_device>> error = scsi_scan_host_selected(shost, channel, id, lun,
+ *   - drivers/scsi/scsi_scan.c|1913| <<do_scsi_scan_host>> scsi_scan_host_selected(shost, SCAN_WILD_CARD, SCAN_WILD_CARD,
+ *   - drivers/scsi/scsi_sysfs.c|150| <<scsi_scan>> res = scsi_scan_host_selected(shost, channel, id, lun,
+ */
 int scsi_scan_host_selected(struct Scsi_Host *shost, unsigned int channel,
 			    unsigned int id, u64 lun,
 			    enum scsi_scan_mode rescan)
@@ -1686,6 +1812,10 @@ int scsi_scan_host_selected(struct Scsi_Host *shost, unsigned int channel,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1784| <<scsi_finish_async_scan>> scsi_sysfs_add_devices(shost);
+ */
 static void scsi_sysfs_add_devices(struct Scsi_Host *shost)
 {
 	struct scsi_device *sdev;
@@ -1712,14 +1842,40 @@ static void scsi_sysfs_add_devices(struct Scsi_Host *shost)
  * that other asynchronous scans started after this one won't affect the
  * ordering of the discovered devices.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1887| <<scsi_scan_host>> data = scsi_prep_async_scan(shost);
+ */
 static struct async_scan_data *scsi_prep_async_scan(struct Scsi_Host *shost)
 {
 	struct async_scan_data *data;
 	unsigned long flags;
 
+	/*
+	 * 在以下使用scsi_scan_type:
+	 *   - drivers/scsi/scsi_scan.c|107| <<global>> module_param_string(scan, scsi_scan_type, sizeof(scsi_scan_type),
+	 *   - drivers/scsi/scsi_pm.c|166| <<scsi_bus_resume_common>> if (strncmp(scsi_scan_type, "async", 5) != 0)
+	 *   - drivers/scsi/scsi_scan.c|1494| <<__scsi_add_device>> if (strncmp(scsi_scan_type, "none", 4) == 0)
+	 *   - drivers/scsi/scsi_scan.c|1634| <<scsi_scan_target>> if (strncmp(scsi_scan_type, "none", 4) == 0)
+	 *   - drivers/scsi/scsi_scan.c|1638| <<scsi_scan_target>> strncmp(scsi_scan_type, "manual", 6) == 0)
+	 *   - drivers/scsi/scsi_scan.c|1752| <<scsi_prep_async_scan>> if (strncmp(scsi_scan_type, "sync", 4) == 0)
+	 *   - drivers/scsi/scsi_scan.c|1881| <<scsi_scan_host>> if (strncmp(scsi_scan_type, "none", 4) == 0 ||
+	 *   - drivers/scsi/scsi_scan.c|1882| <<scsi_scan_host>> strncmp(scsi_scan_type, "manual", 6) == 0)
+	 */
 	if (strncmp(scsi_scan_type, "sync", 4) == 0)
 		return NULL;
 
+	/*
+	 * 在以下使用Scsi_Host->async_scan:
+	 *   - drivers/scsi/scsi_scan.c|1184| <<scsi_probe_and_add_lun>> res = scsi_add_lun(sdev, result, &bflags, shost->async_scan);
+	 *   - drivers/scsi/scsi_scan.c|1503| <<__scsi_add_device>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1642| <<scsi_scan_target>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1699| <<scsi_scan_host_selected>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1755| <<scsi_prep_async_scan>> if (shost->async_scan) {
+	 *   - drivers/scsi/scsi_scan.c|1770| <<scsi_prep_async_scan>> shost->async_scan = 1;
+	 *   - drivers/scsi/scsi_scan.c|1807| <<scsi_finish_async_scan>> if (!shost->async_scan) {
+	 *   - drivers/scsi/scsi_scan.c|1819| <<scsi_finish_async_scan>> shost->async_scan = 0;
+	 */
 	if (shost->async_scan) {
 		shost_printk(KERN_DEBUG, shost, "%s called twice\n", __func__);
 		return NULL;
@@ -1735,11 +1891,34 @@ static struct async_scan_data *scsi_prep_async_scan(struct Scsi_Host *shost)
 
 	mutex_lock(&shost->scan_mutex);
 	spin_lock_irqsave(shost->host_lock, flags);
+	/*
+	 * 在以下使用Scsi_Host->async_scan:
+	 *   - drivers/scsi/scsi_scan.c|1184| <<scsi_probe_and_add_lun>> res = scsi_add_lun(sdev, result, &bflags, shost->async_scan);
+	 *   - drivers/scsi/scsi_scan.c|1503| <<__scsi_add_device>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1642| <<scsi_scan_target>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1699| <<scsi_scan_host_selected>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1755| <<scsi_prep_async_scan>> if (shost->async_scan) {
+	 *   - drivers/scsi/scsi_scan.c|1770| <<scsi_prep_async_scan>> shost->async_scan = 1;
+	 *   - drivers/scsi/scsi_scan.c|1807| <<scsi_finish_async_scan>> if (!shost->async_scan) {
+	 *   - drivers/scsi/scsi_scan.c|1819| <<scsi_finish_async_scan>> shost->async_scan = 0;
+	 */
 	shost->async_scan = 1;
 	spin_unlock_irqrestore(shost->host_lock, flags);
 	mutex_unlock(&shost->scan_mutex);
 
 	spin_lock(&async_scan_lock);
+	/*
+	 * 在以下使用scanning_hosts:
+	 *   - drivers/scsi/scsi_scan.c|138| <<scsi_complete_async_scans>> if (list_empty(&scanning_hosts))
+	 *   - drivers/scsi/scsi_scan.c|154| <<scsi_complete_async_scans>> if (list_empty(&scanning_hosts))
+	 *   - drivers/scsi/scsi_scan.c|156| <<scsi_complete_async_scans>> list_add_tail(&data->list, &scanning_hosts);
+	 *   - drivers/scsi/scsi_scan.c|164| <<scsi_complete_async_scans>> if (!list_empty(&scanning_hosts)) {
+	 *   - drivers/scsi/scsi_scan.c|165| <<scsi_complete_async_scans>> struct async_scan_data *next = list_entry(scanning_hosts.next,
+	 *   - drivers/scsi/scsi_scan.c|1743| <<scsi_prep_async_scan>> if (list_empty(&scanning_hosts))
+	 *   - drivers/scsi/scsi_scan.c|1745| <<scsi_prep_async_scan>> list_add_tail(&data->list, &scanning_hosts);
+	 *   - drivers/scsi/scsi_scan.c|1794| <<scsi_finish_async_scan>> if (!list_empty(&scanning_hosts)) {
+	 *   - drivers/scsi/scsi_scan.c|1795| <<scsi_finish_async_scan>> struct async_scan_data *next = list_entry(scanning_hosts.next,
+	 */
 	if (list_empty(&scanning_hosts))
 		complete(&data->prev_finished);
 	list_add_tail(&data->list, &scanning_hosts);
@@ -1760,6 +1939,10 @@ static struct async_scan_data *scsi_prep_async_scan(struct Scsi_Host *shost)
  * This function announces all the devices it has found to the rest
  * of the system.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1928| <<do_scan_async>> scsi_finish_async_scan(data);
+ */
 static void scsi_finish_async_scan(struct async_scan_data *data)
 {
 	struct Scsi_Host *shost;
@@ -1794,6 +1977,16 @@ static void scsi_finish_async_scan(struct async_scan_data *data)
 	if (!list_empty(&scanning_hosts)) {
 		struct async_scan_data *next = list_entry(scanning_hosts.next,
 				struct async_scan_data, list);
+		/*
+		 * 在以下使用async_scan_data->prev_finished:
+		 *   - drivers/scsi/scsi_scan.c|185| <<scsi_complete_async_scans>> init_completion(&data->prev_finished);
+		 *   - drivers/scsi/scsi_scan.c|195| <<scsi_complete_async_scans>> wait_for_completion(&data->prev_finished);
+		 *   - drivers/scsi/scsi_scan.c|202| <<scsi_complete_async_scans>> complete(&next->prev_finished);
+		 *   - drivers/scsi/scsi_scan.c|1803| <<scsi_prep_async_scan>> init_completion(&data->prev_finished);
+		 *   - drivers/scsi/scsi_scan.c|1836| <<scsi_prep_async_scan>> complete(&data->prev_finished);
+		 *   - drivers/scsi/scsi_scan.c|1874| <<scsi_finish_async_scan>> wait_for_completion(&data->prev_finished);
+		 *   - drivers/scsi/scsi_scan.c|1889| <<scsi_finish_async_scan>> complete(&next->prev_finished);
+		 */
 		complete(&next->prev_finished);
 	}
 	spin_unlock(&async_scan_lock);
@@ -1803,8 +1996,17 @@ static void scsi_finish_async_scan(struct async_scan_data *data)
 	kfree(data);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1918| <<do_scan_async>> do_scsi_scan_host(shost);
+ *   - drivers/scsi/scsi_scan.c|1949| <<scsi_scan_host>> do_scsi_scan_host(shost);
+ */
 static void do_scsi_scan_host(struct Scsi_Host *shost)
 {
+	/*
+	 * 有的支持, 有的不支持.
+	 * virtio-scsi不支持
+	 */
 	if (shost->hostt->scan_finished) {
 		unsigned long start = jiffies;
 		if (shost->hostt->scan_start)
@@ -1818,6 +2020,10 @@ static void do_scsi_scan_host(struct Scsi_Host *shost)
 	}
 }
 
+/*
+ * 在以下使用do_scan_async():
+ *   - drivers/scsi/scsi_scan.c|1957| <<scsi_scan_host>> async_schedule(do_scan_async, data);
+ */
 static void do_scan_async(void *_data, async_cookie_t c)
 {
 	struct async_scan_data *data = _data;
@@ -1831,6 +2037,17 @@ static void do_scan_async(void *_data, async_cookie_t c)
  * scsi_scan_host - scan the given adapter
  * @shost:	adapter to scan
  **/
+/*
+ * 部分调用的例子:
+ *   - drivers/scsi/megaraid/megaraid_mbox.c|653| <<megaraid_io_attach>> scsi_scan_host(host);
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|7423| <<megasas_probe_one>> scsi_scan_host(host);
+ *   - drivers/scsi/mpt3sas/mpt3sas_scsih.c|10817| <<_scsih_probe>> scsi_scan_host(shost);
+ *   - drivers/scsi/qla2xxx/qla_os.c|3431| <<qla2x00_probe_one>> scsi_scan_host(host);
+ *   - drivers/scsi/scsi_debug.c|7440| <<sdebug_driver_probe>> scsi_scan_host(hpnt);
+ *   - drivers/scsi/virtio_scsi.c|513| <<virtscsi_handle_event>> scsi_scan_host(virtio_scsi_host(vscsi->vdev));
+ *   - drivers/scsi/virtio_scsi.c|1181| <<virtscsi_probe>> scsi_scan_host(shost);
+ *   - drivers/scsi/vmw_pvscsi.c|1543| <<pvscsi_probe>> scsi_scan_host(host);
+ */
 void scsi_scan_host(struct Scsi_Host *shost)
 {
 	struct async_scan_data *data;
@@ -1891,6 +2108,11 @@ void scsi_forget_host(struct Scsi_Host *shost)
  *	drivers (including generics), which is probably not
  *	optimal.  We can add hooks later to attach.
  */
+/*
+ * called by:
+ *   - drivers/scsi/gdth.c|378| <<gdth_execute>> struct scsi_device *sdev = scsi_get_host_dev(shost);
+ *   - drivers/scsi/gdth.c|3433| <<gdth_open>> ha->sdev = scsi_get_host_dev(ha->shost);
+ */
 struct scsi_device *scsi_get_host_dev(struct Scsi_Host *shost)
 {
 	struct scsi_device *sdev = NULL;
diff --git a/drivers/scsi/scsi_sysfs.c b/drivers/scsi/scsi_sysfs.c
index 163dbcb741c1..a85da4f1a9fb 100644
--- a/drivers/scsi/scsi_sysfs.c
+++ b/drivers/scsi/scsi_sysfs.c
@@ -129,6 +129,10 @@ static int check_set(unsigned long long *val, char *src)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_sysfs.c|190| <<store_scan>> res = scsi_scan(shost, buf);
+ */
 static int scsi_scan(struct Scsi_Host *shost, const char *str)
 {
 	char s1[15], s2[15], s3[17], junk;
@@ -505,6 +509,9 @@ static struct class sdev_class = {
 };
 
 /* all probing is done in the individual ->probe routines */
+/*
+ * struct bus_type scsi_bus_type.match = scsi_bus_match()
+ */
 static int scsi_bus_match(struct device *dev, struct device_driver *gendrv)
 {
 	struct scsi_device *sdp;
@@ -518,6 +525,9 @@ static int scsi_bus_match(struct device *dev, struct device_driver *gendrv)
 	return (sdp->inq_periph_qual == SCSI_INQ_PQ_CON)? 1: 0;
 }
 
+/*
+ * struct bus_type scsi_bus_type.uevent = scsi_bus_uevent()
+ */
 static int scsi_bus_uevent(struct device *dev, struct kobj_uevent_env *env)
 {
 	struct scsi_device *sdev;
@@ -1313,6 +1323,11 @@ static int scsi_target_add(struct scsi_target *starget)
  * Return value:
  * 	0 on Success / non-zero on Failure
  **/
+/*
+ * called by:
+ *   - drivers/scsi/scsi_scan.c|1044| <<scsi_add_lun>> if (!async && scsi_sysfs_add_sdev(sdev) != 0)
+ *   - drivers/scsi/scsi_scan.c|1788| <<scsi_sysfs_add_devices>> scsi_sysfs_add_sdev(sdev) != 0)
+ */
 int scsi_sysfs_add_sdev(struct scsi_device *sdev)
 {
 	int error, i;
diff --git a/drivers/scsi/scsi_trace.c b/drivers/scsi/scsi_trace.c
index 41a950075913..9a31e3fcc0bb 100644
--- a/drivers/scsi/scsi_trace.c
+++ b/drivers/scsi/scsi_trace.c
@@ -90,6 +90,10 @@ scsi_trace_rw16(struct trace_seq *p, unsigned char *cdb, int len)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_trace.c|336| <<scsi_trace_varlen>> return scsi_trace_rw32(p, cdb, len);
+ */
 static const char *
 scsi_trace_rw32(struct trace_seq *p, unsigned char *cdb, int len)
 {
@@ -325,6 +329,10 @@ scsi_trace_zbc_out(struct trace_seq *p, unsigned char *cdb, int len)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_trace.c|379| <<scsi_trace_parse_cdb>> return scsi_trace_varlen(p, cdb, len);
+ */
 static const char *
 scsi_trace_varlen(struct trace_seq *p, unsigned char *cdb, int len)
 {
@@ -350,6 +358,12 @@ scsi_trace_misc(struct trace_seq *p, unsigned char *cdb, int len)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/trace/events/scsi.h|198| <<__parse_cdb>> #define __parse_cdb(cdb, len) scsi_trace_parse_cdb(p, cdb, len)
+ *   - tools/lib/traceevent/plugins/plugin_scsi.c|379| <<scsi_trace_parse_cdb>> scsi_trace_parse_cdb(struct trace_seq *p, unsigned char *cdb, int len)
+ *   - tools/lib/traceevent/plugins/plugin_scsi.c|413| <<process_scsi_trace_parse_cdb>> scsi_trace_parse_cdb(s, (unsigned char *) (unsigned long ) args[1], args[2]);
+ */
 const char *
 scsi_trace_parse_cdb(struct trace_seq *p, unsigned char *cdb, int len)
 {
diff --git a/drivers/scsi/scsicam.c b/drivers/scsi/scsicam.c
index 682cf08ab041..491514a46be5 100644
--- a/drivers/scsi/scsicam.c
+++ b/drivers/scsi/scsicam.c
@@ -216,6 +216,11 @@ static int setsize(unsigned long capacity, unsigned int *cyls, unsigned int *hds
  *
  * Returns : -1 on failure, 0 on success.
  */
+/*
+ * called by:
+ *   - drivers/scsi/aha152x.c|1238| <<aha152x_biosparam>> if (scsicam_bios_param(bdev, capacity, info) < 0 ||
+ *   - drivers/scsi/sd.c|1469| <<sd_getgeo>> scsicam_bios_param(bdev, capacity, diskinfo);
+ */
 int scsicam_bios_param(struct block_device *bdev, sector_t capacity, int *ip)
 {
 	u64 capacity64 = capacity;	/* Suppress gcc warning */
diff --git a/drivers/scsi/sd.c b/drivers/scsi/sd.c
index 16503e22691e..4ff4e9c4a848 100644
--- a/drivers/scsi/sd.c
+++ b/drivers/scsi/sd.c
@@ -577,6 +577,12 @@ static const struct dev_pm_ops sd_pm_ops = {
 	.runtime_resume		= sd_resume,
 };
 
+/*
+ * 在以下使用sd_template:
+ *   - drivers/scsi/sd.c|3368| <<sd_probe>> sdkp->driver = &sd_template;
+ *   - drivers/scsi/sd.c|3712| <<init_sd>> err = scsi_register_driver(&sd_template.gendrv);
+ *   - drivers/scsi/sd.c|3746| <<exit_sd>> scsi_unregister_driver(&sd_template.gendrv);
+ */
 static struct scsi_driver sd_template = {
 	.gendrv = {
 		.name		= "sd",
@@ -1831,6 +1837,10 @@ static const struct pr_ops sd_pr_ops = {
 	.pr_clear	= sd_pr_clear,
 };
 
+/*
+ * 在以下使用sd_fops:
+ *   - rivers/scsi/sd.c|3403| <<sd_probe>> gd->fops = &sd_fops;
+ */
 static const struct block_device_operations sd_fops = {
 	.owner			= THIS_MODULE,
 	.open			= sd_open,
@@ -3320,9 +3330,16 @@ static int sd_format_disk_name(char *prefix, int index, char *buf, int buflen)
  *	Assume sd_probe is not re-entrant (for time being)
  *	Also think about sd_probe() and sd_remove() running coincidentally.
  **/
+/*
+ * struct scsi_driver sd_template.gendrv.probe = sd_probe()
+ */
 static int sd_probe(struct device *dev)
 {
 	struct scsi_device *sdp = to_scsi_device(dev);
+	/*
+	 * scsi_device已经有了
+	 * 这里下面要分配scsi_disk
+	 */
 	struct scsi_disk *sdkp;
 	struct gendisk *gd;
 	int index;
@@ -3348,6 +3365,9 @@ static int sd_probe(struct device *dev)
 	if (!sdkp)
 		goto out;
 
+	/*
+	 * 在这里分配gd!!!
+	 */
 	gd = alloc_disk(SD_MINORS);
 	if (!gd)
 		goto out_free;
@@ -3389,6 +3409,9 @@ static int sd_probe(struct device *dev)
 		goto out_free_index;
 
 	get_device(dev);
+	/*
+	 * 设置device->driver_data = data;
+	 */
 	dev_set_drvdata(dev, sdkp);
 
 	gd->major = sd_major((index & 0xf0) >> 4);
@@ -3396,6 +3419,12 @@ static int sd_probe(struct device *dev)
 
 	gd->fops = &sd_fops;
 	gd->private_data = &sdkp->driver;
+	/*
+	 * struct scsi_disk *sdkp:
+	 * -> struct scsi_driver *driver;
+	 * -> struct scsi_device *device;
+	 *    -> struct request_queue *request_queue;
+	 */
 	gd->queue = sdkp->device->request_queue;
 
 	/* defaults, until the device tells us otherwise */
@@ -3765,6 +3794,15 @@ void sd_print_sense_hdr(struct scsi_disk *sdkp, struct scsi_sense_hdr *sshdr)
 			     sdkp->disk ? sdkp->disk->disk_name : NULL, sshdr);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/sd.c|1675| <<sd_sync_cache>> sd_print_result(sdkp, "Synchronize Cache(10) failed", res);
+ *   - drivers/scsi/sd.c|2139| <<sd_spinup_disk>> sd_print_result(sdkp, "Test Unit Ready failed",
+ *   - drivers/scsi/sd.c|2343| <<read_capacity_16>> sd_print_result(sdkp, "Read Capacity(16) failed", the_result);
+ *   - drivers/scsi/sd.c|2420| <<read_capacity_10>> sd_print_result(sdkp, "Read Capacity(10) failed", the_result);
+ *   - drivers/scsi/sd.c|3551| <<sd_start_stop_device>> sd_print_result(sdkp, "Start/Stop Unit failed", res);
+ *   - drivers/scsi/sd_zbc.c|118| <<sd_zbc_do_report_zones>> sd_print_result(sdkp, "REPORT ZONES", result);
+ */
 void sd_print_result(const struct scsi_disk *sdkp, const char *msg, int result)
 {
 	const char *hb_string = scsi_hostbyte_string(result);
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 3b1803432090..716f327c9da2 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -32,8 +32,45 @@
 
 #include "sd.h"
 
+/*
+ * [0] bio_endio
+ * [0] scsi_end_request
+ * [0] scsi_io_completion
+ * [0] scsi_finish_command
+ * [0] scsi_softirq_done
+ * [0] blk_done_softirq
+ *
+ * [0] bio_endio
+ * [0] clone_endio
+ * [0] bio_endio
+ * [0] blk_update_request
+ * [0] scsi_end_request
+ * [0] scsi_io_completion
+ * [0] scsi_finish_command
+ * [0] scsi_softirq_done
+ * [0] blk_done_softirq
+ */
+
+/*
+ * 在以下使用VIRTIO_SCSI_MEMPOOL_SZ:
+ *   - drivers/scsi/virtio_scsi.c|1343| <<init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
+ */
 #define VIRTIO_SCSI_MEMPOOL_SZ 64
+/*
+ * 在以下使用VIRTIO_SCSI_EVENT_LEN:
+ *   - drivers/scsi/virtio_scsi.c|100| <<global>> struct virtio_scsi_event_node event_list[VIRTIO_SCSI_EVENT_LEN];
+ *   - drivers/scsi/virtio_scsi.c|385| <<virtscsi_kick_event_all>> for (i = 0; i < VIRTIO_SCSI_EVENT_LEN; i++) {
+ *   - drivers/scsi/virtio_scsi.c|410| <<virtscsi_cancel_event_work>> for (i = 0; i < VIRTIO_SCSI_EVENT_LEN; i++)
+ */
 #define VIRTIO_SCSI_EVENT_LEN 8
+/*
+ * 在以下使用VIRTIO_SCSI_VQ_BASE:
+ *   - drivers/scsi/virtio_scsi.c|302| <<virtscsi_req_done>> int index = vq->index - VIRTIO_SCSI_VQ_BASE;
+ *   - drivers/scsi/virtio_scsi.c|1089| <<virtscsi_init>> num_vqs = vscsi->num_queues + VIRTIO_SCSI_VQ_BASE;
+ *   - drivers/scsi/virtio_scsi.c|1104| <<virtscsi_init>> for (i = VIRTIO_SCSI_VQ_BASE; i < num_vqs; i++) {
+ *   - drivers/scsi/virtio_scsi.c|1116| <<virtscsi_init>> for (i = VIRTIO_SCSI_VQ_BASE; i < num_vqs; i++)
+ *   - drivers/scsi/virtio_scsi.c|1117| <<virtscsi_init>> virtscsi_init_vq(&vscsi->req_vqs[i - VIRTIO_SCSI_VQ_BASE],
+ */
 #define VIRTIO_SCSI_VQ_BASE 2
 
 /* Command queue element */
@@ -72,21 +109,59 @@ struct virtio_scsi {
 	struct virtio_device *vdev;
 
 	/* Get some buffers ready for event vq */
+	/*
+	 * 在以下使用virtio_scsi->event_list[]:
+	 *   - drivers/scsi/virtio_scsi.c|277| <<virtscsi_kick_event_all>> vscsi->event_list[i].vscsi = vscsi;
+	 *   - drivers/scsi/virtio_scsi.c|278| <<virtscsi_kick_event_all>> virtscsi_kick_event(vscsi, &vscsi->event_list[i]);
+	 *   - drivers/scsi/virtio_scsi.c|294| <<virtscsi_cancel_event_work>> cancel_work_sync(&vscsi->event_list[i].work);
+	 */
 	struct virtio_scsi_event_node event_list[VIRTIO_SCSI_EVENT_LEN];
 
 	u32 num_queues;
 
+	/* 似乎没用了 */
 	struct hlist_node node;
 
 	/* Protected by event_vq lock */
+	/*
+	 * 在以下使用virtio_scsi->stop_events:
+	 *   - drivers/scsi/virtio_scsi.c|359| <<virtscsi_cancel_event_work>> vscsi->stop_events = true;
+	 *   - drivers/scsi/virtio_scsi.c|510| <<virtscsi_complete_event>> if (!vscsi->stop_events)
+	 */
 	bool stop_events;
 
+	/*
+	 * 在以下使用virtio_scsi->ctrl_vq:
+	 *   - drivers/scsi/virtio_scsi.c|296| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *   - drivers/scsi/virtio_scsi.c|1024| <<virtscsi_init>> virtscsi_init_vq(&vscsi->ctrl_vq, vqs[0]);
+	 */
 	struct virtio_scsi_vq ctrl_vq;
 	struct virtio_scsi_vq event_vq;
 	struct virtio_scsi_vq req_vqs[];
 };
 
+/*
+ * 在以下使用virtscsi_cmd_cache:
+ *   - drivers/scsi/virtio_scsi.c|1005| <<init>> virtscsi_cmd_cache = KMEM_CACHE(virtio_scsi_cmd, 0);
+ *   - drivers/scsi/virtio_scsi.c|1006| <<init>> if (!virtscsi_cmd_cache) {
+ *   - drivers/scsi/virtio_scsi.c|1014| <<init>> virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1028| <<init>> kmem_cache_destroy(virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1029| <<init>> virtscsi_cmd_cache = NULL;
+ *   - drivers/scsi/virtio_scsi.c|1037| <<fini>> kmem_cache_destroy(virtscsi_cmd_cache);
+ */
 static struct kmem_cache *virtscsi_cmd_cache;
+/*
+ * 在以下使用virtscsi_cmd_pool:
+ *   - drivers/scsi/virtio_scsi.c|636| <<virtscsi_tmf>> mempool_free(cmd, virtscsi_cmd_pool);
+ *   - drivers/scsi/virtio_scsi.c|646| <<virtscsi_device_reset>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+ *   - drivers/scsi/virtio_scsi.c|707| <<virtscsi_abort>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+ *   - drivers/scsi/virtio_scsi.c|1012| <<init>> virtscsi_cmd_pool =
+ *   - drivers/scsi/virtio_scsi.c|1015| <<init>> if (!virtscsi_cmd_pool) {
+ *   - drivers/scsi/virtio_scsi.c|1026| <<init>> mempool_destroy(virtscsi_cmd_pool);
+ *   - drivers/scsi/virtio_scsi.c|1027| <<init>> virtscsi_cmd_pool = NULL;
+ *   - drivers/scsi/virtio_scsi.c|1036| <<fini>> mempool_destroy(virtscsi_cmd_pool);
+ */
 static mempool_t *virtscsi_cmd_pool;
 
 static inline struct Scsi_Host *virtio_scsi_host(struct virtio_device *vdev)
@@ -94,8 +169,19 @@ static inline struct Scsi_Host *virtio_scsi_host(struct virtio_device *vdev)
 	return vdev->priv;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|139| <<virtscsi_complete_cmd>> virtscsi_compute_resid(sc, virtio32_to_cpu(vscsi->vdev, resp->resid));
+ */
 static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
 {
+	/*
+	 * struct scsi_cmnd:
+	 * -> struct scsi_request req;
+	 *    -> unsigned int resid_len
+	 *
+	 * 设置cmd->req.resid_len = resid;
+	 */
 	if (resid)
 		scsi_set_resid(sc, resid);
 }
@@ -105,6 +191,27 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * virtio1-config  -> 不属于virtio-scsi, 是所有virtio都有的
+ * virtio1-control
+ * virtio1-event
+ * virtio1-request
+ *
+ * [0] virtscsi_complete_cmd
+ * [0] virtscsi_req_done
+ * [0] vring_interrupt
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] asm_call_sysvec_on_stack
+ * [0] common_interrupt
+ *
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|219| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|229| <<virtscsi_poll_requests>> virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|615| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -165,9 +272,19 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 			set_driver_byte(sc, DRIVER_SENSE);
 	}
 
+	/*
+	 * 比如scsi_mq_done()
+	 */
 	sc->scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|235| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|244| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
+ *   - drivers/scsi/virtio_scsi.c|261| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|464| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -180,6 +297,9 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_lock_irqsave(&virtscsi_vq->vq_lock, flags);
 	do {
 		virtqueue_disable_cb(vq);
+		/*
+		 * 这里是fn()!!!!
+		 */
 		while ((buf = virtqueue_get_buf(vq, &len)) != NULL)
 			fn(vscsi, buf);
 
@@ -189,6 +309,10 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_unlock_irqrestore(&virtscsi_vq->vq_lock, flags);
 }
 
+/*
+ * 在以下使用virtscsi_req_done():
+ *   - drivers/scsi/virtio_scsi.c|896| <<virtscsi_init>> callbacks[i] = virtscsi_req_done;
+ */
 static void virtscsi_req_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -199,6 +323,10 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|687| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
@@ -209,14 +337,23 @@ static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 				 virtscsi_complete_cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|261| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ */
 static void virtscsi_complete_free(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
 
+	/* 还一个使用cmd->comp的地方是virtscsi_tmf() */
 	if (cmd->comp)
 		complete(cmd->comp);
 }
 
+/*
+ * 在以下使用virtscsi_ctrl_done():
+ *   - drivers/scsi/virtio_scsi.c|1010| <<virtscsi_init>> callbacks[0] = virtscsi_ctrl_done;
+ */
 static void virtscsi_ctrl_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -227,6 +364,27 @@ static void virtscsi_ctrl_done(struct virtqueue *vq)
 
 static void virtscsi_handle_event(struct work_struct *work);
 
+/*
+ * [0] virtscsi_kick_event
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|278| <<virtscsi_kick_event_all>> virtscsi_kick_event(vscsi, &vscsi->event_list[i]);
+ *   - drivers/scsi/virtio_scsi.c|414| <<virtscsi_handle_event>> virtscsi_kick_event(vscsi, event_node);
+ */
 static int virtscsi_kick_event(struct virtio_scsi *vscsi,
 			       struct virtio_scsi_event_node *event_node)
 {
@@ -249,11 +407,20 @@ static int virtscsi_kick_event(struct virtio_scsi *vscsi,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|956| <<virtscsi_probe>> virtscsi_kick_event_all(vscsi);
+ *   - drivers/scsi/virtio_scsi.c|1010| <<virtscsi_restore>> virtscsi_kick_event_all(vscsi);
+ */
 static int virtscsi_kick_event_all(struct virtio_scsi *vscsi)
 {
 	int i;
 
 	for (i = 0; i < VIRTIO_SCSI_EVENT_LEN; i++) {
+		/*
+		 * struct virtio_scsi:
+		 *   -> struct virtio_scsi_event_node event_list[VIRTIO_SCSI_EVENT_LEN];
+		 */
 		vscsi->event_list[i].vscsi = vscsi;
 		virtscsi_kick_event(vscsi, &vscsi->event_list[i]);
 	}
@@ -261,6 +428,10 @@ static int virtscsi_kick_event_all(struct virtio_scsi *vscsi)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|1080| <<virtscsi_remove>> virtscsi_cancel_event_work(vscsi);
+ */
 static void virtscsi_cancel_event_work(struct virtio_scsi *vscsi)
 {
 	int i;
@@ -274,6 +445,10 @@ static void virtscsi_cancel_event_work(struct virtio_scsi *vscsi)
 		cancel_work_sync(&vscsi->event_list[i].work);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|478| <<virtscsi_handle_event>> virtscsi_handle_transport_reset(vscsi, event);
+ */
 static void virtscsi_handle_transport_reset(struct virtio_scsi *vscsi,
 					    struct virtio_scsi_event *event)
 {
@@ -301,6 +476,10 @@ static void virtscsi_handle_transport_reset(struct virtio_scsi *vscsi,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|481| <<virtscsi_handle_event>> virtscsi_handle_param_change(vscsi, event);
+ */
 static void virtscsi_handle_param_change(struct virtio_scsi *vscsi,
 					 struct virtio_scsi_event *event)
 {
@@ -326,6 +505,10 @@ static void virtscsi_handle_param_change(struct virtio_scsi *vscsi,
 	scsi_device_put(sdev);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|432| <<virtscsi_handle_event>> virtscsi_rescan_hotunplug(vscsi);
+ */
 static void virtscsi_rescan_hotunplug(struct virtio_scsi *vscsi)
 {
 	struct scsi_device *sdev;
@@ -364,6 +547,18 @@ static void virtscsi_rescan_hotunplug(struct virtio_scsi *vscsi)
 	kfree(inq_result);
 }
 
+/*
+ * "block_resize drive1 256M"会产生event=3=VIRTIO_SCSI_T_PARAM_CHANGE
+ * 在targetcli为vhost-scsi添加lun的实验的时候也可以模拟这个event
+ * [0] virtscsi_handle_event
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 在以下使用virtscsi_handle_event():
+ *   - drivers/scsi/virtio_scsi.c|257| <<virtscsi_kick_event>> INIT_WORK(&event_node->work, virtscsi_handle_event);
+ */
 static void virtscsi_handle_event(struct work_struct *work)
 {
 	struct virtio_scsi_event_node *event_node =
@@ -391,17 +586,35 @@ static void virtscsi_handle_event(struct work_struct *work)
 	default:
 		pr_err("Unsupport virtio scsi event %x\n", event->event);
 	}
+	/*
+	 * 在以下调用virtscsi_kick_event():
+	 *   - drivers/scsi/virtio_scsi.c|278| <<virtscsi_kick_event_all>> virtscsi_kick_event(vscsi, &vscsi->event_list[i]);
+	 *   - drivers/scsi/virtio_scsi.c|414| <<virtscsi_handle_event>> virtscsi_kick_event(vscsi, event_node);
+	 */
 	virtscsi_kick_event(vscsi, event_node);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|502| <<virtscsi_event_done>> virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_complete_event(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_event_node *event_node = buf;
 
+	/*
+	 * 在以下使用virtio_scsi->stop_events:
+	 *   - drivers/scsi/virtio_scsi.c|359| <<virtscsi_cancel_event_work>> vscsi->stop_events = true;
+	 *   - drivers/scsi/virtio_scsi.c|510| <<virtscsi_complete_event>> if (!vscsi->stop_events)
+	 */
 	if (!vscsi->stop_events)
 		queue_work(system_freezable_wq, &event_node->work);
 }
 
+/*
+ * 在以下使用virtscsi_event_done():
+ *   - drivers/scsi/virtio_scsi.c|930| <<virtscsi_init>> callbacks[1] = virtscsi_event_done;
+ */
 static void virtscsi_event_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -410,6 +623,10 @@ static void virtscsi_event_done(struct virtqueue *vq)
 	virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|586| <<virtscsi_add_cmd>> err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int __virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -455,6 +672,10 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 	return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|828| <<virtscsi_commit_rqs>> virtscsi_kick_vq(&vscsi->req_vqs[hwq]);
+ */
 static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
 {
 	bool needs_kick;
@@ -476,6 +697,11 @@ static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
  * @resp_size	: size of the response buffer
  * @kick	: whether to kick the virtqueue immediately
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|586| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|609| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+ */
 static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size,
@@ -497,10 +723,30 @@ static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|619| <<virtio_scsi_init_hdr_pi>> virtio_scsi_init_hdr(vdev, (struct virtio_scsi_cmd_req *)cmd_pi, sc);
+ *   - drivers/scsi/virtio_scsi.c|677| <<virtscsi_queuecommand>> virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
+ */
 static void virtio_scsi_init_hdr(struct virtio_device *vdev,
 				 struct virtio_scsi_cmd_req *cmd,
 				 struct scsi_cmnd *sc)
 {
+	/*
+	 * struct virtio_scsi_cmd_req {
+	 *     __u8 lun[8];            // Logical Unit Number
+	 *     __virtio64 tag;         // Command identifier
+	 *     __u8 task_attr;         // Task attribute
+	 *     __u8 prio;              // SAM command priority field
+	 *     __u8 crn;
+	 *     __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+	 * } __attribute__((packed));
+	 *
+	 * struct scsi_cmnd *sc:
+	 * -> struct scsi_device *device;
+	 *    -> unsigned int id, channel;
+	 *    -> u64 lun;
+	 */
 	cmd->lun[0] = 1;
 	cmd->lun[1] = sc->device->id;
 	cmd->lun[2] = (sc->device->lun >> 8) | 0x40;
@@ -512,6 +758,10 @@ static void virtio_scsi_init_hdr(struct virtio_device *vdev,
 }
 
 #ifdef CONFIG_BLK_DEV_INTEGRITY
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|671| <<virtscsi_queuecommand>> virtio_scsi_init_hdr_pi(vscsi->vdev, &cmd->req.cmd_pi, sc);
+ */
 static void virtio_scsi_init_hdr_pi(struct virtio_device *vdev,
 				    struct virtio_scsi_cmd_req_pi *cmd_pi,
 				    struct scsi_cmnd *sc)
@@ -537,6 +787,10 @@ static void virtio_scsi_init_hdr_pi(struct virtio_device *vdev,
 }
 #endif
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|650| <<virtscsi_queuecommand>> struct virtio_scsi_vq *req_vq = virtscsi_pick_vq_mq(vscsi, sc);
+ */
 static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 						  struct scsi_cmnd *sc)
 {
@@ -546,6 +800,13 @@ static struct virtio_scsi_vq *virtscsi_pick_vq_mq(struct virtio_scsi *vscsi,
 	return &vscsi->req_vqs[hwq];
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/scsi_error.c|1090| <<scsi_send_eh_cmnd>> rtn = shost->hostt->queuecommand(shost, scmd);
+ *   - drivers/scsi/scsi_lib.c|1554| <<scsi_dispatch_cmd>> rtn = host->hostt->queuecommand(host, cmd);
+ *
+ * struct scsi_host_template virtscsi_host_template.queuecommand = virtscsi_queuecommand()
+ */
 static int virtscsi_queuecommand(struct Scsi_Host *shost,
 				 struct scsi_cmnd *sc)
 {
@@ -577,7 +838,20 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	} else
 #endif
 	{
+		/*
+		 * struct virtio_scsi_cmd *cmd:
+		 * -> union req:
+		 *    -> struct virtio_scsi_cmd_req       cmd;
+		 *       -> __u8 cdb[VIRTIO_SCSI_CDB_SIZE];
+		 *    -> struct virtio_scsi_cmd_req_pi    cmd_pi;
+		 *    -> struct virtio_scsi_ctrl_tmf_req  tmf;
+		 *    -> struct virtio_scsi_ctrl_an_req   an;
+		 */
 		virtio_scsi_init_hdr(vscsi->vdev, &cmd->req.cmd, sc);
+		/*
+		 * struct scsi_cmnd *sc:
+		 * -> unsigned char *cmnd;
+		 */
 		memcpy(cmd->req.cmd.cdb, sc->cmnd, sc->cmd_len);
 		req_size = sizeof(cmd->req.cmd);
 	}
@@ -595,6 +869,11 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|650| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+ */
 static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 {
 	DECLARE_COMPLETION_ONSTACK(comp);
@@ -627,6 +906,9 @@ static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 	return ret;
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.eh_device_reset_handler = virtscsi_device_reset()
+ */
 static int virtscsi_device_reset(struct scsi_cmnd *sc)
 {
 	struct virtio_scsi *vscsi = shost_priv(sc->device->host);
@@ -650,6 +932,9 @@ static int virtscsi_device_reset(struct scsi_cmnd *sc)
 	return virtscsi_tmf(vscsi, cmd);
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_templat.slave_alloc = virtscsi_device_alloc()
+ */
 static int virtscsi_device_alloc(struct scsi_device *sdevice)
 {
 	/*
@@ -677,6 +962,9 @@ static int virtscsi_device_alloc(struct scsi_device *sdevice)
  * @sdev:	Virtscsi target whose queue depth to change
  * @qdepth:	New queue depth
  */
+/*
+ * struct scsi_host_template virtscsi_host_template.change_queue_depth = virtscsi_change_queue_depth()
+ */
 static int virtscsi_change_queue_depth(struct scsi_device *sdev, int qdepth)
 {
 	struct Scsi_Host *shost = sdev->host;
@@ -685,6 +973,9 @@ static int virtscsi_change_queue_depth(struct scsi_device *sdev, int qdepth)
 	return scsi_change_queue_depth(sdev, min(max_depth, qdepth));
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.eh_abort_handler = virtscsi_abort()
+ */
 static int virtscsi_abort(struct scsi_cmnd *sc)
 {
 	struct virtio_scsi *vscsi = shost_priv(sc->device->host);
@@ -696,6 +987,15 @@ static int virtscsi_abort(struct scsi_cmnd *sc)
 		return FAILED;
 
 	memset(cmd, 0, sizeof(*cmd));
+	/*
+	 * // Task Management Request
+	 * struct virtio_scsi_ctrl_tmf_req {
+	 *     __virtio32 type;
+	 *     __virtio32 subtype;
+	 *     __u8 lun[8];
+	 *     __virtio64 tag;
+	 * } __attribute__((packed));
+	 */
 	cmd->req.tmf = (struct virtio_scsi_ctrl_tmf_req){
 		.type = VIRTIO_SCSI_T_TMF,
 		.subtype = VIRTIO_SCSI_T_TMF_ABORT_TASK,
@@ -708,6 +1008,9 @@ static int virtscsi_abort(struct scsi_cmnd *sc)
 	return virtscsi_tmf(vscsi, cmd);
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.virtscsi_map_queues()
+ */
 static int virtscsi_map_queues(struct Scsi_Host *shost)
 {
 	struct virtio_scsi *vscsi = shost_priv(shost);
@@ -716,6 +1019,9 @@ static int virtscsi_map_queues(struct Scsi_Host *shost)
 	return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.commit_rqs = virtscsi_commit_rqs()
+ */
 static void virtscsi_commit_rqs(struct Scsi_Host *shost, u16 hwq)
 {
 	struct virtio_scsi *vscsi = shost_priv(shost);
@@ -728,6 +1034,9 @@ static void virtscsi_commit_rqs(struct Scsi_Host *shost, u16 hwq)
  * latencies might be higher than on bare metal.  Reset the timer
  * unconditionally to give the host a chance to perform EH.
  */
+/*
+ * struct scsi_host_template virtscsi_host_template.eh_timed_out = virtscsi_eh_timed_out()
+ */
 static enum blk_eh_timer_return virtscsi_eh_timed_out(struct scsi_cmnd *scmnd)
 {
 	return BLK_EH_RESET_TIMER;
@@ -752,6 +1061,15 @@ static struct scsi_host_template virtscsi_host_template = {
 	.track_queue_depth = 1,
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|890| <<virtscsi_probe>> num_queues = virtscsi_config_get(vdev, num_queues) ? : 1;
+ *   - drivers/scsi/virtio_scsi.c|893| <<virtscsi_probe>> num_targets = virtscsi_config_get(vdev, max_target) + 1;
+ *   - drivers/scsi/virtio_scsi.c|910| <<virtscsi_probe>> sg_elems = virtscsi_config_get(vdev, seg_max) ?: 1;
+ *   - drivers/scsi/virtio_scsi.c|923| <<virtscsi_probe>> cmd_per_lun = virtscsi_config_get(vdev, cmd_per_lun) ?: 1;
+ *   - drivers/scsi/virtio_scsi.c|925| <<virtscsi_probe>> shost->max_sectors = virtscsi_config_get(vdev, max_sectors) ?: 0xFFFF;
+ *   - drivers/scsi/virtio_scsi.c|930| <<virtscsi_probe>> shost->max_lun = virtscsi_config_get(vdev, max_lun) + 1 + 0x4000;
+ */
 #define virtscsi_config_get(vdev, fld) \
 	({ \
 		__virtio_native_type(struct virtio_scsi_config, fld) __val; \
@@ -765,6 +1083,12 @@ static struct scsi_host_template virtscsi_host_template = {
 		virtio_cwrite(vdev, struct virtio_scsi_config, fld, &__val); \
 	} while(0)
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|851| <<virtscsi_init>> virtscsi_init_vq(&vscsi->ctrl_vq, vqs[0]);
+ *   - drivers/scsi/virtio_scsi.c|852| <<virtscsi_init>> virtscsi_init_vq(&vscsi->event_vq, vqs[1]);
+ *   - drivers/scsi/virtio_scsi.c|854| <<virtscsi_init>> virtscsi_init_vq(&vscsi->req_vqs[i - VIRTIO_SCSI_VQ_BASE],
+ */
 static void virtscsi_init_vq(struct virtio_scsi_vq *virtscsi_vq,
 			     struct virtqueue *vq)
 {
@@ -772,6 +1096,12 @@ static void virtscsi_init_vq(struct virtio_scsi_vq *virtscsi_vq,
 	virtscsi_vq->vq = vq;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|959| <<virtscsi_init>> virtscsi_remove_vqs(vdev);
+ *   - drivers/scsi/virtio_scsi.c|1083| <<virtscsi_remove>> virtscsi_remove_vqs(vdev);
+ *   - drivers/scsi/virtio_scsi.c|1093| <<virtscsi_freeze>> virtscsi_remove_vqs(vdev);
+ */
 static void virtscsi_remove_vqs(struct virtio_device *vdev)
 {
 	/* Stop all the virtqueues. */
@@ -779,6 +1109,11 @@ static void virtscsi_remove_vqs(struct virtio_device *vdev)
 	vdev->config->del_vqs(vdev);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|917| <<virtscsi_probe>> err = virtscsi_init(vdev, vscsi);
+ *   - drivers/scsi/virtio_scsi.c|1003| <<virtscsi_restore>> err = virtscsi_init(vdev, vscsi);
+ */
 static int virtscsi_init(struct virtio_device *vdev,
 			 struct virtio_scsi *vscsi)
 {
@@ -790,6 +1125,9 @@ static int virtscsi_init(struct virtio_device *vdev,
 	struct virtqueue **vqs;
 	struct irq_affinity desc = { .pre_vectors = 2 };
 
+	/*
+	 * virtio1-control和virtio1-event
+	 */
 	num_vqs = vscsi->num_queues + VIRTIO_SCSI_VQ_BASE;
 	vqs = kmalloc_array(num_vqs, sizeof(struct virtqueue *), GFP_KERNEL);
 	callbacks = kmalloc_array(num_vqs, sizeof(vq_callback_t *),
@@ -835,6 +1173,24 @@ static int virtscsi_init(struct virtio_device *vdev,
 	return err;
 }
 
+/*
+ * [0] virtscsi_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * struct virtio_driver virtio_scsi_driver.probe = virtscsi_probe()
+ */
 static int virtscsi_probe(struct virtio_device *vdev)
 {
 	struct Scsi_Host *shost;
@@ -856,11 +1212,26 @@ static int virtscsi_probe(struct virtio_device *vdev)
 
 	num_targets = virtscsi_config_get(vdev, max_target) + 1;
 
+	/*
+	 * struct_size()参数中的vscsi是上面声明的struct virtio_scsi *vscsi;
+	 *
+	 * struct virtio_scsi:
+	 *   -> struct virtio_scsi_vq ctrl_vq;
+	 *   -> struct virtio_scsi_vq event_vq;
+	 *   -> struct virtio_scsi_vq req_vqs[];
+	 *        -> spinlock_t vq_lock;
+	 *        -> struct virtqueue *vq;
+	 *
+	 * 目前假设是Scsi_Host后面跟着"struct virtio_scsi"和若干个"struct virtio_scsi_vq"
+	 */
 	shost = scsi_host_alloc(&virtscsi_host_template,
 				struct_size(vscsi, req_vqs, num_queues));
 	if (!shost)
 		return -ENOMEM;
 
+	/*
+	 * the maximum number of segments that can be in a command
+	 */
 	sg_elems = virtscsi_config_get(vdev, seg_max) ?: 1;
 	shost->sg_tablesize = sg_elems;
 	vscsi = shost_priv(shost);
@@ -872,8 +1243,14 @@ static int virtscsi_probe(struct virtio_device *vdev)
 	if (err)
 		goto virtscsi_init_failed;
 
+	/*
+	 * virtqueue_get_vring_size(): return the size of the virtqueue's vring
+	 */
 	shost->can_queue = virtqueue_get_vring_size(vscsi->req_vqs[0].vq);
 
+	/*
+	 * the maximum number of linked commands it can send to one LUN
+	 */
 	cmd_per_lun = virtscsi_config_get(vdev, cmd_per_lun) ?: 1;
 	shost->cmd_per_lun = min_t(u32, cmd_per_lun, shost->can_queue);
 	shost->max_sectors = virtscsi_config_get(vdev, max_sectors) ?: 0xFFFF;
@@ -919,6 +1296,9 @@ static int virtscsi_probe(struct virtio_device *vdev)
 	return err;
 }
 
+/*
+ * struct virtio_driver virtio_scsi_driver.remove = virtscsi_remove()
+ */
 static void virtscsi_remove(struct virtio_device *vdev)
 {
 	struct Scsi_Host *shost = virtio_scsi_host(vdev);
@@ -933,12 +1313,18 @@ static void virtscsi_remove(struct virtio_device *vdev)
 }
 
 #ifdef CONFIG_PM_SLEEP
+/*
+ * struct virtio_driver virtio_scsi_driver.freeze = virtscsi_freeze()
+ */
 static int virtscsi_freeze(struct virtio_device *vdev)
 {
 	virtscsi_remove_vqs(vdev);
 	return 0;
 }
 
+/*
+ * struct virtio_driver virtio_scsi_driver.restore = virtscsi_restore()
+ */
 static int virtscsi_restore(struct virtio_device *vdev)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vdev);
@@ -963,6 +1349,9 @@ static struct virtio_device_id id_table[] = {
 	{ 0 },
 };
 
+/*
+ * 这feature的意思?
+ */
 static unsigned int features[] = {
 	VIRTIO_SCSI_F_HOTPLUG,
 	VIRTIO_SCSI_F_CHANGE,
@@ -996,6 +1385,17 @@ static int __init init(void)
 	}
 
 
+	/*
+	 * 在以下使用virtscsi_cmd_pool:
+	 *   - drivers/scsi/virtio_scsi.c|636| <<virtscsi_tmf>> mempool_free(cmd, virtscsi_cmd_pool);
+	 *   - drivers/scsi/virtio_scsi.c|646| <<virtscsi_device_reset>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+	 *   - drivers/scsi/virtio_scsi.c|707| <<virtscsi_abort>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+	 *   - drivers/scsi/virtio_scsi.c|1012| <<init>> virtscsi_cmd_pool =
+	 *   - drivers/scsi/virtio_scsi.c|1015| <<init>> if (!virtscsi_cmd_pool) {
+	 *   - drivers/scsi/virtio_scsi.c|1026| <<init>> mempool_destroy(virtscsi_cmd_pool);
+	 *   - drivers/scsi/virtio_scsi.c|1027| <<init>> virtscsi_cmd_pool = NULL;
+	 *   - drivers/scsi/virtio_scsi.c|1036| <<fini>> mempool_destroy(virtscsi_cmd_pool);
+	 */
 	virtscsi_cmd_pool =
 		mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
 					 virtscsi_cmd_cache);
diff --git a/drivers/target/loopback/tcm_loop.c b/drivers/target/loopback/tcm_loop.c
index 16d5a4e117a2..d216db94e3aa 100644
--- a/drivers/target/loopback/tcm_loop.c
+++ b/drivers/target/loopback/tcm_loop.c
@@ -37,6 +37,21 @@
 
 #include "tcm_loop.h"
 
+/*
+ * [0] fd_execute_rw
+ * [0] __target_execute_cmd
+ * [0] target_execute_cmd
+ * [0] tcm_loop_write_pending
+ * [0] transport_generic_new_cmd
+ * [0] transport_handle_cdb_direct
+ * [0] target_submit_cmd_map_sgls
+ * [0] tcm_loop_submission_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
+
 #define to_tcm_loop_hba(hba)	container_of(hba, struct tcm_loop_hba, dev)
 
 static struct workqueue_struct *tcm_loop_workqueue;
@@ -93,6 +108,20 @@ static struct device_driver tcm_loop_driverfs = {
  */
 static struct device *tcm_loop_primary;
 
+/*
+ * [0] fd_execute_rw
+ * [0] __target_execute_cmd
+ * [0] target_execute_cmd
+ * [0] tcm_loop_write_pending
+ * [0] transport_generic_new_cmd
+ * [0] transport_handle_cdb_direct
+ * [0] target_submit_cmd_map_sgls
+ * [0] tcm_loop_submission_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void tcm_loop_submission_work(struct work_struct *work)
 {
 	struct tcm_loop_cmd *tl_cmd =
@@ -296,6 +325,10 @@ static int tcm_loop_target_reset(struct scsi_cmnd *sc)
 	return FAILED;
 }
 
+/*
+ * 在以下使用tcm_loop_driver_template:
+ *   - drivers/target/loopback/tcm_loop.c|326| <<tcm_loop_driver_probe>> sh = scsi_host_alloc(&tcm_loop_driver_template,
+ */
 static struct scsi_host_template tcm_loop_driver_template = {
 	.show_info		= tcm_loop_show_info,
 	.proc_name		= "tcm_loopback",
diff --git a/drivers/target/target_core_configfs.c b/drivers/target/target_core_configfs.c
index f04352285155..537afd7822e1 100644
--- a/drivers/target/target_core_configfs.c
+++ b/drivers/target/target_core_configfs.c
@@ -65,6 +65,13 @@ static void target_core_setup_##_name##_cit(struct target_backend *tb)	\
 
 extern struct t10_alua_lu_gp *default_lu_gp;
 
+/*
+ * 在以下使用g_tf_list:
+ *   - drivers/target/target_core_configfs.c|110| <<target_core_item_dbroot_store>> if (!list_empty(&g_tf_list)) {
+ *   - drivers/target/target_core_configfs.c|166| <<target_core_get_fabric>> list_for_each_entry(tf, &g_tf_list, tf_list) {
+ *   - drivers/target/target_core_configfs.c|470| <<target_register_template>> list_add_tail(&tf->tf_list, &g_tf_list);
+ *   - drivers/target/target_core_configfs.c|482| <<target_unregister_template>> list_for_each_entry(t, &g_tf_list, tf_list) {
+ */
 static LIST_HEAD(g_tf_list);
 static DEFINE_MUTEX(g_tf_lock);
 
@@ -446,6 +453,20 @@ static int target_fabric_tf_ops_check(const struct target_core_fabric_ops *tfo)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3886| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4118| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1927| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1931| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+ *   - drivers/target/iscsi/iscsi_target.c|697| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+ *   - drivers/target/loopback/tcm_loop.c|1174| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+ *   - drivers/target/sbp/sbp_target.c|2339| <<sbp_init>> return target_register_template(&sbp_ops);
+ *   - drivers/target/tcm_fc/tfc_conf.c|460| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+ *   - drivers/usb/gadget/function/f_tcm.c|2339| <<tcm_init>> ret = target_register_template(&usbg_ops);
+ *   - drivers/vhost/scsi.c|2338| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/xen/xen-scsiback.c|1864| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+ */
 int target_register_template(const struct target_core_fabric_ops *fo)
 {
 	struct target_fabric_configfs *tf;
@@ -467,6 +488,13 @@ int target_register_template(const struct target_core_fabric_ops *fo)
 	target_fabric_setup_cits(tf);
 
 	mutex_lock(&g_tf_lock);
+	/*
+	 * 在以下使用g_tf_list:
+	 *   - drivers/target/target_core_configfs.c|110| <<target_core_item_dbroot_store>> if (!list_empty(&g_tf_list)) {
+	 *   - drivers/target/target_core_configfs.c|166| <<target_core_get_fabric>> list_for_each_entry(tf, &g_tf_list, tf_list) {
+	 *   - drivers/target/target_core_configfs.c|470| <<target_register_template>> list_add_tail(&tf->tf_list, &g_tf_list);
+	 *   - drivers/target/target_core_configfs.c|482| <<target_unregister_template>> list_for_each_entry(t, &g_tf_list, tf_list) {
+	 */
 	list_add_tail(&tf->tf_list, &g_tf_list);
 	mutex_unlock(&g_tf_lock);
 
diff --git a/drivers/target/target_core_fabric_configfs.c b/drivers/target/target_core_fabric_configfs.c
index ee85602213f7..6bd3e2824e0e 100644
--- a/drivers/target/target_core_fabric_configfs.c
+++ b/drivers/target/target_core_fabric_configfs.c
@@ -616,6 +616,9 @@ static struct configfs_attribute *target_fabric_port_attrs[] = {
 	NULL,
 };
 
+/*
+ * struct configfs_item_operations target_fabric_port_item_ops.allow_link = target_fabric_port_link()
+ */
 static int target_fabric_port_link(
 	struct config_item *lun_ci,
 	struct config_item *se_dev_ci)
diff --git a/drivers/target/target_core_file.c b/drivers/target/target_core_file.c
index 7143d03f0e02..8218f6cf0f0c 100644
--- a/drivers/target/target_core_file.c
+++ b/drivers/target/target_core_file.c
@@ -28,6 +28,21 @@
 
 #include "target_core_file.h"
 
+/*
+ * [0] fd_execute_rw
+ * [0] __target_execute_cmd
+ * [0] target_execute_cmd
+ * [0] tcm_loop_write_pending
+ * [0] transport_generic_new_cmd
+ * [0] transport_handle_cdb_direct
+ * [0] target_submit_cmd_map_sgls
+ * [0] tcm_loop_submission_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
+
 static inline struct fd_dev *FD_DEV(struct se_device *dev)
 {
 	return container_of(dev, struct fd_dev, dev);
@@ -315,6 +330,13 @@ fd_execute_rw_aio(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|610| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, pfile, dev->prot_length,
+ *   - drivers/target/target_core_file.c|617| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, file, dev->dev_attrib.block_size,
+ *   - drivers/target/target_core_file.c|642| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, file, dev->dev_attrib.block_size,
+ *   - drivers/target/target_core_file.c|663| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, pfile, dev->prot_length,
+ */
 static int fd_do_rw(struct se_cmd *cmd, struct file *fd,
 		    u32 block_size, struct scatterlist *sgl,
 		    u32 sgl_nents, u32 data_length, int is_write)
@@ -591,6 +613,23 @@ fd_execute_unmap(struct se_cmd *cmd, sector_t lba, sector_t nolb)
 	return 0;
 }
 
+/*
+ * [0] fd_execute_rw
+ * [0] __target_execute_cmd
+ * [0] target_execute_cmd
+ * [0] tcm_loop_write_pending
+ * [0] transport_generic_new_cmd
+ * [0] transport_handle_cdb_direct
+ * [0] target_submit_cmd_map_sgls
+ * [0] tcm_loop_submission_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/target/target_core_file.c|698| <<fd_execute_rw>> return fd_execute_rw_buffered(cmd, sgl, sgl_nents, data_direction);
+ */
 static sense_reason_t
 fd_execute_rw_buffered(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 	      enum dma_data_direction data_direction)
diff --git a/drivers/target/target_core_hba.c b/drivers/target/target_core_hba.c
index d508b343ba7b..ce58d5af7606 100644
--- a/drivers/target/target_core_hba.c
+++ b/drivers/target/target_core_hba.c
@@ -26,6 +26,13 @@
 
 #include "target_core_internal.h"
 
+/*
+ * 在以下使用:
+ *   - drivers/target/target_core_hba.c|48| <<transport_backend_register>> list_for_each_entry(old, &backend_list, list) {
+ *   - drivers/target/target_core_hba.c|57| <<transport_backend_register>> list_add_tail(&tb->list, &backend_list);
+ *   - drivers/target/target_core_hba.c|71| <<target_backend_unregister>> list_for_each_entry(tb, &backend_list, list) {
+ *   - drivers/target/target_core_hba.c|95| <<core_get_backend>> list_for_each_entry(tb, &backend_list, list) {
+ */
 static LIST_HEAD(backend_list);
 static DEFINE_MUTEX(backend_mutex);
 
@@ -34,7 +41,14 @@ static u32 hba_id_counter;
 static DEFINE_SPINLOCK(hba_lock);
 static LIST_HEAD(hba_list);
 
-
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|948| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+ *   - drivers/target/target_core_iblock.c|888| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+ *   - drivers/target/target_core_pscsi.c|1093| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+ *   - drivers/target/target_core_rd.c|659| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+ *   - drivers/target/target_core_user.c|3021| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+ */
 int transport_backend_register(const struct target_backend_ops *ops)
 {
 	struct target_backend *tb, *old;
@@ -87,6 +101,10 @@ void target_backend_unregister(const struct target_backend_ops *ops)
 }
 EXPORT_SYMBOL(target_backend_unregister);
 
+/*
+ * called by:
+ *   - drivers/target/target_core_hba.c|126| <<core_alloc_hba>> hba->backend = core_get_backend(plugin_name);
+ */
 static struct target_backend *core_get_backend(const char *name)
 {
 	struct target_backend *tb;
@@ -105,6 +123,11 @@ static struct target_backend *core_get_backend(const char *name)
 	return tb;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_configfs.c|3484| <<target_core_call_addhbatotarget>> hba = core_alloc_hba(se_plugin_str, plugin_dep_id, 0);
+ *   - drivers/target/target_core_device.c|1026| <<core_dev_setup_virtual_lun0>> hba = core_alloc_hba("rd_mcp", 0, HBA_FLAGS_INTERNAL_USE);
+ */
 struct se_hba *
 core_alloc_hba(const char *plugin_name, u32 plugin_dep_id, u32 hba_flags)
 {
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index 6e8b8d30938f..181b42897dc3 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -787,6 +787,12 @@ sbc_check_dpofua(struct se_device *dev, struct se_cmd *cmd, unsigned char *cdb)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|921| <<fd_parse_cdb>> return sbc_parse_cdb(cmd, &fd_sbc_ops);
+ *   - drivers/target/target_core_iblock.c|850| <<iblock_parse_cdb>> return sbc_parse_cdb(cmd, &iblock_sbc_ops);
+ *   - drivers/target/target_core_rd.c|634| <<rd_parse_cdb>> return sbc_parse_cdb(cmd, &rd_sbc_ops);
+ */
 sense_reason_t
 sbc_parse_cdb(struct se_cmd *cmd, struct sbc_ops *ops)
 {
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index ff26ab0a5f60..129a8873ce63 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -1592,6 +1592,14 @@ transport_generic_map_mem_to_cmd(struct se_cmd *cmd, struct scatterlist *sgl,
  * This may only be called from process context, and also currently
  * assumes internal allocation of fabric payload buffer by target-core.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1530| <<srpt_handle_cmd>> rc = target_submit_cmd_map_sgls(cmd, ch->sess, srp_cmd->cdb,
+ *   - drivers/target/loopback/tcm_loop.c|145| <<tcm_loop_submission_work>> rc = target_submit_cmd_map_sgls(se_cmd, tl_nexus->se_sess, sc->cmnd,
+ *   - drivers/target/target_core_transport.c|1750| <<target_submit_cmd>> return target_submit_cmd_map_sgls(se_cmd, se_sess, cdb, sense,
+ *   - drivers/vhost/scsi.c|779| <<vhost_scsi_submission_work>> rc = target_submit_cmd_map_sgls(se_cmd, tv_nexus->tvn_se_sess,
+ *   - drivers/xen/xen-scsiback.c|404| <<scsiback_cmd_exec>> rc = target_submit_cmd_map_sgls(se_cmd, sess, pending_req->cmnd,
+ */
 int target_submit_cmd_map_sgls(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *cdb, unsigned char *sense, u64 unpacked_lun,
 		u32 data_length, int task_attr, int data_dir, int flags,
@@ -1743,6 +1751,15 @@ EXPORT_SYMBOL(target_submit_cmd_map_sgls);
  *
  * It also assumes interal target core SGL memory allocation.
  */
+/*
+ * called by:
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2728| <<ibmvscsis_parse_cmd>> rc = target_submit_cmd(&cmd->se_cmd, nexus->se_sess, srp->cdb,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|484| <<tcm_qla2xxx_handle_cmd>> return target_submit_cmd(se_cmd, se_sess, cdb, &cmd->sense_buffer[0],
+ *   - drivers/target/sbp/sbp_target.c|1221| <<sbp_handle_command>> if (target_submit_cmd(&req->se_cmd, sess->se_sess, req->cmd_buf,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|551| <<ft_send_work>> if (target_submit_cmd(&cmd->se_cmd, cmd->sess->se_sess, fcp->fc_cdb,
+ *   - drivers/usb/gadget/function/f_tcm.c|1061| <<usbg_cmd_work>> if (target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess, cmd->cmd_buf,
+ *   - drivers/usb/gadget/function/f_tcm.c|1192| <<bot_cmd_work>> if (target_submit_cmd(se_cmd, tv_nexus->tvn_se_sess,
+ */
 int target_submit_cmd(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *cdb, unsigned char *sense, u64 unpacked_lun,
 		u32 data_length, int task_attr, int data_dir, int flags)
@@ -2754,6 +2771,37 @@ void target_put_cmd_and_wait(struct se_cmd *cmd)
  * - For aborted commands for which CMD_T_TAS has not been set .aborted_task()
  *   will be called. target_handle_abort() will drop the final reference.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1440| <<isert_put_cmd>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1448| <<isert_put_cmd>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|1471| <<isert_put_cmd>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1300| <<srpt_abort_cmd>> transport_generic_free_cmd(&ioctx->cmd, 0);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1303| <<srpt_abort_cmd>> transport_generic_free_cmd(&ioctx->cmd, 0);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1764| <<srpt_send_done>> transport_generic_free_cmd(&ioctx->cmd, 0);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|239| <<tcm_qla2xxx_complete_mcmd>> transport_generic_free_cmd(&mcmd->se_cmd, 0);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|268| <<tcm_qla2xxx_complete_free>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/target/iscsi/iscsi_target_util.c|771| <<iscsit_free_cmd>> rc = transport_generic_free_cmd(se_cmd, shutdown);
+ *   - drivers/target/loopback/tcm_loop.c|69| <<tcm_loop_check_stop_free>> return transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/target/sbp/sbp_target.c|1795| <<sbp_check_stop_free>> return transport_generic_free_cmd(&req->se_cmd, 0);
+ *   - drivers/target/target_core_xcopy.c|614| <<target_xcopy_read_source>> transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/target/target_core_xcopy.c|656| <<target_xcopy_write_destination>> transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/target/tcm_fc/tfc_cmd.c|95| <<ft_check_stop_free>> return transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/target/tcm_fc/tfc_cmd.c|261| <<ft_recv_seq>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/usb/gadget/function/f_tcm.c|64| <<bot_status_complete>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/usb/gadget/function/f_tcm.c|603| <<uasp_status_data_cmpl>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/usb/gadget/function/f_tcm.c|613| <<uasp_status_data_cmpl>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/usb/gadget/function/f_tcm.c|976| <<usbg_data_write_cmpl>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/usb/gadget/function/f_tcm.c|1071| <<usbg_cmd_work>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/usb/gadget/function/f_tcm.c|1202| <<bot_cmd_work>> transport_generic_free_cmd(&cmd->se_cmd, 0);
+ *   - drivers/vhost/scsi.c|460| <<vhost_scsi_free_cmd>> transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/vhost/scsi.c|830| <<vhost_scsi_submission_work>> transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/xen/xen-scsiback.c|413| <<scsiback_cmd_exec>> transport_generic_free_cmd(se_cmd, 0);
+ *   - drivers/xen/xen-scsiback.c|617| <<scsiback_device_action>> transport_generic_free_cmd(&pending_req->se_cmd, 0);
+ *   - drivers/xen/xen-scsiback.c|774| <<scsiback_do_cmd_fn>> transport_generic_free_cmd(&pending_req->se_cmd, 0);
+ *   - drivers/xen/xen-scsiback.c|790| <<scsiback_do_cmd_fn>> transport_generic_free_cmd(&pending_req->se_cmd, 0);
+ *   - drivers/xen/xen-scsiback.c|1387| <<scsiback_check_stop_free>> return transport_generic_free_cmd(se_cmd, 0);
+ */
 int transport_generic_free_cmd(struct se_cmd *cmd, int wait_for_tasks)
 {
 	DECLARE_COMPLETION_ONSTACK(compl);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 531a00d703cd..341ad63b8179 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -35,6 +35,11 @@
 
 #include "vhost.h"
 
+/*
+ * 在以下使用experimental_zcopytx:
+ *   - drivers/vhost/net.c|344| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+ *   - drivers/vhost/net.c|1784| <<vhost_net_init>> if (experimental_zcopytx)
+ */
 static int experimental_zcopytx = 0;
 module_param(experimental_zcopytx, int, 0444);
 MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
@@ -42,6 +47,11 @@ MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
 
 /* Max number of bytes transferred before requeueing the job.
  * Using this limit prevents one virtqueue from starving others. */
+/*
+ * 在以下使用VHOST_NET_WEIGHT:
+ *   - drivers/vhost/net.c|641| <<tx_can_batch>> return total_len < VHOST_NET_WEIGHT &&
+ *   - drivers/vhost/net.c|1330| <<vhost_net_open>> VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,
+ */
 #define VHOST_NET_WEIGHT 0x80000
 
 /* Max number of packets transferred before requeueing the job.
@@ -220,6 +230,11 @@ static int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)
 	return vhost_net_buf_peek_len(vhost_net_buf_get_ptr(rxq));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|324| <<vhost_net_vq_reset>> vhost_net_buf_init(&n->vqs[i].rxq);
+ *   - drivers/vhost/net.c|1349| <<vhost_net_open>> vhost_net_buf_init(&n->vqs[i].rxq);
+ */
 static void vhost_net_buf_init(struct vhost_net_buf *rxq)
 {
 	rxq->head = rxq->tail = 0;
@@ -1273,6 +1288,19 @@ static void handle_rx_net(struct vhost_work *work)
 	handle_rx(net);
 }
 
+/*
+ * 为每个queue(kthread)调用一次.
+ * [0] vhost_net_open
+ * [0] misc_open
+ * [0] chrdev_open
+ * [0] do_dentry_open
+ * [0] path_openat
+ * [0] do_filp_open
+ * [0] do_sys_openat2
+ * [0] do_sys_open
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int vhost_net_open(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n;
@@ -1282,9 +1310,27 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 	struct xdp_buff *xdp;
 	int i;
 
+	/*
+	 * struct vhost_net:
+	 *  -> struct vhost_dev dev;
+	 *  -> struct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];
+	 *      -> struct vhost_virtqueue vq;
+	 *          -> struct file *kick;
+	 *          -> struct vhost_vring_call call_ctx;
+	 *          -> struct eventfd_ctx *error_ctx;
+	 *          -> struct eventfd_ctx *log_ctx;
+	 *          -> struct vhost_poll poll;
+	 *  -> struct vhost_poll poll[VHOST_NET_VQ_MAX];
+	 *
+	 * 分配一个vhost_net结构
+	 */
 	n = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);
 	if (!n)
 		return -ENOMEM;
+	/*
+	 * 分配若干struct vhost_virtqueue的指针
+	 * 真正的内存是vhost_net的一部分
+	 */
 	vqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);
 	if (!vqs) {
 		kvfree(n);
@@ -1364,6 +1410,12 @@ static void vhost_net_stop(struct vhost_net *n, struct socket **tx_sock,
 	*rx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_RX].vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1385| <<vhost_net_flush>> vhost_net_flush_vq(n, VHOST_NET_VQ_TX);
+ *   - drivers/vhost/net.c|1386| <<vhost_net_flush>> vhost_net_flush_vq(n, VHOST_NET_VQ_RX);
+ *   - drivers/vhost/net.c|1576| <<vhost_net_set_backend>> vhost_net_flush_vq(n, index);
+ */
 static void vhost_net_flush_vq(struct vhost_net *n, int index)
 {
 	vhost_poll_flush(n->poll + index);
@@ -1396,6 +1448,16 @@ static int vhost_net_release(struct inode *inode, struct file *f)
 	vhost_net_stop(n, &tx_sock, &rx_sock);
 	vhost_net_flush(n);
 	vhost_dev_stop(&n->dev);
+	/*
+	 * 在以下调用vhost_dev_cleanup():
+	 *   - drivers/vhost/net.c|1451| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+	 *   - drivers/vhost/scsi.c|1654| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+	 *   - drivers/vhost/test.c|165| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+	 *   - drivers/vhost/vdpa.c|838| <<vhost_vdpa_open>> vhost_dev_cleanup(&v->vdev);
+	 *   - drivers/vhost/vdpa.c|870| <<vhost_vdpa_release>> vhost_dev_cleanup(&v->vdev);
+	 *   - drivers/vhost/vhost.c|787| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+	 *   - drivers/vhost/vsock.c|715| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+	 */
 	vhost_dev_cleanup(&n->dev);
 	vhost_net_vq_reset(n);
 	if (tx_sock)
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index b22adf03f584..1b2603411f00 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -105,6 +105,12 @@ struct vhost_scsi_cmd {
 	/* work item used for cmwq dispatch to vhost_scsi_submission_work() */
 	struct work_struct work;
 	/* Copy of the incoming SCSI command descriptor block (CDB) */
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE]:
+	 *   - drivers/vhost/scsi.c|652| <<vhost_scsi_get_tag>> memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
+	 *   - drivers/vhost/scsi.c|833| <<vhost_scsi_submission_work>> cmd->tvc_cdb, &cmd->tvc_sense_buf[0],
+	 *   - drivers/vhost/scsi.c|1142| <<vhost_scsi_handle_vq>> cmd->tvc_cdb[0], cmd->tvc_lun);
+	 */
 	unsigned char tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE];
 	/* Sense buffer that will be mapped into outgoing status */
 	unsigned char tvc_sense_buf[TRANSPORT_SENSE_BUFFER];
@@ -183,6 +189,12 @@ struct vhost_scsi_virtqueue {
 	 * each time, one reference tracks new commands submitted, while we
 	 * wait for another one to reach 0.
 	 */
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->inflights:
+	 *   - drivers/vhost/scsi.c|254| <<vhost_scsi_init_inflight>> old_inflight[i] = &vs->vqs[i].inflights[idx];
+	 *   - drivers/vhost/scsi.c|258| <<vhost_scsi_init_inflight>> new_inflight = &vs->vqs[i].inflights[idx ^ 1];
+	 *   - drivers/vhost/scsi.c|273| <<vhost_scsi_get_inflight>> inflight = &svq->inflights[svq->inflight_idx];
+	 */
 	struct vhost_scsi_inflight inflights[2];
 	/*
 	 * Indicate current inflight in use, protected by vq->mutex.
@@ -202,6 +214,13 @@ struct vhost_scsi {
 	struct vhost_work vs_completion_work; /* cmd completion work item */
 	struct llist_head vs_completion_list; /* cmd completion queue */
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|499| <<vhost_scsi_evt_work>> vs_event_work);
+	 *   - drivers/vhost/scsi.c|1315| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+	 *   - drivers/vhost/scsi.c|1369| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_event_work);
+	 *   - drivers/vhost/scsi.c|1617| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	struct vhost_work vs_event_work; /* evt injection work item */
 	struct llist_head vs_event_list; /* evt injection queue */
 
@@ -228,6 +247,11 @@ static struct workqueue_struct *vhost_scsi_workqueue;
 static DEFINE_MUTEX(vhost_scsi_mutex);
 static LIST_HEAD(vhost_scsi_list);
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|281| <<vhost_scsi_put_inflight>> kref_put(&inflight->kref, vhost_scsi_done_inflight);
+ *   - drivers/vhost/scsi.c|1363| <<vhost_scsi_flush>> kref_put(&old_inflight[i]->kref, vhost_scsi_done_inflight);
+ */
 static void vhost_scsi_done_inflight(struct kref *kref)
 {
 	struct vhost_scsi_inflight *inflight;
@@ -362,6 +386,11 @@ static int vhost_scsi_get_cmd_state(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|378| <<vhost_scsi_queue_data_in>> vhost_scsi_complete_cmd(cmd);
+ *   - drivers/vhost/scsi.c|386| <<vhost_scsi_queue_status>> vhost_scsi_complete_cmd(cmd);
+ */
 static void vhost_scsi_complete_cmd(struct vhost_scsi_cmd *cmd)
 {
 	struct vhost_scsi *vs = cmd->tvc_vhost;
@@ -443,6 +472,10 @@ static int vhost_scsi_check_stop_free(struct se_cmd *se_cmd)
 	return target_put_sess_cmd(se_cmd);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|507| <<vhost_scsi_evt_work>> vhost_scsi_do_evt_work(vs, evt);
+ */
 static void
 vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 {
@@ -493,6 +526,16 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 		vq_err(vq, "Faulted on vhost_scsi_send_event\n");
 }
 
+/*
+ * 在以下使用vhost_scsi->vs_event_work:
+ *   - drivers/vhost/scsi.c|499| <<vhost_scsi_evt_work>> vs_event_work);
+ *   - drivers/vhost/scsi.c|1315| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/scsi.c|1369| <<vhost_scsi_flush>> vhost_work_flush(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/scsi.c|1617| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *
+ * 在以下使用vhost_scsi_evt_work():
+ *   - drivers/vhost/scsi.c|1617| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ */
 static void vhost_scsi_evt_work(struct vhost_work *work)
 {
 	struct vhost_scsi *vs = container_of(work, struct vhost_scsi,
@@ -565,6 +608,10 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_signal(&vs->dev, &vs->vqs[vq].vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_tag(vq, tpg, cdb, tag, lun, task_attr,
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_tag(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -608,6 +655,12 @@ vhost_scsi_get_tag(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 	cmd->tvc_nexus = tv_nexus;
 	cmd->inflight = vhost_scsi_get_inflight(vq);
 
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE]:
+	 *   - drivers/vhost/scsi.c|652| <<vhost_scsi_get_tag>> memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
+	 *   - drivers/vhost/scsi.c|833| <<vhost_scsi_submission_work>> cmd->tvc_cdb, &cmd->tvc_sense_buf[0],
+	 *   - drivers/vhost/scsi.c|1142| <<vhost_scsi_handle_vq>> cmd->tvc_cdb[0], cmd->tvc_lun);
+	 */
 	memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
 
 	return cmd;
@@ -753,6 +806,10 @@ static int vhost_scsi_to_tcm_attr(int attr)
 	return TCM_SIMPLE_TAG;
 }
 
+/*
+ * 在以下使用vhost_scsi_submission_work():
+ *   - drivers/vhost/scsi.c|1152| <<vhost_scsi_handle_vq>> INIT_WORK(&cmd->work, vhost_scsi_submission_work);
+ */
 static void vhost_scsi_submission_work(struct work_struct *work)
 {
 	struct vhost_scsi_cmd *cmd =
@@ -776,6 +833,19 @@ static void vhost_scsi_submission_work(struct work_struct *work)
 	tv_nexus = cmd->tvc_nexus;
 
 	se_cmd->tag = 0;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE]:
+	 *   - drivers/vhost/scsi.c|652| <<vhost_scsi_get_tag>> memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
+	 *   - drivers/vhost/scsi.c|833| <<vhost_scsi_submission_work>> cmd->tvc_cdb, &cmd->tvc_sense_buf[0],
+	 *   - drivers/vhost/scsi.c|1142| <<vhost_scsi_handle_vq>> cmd->tvc_cdb[0], cmd->tvc_lun);
+	 *
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1530| <<srpt_handle_cmd>> rc = target_submit_cmd_map_sgls(cmd, ch->sess, srp_cmd->cdb,
+	 *   - drivers/target/loopback/tcm_loop.c|145| <<tcm_loop_submission_work>> rc = target_submit_cmd_map_sgls(se_cmd, tl_nexus->se_sess, sc->cmnd,
+	 *   - drivers/target/target_core_transport.c|1750| <<target_submit_cmd>> return target_submit_cmd_map_sgls(se_cmd, se_sess, cdb, sense,
+	 *   - drivers/vhost/scsi.c|779| <<vhost_scsi_submission_work>> rc = target_submit_cmd_map_sgls(se_cmd, tv_nexus->tvn_se_sess,
+	 *   - drivers/xen/xen-scsiback.c|404| <<scsiback_cmd_exec>> rc = target_submit_cmd_map_sgls(se_cmd, sess, pending_req->cmnd,
+	 */
 	rc = target_submit_cmd_map_sgls(se_cmd, tv_nexus->tvn_se_sess,
 			cmd->tvc_cdb, &cmd->tvc_sense_buf[0],
 			cmd->tvc_lun, cmd->tvc_exp_data_len,
@@ -907,6 +977,10 @@ vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1340| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -953,6 +1027,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			vc.lunp = &v_req_pi.lun[0];
 			vc.target = &v_req_pi.lun[1];
 		} else {
+			/*
+			 * struct vhost_scsi_ctx vc;
+			 */
 			vc.req = &v_req;
 			vc.req_size = sizeof(v_req);
 			vc.lunp = &v_req.lun[0];
@@ -1078,6 +1155,12 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		cmd->tvc_resp_iov = vq->iov[vc.out];
 		cmd->tvc_in_iovs = vc.in;
 
+		/*
+		 * 在以下使用vhost_scsi_cmd->tvc_cdb[VHOST_SCSI_MAX_CDB_SIZE]:
+		 *   - drivers/vhost/scsi.c|652| <<vhost_scsi_get_tag>> memcpy(cmd->tvc_cdb, cdb, VHOST_SCSI_MAX_CDB_SIZE);
+		 *   - drivers/vhost/scsi.c|833| <<vhost_scsi_submission_work>> cmd->tvc_cdb, &cmd->tvc_sense_buf[0],
+		 *   - drivers/vhost/scsi.c|1142| <<vhost_scsi_handle_vq>> cmd->tvc_cdb[0], cmd->tvc_lun);
+		 */
 		pr_debug("vhost_scsi got command opcode: %#02x, lun: %d\n",
 			 cmd->tvc_cdb[0], cmd->tvc_lun);
 		pr_debug("cmd: %p exp_data_len: %d, prot_bytes: %d data_direction:"
@@ -1104,6 +1187,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		 * cmd is executed on the same kworker CPU as this vhost
 		 * thread to gain positive L2 cache locality effects.
 		 */
+		/*
+		 * struct vhost_scsi_cmd *cmd;
+		 */
 		INIT_WORK(&cmd->work, vhost_scsi_submission_work);
 		queue_work(vhost_scsi_workqueue, &cmd->work);
 		ret = 0;
@@ -1331,6 +1417,10 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_handle_kick():
+ *   - drivers/vhost/scsi.c|1673| <<vhost_scsi_open>> vs->vqs[i].vq.handle_kick = vhost_scsi_handle_kick;
+ */
 static void vhost_scsi_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1609,6 +1699,9 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 			goto err_vs;
 	}
 
+	/*
+	 * 分配了128个!!!
+	 */
 	vqs = kmalloc_array(VHOST_SCSI_MAX_VQ, sizeof(*vqs), GFP_KERNEL);
 	if (!vqs)
 		goto err_vqs;
@@ -1768,6 +1861,21 @@ static char *vhost_scsi_dump_proto_id(struct vhost_scsi_tport *tport)
 	return "Unknown";
 }
 
+/*
+ * 一个4.14的例子
+ * [0] vhost_scsi_do_plug
+ * [0] target_fabric_port_unlink
+ * [0] configfs_unlink
+ * [0] vfs_unlink
+ * [0] do_unlinkat
+ * [0] sys_unlink
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|1894| <<vhost_scsi_hotplug>> vhost_scsi_do_plug(tpg, lun, true);
+ *   - drivers/vhost/scsi.c|1899| <<vhost_scsi_hotunplug>> vhost_scsi_do_plug(tpg, lun, false);
+ */
 static void
 vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 		  struct se_lun *lun, bool plug)
@@ -1796,6 +1904,10 @@ vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 	mutex_unlock(&vs->dev.mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1919| <<vhost_scsi_port_link>> vhost_scsi_hotplug(tpg, lun);
+ */
 static void vhost_scsi_hotplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 {
 	vhost_scsi_do_plug(tpg, lun, true);
@@ -1806,6 +1918,9 @@ static void vhost_scsi_hotunplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 	vhost_scsi_do_plug(tpg, lun, false);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_post_link = vhost_scsi_port_link()
+ */
 static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 			       struct se_lun *lun)
 {
@@ -2335,6 +2450,20 @@ static int __init vhost_scsi_init(void)
 	if (ret < 0)
 		goto out_destroy_workqueue;
 
+	/*
+	 * 在以下调用target_register_template()
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3886| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4118| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1927| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1931| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+	 *   - drivers/target/iscsi/iscsi_target.c|697| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+	 *   - drivers/target/loopback/tcm_loop.c|1174| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+	 *   - drivers/target/sbp/sbp_target.c|2339| <<sbp_init>> return target_register_template(&sbp_ops);
+	 *   - drivers/target/tcm_fc/tfc_conf.c|460| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+	 *   - drivers/usb/gadget/function/f_tcm.c|2339| <<tcm_init>> ret = target_register_template(&usbg_ops);
+	 *   - drivers/vhost/scsi.c|2338| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+	 *   - drivers/xen/xen-scsiback.c|1864| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+	 */
 	ret = target_register_template(&vhost_scsi_ops);
 	if (ret < 0)
 		goto out_vhost_scsi_deregister;
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 9ad45e1d27f0..27e06e183e05 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -33,10 +33,18 @@
 
 #include "vhost.h"
 
+/*
+ * 只在下面使用max_mem_regions:
+ *   - drivers/vhost/vhost.c|1420| <<vhost_set_memory>> if (mem.nregions > max_mem_regions)
+ */
 static ushort max_mem_regions = 64;
 module_param(max_mem_regions, ushort, 0444);
 MODULE_PARM_DESC(max_mem_regions,
 	"Maximum number of memory regions in memory map. (default: 64)");
+/*
+ * 只在下面使用max_iotlb_entries:
+ *   - drivers/vhost/vhost.c|628| <<iotlb_alloc>> return vhost_iotlb_alloc(max_iotlb_entries,
+ */
 static int max_iotlb_entries = 2048;
 module_param(max_iotlb_entries, int, 0444);
 MODULE_PARM_DESC(max_iotlb_entries,
@@ -151,6 +159,12 @@ static void vhost_flush_work(struct vhost_work *work)
 	complete(&s->wait_event);
 }
 
+/*
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|228| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ *
+ * 把包含参数poll_table的vhost_poll的poll->wait加入到参数的wqh (wait_queue_head_t)
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt)
 {
@@ -158,12 +172,23 @@ static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 
 	poll = container_of(pt, struct vhost_poll, table);
 	poll->wqh = wqh;
+	/*
+	 * 把&poll->wait加入到wqh
+	 */
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|206| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|234| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
+	/*
+	 * 对于vhost-scsi来说, 似乎是每一个queue一个fd (wait)??
+	 */
 	struct vhost_poll *poll = container_of(wait, struct vhost_poll, wait);
 	struct vhost_work *work = &poll->work;
 
@@ -178,6 +203,16 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1616| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/scsi.c|1617| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/vhost.c|212| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|261| <<vhost_work_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vhost.c|563| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ *   - drivers/vhost/vhost.h|39| <<vhost_attach_cgroups>> void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn);
+ *   - drivers/vhost/vsock.c|640| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -186,10 +221,42 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
 /* Init poll structure */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1343| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);
+ *   - drivers/vhost/net.c|1344| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);
+ *   - drivers/vhost/vhost.c|509| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick, EPOLLIN, dev);
+ */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev)
 {
+	/*
+	 * 设置poll->wait.func = vhost_poll_wakeup()
+	 */
 	init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	/*
+	 * socket : tun_chr_poll()
+	 * eventfd: eventfd_poll()
+	 *
+	 * poll_wait()就是调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * vhost_poll_func()把&poll->wait加入到fd的wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以到时候waitqueue唤醒的时候都是调用vhost_poll_wakeup()
+	 *
+	 * vfs_poll()调用file->f_op->poll()
+	 *
+	 * 调用_qproc的地方:
+	 *   - include/linux/poll.h|51| <<poll_wait>> p->_qproc(filp, wait_address, p);
+	 *
+	 * poll_wait()的时候会调用p->_qproc=vhost_poll_func()
+	 *
+	 * 设置poll->table._qproc = vhost_poll_func()
+	 * 这个vhost_poll_func()是把包含参数poll_table的vhost_poll的poll->wait加入到参数的wqh (wait_queue_head_t)
+	 */
 	init_poll_funcptr(&poll->table, vhost_poll_func);
 	poll->mask = mask;
 	poll->dev = dev;
@@ -201,6 +268,13 @@ EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/test.c|299| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|1709| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.h|45| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	__poll_t mask;
@@ -208,6 +282,21 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (poll->wqh)
 		return 0;
 
+	/*
+	 * socket : tun_chr_poll()
+	 * eventfd: eventfd_poll()
+	 *
+	 * poll_wait()就是调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * vhost_poll_func()把&poll->wait加入到fd的wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以到时候waitqueue唤醒的时候都是调用vhost_poll_wakeup()
+	 *
+	 * vfs_poll()调用file->f_op->poll()
+	 */
 	mask = vfs_poll(file, &poll->table);
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
@@ -222,6 +311,14 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/test.c|292| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+ *   - drivers/vhost/vhost.c|301| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|781| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1822| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
 	if (poll->wqh) {
@@ -253,6 +350,16 @@ void vhost_poll_flush(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_flush);
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|371| <<vhost_scsi_complete_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1315| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|284| <<vhost_work_flush>> vhost_work_queue(dev, &flush.work);
+ *   - drivers/vhost/vhost.c|323| <<vhost_poll_queue>> vhost_work_queue(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|585| <<vhost_attach_cgroups>> vhost_work_queue(dev, &attach.work);
+ *   - drivers/vhost/vsock.c|268| <<vhost_transport_send_pkt>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ *   - drivers/vhost/vsock.c|555| <<vhost_vsock_start>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ */
 void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 {
 	if (!dev->worker)
@@ -276,6 +383,21 @@ bool vhost_has_work(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_has_work);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|420| <<vhost_zerocopy_callback>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|516| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|519| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|807| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|899| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1175| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1251| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|198| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|556| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|1229| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vsock.c|231| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|311| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
 	vhost_work_queue(poll->dev, &poll->work);
@@ -298,13 +420,33 @@ static void vhost_vq_meta_reset(struct vhost_dev *d)
 		__vhost_vq_meta_reset(d->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|431| <<vhost_vq_reset>> vhost_vring_call_reset(&vq->call_ctx);
+ */
 static void vhost_vring_call_reset(struct vhost_vring_call *call_ctx)
 {
 	call_ctx->ctx = NULL;
 	memset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer));
+	/*
+	 * 在以下使用vhost_vring_call->ctx_lock:
+	 *   - drivers/vhost/vdpa.c|100| <<vhost_vdpa_setup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|103| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|110| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_unsetup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|119| <<vhost_vdpa_unsetup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_vring_call_reset>> spin_lock_init(&call_ctx->ctx_lock);
+	 *   - drivers/vhost/vhost.c|1782| <<vhost_vring_ioctl>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|1784| <<vhost_vring_ioctl>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 */
 	spin_lock_init(&call_ctx->ctx_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|630| <<vhost_dev_init>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|865| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -336,6 +478,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	__vhost_vq_meta_reset(vq);
 }
 
+/*
+ * 在以下创建vhost_worker():
+ *   - drivers/vhost/vhost.c|710| <<vhost_dev_set_owner>> worker = kthread_create(vhost_worker, dev,
+ */
 static int vhost_worker(void *data)
 {
 	struct vhost_dev *dev = data;
@@ -459,6 +605,14 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1338| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
+ *   - drivers/vhost/scsi.c|1630| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, UIO_MAXIOV,
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+ *   - drivers/vhost/vdpa.c|820| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,
+ *   - drivers/vhost/vsock.c|633| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -497,6 +651,19 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
 		vhost_vq_reset(dev, vq);
+		/*
+		 * &vq->poll用来poll在eventfd或者socket上
+		 *
+		 * struct vhost_dev *dev:
+		 * -> struct vhost_virtqueue **vqs;
+		 *    -> struct vhost_poll poll;
+		 *        -> poll_table                table;
+		 *        -> wait_queue_head_t        *wqh;
+		 *        -> wait_queue_entry_t        wait;
+		 *        -> struct vhost_work         work;
+		 *        -> __poll_t                  mask;
+		 *        -> struct vhost_dev         *dev;
+		 */
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
 					EPOLLIN, dev);
@@ -636,6 +803,11 @@ struct vhost_iotlb *vhost_dev_reset_owner_prepare(void)
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1659| <<vhost_net_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ *   - drivers/vhost/test.c|242| <<vhost_test_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ */
 void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 {
 	int i;
@@ -683,6 +855,25 @@ static void vhost_clear_msg(struct vhost_dev *dev)
 	spin_unlock(&dev->iotlb_lock);
 }
 
+/*
+ * 每一个vhost的queue(内核线程)都会调用一次
+ * [0] vhost_dev_cleanup
+ * [0] vhost_net_release
+ * [0] __fput
+ * [0] task_work_run
+ * [0] exit_to_user_mode_prepare
+ * [0] syscall_exit_to_user_mode
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/vhost/net.c|1451| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/scsi.c|1654| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+ *   - drivers/vhost/test.c|165| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/vdpa.c|838| <<vhost_vdpa_open>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vdpa.c|870| <<vhost_vdpa_release>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vhost.c|787| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+ *   - drivers/vhost/vsock.c|715| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev)
 {
 	int i;
@@ -1588,6 +1779,9 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 		return -ENOBUFS;
 
 	idx = array_index_nospec(idx, d->nvqs);
+	/*
+	 * vq的类型是struct vhost_virtqueue *vq;
+	 */
 	vq = d->vqs[idx];
 
 	if (ioctl == VHOST_SET_VRING_NUM ||
@@ -1624,10 +1818,18 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 			r = -EFAULT;
 		break;
 	case VHOST_SET_VRING_KICK:
+		/*
+		 * struct vhost_vring_file f:
+		 *  -> unsigned int index;
+		 *  -> int fd; // Pass -1 to unbind from file.
+		 */
 		if (copy_from_user(&f, argp, sizeof f)) {
 			r = -EFAULT;
 			break;
 		}
+		/*
+		 * 如果(file->f_op != &eventfd_fops), eventfd_fget()会返回-EINVAL
+		 */
 		eventfp = f.fd == VHOST_FILE_UNBIND ? NULL : eventfd_fget(f.fd);
 		if (IS_ERR(eventfp)) {
 			r = PTR_ERR(eventfp);
@@ -1650,6 +1852,17 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_vring_call->ctx_lock:
+		 *   - drivers/vhost/vdpa.c|100| <<vhost_vdpa_setup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|103| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|110| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_unsetup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|119| <<vhost_vdpa_unsetup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vhost.c|401| <<vhost_vring_call_reset>> spin_lock_init(&call_ctx->ctx_lock);
+		 *   - drivers/vhost/vhost.c|1782| <<vhost_vring_ioctl>> spin_lock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vhost.c|1784| <<vhost_vring_ioctl>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 */
 		spin_lock(&vq->call_ctx.ctx_lock);
 		swap(ctx, vq->call_ctx.ctx);
 		spin_unlock(&vq->call_ctx.ctx_lock);
@@ -2417,6 +2630,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2590| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2455,9 +2672,22 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|565| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2601| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2611| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|225| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|504| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
+	/*
+	 * struct vhost_virtqueue *vq:
+	 *  -> struct file *kick;
+	 *  -> struct vhost_vring_call call_ctx;
+	 */
 	if (vq->call_ctx.ctx && vhost_notify(dev, vq))
 		eventfd_signal(vq->call_ctx.ctx, 1);
 }
@@ -2502,6 +2732,20 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
 /* OK, now we need to know about added descriptors. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|512| <<vhost_net_busy_poll_try_queue>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|564| <<vhost_net_busy_poll>> vhost_enable_notify(&net->dev, rvq);
+ *   - drivers/vhost/net.c|803| <<handle_tx_copy>> } else if (unlikely(vhost_enable_notify(&net->dev,
+ *   - drivers/vhost/net.c|895| <<handle_tx_zerocopy>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1171| <<handle_rx>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|470| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|831| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|70| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|111| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|137| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|470| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
@@ -2540,6 +2784,26 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
 /* We don't need to be notified again. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|513| <<vhost_net_busy_poll_try_queue>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|536| <<vhost_net_busy_poll>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|805| <<handle_tx_copy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|896| <<handle_tx_zerocopy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|975| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1144| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1174| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|461| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|832| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|939| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1192| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|58| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|71| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|98| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|138| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|452| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|471| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 9032d3c2a9f4..bb0a13ae7ddd 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -64,6 +64,17 @@ enum vhost_uaddr_type {
 struct vhost_vring_call {
 	struct eventfd_ctx *ctx;
 	struct irq_bypass_producer producer;
+	/*
+	 * 在以下使用vhost_vring_call->ctx_lock:
+	 *   - drivers/vhost/vdpa.c|100| <<vhost_vdpa_setup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|103| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|110| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_unsetup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|119| <<vhost_vdpa_unsetup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_vring_call_reset>> spin_lock_init(&call_ctx->ctx_lock);
+	 *   - drivers/vhost/vhost.c|1782| <<vhost_vring_ioctl>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|1784| <<vhost_vring_ioctl>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 */
 	spinlock_t ctx_lock;
 };
 
@@ -149,7 +160,34 @@ struct vhost_dev {
 	struct vhost_virtqueue **vqs;
 	int nvqs;
 	struct eventfd_ctx *log_ctx;
+	/*
+	 * 在以下使用vhost_dev->work_list:
+	 *   - drivers/vhost/vhost.c|370| <<vhost_work_queue>> llist_add(&work->node, &dev->work_list);
+	 *   - drivers/vhost/vhost.c|379| <<vhost_has_work>> return !llist_empty(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|484| <<vhost_worker>> node = llist_del_all(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|621| <<vhost_dev_init>> init_llist_head(&dev->work_list);
+	 *   - drivers/vhost/vhost.c|883| <<vhost_dev_cleanup>> WARN_ON(!llist_empty(&dev->work_list));
+	 */
 	struct llist_head work_list;
+	/*
+	 * 在以下使用vhost_dev->worker:
+	 *   - drivers/vhost/vhost.c|332| <<vhost_work_flush>> if (dev->worker) {
+	 *   - drivers/vhost/vhost.c|362| <<vhost_work_queue>> if (!dev->worker)
+	 *   - drivers/vhost/vhost.c|371| <<vhost_work_queue>> wake_up_process(dev->worker);
+	 *   - drivers/vhost/vhost.c|615| <<vhost_dev_init>> dev->worker = NULL;
+	 *   - drivers/vhost/vhost.c|729| <<vhost_dev_set_owner>> struct task_struct *worker;
+	 *   - drivers/vhost/vhost.c|742| <<vhost_dev_set_owner>> worker = kthread_create(vhost_worker, dev,
+	 *   - drivers/vhost/vhost.c|744| <<vhost_dev_set_owner>> if (IS_ERR(worker)) {
+	 *   - drivers/vhost/vhost.c|745| <<vhost_dev_set_owner>> err = PTR_ERR(worker);
+	 *   - drivers/vhost/vhost.c|749| <<vhost_dev_set_owner>> dev->worker = worker;
+	 *   - drivers/vhost/vhost.c|750| <<vhost_dev_set_owner>> wake_up_process(worker);
+	 *   - drivers/vhost/vhost.c|763| <<vhost_dev_set_owner>> if (dev->worker) {
+	 *   - drivers/vhost/vhost.c|764| <<vhost_dev_set_owner>> kthread_stop(dev->worker);
+	 *   - drivers/vhost/vhost.c|765| <<vhost_dev_set_owner>> dev->worker = NULL;
+	 *   - drivers/vhost/vhost.c|884| <<vhost_dev_cleanup>> if (dev->worker) {
+	 *   - drivers/vhost/vhost.c|885| <<vhost_dev_cleanup>> kthread_stop(dev->worker);
+	 *   - drivers/vhost/vhost.c|886| <<vhost_dev_cleanup>> dev->worker = NULL;
+	 */
 	struct task_struct *worker;
 	struct vhost_iotlb *umem;
 	struct vhost_iotlb *iotlb;
diff --git a/fs/eventfd.c b/fs/eventfd.c
index df466ef81ddd..38539c12f267 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -59,6 +59,17 @@ struct eventfd_ctx {
  * Returns the amount by which the counter was incremented.  This will be less
  * than @n if the counter has overflowed.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vdpa.c|73| <<vhost_vdpa_virtqueue_cb>> eventfd_signal(call_ctx, 1);
+ *   - drivers/vhost/vdpa.c|84| <<vhost_vdpa_config_cb>> eventfd_signal(config_ctx, 1);
+ *   - drivers/vhost/vhost.c|2142| <<vhost_log_write>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2165| <<vhost_update_used_flags>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2183| <<vhost_update_avail_event>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2609| <<vhost_add_used_n>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2674| <<vhost_signal>> eventfd_signal(vq->call_ctx.ctx, 1);
+ *   - drivers/vhost/vhost.h|263| <<vq_err>> eventfd_signal((vq)->error_ctx, 1);\
+ */
 __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 {
 	unsigned long flags;
diff --git a/include/linux/blk-cgroup.h b/include/linux/blk-cgroup.h
index c8fc9792ac77..7c922a21d405 100644
--- a/include/linux/blk-cgroup.h
+++ b/include/linux/blk-cgroup.h
@@ -124,6 +124,14 @@ struct blkcg_gq {
 	struct blkg_policy_data		*pd[BLKCG_MAX_POLS];
 
 	spinlock_t			async_bio_lock;
+	/*
+	 * 在以下使用blkcg_gq->async_bios:
+	 *   - block/blk-cgroup.c|100| <<__blkg_release>> WARN_ON(!bio_list_empty(&blkg->async_bios));
+	 *   - block/blk-cgroup.c|133| <<blkg_async_bio_workfn>> bio_list_merge(&bios, &blkg->async_bios);
+	 *   - block/blk-cgroup.c|134| <<blkg_async_bio_workfn>> bio_list_init(&blkg->async_bios);
+	 *   - block/blk-cgroup.c|170| <<blkg_alloc>> bio_list_init(&blkg->async_bios);
+	 *   - block/blk-cgroup.c|1594| <<__blkcg_punt_bio_submit>> bio_list_add(&blkg->async_bios, bio);
+	 */
 	struct bio_list			async_bios;
 	struct work_struct		async_bio_work;
 
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9d2d5ad367a4..247636a0cb3b 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -35,6 +35,14 @@ struct blk_mq_hw_ctx {
 	/**
 	 * @run_work: Used for scheduling a hardware queue run at a later time.
 	 */
+	/*
+	 * 在以下使用blk_mq_hw_ctx->blk_mq_run_work_fn:
+	 *   - block/blk-mq-sysfs.c|39| <<blk_mq_hw_sysfs_release>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|1839| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|1957| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq.c|2024| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 *   - block/blk-mq.c|2948| <<blk_mq_alloc_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 */
 	struct delayed_work	run_work;
 	/** @cpumask: Map of available CPUs where this hctx can run. */
 	cpumask_var_t		cpumask;
@@ -74,6 +82,22 @@ struct blk_mq_hw_ctx {
 	 * @ctx_map: Bitmap for each software queue. If bit is on, there is a
 	 * pending request in that software queue.
 	 */
+	/*
+	 * 在以下使用blk_mq_hw_ctx->ctx_map, Bitmap for each software queue. If bit is on, there is a
+	 * pending request in that software queue:
+	 *   - block/blk-mq-debugfs.c|445| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|253| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq-sysfs.c|44| <<blk_mq_hw_sysfs_release>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|81| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|93| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|94| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|102| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|1077| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1115| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2747| <<blk_mq_alloc_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2767| <<blk_mq_alloc_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2930| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 */
 	struct sbitmap		ctx_map;
 
 	/**
@@ -253,6 +277,21 @@ struct blk_mq_tag_set {
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 在以下使用blk_mq_tag_set->tag_list
+	 *   - block/blk-mq.c|2911| <<blk_mq_update_tag_set_depth>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|2924| <<blk_mq_del_queue_tag_set>> if (list_is_singular(&set->tag_list)) {
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> if (!list_empty(&set->tag_list) &&
+	 *   - block/blk-mq.c|2950| <<blk_mq_add_queue_tag_set>> list_add_tail(&q->tag_set_list, &set->tag_list);
+	 *   - block/blk-mq.c|3452| <<blk_mq_alloc_tag_set>> INIT_LIST_HEAD(&set->tag_list);
+	 *   - block/blk-mq.c|3615| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|3622| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|3626| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3639| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3652| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3658| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|3661| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 */
 	struct list_head	tag_list;
 };
 
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 868e11face00..1fecf81c8600 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -245,6 +245,11 @@ struct request {
 	void *end_io_data;
 };
 
+/*
+ * called by:
+ *   - include/linux/blkdev.h|260| <<blk_rq_is_scsi>> return blk_op_is_scsi(req_op(rq));
+ *   - include/linux/blkdev.h|277| <<bio_is_passthrough>> return blk_op_is_scsi(op) || blk_op_is_private(op);
+ */
 static inline bool blk_op_is_scsi(unsigned int op)
 {
 	return op == REQ_OP_SCSI_IN || op == REQ_OP_SCSI_OUT;
@@ -562,6 +567,17 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	/*
+	 * 在以下使用request_queue->mq_freeze_wq:
+	 *   - block/blk-core.c|318| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|353| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|462| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|512| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|575| <<blk_alloc_queue>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|157| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|164| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|204| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_queue_head_t	mq_freeze_wq;
 	/*
 	 * Protect concurrent access to q_usage_counter by
diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index c25b8e41c0ea..b6ffc31be796 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -589,6 +589,16 @@ extern enum system_states {
 #define TAINT_CPU_OUT_OF_SPEC		2
 #define TAINT_FORCED_RMMOD		3
 #define TAINT_MACHINE_CHECK		4
+/*
+ * 在以下使用TAINT_BAD_PAGE:
+ *   - kernel/panic.c|385| <<global>> [ TAINT_BAD_PAGE ] = { 'B', ' ', false },
+ *   - mm/filemap.c|181| <<unaccount_page_cache_page>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+ *   - mm/kasan/report.c|94| <<end_report>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+ *   - mm/memory.c|550| <<print_bad_pte>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+ *   - mm/page_alloc.c|641| <<bad_page>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+ *   - mm/slab.c|444| <<__slab_error>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+ *   - mm/slub.c|658| <<slab_bug>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+ */
 #define TAINT_BAD_PAGE			5
 #define TAINT_USER			6
 #define TAINT_DIE			7
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 05e3c2fb3ef7..dd8b1a14e444 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -498,6 +498,17 @@ struct kvm {
 	long tlbs_dirty;
 	struct list_head devices;
 	u64 manual_dirty_log_protect;
+	/*
+	 * 在以下使用kvm->debugfs_dentry:
+	 *   - arch/arm64/kvm/vgic/vgic-debug.c|294| <<vgic_debug_init>> debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
+	 *   - virt/kvm/kvm_main.c|677| <<kvm_destroy_vm_debugfs>> if (!kvm->debugfs_dentry)
+	 *   - virt/kvm/kvm_main.c|680| <<kvm_destroy_vm_debugfs>> debugfs_remove_recursive(kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|716| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry, stat_data,
+	 *   - virt/kvm/kvm_main.c|3077| <<kvm_create_vcpu_debugfs>> vcpu->kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|4640| <<kvm_uevent_notify_change>> if (!IS_ERR_OR_NULL(kvm->debugfs_dentry)) {
+	 *   - virt/kvm/kvm_main.c|4644| <<kvm_uevent_notify_change>> tmp = dentry_path_raw(kvm->debugfs_dentry, p, PATH_MAX);
+	 */
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
diff --git a/include/linux/overflow.h b/include/linux/overflow.h
index 93fcef105061..4784a17c570e 100644
--- a/include/linux/overflow.h
+++ b/include/linux/overflow.h
@@ -311,6 +311,15 @@ static inline __must_check size_t __ab_c_size(size_t a, size_t b, size_t c)
  *
  * Return: number of bytes needed or SIZE_MAX on overflow.
  */
+/*
+ * 关于__ab_c_size():
+ * Compute a*b+c, returning SIZE_MAX on overflow. Internal helper for
+ * struct_size() below.
+ *
+ * a: count
+ * b: sizeof(*(p)->member) + __must_be_array((p)->member)
+ * c: sizeof(*(p))
+ */
 #define struct_size(p, member, count)					\
 	__ab_c_size(count,						\
 		    sizeof(*(p)->member) + __must_be_array((p)->member),\
diff --git a/include/linux/poll.h b/include/linux/poll.h
index 1cdc32b1f1b0..9d24cdd6d56f 100644
--- a/include/linux/poll.h
+++ b/include/linux/poll.h
@@ -45,6 +45,9 @@ typedef struct poll_table_struct {
 	__poll_t _key;
 } poll_table;
 
+/*
+ * vhost的_qproc的例子vhost_poll_func()
+ */
 static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
 {
 	if (p && p->_qproc && wait_address)
@@ -74,6 +77,10 @@ static inline __poll_t poll_requested_events(const poll_table *p)
 
 static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)
 {
+	/*
+	 * 调用_qproc的地方:
+	 *   - include/linux/poll.h|51| <<poll_wait>> p->_qproc(filp, wait_address, p);
+	 */
 	pt->_qproc = qproc;
 	pt->_key   = ~(__poll_t)0; /* all events enabled */
 }
diff --git a/include/linux/sched.h b/include/linux/sched.h
index afe01e232935..1e262a89a0e6 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -781,6 +781,23 @@ struct task_struct {
 
 	/* Bit to tell LSMs we're in execve(): */
 	unsigned			in_execve:1;
+	/*
+	 * 在以下使用task_struct->in_iowait:
+	 *   - kernel/sched/core.c|2895| <<try_to_wake_up>> if (p->in_iowait) {
+	 *   - kernel/sched/core.c|4487| <<__schedule>> if (prev->in_iowait) {
+	 *   - kernel/sched/core.c|6261| <<io_schedule_prepare>> int old_iowait = current->in_iowait;
+	 *   - kernel/sched/core.c|6263| <<io_schedule_prepare>> current->in_iowait = 1;
+	 *   - kernel/sched/core.c|6271| <<io_schedule_finish>> current->in_iowait = token;
+	 *   - kernel/sched/fair.c|977| <<update_stats_enqueue_sleeper>> if (tsk->in_iowait) {
+	 *   - kernel/sched/fair.c|5490| <<enqueue_task_fair>> if (p->in_iowait)
+	 *   - kernel/sched/psi.c|986| <<cgroup_move_task>> } else if (task->in_iowait)
+	 *   - kernel/sched/stats.h|78| <<psi_enqueue>> if (p->in_iowait)
+	 *   - kernel/sched/stats.h|104| <<psi_dequeue>> if (p->in_iowait)
+	 *   - kernel/sched/stats.h|120| <<psi_ttwu_dequeue>> if (unlikely(p->in_iowait || p->in_memstall)) {
+	 *   - kernel/sched/stats.h|125| <<psi_ttwu_dequeue>> if (p->in_iowait)
+	 *   - lib/test_lockup.c|364| <<test_lockup>> current->in_iowait = 1;
+	 *   - lib/test_lockup.c|369| <<test_lockup>> current->in_iowait = 0;
+	 */
 	unsigned			in_iowait:1;
 #ifndef TIF_RESTORE_SIGMASK
 	unsigned			restore_sigmask:1;
diff --git a/include/scsi/scsi_device.h b/include/scsi/scsi_device.h
index bc5909033d13..9f1d7fd706fa 100644
--- a/include/scsi/scsi_device.h
+++ b/include/scsi/scsi_device.h
@@ -443,6 +443,26 @@ extern int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
 			int timeout, int retries, u64 flags,
 			req_flags_t rq_flags, int *resid);
 /* Make sure any sense buffer is the correct size. */
+/*
+ * called by:
+ *   - drivers/ata/libata-scsi.c|409| <<ata_cmd_ioctl>> cmd_result = scsi_execute(scsidev, scsi_cmd, data_dir, argbuf, argsize,
+ *   - drivers/ata/libata-scsi.c|490| <<ata_task_ioctl>> cmd_result = scsi_execute(scsidev, scsi_cmd, DMA_NONE, NULL, 0,
+ *   - drivers/scsi/cxlflash/superpipe.c|360| <<read_cap16>> result = scsi_execute(sdev, scsi_cmd, DMA_FROM_DEVICE, cmd_buf,
+ *   - drivers/scsi/cxlflash/vlun.c|453| <<write_same16>> result = scsi_execute(sdev, scsi_cmd, DMA_TO_DEVICE, cmd_buf,
+ *   - drivers/scsi/device_handler/scsi_dh_alua.c|141| <<submit_rtpg>> return scsi_execute(sdev, cdb, DMA_FROM_DEVICE, buff, bufflen, NULL,
+ *   - drivers/scsi/device_handler/scsi_dh_alua.c|173| <<submit_stpg>> return scsi_execute(sdev, cdb, DMA_TO_DEVICE, stpg_data, stpg_len, NULL,
+ *   - drivers/scsi/device_handler/scsi_dh_emc.c|266| <<send_trespass_cmd>> err = scsi_execute(sdev, cdb, DMA_TO_DEVICE, csdev->buffer, len, NULL,
+ *   - drivers/scsi/device_handler/scsi_dh_hp_sw.c|90| <<hp_sw_tur>> res = scsi_execute(sdev, cmd, DMA_NONE, NULL, 0, NULL, &sshdr,
+ *   - drivers/scsi/device_handler/scsi_dh_hp_sw.c|128| <<hp_sw_start_stop>> res = scsi_execute(sdev, cmd, DMA_NONE, NULL, 0, NULL, &sshdr,
+ *   - drivers/scsi/device_handler/scsi_dh_rdac.c|558| <<send_mode_select>> if (scsi_execute(sdev, cdb, DMA_TO_DEVICE, &h->ctlr->mode_select,
+ *   - drivers/scsi/scsi_transport_spi.c|120| <<spi_execute>> result = scsi_execute(sdev, cmd, dir, buffer, bufflen, sense,
+ *   - drivers/scsi/sd.c|1674| <<sd_sync_cache>> res = scsi_execute(sdp, cmd, DMA_NONE, NULL, 0, NULL, sshdr,
+ *   - drivers/scsi/sd.c|3577| <<sd_start_stop_device>> res = scsi_execute(sdp, cmd, DMA_NONE, NULL, 0, NULL, &sshdr,
+ *   - drivers/scsi/sr_ioctl.c|203| <<sr_do_ioctl>> result = scsi_execute(SDev, cgc->cmd, cgc->data_direction,
+ *   - drivers/scsi/ufs/ufshcd.c|7980| <<ufshcd_send_request_sense>> ret = scsi_execute(sdp, cmd, DMA_FROM_DEVICE, buffer,
+ *   - drivers/scsi/ufs/ufshcd.c|8047| <<ufshcd_set_dev_pwr_mode>> ret = scsi_execute(sdp, cmd, DMA_NONE, NULL, 0, NULL, &sshdr,
+ *   - include/scsi/scsi_device.h|460| <<scsi_execute_req>> return scsi_execute(sdev, cmd, data_direction, buffer,
+ */
 #define scsi_execute(sdev, cmd, data_direction, buffer, bufflen, sense,	\
 		     sshdr, timeout, retries, flags, rq_flags, resid)	\
 ({									\
@@ -452,6 +472,35 @@ extern int __scsi_execute(struct scsi_device *sdev, const unsigned char *cmd,
 		       sense, sshdr, timeout, retries, flags, rq_flags,	\
 		       resid);						\
 })
+/*
+ * called by:
+ *   - drivers/hwmon/drivetemp.c|195| <<drivetemp_scsi_command>> return scsi_execute_req(st->sdev, scsi_cmd, data_dir,
+ *   - drivers/scsi/ch.c|198| <<ch_do_scsi>> result = scsi_execute_req(ch->device, cmd, direction, buffer,
+ *   - drivers/scsi/scsi.c|338| <<scsi_vpd_inquiry>> result = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buffer,
+ *   - drivers/scsi/scsi.c|523| <<scsi_report_opcode>> result = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buffer, len,
+ *   - drivers/scsi/scsi_ioctl.c|98| <<ioctl_internal_command>> result = scsi_execute_req(sdev, cmd, DMA_NONE, NULL, 0,
+ *   - drivers/scsi/scsi_lib.c|2059| <<scsi_mode_select>> ret = scsi_execute_req(sdev, cmd, DMA_TO_DEVICE, real_buffer, len,
+ *   - drivers/scsi/scsi_lib.c|2126| <<scsi_mode_sense>> result = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buffer, len,
+ *   - drivers/scsi/scsi_lib.c|2204| <<scsi_test_unit_ready>> result = scsi_execute_req(sdev, cmd, DMA_NONE, NULL, 0, sshdr,
+ *   - drivers/scsi/scsi_scan.c|242| <<scsi_unlock_floptical>> scsi_execute_req(sdev, scsi_cmd, DMA_FROM_DEVICE, result, 0x2a, NULL,
+ *   - drivers/scsi/scsi_scan.c|649| <<scsi_probe_lun>> result = scsi_execute_req(sdev, scsi_cmd, DMA_FROM_DEVICE,
+ *   - drivers/scsi/scsi_scan.c|1445| <<scsi_report_lun_scan>> result = scsi_execute_req(sdev, scsi_cmd, DMA_FROM_DEVICE,
+ *   - drivers/scsi/sd.c|683| <<sd_sec_submit>> ret = scsi_execute_req(sdev, cdb,
+ *   - drivers/scsi/sd.c|1785| <<sd_pr_command>> result = scsi_execute_req(sdev, cmd, DMA_TO_DEVICE, &data, sizeof(data),
+ *   - drivers/scsi/sd.c|2124| <<sd_spinup_disk>> the_result = scsi_execute_req(sdkp->device, cmd,
+ *   - drivers/scsi/sd.c|2181| <<sd_spinup_disk>> scsi_execute_req(sdkp->device, cmd, DMA_NONE,
+ *   - drivers/scsi/sd.c|2323| <<read_capacity_16>> the_result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,
+ *   - drivers/scsi/sd.c|2408| <<read_capacity_10>> the_result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,
+ *   - drivers/scsi/sd_zbc.c|112| <<sd_zbc_do_report_zones>> result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,
+ *   - drivers/scsi/ses.c|91| <<ses_recv_diag>> ret = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buf, bufflen,
+ *   - drivers/scsi/ses.c|125| <<ses_send_diag>> result = scsi_execute_req(sdev, cmd, DMA_TO_DEVICE, buf, bufflen,
+ *   - drivers/scsi/sr.c|208| <<sr_get_events>> result = scsi_execute_req(sdev, cmd, DMA_FROM_DEVICE, buf, sizeof(buf),
+ *   - drivers/scsi/sr.c|842| <<get_sectorsize>> the_result = scsi_execute_req(cd->device, cmd, DMA_FROM_DEVICE,
+ *   - drivers/scsi/virtio_scsi.c|476| <<virtscsi_rescan_hotunplug>> result = scsi_execute_req(sdev, scsi_cmd, DMA_FROM_DEVICE,
+ *   - drivers/target/target_core_pscsi.c|150| <<pscsi_tape_read_blocksize>> ret = scsi_execute_req(sdev, cdb, DMA_FROM_DEVICE, buf, 12, NULL,
+ *   - drivers/target/target_core_pscsi.c|201| <<pscsi_get_inquiry_vpd_serial>> ret = scsi_execute_req(sdev, cdb, DMA_FROM_DEVICE, buf,
+ *   - drivers/target/target_core_pscsi.c|236| <<pscsi_get_inquiry_vpd_device_ident>> ret = scsi_execute_req(sdev, cdb, DMA_FROM_DEVICE, buf,
+ */
 static inline int scsi_execute_req(struct scsi_device *sdev,
 	const unsigned char *cmd, int data_direction, void *buffer,
 	unsigned bufflen, struct scsi_sense_hdr *sshdr, int timeout,
diff --git a/include/scsi/scsi_host.h b/include/scsi/scsi_host.h
index 46ef8cccc982..ec19271c48e5 100644
--- a/include/scsi/scsi_host.h
+++ b/include/scsi/scsi_host.h
@@ -527,7 +527,26 @@ struct Scsi_Host {
 	 * their __ prefixed variants with the lock held. NEVER
 	 * access this list directly from a driver.
 	 */
+	/*
+	 * 在以下使用Scsi_Host->__devices:
+	 *   - drivers/scsi/hosts.c|387| <<scsi_host_alloc>> INIT_LIST_HEAD(&shost->__devices);
+	 *   - drivers/scsi/scsi.c|557| <<__scsi_iterate_devices>> struct list_head *list = (prev ? &prev->siblings : &shost->__devices);
+	 *   - drivers/scsi/scsi.c|562| <<__scsi_iterate_devices>> while (list->next != &shost->__devices) {
+	 *   - drivers/scsi/scsi.c|708| <<__scsi_device_lookup>> list_for_each_entry(sdev, &shost->__devices, siblings) {
+	 *   - drivers/scsi/scsi_scan.c|1867| <<scsi_forget_host>> list_for_each_entry(sdev, &shost->__devices, siblings) {
+	 *   - drivers/scsi/scsi_sysfs.c|1480| <<__scsi_remove_target>> list_for_each_entry(sdev, &shost->__devices, siblings) {
+	 *   - drivers/scsi/scsi_sysfs.c|1618| <<scsi_sysfs_device_initialize>> list_add_tail(&sdev->siblings, &shost->__devices);
+	 *   - drivers/target/target_core_pscsi.c|499| <<pscsi_configure_device>> list_for_each_entry(sd, &sh->__devices, siblings) {
+	 *   - include/scsi/scsi_device.h|396| <<__shost_for_each_device>> list_for_each_entry((sdev), &((shost)->__devices), siblings)
+	 */
 	struct list_head	__devices;
+	/*
+	 * 在以下使用Scsi_Host->__targets:
+	 *   - drivers/scsi/hosts.c|388| <<scsi_host_alloc>> INIT_LIST_HEAD(&shost->__targets);
+	 *   - drivers/scsi/scsi_scan.c|352| <<__scsi_find_target>> list_for_each_entry(starget, &shost->__targets, siblings) {
+	 *   - drivers/scsi/scsi_scan.c|448| <<scsi_alloc_target>> list_add_tail(&starget->siblings, &shost->__targets);
+	 *   - drivers/scsi/scsi_sysfs.c|1519| <<scsi_remove_target>> list_for_each_entry(starget, &shost->__targets, siblings) {
+	 */
 	struct list_head	__targets;
 	
 	struct list_head	starved_list;
@@ -626,6 +645,17 @@ struct Scsi_Host {
 	unsigned tmf_in_progress:1;
 
 	/* Asynchronous scan in progress */
+	/*
+	 * 在以下使用Scsi_Host->async_scan:
+	 *   - drivers/scsi/scsi_scan.c|1184| <<scsi_probe_and_add_lun>> res = scsi_add_lun(sdev, result, &bflags, shost->async_scan);
+	 *   - drivers/scsi/scsi_scan.c|1503| <<__scsi_add_device>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1642| <<scsi_scan_target>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1699| <<scsi_scan_host_selected>> if (!shost->async_scan)
+	 *   - drivers/scsi/scsi_scan.c|1755| <<scsi_prep_async_scan>> if (shost->async_scan) {
+	 *   - drivers/scsi/scsi_scan.c|1770| <<scsi_prep_async_scan>> shost->async_scan = 1;
+	 *   - drivers/scsi/scsi_scan.c|1807| <<scsi_finish_async_scan>> if (!shost->async_scan) {
+	 *   - drivers/scsi/scsi_scan.c|1819| <<scsi_finish_async_scan>> shost->async_scan = 0;
+	 */
 	unsigned async_scan:1;
 
 	/* Don't resume host in EH */
@@ -743,6 +773,18 @@ extern const char *scsi_host_state_name(enum scsi_host_state);
 extern void scsi_host_complete_all_commands(struct Scsi_Host *shost,
 					    int status);
 
+/*
+ * 部分调用的例子:
+ *   - drivers/scsi/libiscsi.c|2603| <<iscsi_host_add>> return scsi_add_host(shost, pdev);
+ *   - drivers/scsi/megaraid.c|4463| <<megaraid_probe_one>> error = scsi_add_host(host, &pdev->dev);
+ *   - drivers/scsi/megaraid/megaraid_mbox.c|643| <<megaraid_io_attach>> if (scsi_add_host(host, &adapter->pdev->dev)) {
+ *   - drivers/scsi/megaraid/megaraid_sas_base.c|6814| <<megasas_io_attach>> if (scsi_add_host(host, &instance->pdev->dev)) {
+ *   - drivers/scsi/qla2xxx/qla_os.c|3418| <<qla2x00_probe_one>> ret = scsi_add_host(host, &pdev->dev);
+ *   - drivers/scsi/qla4xxx/ql4_os.c|8730| <<qla4xxx_probe_adapter>> ret = scsi_add_host(host, &pdev->dev);
+ *   - drivers/scsi/virtio_scsi.c|1172| <<virtscsi_probe>> err = scsi_add_host(shost, &vdev->dev);
+ *   - drivers/scsi/vmw_pvscsi.c|1531| <<pvscsi_probe>> error = scsi_add_host(host, &pdev->dev);
+ *   - drivers/target/loopback/tcm_loop.c|353| <<tcm_loop_driver_probe>> error = scsi_add_host(sh, &tl_hba->dev);
+ */
 static inline int __must_check scsi_add_host(struct Scsi_Host *host,
 					     struct device *dev)
 {
diff --git a/include/uapi/linux/virtio_scsi.h b/include/uapi/linux/virtio_scsi.h
index 0abaae4027c0..bb63c182e3da 100644
--- a/include/uapi/linux/virtio_scsi.h
+++ b/include/uapi/linux/virtio_scsi.h
@@ -47,6 +47,11 @@ struct virtio_scsi_cmd_req {
 	__u8 task_attr;		/* Task attribute */
 	__u8 prio;		/* SAM command priority field */
 	__u8 crn;
+	/*
+	 * 在以下使用virtio_scsi_cmd_req->cdb[]:
+	 *   - drivers/scsi/virtio_scsi.c|794| <<virtscsi_queuecommand>> memcpy(cmd->req.cmd_pi.cdb, sc->cmnd, sc->cmd_len);
+	 *   - drivers/scsi/virtio_scsi.c|813| <<virtscsi_queuecommand>> memcpy(cmd->req.cmd.cdb, sc->cmnd, sc->cmd_len);
+	 */
 	__u8 cdb[VIRTIO_SCSI_CDB_SIZE];
 } __attribute__((packed));
 
@@ -104,8 +109,10 @@ struct virtio_scsi_event {
 
 struct virtio_scsi_config {
 	__virtio32 num_queues;
+	/* the maximum number of segments that can be in a command */
 	__virtio32 seg_max;
 	__virtio32 max_sectors;
+	/* the maximum number of linked commands it can send to one LUN */
 	__virtio32 cmd_per_lun;
 	__virtio32 event_info_size;
 	__virtio32 sense_size;
diff --git a/kernel/locking/mutex.c b/kernel/locking/mutex.c
index 5352ce50a97e..6f6381d16601 100644
--- a/kernel/locking/mutex.c
+++ b/kernel/locking/mutex.c
@@ -1348,6 +1348,19 @@ EXPORT_SYMBOL(mutex_lock_killable);
  *
  * Context: Process context.
  */
+/*
+ * called by:
+ *   - fs/jbd2/checkpoint.c|116| <<__jbd2_log_wait_for_space>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/checkpoint.c|291| <<jbd2_log_do_checkpoint>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/commit.c|397| <<jbd2_journal_commit_transaction>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|968| <<jbd2_update_log_tail>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|1343| <<journal_reset>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|1768| <<jbd2_journal_destroy>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|1790| <<jbd2_journal_destroy>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|2036| <<jbd2_journal_flush>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|2046| <<jbd2_journal_flush>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ *   - fs/jbd2/journal.c|2106| <<jbd2_journal_wipe>> mutex_lock_io(&journal->j_checkpoint_mutex);
+ */
 void __sched mutex_lock_io(struct mutex *lock)
 {
 	int token;
diff --git a/kernel/panic.c b/kernel/panic.c
index aef8872ba843..b4e4481be8a6 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -45,6 +45,15 @@ unsigned int __read_mostly sysctl_oops_all_cpu_backtrace;
 #endif /* CONFIG_SMP */
 
 int panic_on_oops = CONFIG_PANIC_ON_OOPS_VALUE;
+/*
+ * 在以下设置和使用tainted_mask:
+ *   - kernel/panic.c|405| <<print_tainted>> if (tainted_mask) {
+ *   - kernel/panic.c|412| <<print_tainted>> *s++ = test_bit(i, &tainted_mask) ?
+ *   - kernel/panic.c|424| <<test_taint>> return test_bit(flag, &tainted_mask);
+ *   - kernel/panic.c|430| <<get_taint>> return tainted_mask;
+ *   - kernel/panic.c|446| <<add_taint>> set_bit(flag, &tainted_mask);
+ *   - kernel/panic.c|448| <<add_taint>> if (tainted_mask & panic_on_taint) {
+ */
 static unsigned long tainted_mask =
 	IS_ENABLED(CONFIG_GCC_PLUGIN_RANDSTRUCT) ? (1 << TAINT_RANDSTRUCT) : 0;
 static int pause_on_oops;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2d95dc3f4644..e6e075e0e54c 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4485,6 +4485,14 @@ static void __sched notrace __schedule(bool preempt)
 			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);
 
 			if (prev->in_iowait) {
+				/*
+				 * 在以下使用rq->nr_iowait:
+				 *   - kernel/sched/core.c|2897| <<try_to_wake_up>> atomic_dec(&task_rq(p)->nr_iowait);
+				 *   - kernel/sched/core.c|3839| <<nr_iowait_cpu>> return atomic_read(&cpu_rq(cpu)->nr_iowait);
+				 *   - kernel/sched/core.c|4488| <<__schedule>> atomic_inc(&rq->nr_iowait);
+				 *   - kernel/sched/core.c|7196| <<sched_init>> atomic_set(&rq->nr_iowait, 0);
+				 *   - kernel/sched/cputime.c|223| <<account_idle_time>> if (atomic_read(&rq->nr_iowait) > 0)
+				 */
 				atomic_inc(&rq->nr_iowait);
 				delayacct_blkio_start();
 			}
@@ -6256,6 +6264,14 @@ int __sched yield_to(struct task_struct *p, bool preempt)
 }
 EXPORT_SYMBOL_GPL(yield_to);
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1705| <<blkcg_maybe_throttle_blkg>> tok = io_schedule_prepare();
+ *   - kernel/locking/mutex.c|1151| <<mutex_lock_io_nested>> token = io_schedule_prepare();
+ *   - kernel/locking/mutex.c|1355| <<mutex_lock_io>> token = io_schedule_prepare();
+ *   - kernel/sched/core.c|6283| <<io_schedule_timeout>> token = io_schedule_prepare();
+ *   - kernel/sched/core.c|6295| <<io_schedule>> token = io_schedule_prepare();
+ */
 int io_schedule_prepare(void)
 {
 	int old_iowait = current->in_iowait;
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index 5a55d2300452..846d71f4e4fa 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -220,6 +220,14 @@ void account_idle_time(u64 cputime)
 	u64 *cpustat = kcpustat_this_cpu->cpustat;
 	struct rq *rq = this_rq();
 
+	/*
+	 * 在以下使用rq->nr_iowait:
+	 *   - kernel/sched/core.c|2897| <<try_to_wake_up>> atomic_dec(&task_rq(p)->nr_iowait);
+	 *   - kernel/sched/core.c|3839| <<nr_iowait_cpu>> return atomic_read(&cpu_rq(cpu)->nr_iowait);
+	 *   - kernel/sched/core.c|4488| <<__schedule>> atomic_inc(&rq->nr_iowait);
+	 *   - kernel/sched/core.c|7196| <<sched_init>> atomic_set(&rq->nr_iowait, 0);
+	 *   - kernel/sched/cputime.c|223| <<account_idle_time>> if (atomic_read(&rq->nr_iowait) > 0)
+	 */
 	if (atomic_read(&rq->nr_iowait) > 0)
 		cpustat[CPUTIME_IOWAIT] += cputime;
 	else
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 28709f6b0975..a1011ff40597 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -959,6 +959,14 @@ struct rq {
 	u64			clock_pelt;
 	unsigned long		lost_idle_time;
 
+	/*
+	 * 在以下使用rq->nr_iowait:
+	 *   - kernel/sched/core.c|2897| <<try_to_wake_up>> atomic_dec(&task_rq(p)->nr_iowait);
+	 *   - kernel/sched/core.c|3839| <<nr_iowait_cpu>> return atomic_read(&cpu_rq(cpu)->nr_iowait);
+	 *   - kernel/sched/core.c|4488| <<__schedule>> atomic_inc(&rq->nr_iowait);
+	 *   - kernel/sched/core.c|7196| <<sched_init>> atomic_set(&rq->nr_iowait, 0);
+	 *   - kernel/sched/cputime.c|223| <<account_idle_time>> if (atomic_read(&rq->nr_iowait) > 0)
+	 */
 	atomic_t		nr_iowait;
 
 #ifdef CONFIG_MEMBARRIER
diff --git a/mm/slub.c b/mm/slub.c
index 6d3574013b2f..4efa85963a1f 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -655,6 +655,16 @@ static void slab_bug(struct kmem_cache *s, char *fmt, ...)
 	pr_err("BUG %s (%s): %pV\n", s->name, print_tainted(), &vaf);
 	pr_err("-----------------------------------------------------------------------------\n\n");
 
+	/*
+	 * 在以下使用TAINT_BAD_PAGE:
+	 *   - kernel/panic.c|385| <<global>> [ TAINT_BAD_PAGE ] = { 'B', ' ', false },
+	 *   - mm/filemap.c|181| <<unaccount_page_cache_page>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+	 *   - mm/kasan/report.c|94| <<end_report>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+	 *   - mm/memory.c|550| <<print_bad_pte>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+	 *   - mm/page_alloc.c|641| <<bad_page>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+	 *   - mm/slab.c|444| <<__slab_error>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+	 *   - mm/slub.c|658| <<slab_bug>> add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
+	 */
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
 	va_end(args);
 }
diff --git a/net/core/dev.c b/net/core/dev.c
index 4906b44af850..61e5d6be7c76 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -5076,6 +5076,23 @@ static inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,
 	return 0;
 }
 
+/*
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb_list_core
+ * [0] netif_receive_skb_list_internal
+ * [0] gro_normal_list.part.170
+ * [0] napi_complete_done
+ * [0] virtqueue_napi_complete
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ */
 static int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,
 				    struct packet_type **ppt_prev)
 {
diff --git a/net/ipv4/devinet.c b/net/ipv4/devinet.c
index 123a6d39438f..23915c1612ab 100644
--- a/net/ipv4/devinet.c
+++ b/net/ipv4/devinet.c
@@ -1502,6 +1502,19 @@ static void inetdev_changename(struct net_device *dev, struct in_device *in_dev)
 	}
 }
 
+/*
+ * [0] inetdev_send_gratuitous_arp
+ * [0] inetdev_event
+ * [0] notifier_call_chain
+ * [0] netdev_notify_peers
+ * [0] netback_changed
+ * [0] xenwatch_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - net/ipv4/devinet.c|1582| <<inetdev_event>> inetdev_send_gratuitous_arp(dev, in_dev);
+ */
 static void inetdev_send_gratuitous_arp(struct net_device *dev,
 					struct in_device *in_dev)
 
diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
index 2b33e977a905..5430def31043 100644
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@ -2159,6 +2159,97 @@ static int packet_rcv(struct sk_buff *skb, struct net_device *dev,
 	return 0;
 }
 
+/*
+ * [0] tpacket_rcv
+ * [0] dev_queue_xmit_nit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] ip_send_skb
+ * [0] icmp_reply.constprop.38
+ * [0] icmp_echo.part.31
+ * [0] icmp_echo
+ * [0] icmp_rcv
+ * [0] ip_protocol_deliver_rcu
+ * [0] ip_local_deliver_finish
+ * [0] ip_local_deliver
+ * [0] ip_sublist_rcv_finish
+ * [0] ip_sublist_rcv
+ * [0] ip_list_rcv
+ * [0] __netif_receive_skb_list_core
+ * [0] netif_receive_skb_list_internal
+ * [0] gro_normal_list.part.170
+ * [0] napi_complete_done
+ * [0] virtqueue_napi_complete
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ * 
+ * [0] tpacket_rcv
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb_list_core
+ * [0] netif_receive_skb_list_internal
+ * [0] gro_normal_list.part.170
+ * [0] napi_complete_done
+ * [0] virtqueue_napi_complete
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ *
+ * [0] tpacket_rcv
+ * [0] dev_queue_xmit_nit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] __ip_queue_xmit
+ * [0] __tcp_transmit_skb
+ * [0] tcp_rcv_established
+ * [0] tcp_v4_do_rcv
+ * [0] __release_sock
+ * [0] release_sock
+ * [0] tcp_recvmsg
+ * [0] inet6_recvmsg
+ * [0] sock_read_iter
+ * [0] new_sync_read
+ * [0] vfs_read
+ * [0] ksys_read
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] tpacket_rcv
+ * [0] dev_queue_xmit_nit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] ip_send_skb
+ * [0] raw_sendmsg
+ * [0] sock_sendmsg
+ * [0] __sys_sendto
+ * [0] __x64_sys_sendto
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,
 		       struct packet_type *pt, struct net_device *orig_dev)
 {
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index cf88233b819a..78734271e2b8 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -108,6 +108,16 @@ static struct kmem_cache *kvm_vcpu_cache;
 static __read_mostly struct preempt_ops kvm_preempt_ops;
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
+/*
+ * 在以下使用kvm_debugfs_dir:
+ *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+ *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+ *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+ *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+ *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+ *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+ *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+ */
 struct dentry *kvm_debugfs_dir;
 EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
 
@@ -696,6 +706,26 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 		return 0;
 
 	snprintf(dir_name, sizeof(dir_name), "%d-%d", task_pid_nr(current), fd);
+	/*
+	 * 在以下使用kvm->debugfs_dentry:
+	 *   - arch/arm64/kvm/vgic/vgic-debug.c|294| <<vgic_debug_init>> debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
+	 *   - virt/kvm/kvm_main.c|677| <<kvm_destroy_vm_debugfs>> if (!kvm->debugfs_dentry)
+	 *   - virt/kvm/kvm_main.c|680| <<kvm_destroy_vm_debugfs>> debugfs_remove_recursive(kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|716| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry, stat_data,
+	 *   - virt/kvm/kvm_main.c|3077| <<kvm_create_vcpu_debugfs>> vcpu->kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|4640| <<kvm_uevent_notify_change>> if (!IS_ERR_OR_NULL(kvm->debugfs_dentry)) {
+	 *   - virt/kvm/kvm_main.c|4644| <<kvm_uevent_notify_change>> tmp = dentry_path_raw(kvm->debugfs_dentry, p, PATH_MAX);
+	 *
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
 
 	kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
@@ -4657,6 +4687,16 @@ static void kvm_init_debug(void)
 {
 	struct kvm_stats_debugfs_item *p;
 
+	/*
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
 
 	kvm_debugfs_num_entries = 0;
@@ -4868,6 +4908,16 @@ EXPORT_SYMBOL_GPL(kvm_init);
 
 void kvm_exit(void)
 {
+	/*
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	debugfs_remove_recursive(kvm_debugfs_dir);
 	misc_deregister(&kvm_dev);
 	kmem_cache_destroy(kvm_vcpu_cache);
-- 
2.17.1

