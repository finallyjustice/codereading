From a65ca14ced4b8057bae9329d758a5abd78bb95e2 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 26 Oct 2020 09:21:57 -0700
Subject: [PATCH 1/1] linux v5.9

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/bfq-iosched.c       |  14 +++
 block/blk-cgroup-rwstat.c |  11 ++
 block/blk-cgroup.c        |  79 ++++++++++++
 block/blk-core.c          |  13 ++
 block/blk-iocost.c        |  47 ++++++++
 block/blk-iolatency.c     |  59 +++++++++
 block/blk-mq-cpumap.c     |  14 +++
 block/blk-mq-debugfs.c    |   3 +
 block/blk-mq-tag.c        | 235 ++++++++++++++++++++++++++++++++++++
 block/blk-mq-tag.h        |  40 +++++++
 block/blk-mq-virtio.c     |   5 +
 block/blk-mq.c            |  14 +++
 block/blk-throttle.c      |  94 +++++++++++++++
 block/elevator.c          |   6 +
 drivers/net/tap.c         |   5 +
 drivers/net/virtio_net.c  |  48 ++++++++
 drivers/vhost/net.c       |  62 ++++++++++
 drivers/vhost/vhost.c     | 246 ++++++++++++++++++++++++++++++++++++++
 drivers/vhost/vhost.h     |  30 +++++
 fs/eventfd.c              |  11 ++
 include/linux/blk-mq.h    |  15 +++
 include/linux/kvm_host.h  |  11 ++
 include/linux/poll.h      |   7 ++
 net/core/dev.c            |  17 +++
 net/packet/af_packet.c    |  91 ++++++++++++++
 virt/kvm/kvm_main.c       |  50 ++++++++
 26 files changed, 1227 insertions(+)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index fa98470df3f0..750660c538ee 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -132,6 +132,13 @@
 #include "bfq-iosched.h"
 #include "blk-wbt.h"
 
+/*
+ * 从block/bfq-iosched.c又分裂出了如下三个文件:
+ * create mode 100644 block/bfq-cgroup.c
+ * create mode 100644 block/bfq-iosched.h
+ * create mode 100644 block/bfq-wf2q.c
+ */
+
 #define BFQ_BFQQ_FNS(name)						\
 void bfq_mark_bfqq_##name(struct bfq_queue *bfqq)			\
 {									\
@@ -6809,6 +6816,13 @@ static int __init bfq_init(void)
 	int ret;
 
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	ret = blkcg_policy_register(&blkcg_policy_bfq);
 	if (ret)
 		return ret;
diff --git a/block/blk-cgroup-rwstat.c b/block/blk-cgroup-rwstat.c
index 85d5790ac49b..8468dacc5982 100644
--- a/block/blk-cgroup-rwstat.c
+++ b/block/blk-cgroup-rwstat.c
@@ -5,6 +5,17 @@
  */
 #include "blk-cgroup-rwstat.h"
 
+/*
+ * called by:
+ *   - block/bfq-cgroup.c|464| <<bfqg_stats_init>> if (blkg_rwstat_init(&stats->bytes, gfp) ||
+ *   - block/bfq-cgroup.c|465| <<bfqg_stats_init>> blkg_rwstat_init(&stats->ios, gfp))
+ *   - block/bfq-cgroup.c|469| <<bfqg_stats_init>> if (blkg_rwstat_init(&stats->merged, gfp) ||
+ *   - block/bfq-cgroup.c|470| <<bfqg_stats_init>> blkg_rwstat_init(&stats->service_time, gfp) ||
+ *   - block/bfq-cgroup.c|471| <<bfqg_stats_init>> blkg_rwstat_init(&stats->wait_time, gfp) ||
+ *   - block/bfq-cgroup.c|472| <<bfqg_stats_init>> blkg_rwstat_init(&stats->queued, gfp) ||
+ *   - block/blk-throttle.c|496| <<throtl_pd_alloc>> if (blkg_rwstat_init(&tg->stat_bytes, gfp))
+ *   - block/blk-throttle.c|499| <<throtl_pd_alloc>> if (blkg_rwstat_init(&tg->stat_ios, gfp))
+ */
 int blkg_rwstat_init(struct blkg_rwstat *rwstat, gfp_t gfp)
 {
 	int i, ret;
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index c195365c9817..464738113f36 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -34,6 +34,14 @@
 
 #define MAX_KEY_LEN 100
 
+/*
+ * 在以下调用blkcg_policy_register():
+ *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+ *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+ *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+ */
+
 /*
  * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation.
  * blkcg_pol_register_mutex nests outside of it and synchronizes entire
@@ -219,6 +227,12 @@ EXPORT_SYMBOL_GPL(blkg_lookup_slowpath);
  * If @new_blkg is %NULL, this function tries to allocate a new one as
  * necessary using %GFP_NOWAIT.  @new_blkg is always consumed on return.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|354| <<blkg_lookup_create>> blkg = blkg_create(pos, q, NULL);
+ *   - block/blk-cgroup.c|663| <<__acquires>> blkg = blkg_create(pos, q, new_blkg);
+ *   - block/blk-cgroup.c|1145| <<blkcg_init_queue>> blkg = blkg_create(&blkcg_root, q, new_blkg);
+ */
 static struct blkcg_gq *blkg_create(struct blkcg *blkcg,
 				    struct request_queue *q,
 				    struct blkcg_gq *new_blkg)
@@ -935,6 +949,9 @@ static struct cftype blkcg_files[] = {
 	{ }	/* terminate */
 };
 
+/*
+ * struct cgroup_subsys io_cgrp_subsys.legacy_cftypes = blkcg_legacy_files[]
+ */
 static struct cftype blkcg_legacy_files[] = {
 	{
 		.name = "reset_stats",
@@ -1127,6 +1144,10 @@ static int blkcg_css_online(struct cgroup_subsys_state *css)
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|574| <<blk_alloc_queue>> if (blkcg_init_queue(q))
+ */
 int blkcg_init_queue(struct request_queue *q)
 {
 	struct blkcg_gq *new_blkg, *blkg;
@@ -1142,6 +1163,12 @@ int blkcg_init_queue(struct request_queue *q)
 	/* Make sure the root blkg exists. */
 	rcu_read_lock();
 	spin_lock_irq(&q->queue_lock);
+	/*
+	 * called by:
+	 *   - block/blk-cgroup.c|354| <<blkg_lookup_create>> blkg = blkg_create(pos, q, NULL);
+	 *   - block/blk-cgroup.c|663| <<__acquires>> blkg = blkg_create(pos, q, new_blkg);
+	 *   - block/blk-cgroup.c|1145| <<blkcg_init_queue>> blkg = blkg_create(&blkcg_root, q, new_blkg);
+	 */
 	blkg = blkg_create(&blkcg_root, q, new_blkg);
 	if (IS_ERR(blkg))
 		goto err_unlock;
@@ -1239,6 +1266,23 @@ static void blkcg_exit(struct task_struct *tsk)
 	tsk->throttle_queue = NULL;
 }
 
+/*
+ * 在以下使用io_cgrp_subsys:
+ *   - block/bfq-cgroup.c|512| <<bfq_cpd_init>> d->weight = cgroup_subsys_on_dfl(io_cgrp_subsys) ?
+ *   - block/bfq-iosched.c|5499| <<bfq_insert_request>> if (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)
+ *   - block/blk-cgroup.c|1472| <<blkcg_policy_register>> WARN_ON(cgroup_add_dfl_cftypes(&io_cgrp_subsys,
+ *   - block/blk-cgroup.c|1475| <<blkcg_policy_register>> WARN_ON(cgroup_add_legacy_cftypes(&io_cgrp_subsys,
+ *   - block/blk-cgroup.c|1919| <<blk_cgroup_bio_start>> if (cgroup_subsys_on_dfl(io_cgrp_subsys))
+ *   - block/blk-throttle.c|302| <<tg_bps_limit>> if (cgroup_subsys_on_dfl(io_cgrp_subsys) && !blkg->parent)
+ *   - block/blk-throttle.c|332| <<tg_iops_limit>> if (cgroup_subsys_on_dfl(io_cgrp_subsys) && !blkg->parent)
+ *   - block/blk-throttle.c|555| <<throtl_pd_init>> if (cgroup_subsys_on_dfl(io_cgrp_subsys) && blkg->parent)
+ *   - block/blk-throttle.c|1409| <<tg_conf_updated>> if (!cgroup_subsys_on_dfl(io_cgrp_subsys) || !blkg->parent ||
+ *   - block/blk-throttle.c|2178| <<blk_throtl_bio>> if (!cgroup_subsys_on_dfl(io_cgrp_subsys)) {
+ *   - include/linux/backing-dev.h|248| <<inode_cgwb_enabled>> cgroup_subsys_on_dfl(io_cgrp_subsys) &&
+ *   - mm/backing-dev.c|442| <<cgwb_create>> blkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);
+ *   - mm/backing-dev.c|561| <<wb_get_lookup>> blkcg_css = cgroup_get_e_css(memcg_css->cgroup, &io_cgrp_subsys);
+ *   - mm/page_io.c|289| <<bio_associate_blkg_from_page>> css = cgroup_e_css(page->mem_cgroup->css.cgroup, &io_cgrp_subsys);
+ */
 struct cgroup_subsys io_cgrp_subsys = {
 	.css_alloc = blkcg_css_alloc,
 	.css_online = blkcg_css_online,
@@ -1415,6 +1459,13 @@ EXPORT_SYMBOL_GPL(blkcg_deactivate_policy);
  * Register @pol with blkcg core.  Might sleep and @pol may be modified on
  * successful registration.  Returns 0 on success and -errno on failure.
  */
+/*
+ * 在以下调用blkcg_policy_register():
+ *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+ *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+ *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+ */
 int blkcg_policy_register(struct blkcg_policy *pol)
 {
 	struct blkcg *blkcg;
@@ -1862,6 +1913,34 @@ static int blk_cgroup_io_type(struct bio *bio)
 	return BLKG_IOSTAT_READ;
 }
 
+/*
+ * [0] blk_cgroup_bio_start
+ * [0] submit_bio_checks
+ * [0] submit_bio_noacct
+ * [0] submit_bio
+ * [0] submit_bh_wbc.isra.57
+ * [0] ll_rw_block
+ * [0] jread
+ * [0] do_one_pass
+ * [0] jbd2_journal_recover
+ * [0] jbd2_journal_load
+ * [0] ext4_fill_super
+ * [0] mount_bdev
+ * [0] legacy_get_tree
+ * [0] vfs_get_tree
+ * [0] path_mount
+ * [0] init_mount
+ * [0] do_mount_root
+ * [0] mount_block_root
+ * [0] mount_root
+ * [0] prepare_namespace
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - block/blk-core.c|1059| <<submit_bio_checks>> blk_cgroup_bio_start(bio);
+ */
 void blk_cgroup_bio_start(struct bio *bio)
 {
 	int rwd = blk_cgroup_io_type(bio), cpu;
diff --git a/block/blk-core.c b/block/blk-core.c
index 10c08ac50697..43ac30695406 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -486,6 +486,19 @@ static inline int bio_queue_enter(struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1089| <<__submit_bio>> blk_queue_exit(disk->queue);
+ *   - block/blk-core.c|1175| <<__submit_bio_noacct_mq>> blk_queue_exit(disk->queue);
+ *   - block/blk-mq-tag.c|453| <<blk_mq_queue_tag_busy_iter>> blk_queue_exit(q);
+ *   - block/blk-mq.c|422| <<blk_mq_alloc_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|481| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|501| <<__blk_mq_free_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|1012| <<blk_mq_timeout_work>> blk_queue_exit(q);
+ *   - block/blk-mq.c|2264| <<blk_mq_submit_bio>> blk_queue_exit(q);
+ *   - fs/block_dev.c|701| <<bdev_read_page>> blk_queue_exit(bdev->bd_disk->queue);
+ *   - fs/block_dev.c|745| <<bdev_write_page>> blk_queue_exit(bdev->bd_disk->queue);
+ */
 void blk_queue_exit(struct request_queue *q)
 {
 	percpu_ref_put(&q->q_usage_counter);
diff --git a/block/blk-iocost.c b/block/blk-iocost.c
index d37b55db2409..298e72f08506 100644
--- a/block/blk-iocost.c
+++ b/block/blk-iocost.c
@@ -1718,6 +1718,9 @@ static u64 calc_size_vtime_cost(struct request *rq, struct ioc *ioc)
 	return cost;
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.throttle = ioc_rqos_throttle()
+ */
 static void ioc_rqos_throttle(struct rq_qos *rqos, struct bio *bio)
 {
 	struct blkcg_gq *blkg = bio->bi_blkg;
@@ -1844,6 +1847,9 @@ static void ioc_rqos_throttle(struct rq_qos *rqos, struct bio *bio)
 	finish_wait(&iocg->waitq, &wait.wait);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.merge = ioc_rqos_merge()
+ */
 static void ioc_rqos_merge(struct rq_qos *rqos, struct request *rq,
 			   struct bio *bio)
 {
@@ -1897,6 +1903,9 @@ static void ioc_rqos_merge(struct rq_qos *rqos, struct request *rq,
 	spin_unlock_irqrestore(&iocg->waitq.lock, flags);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.done_bio = ioc_rqos_done_bio()
+ */
 static void ioc_rqos_done_bio(struct rq_qos *rqos, struct bio *bio)
 {
 	struct ioc_gq *iocg = blkg_to_iocg(bio->bi_blkg);
@@ -1905,6 +1914,9 @@ static void ioc_rqos_done_bio(struct rq_qos *rqos, struct bio *bio)
 		atomic64_add(bio->bi_iocost_cost, &iocg->done_vtime);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.done = ioc_rqos_done()
+ */
 static void ioc_rqos_done(struct rq_qos *rqos, struct request *rq)
 {
 	struct ioc *ioc = rqos_to_ioc(rqos);
@@ -1940,6 +1952,9 @@ static void ioc_rqos_done(struct rq_qos *rqos, struct request *rq)
 	this_cpu_add(ioc->pcpu_stat->rq_wait_ns, rq_wait_ns);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.queue_depth_changed = ioc_rqos_queue_depth_changed()
+ */
 static void ioc_rqos_queue_depth_changed(struct rq_qos *rqos)
 {
 	struct ioc *ioc = rqos_to_ioc(rqos);
@@ -1949,6 +1964,9 @@ static void ioc_rqos_queue_depth_changed(struct rq_qos *rqos)
 	spin_unlock_irq(&ioc->lock);
 }
 
+/*
+ * struct rq_qos_ops ioc_rqos_ops.exit = ioc_rqos_exit()
+ */
 static void ioc_rqos_exit(struct rq_qos *rqos)
 {
 	struct ioc *ioc = rqos_to_ioc(rqos);
@@ -1964,6 +1982,10 @@ static void ioc_rqos_exit(struct rq_qos *rqos)
 	kfree(ioc);
 }
 
+/*
+ * 在以下使用ioc_rqos_ops:
+ *   - block/blk-iocost.c|1994| <<blk_iocost_init>> rqos->ops = &ioc_rqos_ops;
+ */
 static struct rq_qos_ops ioc_rqos_ops = {
 	.throttle = ioc_rqos_throttle,
 	.merge = ioc_rqos_merge,
@@ -1973,6 +1995,11 @@ static struct rq_qos_ops ioc_rqos_ops = {
 	.exit = ioc_rqos_exit,
 };
 
+/*
+ * called by:
+ *   - block/blk-iocost.c|2262| <<ioc_qos_write>> ret = blk_iocost_init(disk->queue);
+ *   - block/blk-iocost.c|2429| <<ioc_cost_model_write>> ret = blk_iocost_init(disk->queue);
+ */
 static int blk_iocost_init(struct request_queue *q)
 {
 	struct ioc *ioc;
@@ -2514,6 +2541,19 @@ static struct cftype ioc_files[] = {
 	{}
 };
 
+/*
+ * 在以下使用blkcg_policy_iocost:
+ *   - block/blk-iocost.c|639| <<blkg_to_iocg>> return pd_to_iocg(blkg_to_pd(blkg, &blkcg_policy_iocost));
+ *   - block/blk-iocost.c|649| <<blkcg_to_iocc>> return container_of(blkcg_to_cpd(blkcg, &blkcg_policy_iocost),
+ *   - block/blk-iocost.c|1956| <<ioc_rqos_exit>> blkcg_deactivate_policy(rqos->q, &blkcg_policy_iocost);
+ *   - block/blk-iocost.c|2014| <<blk_iocost_init>> ret = blkcg_activate_policy(q, &blkcg_policy_iocost);
+ *   - block/blk-iocost.c|2130| <<ioc_weight_show>> &blkcg_policy_iocost, seq_cft(sf)->private, false);
+ *   - block/blk-iocost.c|2169| <<ioc_weight_write>> ret = blkg_conf_prep(blkcg, &blkcg_policy_iocost, buf, &ctx);
+ *   - block/blk-iocost.c|2226| <<ioc_qos_show>> &blkcg_policy_iocost, seq_cft(sf)->private, false);
+ *   - block/blk-iocost.c|2393| <<ioc_cost_model_show>> &blkcg_policy_iocost, seq_cft(sf)->private, false);
+ *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+ *   - block/blk-iocost.c|2533| <<ioc_exit>> return blkcg_policy_unregister(&blkcg_policy_iocost);
+ */
 static struct blkcg_policy blkcg_policy_iocost = {
 	.dfl_cftypes	= ioc_files,
 	.cpd_alloc_fn	= ioc_cpd_alloc,
@@ -2525,6 +2565,13 @@ static struct blkcg_policy blkcg_policy_iocost = {
 
 static int __init ioc_init(void)
 {
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	return blkcg_policy_register(&blkcg_policy_iocost);
 }
 
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index f90429cf4edf..3092e9a6d024 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -715,6 +715,10 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	rcu_read_unlock();
 }
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1173| <<blkcg_init_queue>> ret = blk_iolatency_init(q);
+ */
 int blk_iolatency_init(struct request_queue *q)
 {
 	struct blk_iolatency *blkiolat;
@@ -935,6 +939,39 @@ static size_t iolatency_pd_stat(struct blkg_policy_data *pd, char *buf,
 }
 
 
+/*
+ * [0] iolatency_pd_alloc
+ * [0] blkg_alloc
+ * [0] blkg_conf_prep
+ * [0] tg_set_conf.constprop.31
+ * [0] cgroup_file_write
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] iolatency_pd_alloc
+ * [0] blkcg_activate_policy
+ * [0] blk_iolatency_init
+ * [0] blkcg_init_queue
+ * [0] blk_alloc_queue
+ * [0] blk_mq_init_queue_data
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp,
 						   struct request_queue *q,
 						   struct blkcg *blkcg)
@@ -950,6 +987,10 @@ static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp,
 		kfree(iolat);
 		return NULL;
 	}
+	/*
+	 * struct iolatency_grp:
+	 *  -> struct blkg_policy_data pd;
+	 */
 	return &iolat->pd;
 }
 
@@ -1030,6 +1071,17 @@ static struct cftype iolatency_files[] = {
 	{}
 };
 
+/*
+ * 在以下使用blkcg_policy_iolatency:
+ *   - block/blk-iolatency.c|184| <<blkg_to_lat>> return pd_to_lat(blkg_to_pd(blkg, &blkcg_policy_iolatency));
+ *   - block/blk-iolatency.c|647| <<blkcg_iolatency_exit>> blkcg_deactivate_policy(rqos->q, &blkcg_policy_iolatency);
+ *   - block/blk-iolatency.c|735| <<blk_iolatency_init>> ret = blkcg_activate_policy(q, &blkcg_policy_iolatency);
+ *   - block/blk-iolatency.c|801| <<iolatency_set_limit>> ret = blkg_conf_prep(blkcg, &blkcg_policy_iolatency, buf, &ctx);
+ *   - block/blk-iolatency.c|885| <<iolatency_print_limit>> &blkcg_policy_iolatency, seq_cft(sf)->private, false);
+ *   - block/blk-iolatency.c|990| <<iolatency_pd_init>> if (blkg->parent && blkg_to_pd(blkg->parent, &blkcg_policy_iolatency)) {
+ *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+ *   - block/blk-iolatency.c|1049| <<iolatency_exit>> return blkcg_policy_unregister(&blkcg_policy_iolatency);
+ */
 static struct blkcg_policy blkcg_policy_iolatency = {
 	.dfl_cftypes	= iolatency_files,
 	.pd_alloc_fn	= iolatency_pd_alloc,
@@ -1041,6 +1093,13 @@ static struct blkcg_policy blkcg_policy_iolatency = {
 
 static int __init iolatency_init(void)
 {
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	return blkcg_policy_register(&blkcg_policy_iolatency);
 }
 
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 0157f2b3485a..a1618920df71 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -32,6 +32,20 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|42| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|44| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3350| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3649| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|450| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|2186| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2348| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2349| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/nvme/host/tcp.c|2358| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|7751| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1766| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index 3f09bcb8a6fd..e9d3a7722a32 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -414,6 +414,9 @@ static int hctx_busy_show(void *data, struct seq_file *m)
 	struct blk_mq_hw_ctx *hctx = data;
 	struct show_busy_params params = { .m = m, .hctx = hctx };
 
+	/*
+	 * iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+	 */
 	blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
 				&params);
 
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 32d82e23b095..e2394d2ffbc3 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -21,6 +21,10 @@
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|62| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
@@ -33,6 +37,11 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|56| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|263| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -44,6 +53,10 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|70| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -56,6 +69,12 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|92| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|114| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|120| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
@@ -68,6 +87,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|383| <<__blk_mq_alloc_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|475| <<blk_mq_alloc_request_hctx>> tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
@@ -160,6 +184,13 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|157| <<blk_mq_get_tag>> blk_mq_put_tag(tags, data->ctx, tag + tag_offset);
+ *   - block/blk-mq.c|497| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|499| <<__blk_mq_free_request>> blk_mq_put_tag(hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|199| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,
 		    unsigned int tag)
 {
@@ -181,6 +212,16 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * 在以下使用bt_iter():
+ *   - block/blk-mq-tag.c|229| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ *
+ * blk_mq_queue_tag_busy_iter()
+ *  -> bt_for_each() --> 两处
+ *     -> bt_iter()
+ *
+ * 使用的是tags->rqs[bitnr] 不是static
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_iter_data *iter_data = data;
@@ -216,6 +257,11 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * 在以下使用bt_for_each():
+ *   - block/blk-mq-tag.c|419| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|420| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -226,6 +272,13 @@ static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 		.reserved = reserved,
 	};
 
+	/*
+	 * blk_mq_queue_tag_busy_iter()
+	 *  -> bt_for_each() --> 两处
+	 *     -> bt_iter()
+	 *
+	 * bt_iter(): 使用的是tags->rqs[bitnr] 不是static
+	 */
 	sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
 }
 
@@ -236,10 +289,36 @@ struct bt_tags_iter_data {
 	unsigned int flags;
 };
 
+/*
+ * 在以下使用BT_TAG_ITER_RESERVED:
+ *   - block/blk-mq-tag.c|301| <<bt_tags_iter>> bool reserved = iter_data->flags & BT_TAG_ITER_RESERVED;
+ *   - block/blk-mq-tag.c|369| <<__blk_mq_all_tag_iter>> WARN_ON_ONCE(flags & BT_TAG_ITER_RESERVED);
+ *   - block/blk-mq-tag.c|373| <<__blk_mq_all_tag_iter>> flags | BT_TAG_ITER_RESERVED);
+ */
 #define BT_TAG_ITER_RESERVED		(1 << 0)
+/*
+ * 在以下使用BT_TAG_ITER_STARTED:
+ *   - block/blk-mq-tag.c|263| <<bt_tags_iter>> if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
+ *   - block/blk-mq-tag.c|342| <<blk_mq_tagset_busy_iter>> BT_TAG_ITER_STARTED);
+ */
 #define BT_TAG_ITER_STARTED		(1 << 1)
+/*
+ * 在以下使用BT_TAG_ITER_STATIC_RQS:
+ *   - block/blk-mq-tag.c|257| <<bt_tags_iter>> if (iter_data->flags & BT_TAG_ITER_STATIC_RQS)
+ *   - block/blk-mq-tag.c|321| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+ */
 #define BT_TAG_ITER_STATIC_RQS		(1 << 2)
 
+/*
+ * 在以下使用bt_tags_iter():
+ *   - block/blk-mq-tag.c|292| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ *
+ * __blk_mq_all_tag_iter()
+ * -> bt_tags_for_each() --> 两处调用
+ *    -> bt_tags_iter()
+ *
+ * 根据BT_TAG_ITER_STATIC_RQS的情况决定用static还是rqs
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -254,12 +333,24 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	 * We can hit rq == NULL here, because the tagging functions
 	 * test and set the bit before assigning ->rqs[].
 	 */
+	/*
+	 * 在以下使用BT_TAG_ITER_STATIC_RQS:
+	 *   - block/blk-mq-tag.c|257| <<bt_tags_iter>> if (iter_data->flags & BT_TAG_ITER_STATIC_RQS)
+	 *   - block/blk-mq-tag.c|321| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+	 */
 	if (iter_data->flags & BT_TAG_ITER_STATIC_RQS)
 		rq = tags->static_rqs[bitnr];
 	else
 		rq = tags->rqs[bitnr];
 	if (!rq)
 		return true;
+	/*
+	 * 在以下使用BT_TAG_ITER_STARTED:
+	 *   - block/blk-mq-tag.c|263| <<bt_tags_iter>> if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
+	 *   - block/blk-mq-tag.c|342| <<blk_mq_tagset_busy_iter>> BT_TAG_ITER_STARTED);
+	 *
+	 * 这整个函数Return true to continue iterating tags, false to stop.
+	 */
 	if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
 	    !blk_mq_request_started(rq))
 		return true;
@@ -278,6 +369,11 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @data:	Will be passed as second argument to @fn.
  * @flags:	BT_TAG_ITER_*
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|301| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, flags | BT_TAG_ITER_RESERVED);
+ *   - block/blk-mq-tag.c|303| <<__blk_mq_all_tag_iter>> bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, flags);
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, unsigned int flags)
 {
@@ -288,10 +384,24 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 		.flags = flags,
 	};
 
+	/*
+	 * sbitmap_for_each_set() - Iterate over each set bit in a &struct sbitmap.
+	 * @sb: Bitmap to iterate over.
+	 * @fn: Callback. Should return true to continue or false to break early.
+	 * @data: Pointer to pass to callback.
+	 */
 	if (tags->rqs)
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|352| <<blk_mq_all_tag_iter>> __blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);
+ *   - block/blk-mq-tag.c|372| <<blk_mq_tagset_busy_iter>> __blk_mq_all_tag_iter(tagset->tags[i], fn, priv,
+ *
+ * blk_mq_all_tag_iter(): iterate over all requests in a tag map (所以用了BT_TAG_ITER_STATIC_RQS)
+ * blk_mq_tagset_busy_iter(): iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+ */
 static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv, unsigned int flags)
 {
@@ -315,6 +425,10 @@ static void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,
  *
  * Caller has to pass the tag map from which requests are allocated.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|2472| <<blk_mq_hctx_has_requests>> blk_mq_all_tag_iter(tags, blk_mq_has_request, &data);
+ */
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv)
 {
@@ -331,12 +445,45 @@ void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|417| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq,
+ *   - block/blk-mq-tag.c|436| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_busy_iter(tagset,
+ *   - drivers/block/mtip32xx/mtip32xx.c|2685| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags, mtip_queue_cmd, dd);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2690| <<mtip_service_thread>> blk_mq_tagset_busy_iter(&dd->tags,
+ *   - drivers/block/mtip32xx/mtip32xx.c|3804| <<mtip_block_remove>> blk_mq_tagset_busy_iter(&dd->tags, mtip_no_dev_cleanup, dd);
+ *   - drivers/block/nbd.c|825| <<nbd_clear_que>> blk_mq_tagset_busy_iter(&nbd->tag_set, nbd_clear_req, NULL);
+ *   - drivers/block/skd_main.c|396| <<skd_in_flight>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_inc_in_flight, &count);
+ *   - drivers/block/skd_main.c|1920| <<skd_recover_requests>> blk_mq_tagset_busy_iter(&skdev->tag_set, skd_recover_request, skdev);
+ *   - drivers/nvme/host/fc.c|3127| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/host/fc.c|3150| <<nvme_fc_delete_association>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/nvme/host/pci.c|2434| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2435| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|1017| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->ctrl.admin_tagset,
+ *   - drivers/nvme/host/rdma.c|1036| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->ctrl.tagset,
+ *   - drivers/nvme/host/tcp.c|1893| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset,
+ *   - drivers/nvme/host/tcp.c|1914| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|410| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|420| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+ *   - drivers/scsi/hosts.c|580| <<scsi_host_busy>> blk_mq_tagset_busy_iter(&shost->tag_set,
+ *   - drivers/scsi/hosts.c|679| <<scsi_host_complete_all_commands>> blk_mq_tagset_busy_iter(&shost->tag_set, complete_all_cmds_iter,
+ *   - drivers/scsi/hosts.c|716| <<bool>> blk_mq_tagset_busy_iter(&shost->tag_set, __scsi_host_busy_iter_fn,
+ *   - drivers/scsi/ufs/ufshcd.c|1309| <<ufshcd_any_tag_in_use>> blk_mq_tagset_busy_iter(q->tag_set, ufshcd_is_busy, &busy);
+ *   - drivers/scsi/ufs/ufshcd.c|5899| <<ufshcd_tmc_handler>> blk_mq_tagset_busy_iter(q->tag_set, ufshcd_compl_tm, &ci);
+ *
+ * iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
 	int i;
 
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
+		/*
+		 * 在以下使用BT_TAG_ITER_STARTED:
+		 *   - block/blk-mq-tag.c|263| <<bt_tags_iter>> if ((iter_data->flags & BT_TAG_ITER_STARTED) &&
+		 *   - block/blk-mq-tag.c|342| <<blk_mq_tagset_busy_iter>> BT_TAG_ITER_STARTED);
+		 */
 		if (tagset->tags && tagset->tags[i])
 			__blk_mq_all_tag_iter(tagset->tags[i], fn, priv,
 					      BT_TAG_ITER_STARTED);
@@ -344,6 +491,10 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 }
 EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
 
+/*
+ * 在以下使用blk_mq_tagset_count_completed_rqs():
+ *   - block/blk-mq-tag.c|472| <<blk_mq_tagset_wait_completed_request>> blk_mq_tagset_count_completed_rqs, &count);
+ */
 static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
 		void *data, bool reserved)
 {
@@ -361,11 +512,29 @@ static bool blk_mq_tagset_count_completed_rqs(struct request *rq,
  *
  * Note: This function has to be run after all IO queues are shutdown
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3129| <<nvme_fc_delete_association>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/host/fc.c|3152| <<nvme_fc_delete_association>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *   - drivers/nvme/host/pci.c|2436| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->tagset);
+ *   - drivers/nvme/host/pci.c|2437| <<nvme_dev_disable>> blk_mq_tagset_wait_completed_request(&dev->admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1019| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.admin_tagset);
+ *   - drivers/nvme/host/rdma.c|1038| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->ctrl.tagset);
+ *   - drivers/nvme/host/tcp.c|1895| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_wait_completed_request(ctrl->admin_tagset);
+ *   - drivers/nvme/host/tcp.c|1916| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_wait_completed_request(ctrl->tagset);
+ *   - drivers/nvme/target/loop.c|412| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->tag_set);
+ *   - drivers/nvme/target/loop.c|422| <<nvme_loop_shutdown_ctrl>> blk_mq_tagset_wait_completed_request(&ctrl->admin_tag_set);
+ *
+ * 调用blk_mq_tagset_busy_iter()的时候用上了BT_TAG_ITER_STARTED
+ */
 void blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)
 {
 	while (true) {
 		unsigned count = 0;
 
+		/*
+		 * iterate over all started requests in a tag set (BT_TAG_ITER_STARTED用上了)
+		 */
 		blk_mq_tagset_busy_iter(tagset,
 				blk_mq_tagset_count_completed_rqs, &count);
 		if (!count)
@@ -389,6 +558,13 @@ EXPORT_SYMBOL(blk_mq_tagset_wait_completed_request);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|118| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|128| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|890| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|995| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -417,11 +593,26 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 
 		if (tags->nr_reserved_tags)
 			bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+		/*
+		 * 在以下使用bt_for_each():
+		 *   - block/blk-mq-tag.c|419| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+		 *   - block/blk-mq-tag.c|420| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+		 *
+		 * fn()是:
+		 *   - blk_mq_check_inflight()
+		 *   - blk_mq_rq_inflight()
+		 *   - blk_mq_check_expired()
+		 */
 		bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
 	}
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|469| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|471| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, round_robin,
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -429,6 +620,14 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|501| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ *
+ * blk_mq_alloc_rq_map()
+ * -> blk_mq_init_tags()
+ *    -> blk_mq_init_bitmap_tags()
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -449,6 +648,10 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2321| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node, BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -470,6 +673,12 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2306| <<blk_mq_free_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2330| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2339| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ */
 void blk_mq_free_tags(struct blk_mq_tags *tags)
 {
 	sbitmap_queue_free(&tags->bitmap_tags);
@@ -477,6 +686,15 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3498| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3501| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ *
+ * queue_requests_store()
+ * -> blk_mq_update_nr_requests() --> 两处调用
+ *    -> blk_mq_tag_update_depth()
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -542,6 +760,23 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * called by:
+ *   - drivers/block/nbd.c|178| <<nbd_cmd_handle>> u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/block/skd_main.c|486| <<skd_mq_queue_rq>> const u32 tag = blk_mq_unique_tag(req);
+ *   - drivers/block/skd_main.c|607| <<skd_timed_out>> blk_mq_unique_tag(req));
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2186| <<srp_queuecommand>> tag = blk_mq_unique_tag(scmnd->request);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|2817| <<srp_abort>> tag = blk_mq_unique_tag(scmnd->request);
+ *   - drivers/nvme/host/nvme.h|181| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/cxlflash/main.c|436| <<cmd_to_target_hwq>> tag = blk_mq_unique_tag(scp->request);
+ *   - drivers/scsi/lpfc/lpfc_scsi.c|645| <<lpfc_get_scsi_buf_s4>> tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/qla2xxx/qla_os.c|843| <<qla2xxx_queuecommand>> tag = blk_mq_unique_tag(cmd->request);
+ *   - drivers/scsi/scsi_debug.c|4704| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|4717| <<get_tag>> return blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|7189| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5298| <<pqi_get_hw_queue>> hw_queue = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));
+ *   - drivers/scsi/virtio_scsi.c|543| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index b1acac518c4e..d58804145b98 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -11,6 +11,13 @@ struct blk_mq_tags {
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在以下使用blk_mq_tags->active_queues:
+	 *   - block/blk-mq-debugfs.c|452| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|67| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.h|93| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
@@ -37,6 +44,12 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 void blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|123| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|170| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1189| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(sbq, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -54,19 +67,36 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|376| <<__blk_mq_alloc_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|472| <<blk_mq_alloc_request_hctx>> blk_mq_tag_busy(data.hctx);
+ *   - block/blk-mq.c|1103| <<__blk_mq_get_driver_tag>> blk_mq_tag_busy(rq->mq_hctx);
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return false;
 
+	/*
+	 * 只在这里被调用
+	 */
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1009| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2581| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return;
 
+	/*
+	 * 只在这里被调用
+	 */
 	__blk_mq_tag_idle(hctx);
 }
 
@@ -74,6 +104,11 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|81| <<__blk_mq_get_tag>> if (!data->q->elevator && !hctx_may_queue(data->hctx, bt))
+ *   - block/blk-mq.c|1110| <<__blk_mq_get_driver_tag>> if (!hctx_may_queue(rq->mq_hctx, bt))
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
@@ -101,6 +136,11 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|197| <<blk_mq_put_tag>> if (!blk_mq_tag_is_reserved(tags, tag)) {
+ *   - block/blk-mq.c|1105| <<__blk_mq_get_driver_tag>> if (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 7b8a42c35102..332f74f1f79b 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -21,6 +21,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|685| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+ *   - drivers/scsi/virtio_scsi.c|716| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index cdced4aca2e8..7a05fd2c5200 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -2306,6 +2306,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|633| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|550| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2750| <<__blk_mq_alloc_map_and_request>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2455,6 +2461,10 @@ static bool blk_mq_has_request(struct request *rq, void *data, bool reserved)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2511| <<blk_mq_hctx_notify_offline>> while (blk_mq_hctx_has_requests(hctx))
+ */
 static bool blk_mq_hctx_has_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->sched_tags ?
@@ -3471,6 +3481,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|82| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index fee3325edf27..a91b3ff07e34 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -14,6 +14,20 @@
 #include "blk.h"
 #include "blk-cgroup-rwstat.h"
 
+/*
+ * 似乎是cgroup-blkio有多种限流的算法 (block/Kconfig也有一些解释).
+ * - throttle
+ * - iolatency
+ * - iocost
+ * - bfq
+ *
+ * # mkdir /sys/fs/cgroup/blkio/test
+ *
+ * # echo "253:0 1" > /sys/fs/cgroup/blkio/test/blkio.throttle.write_iops_device
+ *
+ * # cgexec -g blkio:test dd if=/dev/zero of=/dev/vda bs=1M count=10 oflag=direct
+ */
+
 /* Max dispatch from a group in 1 round */
 static int throtl_grp_quantum = 8;
 
@@ -323,6 +337,15 @@ static uint64_t tg_bps_limit(struct throtl_grp *tg, int rw)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|575| <<tg_update_has_rules>> tg_iops_limit(tg, rw) != UINT_MAX));
+ *   - block/blk-throttle.c|872| <<throtl_trim_slice>> io_trim = (tg_iops_limit(tg, rw) * tg->td->throtl_slice * nr_slices) /
+ *   - block/blk-throttle.c|916| <<tg_with_in_iops_limit>> tmp = (u64)tg_iops_limit(tg, rw) * jiffy_elapsed_rnd;
+ *   - block/blk-throttle.c|1002| <<tg_may_dispatch>> tg_iops_limit(tg, rw) == UINT_MAX) {
+ *   - block/blk-throttle.c|1393| <<tg_conf_updated>> tg_iops_limit(tg, READ), tg_iops_limit(tg, WRITE));
+ *   - block/blk-throttle.c|2252| <<blk_throtl_bio>> tg->io_disp[rw], tg_iops_limit(tg, rw),
+ */
 static unsigned int tg_iops_limit(struct throtl_grp *tg, int rw)
 {
 	struct blkcg_gq *blkg = tg_to_blkg(tg);
@@ -482,6 +505,39 @@ static void throtl_service_queue_init(struct throtl_service_queue *sq)
 	timer_setup(&sq->pending_timer, throtl_pending_timer_fn, 0);
 }
 
+/*
+ * [0] throtl_pd_alloc
+ * [0] blkg_alloc
+ * [0] blkg_conf_prep
+ * [0] tg_set_conf.constprop.31
+ * [0] cgroup_file_write
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] throtl_pd_alloc
+ * [0] blkcg_activate_policy
+ * [0] blk_throtl_init
+ * [0] blkcg_init_queue
+ * [0] blk_alloc_queue
+ * [0] blk_mq_init_queue_data
+ * [0] virtblk_probe
+ * [0] virtio_dev_probe
+ * [0] really_probe
+ * [0] driver_probe_device
+ * [0] device_driver_attach
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] init
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ */
 static struct blkg_policy_data *throtl_pd_alloc(gfp_t gfp,
 						struct request_queue *q,
 						struct blkcg *blkcg)
@@ -575,6 +631,18 @@ static void tg_update_has_rules(struct throtl_grp *tg)
 			  tg_iops_limit(tg, rw) != UINT_MAX));
 }
 
+/*
+ * [0] throtl_pd_online
+ * [0] blkg_create
+ * [0] blkg_conf_prep
+ * [0] tg_set_conf.constprop.31
+ * [0] cgroup_file_write
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void throtl_pd_online(struct blkg_policy_data *pd)
 {
 	struct throtl_grp *tg = pd_to_tg(pd);
@@ -1508,6 +1576,9 @@ static int tg_print_rwstat_recursive(struct seq_file *sf, void *v)
 	return 0;
 }
 
+/*
+ * struct blkcg_policy blkcg_policy_throtl.legacy_cftypes = throtl_legacy_files[]
+ */
 static struct cftype throtl_legacy_files[] = {
 	{
 		.name = "throttle.read_bps_device",
@@ -2158,6 +2229,10 @@ static inline void throtl_update_latency_buckets(struct throtl_data *td)
 }
 #endif
 
+/*
+ * called by:
+ *   - block/blk-core.c|1054| <<submit_bio_checks>> if (blk_throtl_bio(bio)) {
+ */
 bool blk_throtl_bio(struct bio *bio)
 {
 	struct request_queue *q = bio->bi_disk->queue;
@@ -2307,6 +2382,10 @@ void blk_throtl_stat_add(struct request *rq, u64 time_ns)
 			     time_ns >> 10);
 }
 
+/*
+ * called by:
+ *   - block/bio.c|1445| <<bio_endio>> blk_throtl_bio_endio(bio);
+ */
 void blk_throtl_bio_endio(struct bio *bio)
 {
 	struct blkcg_gq *blkg;
@@ -2360,6 +2439,10 @@ void blk_throtl_bio_endio(struct bio *bio)
 }
 #endif
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1169| <<blkcg_init_queue>> ret = blk_throtl_init(q);
+ */
 int blk_throtl_init(struct request_queue *q)
 {
 	struct throtl_data *td;
@@ -2413,6 +2496,10 @@ void blk_throtl_exit(struct request_queue *q)
 	kfree(q->td);
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|1049| <<blk_register_queue>> blk_throtl_register_queue(q);
+ */
 void blk_throtl_register_queue(struct request_queue *q)
 {
 	struct throtl_data *td;
@@ -2474,6 +2561,13 @@ static int __init throtl_init(void)
 	if (!kthrotld_workqueue)
 		panic("Failed to create kthrotld\n");
 
+	/*
+	 * 在以下调用blkcg_policy_register():
+	 *   - block/bfq-iosched.c|6812| <<bfq_init>> ret = blkcg_policy_register(&blkcg_policy_bfq);
+	 *   - block/blk-iocost.c|2528| <<ioc_init>> return blkcg_policy_register(&blkcg_policy_iocost);
+	 *   - block/blk-iolatency.c|1044| <<iolatency_init>> return blkcg_policy_register(&blkcg_policy_iolatency);
+	 *   - block/blk-throttle.c|2477| <<throtl_init>> return blkcg_policy_register(&blkcg_policy_throtl);
+	 */
 	return blkcg_policy_register(&blkcg_policy_throtl);
 }
 
diff --git a/block/elevator.c b/block/elevator.c
index 90ed7a28c21d..9b78a5d154ae 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -527,6 +527,12 @@ void elv_unregister_queue(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|6844| <<bfq_init>> ret = elv_register(&iosched_bfq_mq);
+ *   - block/kyber-iosched.c|1037| <<kyber_init>> return elv_register(&kyber_sched);
+ *   - block/mq-deadline.c|804| <<deadline_init>> return elv_register(&mq_deadline);
+ */
 int elv_register(struct elevator_type *e)
 {
 	/* create icq_cache if requested */
diff --git a/drivers/net/tap.c b/drivers/net/tap.c
index 1f4bdd94407a..ed52dda0c067 100644
--- a/drivers/net/tap.c
+++ b/drivers/net/tap.c
@@ -315,6 +315,11 @@ void tap_del_queues(struct tap_dev *tap)
 }
 EXPORT_SYMBOL_GPL(tap_del_queues);
 
+/*
+ * 在以下使用tap_handle_frame():
+ *   - drivers/net/ipvlan/ipvtap.c|94| <<ipvtap_newlink>> err = netdev_rx_handler_register(dev, tap_handle_frame, &vlantap->tap);
+ *   - drivers/net/macvtap.c|102| <<macvtap_newlink>> err = netdev_rx_handler_register(dev, tap_handle_frame, &vlantap->tap);
+ */
 rx_handler_result_t tap_handle_frame(struct sk_buff **pskb)
 {
 	struct sk_buff *skb = *pskb;
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 668685c09e65..a8faee130759 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -1442,6 +1442,17 @@ static void virtnet_poll_cleantx(struct receive_queue *rq)
 		netif_tx_wake_queue(txq);
 }
 
+/*
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ */
 static int virtnet_poll(struct napi_struct *napi, int budget)
 {
 	struct receive_queue *rq =
@@ -1503,6 +1514,43 @@ static int virtnet_open(struct net_device *dev)
 	return 0;
 }
 
+/*
+ * commit b92f1e6751a6aaaf0b1e05128661ce0f23ed3695
+ * Author: Willem de Bruijn <willemb@google.com>
+ * Date:   Mon Apr 24 13:49:27 2017 -0400
+ *
+ * virtio-net: transmit napi
+ *
+ * Convert virtio-net to a standard napi tx completion path. This enables
+ * better TCP pacing using TCP small queues and increases single stream
+ * throughput.
+ *
+ * The virtio-net driver currently cleans tx descriptors on transmission
+ * of new packets in ndo_start_xmit. Latency depends on new traffic, so
+ * is unbounded. To avoid deadlock when a socket reaches its snd limit,
+ * packets are orphaned on tranmission. This breaks socket backpressure,
+ * including TSQ.
+ *
+ * Napi increases the number of interrupts generated compared to the
+ * current model, which keeps interrupts disabled as long as the ring
+ * has enough free descriptors. Keep tx napi optional and disabled for
+ * now. Follow-on patches will reduce the interrupt cost.
+ *
+ * Signed-off-by: Willem de Bruijn <willemb@google.com>
+ * Signed-off-by: Jason Wang <jasowang@redhat.com>
+ * Signed-off-by: David S. Miller <davem@davemloft.net>
+ *
+ *
+ * [0] virtnet_poll_tx
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ */
 static int virtnet_poll_tx(struct napi_struct *napi, int budget)
 {
 	struct send_queue *sq = container_of(napi, struct send_queue, napi);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 531a00d703cd..341ad63b8179 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -35,6 +35,11 @@
 
 #include "vhost.h"
 
+/*
+ * 在以下使用experimental_zcopytx:
+ *   - drivers/vhost/net.c|344| <<vhost_sock_zcopy>> return unlikely(experimental_zcopytx) &&
+ *   - drivers/vhost/net.c|1784| <<vhost_net_init>> if (experimental_zcopytx)
+ */
 static int experimental_zcopytx = 0;
 module_param(experimental_zcopytx, int, 0444);
 MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
@@ -42,6 +47,11 @@ MODULE_PARM_DESC(experimental_zcopytx, "Enable Zero Copy TX;"
 
 /* Max number of bytes transferred before requeueing the job.
  * Using this limit prevents one virtqueue from starving others. */
+/*
+ * 在以下使用VHOST_NET_WEIGHT:
+ *   - drivers/vhost/net.c|641| <<tx_can_batch>> return total_len < VHOST_NET_WEIGHT &&
+ *   - drivers/vhost/net.c|1330| <<vhost_net_open>> VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,
+ */
 #define VHOST_NET_WEIGHT 0x80000
 
 /* Max number of packets transferred before requeueing the job.
@@ -220,6 +230,11 @@ static int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)
 	return vhost_net_buf_peek_len(vhost_net_buf_get_ptr(rxq));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|324| <<vhost_net_vq_reset>> vhost_net_buf_init(&n->vqs[i].rxq);
+ *   - drivers/vhost/net.c|1349| <<vhost_net_open>> vhost_net_buf_init(&n->vqs[i].rxq);
+ */
 static void vhost_net_buf_init(struct vhost_net_buf *rxq)
 {
 	rxq->head = rxq->tail = 0;
@@ -1273,6 +1288,19 @@ static void handle_rx_net(struct vhost_work *work)
 	handle_rx(net);
 }
 
+/*
+ * 为每个queue(kthread)调用一次.
+ * [0] vhost_net_open
+ * [0] misc_open
+ * [0] chrdev_open
+ * [0] do_dentry_open
+ * [0] path_openat
+ * [0] do_filp_open
+ * [0] do_sys_openat2
+ * [0] do_sys_open
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int vhost_net_open(struct inode *inode, struct file *f)
 {
 	struct vhost_net *n;
@@ -1282,9 +1310,27 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 	struct xdp_buff *xdp;
 	int i;
 
+	/*
+	 * struct vhost_net:
+	 *  -> struct vhost_dev dev;
+	 *  -> struct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];
+	 *      -> struct vhost_virtqueue vq;
+	 *          -> struct file *kick;
+	 *          -> struct vhost_vring_call call_ctx;
+	 *          -> struct eventfd_ctx *error_ctx;
+	 *          -> struct eventfd_ctx *log_ctx;
+	 *          -> struct vhost_poll poll;
+	 *  -> struct vhost_poll poll[VHOST_NET_VQ_MAX];
+	 *
+	 * 分配一个vhost_net结构
+	 */
 	n = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);
 	if (!n)
 		return -ENOMEM;
+	/*
+	 * 分配若干struct vhost_virtqueue的指针
+	 * 真正的内存是vhost_net的一部分
+	 */
 	vqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);
 	if (!vqs) {
 		kvfree(n);
@@ -1364,6 +1410,12 @@ static void vhost_net_stop(struct vhost_net *n, struct socket **tx_sock,
 	*rx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_RX].vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1385| <<vhost_net_flush>> vhost_net_flush_vq(n, VHOST_NET_VQ_TX);
+ *   - drivers/vhost/net.c|1386| <<vhost_net_flush>> vhost_net_flush_vq(n, VHOST_NET_VQ_RX);
+ *   - drivers/vhost/net.c|1576| <<vhost_net_set_backend>> vhost_net_flush_vq(n, index);
+ */
 static void vhost_net_flush_vq(struct vhost_net *n, int index)
 {
 	vhost_poll_flush(n->poll + index);
@@ -1396,6 +1448,16 @@ static int vhost_net_release(struct inode *inode, struct file *f)
 	vhost_net_stop(n, &tx_sock, &rx_sock);
 	vhost_net_flush(n);
 	vhost_dev_stop(&n->dev);
+	/*
+	 * 在以下调用vhost_dev_cleanup():
+	 *   - drivers/vhost/net.c|1451| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+	 *   - drivers/vhost/scsi.c|1654| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+	 *   - drivers/vhost/test.c|165| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+	 *   - drivers/vhost/vdpa.c|838| <<vhost_vdpa_open>> vhost_dev_cleanup(&v->vdev);
+	 *   - drivers/vhost/vdpa.c|870| <<vhost_vdpa_release>> vhost_dev_cleanup(&v->vdev);
+	 *   - drivers/vhost/vhost.c|787| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+	 *   - drivers/vhost/vsock.c|715| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+	 */
 	vhost_dev_cleanup(&n->dev);
 	vhost_net_vq_reset(n);
 	if (tx_sock)
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 9ad45e1d27f0..da465c073db7 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -33,10 +33,18 @@
 
 #include "vhost.h"
 
+/*
+ * 只在下面使用max_mem_regions:
+ *   - drivers/vhost/vhost.c|1420| <<vhost_set_memory>> if (mem.nregions > max_mem_regions)
+ */
 static ushort max_mem_regions = 64;
 module_param(max_mem_regions, ushort, 0444);
 MODULE_PARM_DESC(max_mem_regions,
 	"Maximum number of memory regions in memory map. (default: 64)");
+/*
+ * 只在下面使用max_iotlb_entries:
+ *   - drivers/vhost/vhost.c|628| <<iotlb_alloc>> return vhost_iotlb_alloc(max_iotlb_entries,
+ */
 static int max_iotlb_entries = 2048;
 module_param(max_iotlb_entries, int, 0444);
 MODULE_PARM_DESC(max_iotlb_entries,
@@ -151,6 +159,12 @@ static void vhost_flush_work(struct vhost_work *work)
 	complete(&s->wait_event);
 }
 
+/*
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|228| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ *
+ * 把包含参数poll_table的vhost_poll的poll->wait加入到参数的wqh (wait_queue_head_t)
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt)
 {
@@ -158,9 +172,17 @@ static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 
 	poll = container_of(pt, struct vhost_poll, table);
 	poll->wqh = wqh;
+	/*
+	 * 把&poll->wait加入到wqh
+	 */
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|206| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|234| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -178,6 +200,16 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1616| <<vhost_scsi_open>> vhost_work_init(&vs->vs_completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/scsi.c|1617| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/vhost.c|212| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|261| <<vhost_work_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vhost.c|563| <<vhost_attach_cgroups>> vhost_work_init(&attach.work, vhost_attach_cgroups_work);
+ *   - drivers/vhost/vhost.h|39| <<vhost_attach_cgroups>> void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn);
+ *   - drivers/vhost/vsock.c|640| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -186,10 +218,42 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
 /* Init poll structure */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1343| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);
+ *   - drivers/vhost/net.c|1344| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);
+ *   - drivers/vhost/vhost.c|509| <<vhost_dev_init>> vhost_poll_init(&vq->poll, vq->handle_kick, EPOLLIN, dev);
+ */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev)
 {
+	/*
+	 * 设置poll->wait.func = vhost_poll_wakeup()
+	 */
 	init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+	/*
+	 * socket : tun_chr_poll()
+	 * eventfd: eventfd_poll()
+	 *
+	 * poll_wait()就是调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * vhost_poll_func()把&poll->wait加入到fd的wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以到时候waitqueue唤醒的时候都是调用vhost_poll_wakeup()
+	 *
+	 * vfs_poll()调用file->f_op->poll()
+	 *
+	 * 调用_qproc的地方:
+	 *   - include/linux/poll.h|51| <<poll_wait>> p->_qproc(filp, wait_address, p);
+	 *
+	 * poll_wait()的时候会调用p->_qproc=vhost_poll_func()
+	 *
+	 * 设置poll->table._qproc = vhost_poll_func()
+	 * 这个vhost_poll_func()是把包含参数poll_table的vhost_poll的poll->wait加入到参数的wqh (wait_queue_head_t)
+	 */
 	init_poll_funcptr(&poll->table, vhost_poll_func);
 	poll->mask = mask;
 	poll->dev = dev;
@@ -201,6 +265,13 @@ EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/test.c|299| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|1709| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.h|45| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	__poll_t mask;
@@ -208,6 +279,21 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (poll->wqh)
 		return 0;
 
+	/*
+	 * socket : tun_chr_poll()
+	 * eventfd: eventfd_poll()
+	 *
+	 * poll_wait()就是调用poll_table->_qproc=vhost_poll_func()
+	 *
+	 * vhost_poll_func()把&poll->wait加入到fd的wqh
+	 *
+	 * 对于socket: 把n->poll->wait加入到socket的wqh, wait的func是vhost_poll_wakeup()
+	 * 对于eventfd, 把vhost_virtqueue->poll->wait加入到eventfd的wqh, wait的func是vhost_poll_wakeup()
+	 *
+	 * 所以到时候waitqueue唤醒的时候都是调用vhost_poll_wakeup()
+	 *
+	 * vfs_poll()调用file->f_op->poll()
+	 */
 	mask = vfs_poll(file, &poll->table);
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
@@ -222,6 +308,14 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/test.c|292| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+ *   - drivers/vhost/vhost.c|301| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|781| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|1822| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
 	if (poll->wqh) {
@@ -253,6 +347,16 @@ void vhost_poll_flush(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_flush);
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|371| <<vhost_scsi_complete_cmd>> vhost_work_queue(&vs->dev, &vs->vs_completion_work);
+ *   - drivers/vhost/scsi.c|1315| <<vhost_scsi_send_evt>> vhost_work_queue(&vs->dev, &vs->vs_event_work);
+ *   - drivers/vhost/vhost.c|284| <<vhost_work_flush>> vhost_work_queue(dev, &flush.work);
+ *   - drivers/vhost/vhost.c|323| <<vhost_poll_queue>> vhost_work_queue(poll->dev, &poll->work);
+ *   - drivers/vhost/vhost.c|585| <<vhost_attach_cgroups>> vhost_work_queue(dev, &attach.work);
+ *   - drivers/vhost/vsock.c|268| <<vhost_transport_send_pkt>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ *   - drivers/vhost/vsock.c|555| <<vhost_vsock_start>> vhost_work_queue(&vsock->dev, &vsock->send_pkt_work);
+ */
 void vhost_work_queue(struct vhost_dev *dev, struct vhost_work *work)
 {
 	if (!dev->worker)
@@ -298,13 +402,33 @@ static void vhost_vq_meta_reset(struct vhost_dev *d)
 		__vhost_vq_meta_reset(d->vqs[i]);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|431| <<vhost_vq_reset>> vhost_vring_call_reset(&vq->call_ctx);
+ */
 static void vhost_vring_call_reset(struct vhost_vring_call *call_ctx)
 {
 	call_ctx->ctx = NULL;
 	memset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer));
+	/*
+	 * 在以下使用vhost_vring_call->ctx_lock:
+	 *   - drivers/vhost/vdpa.c|100| <<vhost_vdpa_setup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|103| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|110| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_unsetup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|119| <<vhost_vdpa_unsetup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_vring_call_reset>> spin_lock_init(&call_ctx->ctx_lock);
+	 *   - drivers/vhost/vhost.c|1782| <<vhost_vring_ioctl>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|1784| <<vhost_vring_ioctl>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 */
 	spin_lock_init(&call_ctx->ctx_lock);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|630| <<vhost_dev_init>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|865| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -336,6 +460,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	__vhost_vq_meta_reset(vq);
 }
 
+/*
+ * 在以下创建vhost_worker():
+ *   - drivers/vhost/vhost.c|710| <<vhost_dev_set_owner>> worker = kthread_create(vhost_worker, dev,
+ */
 static int vhost_worker(void *data)
 {
 	struct vhost_dev *dev = data;
@@ -459,6 +587,14 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1338| <<vhost_net_open>> vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
+ *   - drivers/vhost/scsi.c|1630| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, UIO_MAXIOV,
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+ *   - drivers/vhost/vdpa.c|820| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,
+ *   - drivers/vhost/vsock.c|633| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -497,6 +633,19 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
 		vhost_vq_reset(dev, vq);
+		/*
+		 * &vq->poll用来poll在eventfd或者socket上
+		 *
+		 * struct vhost_dev *dev:
+		 * -> struct vhost_virtqueue **vqs;
+		 *    -> struct vhost_poll poll;
+		 *        -> poll_table                table;
+		 *        -> wait_queue_head_t        *wqh;
+		 *        -> wait_queue_entry_t        wait;
+		 *        -> struct vhost_work         work;
+		 *        -> __poll_t                  mask;
+		 *        -> struct vhost_dev         *dev;
+		 */
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
 					EPOLLIN, dev);
@@ -636,6 +785,11 @@ struct vhost_iotlb *vhost_dev_reset_owner_prepare(void)
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);
 
 /* Caller should have device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1659| <<vhost_net_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ *   - drivers/vhost/test.c|242| <<vhost_test_reset_owner>> vhost_dev_reset_owner(&n->dev, umem);
+ */
 void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 {
 	int i;
@@ -683,6 +837,25 @@ static void vhost_clear_msg(struct vhost_dev *dev)
 	spin_unlock(&dev->iotlb_lock);
 }
 
+/*
+ * 每一个vhost的queue(内核线程)都会调用一次
+ * [0] vhost_dev_cleanup
+ * [0] vhost_net_release
+ * [0] __fput
+ * [0] task_work_run
+ * [0] exit_to_user_mode_prepare
+ * [0] syscall_exit_to_user_mode
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/vhost/net.c|1451| <<vhost_net_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/scsi.c|1654| <<vhost_scsi_release>> vhost_dev_cleanup(&vs->dev);
+ *   - drivers/vhost/test.c|165| <<vhost_test_release>> vhost_dev_cleanup(&n->dev);
+ *   - drivers/vhost/vdpa.c|838| <<vhost_vdpa_open>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vdpa.c|870| <<vhost_vdpa_release>> vhost_dev_cleanup(&v->vdev);
+ *   - drivers/vhost/vhost.c|787| <<vhost_dev_reset_owner>> vhost_dev_cleanup(dev);
+ *   - drivers/vhost/vsock.c|715| <<vhost_vsock_dev_release>> vhost_dev_cleanup(&vsock->dev);
+ */
 void vhost_dev_cleanup(struct vhost_dev *dev)
 {
 	int i;
@@ -1588,6 +1761,9 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 		return -ENOBUFS;
 
 	idx = array_index_nospec(idx, d->nvqs);
+	/*
+	 * vq的类型是struct vhost_virtqueue *vq;
+	 */
 	vq = d->vqs[idx];
 
 	if (ioctl == VHOST_SET_VRING_NUM ||
@@ -1624,10 +1800,18 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 			r = -EFAULT;
 		break;
 	case VHOST_SET_VRING_KICK:
+		/*
+		 * struct vhost_vring_file f:
+		 *  -> unsigned int index;
+		 *  -> int fd; // Pass -1 to unbind from file.
+		 */
 		if (copy_from_user(&f, argp, sizeof f)) {
 			r = -EFAULT;
 			break;
 		}
+		/*
+		 * 如果(file->f_op != &eventfd_fops), eventfd_fget()会返回-EINVAL
+		 */
 		eventfp = f.fd == VHOST_FILE_UNBIND ? NULL : eventfd_fget(f.fd);
 		if (IS_ERR(eventfp)) {
 			r = PTR_ERR(eventfp);
@@ -1650,6 +1834,17 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 			break;
 		}
 
+		/*
+		 * 在以下使用vhost_vring_call->ctx_lock:
+		 *   - drivers/vhost/vdpa.c|100| <<vhost_vdpa_setup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|103| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|110| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_unsetup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vdpa.c|119| <<vhost_vdpa_unsetup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vhost.c|401| <<vhost_vring_call_reset>> spin_lock_init(&call_ctx->ctx_lock);
+		 *   - drivers/vhost/vhost.c|1782| <<vhost_vring_ioctl>> spin_lock(&vq->call_ctx.ctx_lock);
+		 *   - drivers/vhost/vhost.c|1784| <<vhost_vring_ioctl>> spin_unlock(&vq->call_ctx.ctx_lock);
+		 */
 		spin_lock(&vq->call_ctx.ctx_lock);
 		swap(ctx, vq->call_ctx.ctx);
 		spin_unlock(&vq->call_ctx.ctx_lock);
@@ -2417,6 +2612,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2590| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2455,9 +2654,22 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 
 /* This actually signals the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|565| <<vhost_scsi_complete_cmd_work>> vhost_signal(&vs->dev, &vs->vqs[vq].vq);
+ *   - drivers/vhost/vhost.c|2601| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|2611| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|225| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|504| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	/* Signal the Guest tell them we used something up. */
+	/*
+	 * struct vhost_virtqueue *vq:
+	 *  -> struct file *kick;
+	 *  -> struct vhost_vring_call call_ctx;
+	 */
 	if (vq->call_ctx.ctx && vhost_notify(dev, vq))
 		eventfd_signal(vq->call_ctx.ctx, 1);
 }
@@ -2502,6 +2714,20 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
 /* OK, now we need to know about added descriptors. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|512| <<vhost_net_busy_poll_try_queue>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|564| <<vhost_net_busy_poll>> vhost_enable_notify(&net->dev, rvq);
+ *   - drivers/vhost/net.c|803| <<handle_tx_copy>> } else if (unlikely(vhost_enable_notify(&net->dev,
+ *   - drivers/vhost/net.c|895| <<handle_tx_zerocopy>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1171| <<handle_rx>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|470| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|831| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|70| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|111| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|137| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|470| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__virtio16 avail_idx;
@@ -2540,6 +2766,26 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
 /* We don't need to be notified again. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|513| <<vhost_net_busy_poll_try_queue>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|536| <<vhost_net_busy_poll>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|805| <<handle_tx_copy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|896| <<handle_tx_zerocopy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|975| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1144| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1174| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|461| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|832| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|939| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1192| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|58| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|71| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|98| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|138| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|452| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|471| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index 9032d3c2a9f4..551d802e9537 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -64,6 +64,17 @@ enum vhost_uaddr_type {
 struct vhost_vring_call {
 	struct eventfd_ctx *ctx;
 	struct irq_bypass_producer producer;
+	/*
+	 * 在以下使用vhost_vring_call->ctx_lock:
+	 *   - drivers/vhost/vdpa.c|100| <<vhost_vdpa_setup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|103| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|110| <<vhost_vdpa_setup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|117| <<vhost_vdpa_unsetup_vq_irq>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vdpa.c|119| <<vhost_vdpa_unsetup_vq_irq>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|401| <<vhost_vring_call_reset>> spin_lock_init(&call_ctx->ctx_lock);
+	 *   - drivers/vhost/vhost.c|1782| <<vhost_vring_ioctl>> spin_lock(&vq->call_ctx.ctx_lock);
+	 *   - drivers/vhost/vhost.c|1784| <<vhost_vring_ioctl>> spin_unlock(&vq->call_ctx.ctx_lock);
+	 */
 	spinlock_t ctx_lock;
 };
 
@@ -150,6 +161,25 @@ struct vhost_dev {
 	int nvqs;
 	struct eventfd_ctx *log_ctx;
 	struct llist_head work_list;
+	/*
+	 * 在以下使用vhost_dev->worker:
+	 *   - drivers/vhost/vhost.c|332| <<vhost_work_flush>> if (dev->worker) {
+	 *   - drivers/vhost/vhost.c|362| <<vhost_work_queue>> if (!dev->worker)
+	 *   - drivers/vhost/vhost.c|371| <<vhost_work_queue>> wake_up_process(dev->worker);
+	 *   - drivers/vhost/vhost.c|615| <<vhost_dev_init>> dev->worker = NULL;
+	 *   - drivers/vhost/vhost.c|729| <<vhost_dev_set_owner>> struct task_struct *worker;
+	 *   - drivers/vhost/vhost.c|742| <<vhost_dev_set_owner>> worker = kthread_create(vhost_worker, dev,
+	 *   - drivers/vhost/vhost.c|744| <<vhost_dev_set_owner>> if (IS_ERR(worker)) {
+	 *   - drivers/vhost/vhost.c|745| <<vhost_dev_set_owner>> err = PTR_ERR(worker);
+	 *   - drivers/vhost/vhost.c|749| <<vhost_dev_set_owner>> dev->worker = worker;
+	 *   - drivers/vhost/vhost.c|750| <<vhost_dev_set_owner>> wake_up_process(worker);
+	 *   - drivers/vhost/vhost.c|763| <<vhost_dev_set_owner>> if (dev->worker) {
+	 *   - drivers/vhost/vhost.c|764| <<vhost_dev_set_owner>> kthread_stop(dev->worker);
+	 *   - drivers/vhost/vhost.c|765| <<vhost_dev_set_owner>> dev->worker = NULL;
+	 *   - drivers/vhost/vhost.c|884| <<vhost_dev_cleanup>> if (dev->worker) {
+	 *   - drivers/vhost/vhost.c|885| <<vhost_dev_cleanup>> kthread_stop(dev->worker);
+	 *   - drivers/vhost/vhost.c|886| <<vhost_dev_cleanup>> dev->worker = NULL;
+	 */
 	struct task_struct *worker;
 	struct vhost_iotlb *umem;
 	struct vhost_iotlb *iotlb;
diff --git a/fs/eventfd.c b/fs/eventfd.c
index df466ef81ddd..38539c12f267 100644
--- a/fs/eventfd.c
+++ b/fs/eventfd.c
@@ -59,6 +59,17 @@ struct eventfd_ctx {
  * Returns the amount by which the counter was incremented.  This will be less
  * than @n if the counter has overflowed.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vdpa.c|73| <<vhost_vdpa_virtqueue_cb>> eventfd_signal(call_ctx, 1);
+ *   - drivers/vhost/vdpa.c|84| <<vhost_vdpa_config_cb>> eventfd_signal(config_ctx, 1);
+ *   - drivers/vhost/vhost.c|2142| <<vhost_log_write>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2165| <<vhost_update_used_flags>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2183| <<vhost_update_avail_event>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2609| <<vhost_add_used_n>> eventfd_signal(vq->log_ctx, 1);
+ *   - drivers/vhost/vhost.c|2674| <<vhost_signal>> eventfd_signal(vq->call_ctx.ctx, 1);
+ *   - drivers/vhost/vhost.h|263| <<vq_err>> eventfd_signal((vq)->error_ctx, 1);\
+ */
 __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)
 {
 	unsigned long flags;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index 9d2d5ad367a4..03daf6419889 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -253,6 +253,21 @@ struct blk_mq_tag_set {
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 在以下使用blk_mq_tag_set->tag_list
+	 *   - block/blk-mq.c|2911| <<blk_mq_update_tag_set_depth>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|2924| <<blk_mq_del_queue_tag_set>> if (list_is_singular(&set->tag_list)) {
+	 *   - block/blk-mq.c|2942| <<blk_mq_add_queue_tag_set>> if (!list_empty(&set->tag_list) &&
+	 *   - block/blk-mq.c|2950| <<blk_mq_add_queue_tag_set>> list_add_tail(&q->tag_set_list, &set->tag_list);
+	 *   - block/blk-mq.c|3452| <<blk_mq_alloc_tag_set>> INIT_LIST_HEAD(&set->tag_list);
+	 *   - block/blk-mq.c|3615| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|3622| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|3626| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3639| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3652| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list) {
+	 *   - block/blk-mq.c|3658| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 *   - block/blk-mq.c|3661| <<__blk_mq_update_nr_hw_queues>> list_for_each_entry(q, &set->tag_list, tag_set_list)
+	 */
 	struct list_head	tag_list;
 };
 
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 05e3c2fb3ef7..dd8b1a14e444 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -498,6 +498,17 @@ struct kvm {
 	long tlbs_dirty;
 	struct list_head devices;
 	u64 manual_dirty_log_protect;
+	/*
+	 * 在以下使用kvm->debugfs_dentry:
+	 *   - arch/arm64/kvm/vgic/vgic-debug.c|294| <<vgic_debug_init>> debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
+	 *   - virt/kvm/kvm_main.c|677| <<kvm_destroy_vm_debugfs>> if (!kvm->debugfs_dentry)
+	 *   - virt/kvm/kvm_main.c|680| <<kvm_destroy_vm_debugfs>> debugfs_remove_recursive(kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|716| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry, stat_data,
+	 *   - virt/kvm/kvm_main.c|3077| <<kvm_create_vcpu_debugfs>> vcpu->kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|4640| <<kvm_uevent_notify_change>> if (!IS_ERR_OR_NULL(kvm->debugfs_dentry)) {
+	 *   - virt/kvm/kvm_main.c|4644| <<kvm_uevent_notify_change>> tmp = dentry_path_raw(kvm->debugfs_dentry, p, PATH_MAX);
+	 */
 	struct dentry *debugfs_dentry;
 	struct kvm_stat_data **debugfs_stat_data;
 	struct srcu_struct srcu;
diff --git a/include/linux/poll.h b/include/linux/poll.h
index 1cdc32b1f1b0..9d24cdd6d56f 100644
--- a/include/linux/poll.h
+++ b/include/linux/poll.h
@@ -45,6 +45,9 @@ typedef struct poll_table_struct {
 	__poll_t _key;
 } poll_table;
 
+/*
+ * vhost的_qproc的例子vhost_poll_func()
+ */
 static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
 {
 	if (p && p->_qproc && wait_address)
@@ -74,6 +77,10 @@ static inline __poll_t poll_requested_events(const poll_table *p)
 
 static inline void init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)
 {
+	/*
+	 * 调用_qproc的地方:
+	 *   - include/linux/poll.h|51| <<poll_wait>> p->_qproc(filp, wait_address, p);
+	 */
 	pt->_qproc = qproc;
 	pt->_key   = ~(__poll_t)0; /* all events enabled */
 }
diff --git a/net/core/dev.c b/net/core/dev.c
index 4906b44af850..61e5d6be7c76 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -5076,6 +5076,23 @@ static inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,
 	return 0;
 }
 
+/*
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb_list_core
+ * [0] netif_receive_skb_list_internal
+ * [0] gro_normal_list.part.170
+ * [0] napi_complete_done
+ * [0] virtqueue_napi_complete
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ */
 static int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,
 				    struct packet_type **ppt_prev)
 {
diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
index 2b33e977a905..5430def31043 100644
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@ -2159,6 +2159,97 @@ static int packet_rcv(struct sk_buff *skb, struct net_device *dev,
 	return 0;
 }
 
+/*
+ * [0] tpacket_rcv
+ * [0] dev_queue_xmit_nit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] ip_send_skb
+ * [0] icmp_reply.constprop.38
+ * [0] icmp_echo.part.31
+ * [0] icmp_echo
+ * [0] icmp_rcv
+ * [0] ip_protocol_deliver_rcu
+ * [0] ip_local_deliver_finish
+ * [0] ip_local_deliver
+ * [0] ip_sublist_rcv_finish
+ * [0] ip_sublist_rcv
+ * [0] ip_list_rcv
+ * [0] __netif_receive_skb_list_core
+ * [0] netif_receive_skb_list_internal
+ * [0] gro_normal_list.part.170
+ * [0] napi_complete_done
+ * [0] virtqueue_napi_complete
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ * 
+ * [0] tpacket_rcv
+ * [0] __netif_receive_skb_core
+ * [0] __netif_receive_skb_list_core
+ * [0] netif_receive_skb_list_internal
+ * [0] gro_normal_list.part.170
+ * [0] napi_complete_done
+ * [0] virtqueue_napi_complete
+ * [0] virtnet_poll
+ * [0] net_rx_action
+ * [0] __do_softirq
+ * [0] asm_call_irq_on_stack
+ * [0] </IRQ>
+ * [0] do_softirq_own_stack
+ * [0] irq_exit_rcu
+ * [0] common_interrupt
+ * [0] asm_common_interrupt
+ *
+ * [0] tpacket_rcv
+ * [0] dev_queue_xmit_nit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] __ip_queue_xmit
+ * [0] __tcp_transmit_skb
+ * [0] tcp_rcv_established
+ * [0] tcp_v4_do_rcv
+ * [0] __release_sock
+ * [0] release_sock
+ * [0] tcp_recvmsg
+ * [0] inet6_recvmsg
+ * [0] sock_read_iter
+ * [0] new_sync_read
+ * [0] vfs_read
+ * [0] ksys_read
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ *
+ * [0] tpacket_rcv
+ * [0] dev_queue_xmit_nit
+ * [0] dev_hard_start_xmit
+ * [0] sch_direct_xmit
+ * [0] __qdisc_run
+ * [0] __dev_queue_xmit
+ * [0] ip_finish_output2
+ * [0] ip_output
+ * [0] ip_send_skb
+ * [0] raw_sendmsg
+ * [0] sock_sendmsg
+ * [0] __sys_sendto
+ * [0] __x64_sys_sendto
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,
 		       struct packet_type *pt, struct net_device *orig_dev)
 {
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index cf88233b819a..78734271e2b8 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -108,6 +108,16 @@ static struct kmem_cache *kvm_vcpu_cache;
 static __read_mostly struct preempt_ops kvm_preempt_ops;
 static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_running_vcpu);
 
+/*
+ * 在以下使用kvm_debugfs_dir:
+ *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+ *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+ *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+ *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+ *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+ *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+ *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+ */
 struct dentry *kvm_debugfs_dir;
 EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
 
@@ -696,6 +706,26 @@ static int kvm_create_vm_debugfs(struct kvm *kvm, int fd)
 		return 0;
 
 	snprintf(dir_name, sizeof(dir_name), "%d-%d", task_pid_nr(current), fd);
+	/*
+	 * 在以下使用kvm->debugfs_dentry:
+	 *   - arch/arm64/kvm/vgic/vgic-debug.c|294| <<vgic_debug_init>> debugfs_create_file("vgic-state", 0444, kvm->debugfs_dentry, kvm,
+	 *   - virt/kvm/kvm_main.c|677| <<kvm_destroy_vm_debugfs>> if (!kvm->debugfs_dentry)
+	 *   - virt/kvm/kvm_main.c|680| <<kvm_destroy_vm_debugfs>> debugfs_remove_recursive(kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|716| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry, stat_data,
+	 *   - virt/kvm/kvm_main.c|3077| <<kvm_create_vcpu_debugfs>> vcpu->kvm->debugfs_dentry);
+	 *   - virt/kvm/kvm_main.c|4640| <<kvm_uevent_notify_change>> if (!IS_ERR_OR_NULL(kvm->debugfs_dentry)) {
+	 *   - virt/kvm/kvm_main.c|4644| <<kvm_uevent_notify_change>> tmp = dentry_path_raw(kvm->debugfs_dentry, p, PATH_MAX);
+	 *
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
 
 	kvm->debugfs_stat_data = kcalloc(kvm_debugfs_num_entries,
@@ -4657,6 +4687,16 @@ static void kvm_init_debug(void)
 {
 	struct kvm_stats_debugfs_item *p;
 
+	/*
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
 
 	kvm_debugfs_num_entries = 0;
@@ -4868,6 +4908,16 @@ EXPORT_SYMBOL_GPL(kvm_init);
 
 void kvm_exit(void)
 {
+	/*
+	 * 在以下使用kvm_debugfs_dir:
+	 *   - virt/kvm/kvm_main.c|112| <<global>> EXPORT_SYMBOL_GPL(kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/book3s_hv.c|4978| <<kvmppc_core_init_vm_hv>> kvm->arch.debugfs_dir = debugfs_create_dir(buf, kvm_debugfs_dir);
+	 *   - arch/powerpc/kvm/timing.c|214| <<kvmppc_create_vcpu_debugfs>> debugfs_file = debugfs_create_file(dbg_fname, 0666, kvm_debugfs_dir,
+	 *   - virt/kvm/kvm_main.c|699| <<kvm_create_vm_debugfs>> kvm->debugfs_dentry = debugfs_create_dir(dir_name, kvm_debugfs_dir);
+	 *   - virt/kvm/kvm_main.c|4660| <<kvm_init_debug>> kvm_debugfs_dir = debugfs_create_dir("kvm", NULL);
+	 *   - virt/kvm/kvm_main.c|4665| <<kvm_init_debug>> kvm_debugfs_dir, (void *)(long )p->offset,
+	 *   - virt/kvm/kvm_main.c|4871| <<kvm_exit>> debugfs_remove_recursive(kvm_debugfs_dir);
+	 */
 	debugfs_remove_recursive(kvm_debugfs_dir);
 	misc_deregister(&kvm_dev);
 	kmem_cache_destroy(kvm_vcpu_cache);
-- 
2.17.1

