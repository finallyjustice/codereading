From 6432a29eb9a08b974d55ae93f37bae61fa359ea4 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 2 Mar 2025 13:23:28 -0800
Subject: [PATCH 1/1] qemu for v9.2.0

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 accel/kvm/kvm-accel-ops.c         |   4 +
 accel/kvm/kvm-all.c               |  31 +++
 accel/tcg/plugin-gen.c            |   4 +
 backends/hostmem-ram.c            |   4 +
 block/block-backend.c             |  22 +++
 cpu-common.c                      |  71 +++++++
 dump/dump.c                       |  36 ++++
 hw/core/cpu-common.c              |  13 ++
 hw/net/virtio-net.c               |  11 ++
 hw/pci/msix.c                     |   9 +
 hw/scsi/vhost-scsi-common.c       |   3 +
 hw/scsi/virtio-scsi.c             |  75 ++++++++
 hw/virtio/vhost-user.c            |  10 +
 hw/virtio/vhost.c                 | 249 ++++++++++++++++++++++++
 hw/virtio/virtio.c                | 309 ++++++++++++++++++++++++++++++
 include/exec/memory.h             |  15 ++
 include/exec/ramblock.h           |  69 +++++++
 include/hw/core/cpu.h             |  22 +++
 include/hw/pci/pci_device.h       |  10 +
 include/hw/virtio/vhost.h         |  19 ++
 include/hw/virtio/virtio-access.h |  82 ++++++++
 include/hw/virtio/virtio-scsi.h   |   8 +
 include/net/filter.h              |  10 +
 include/sysemu/dump.h             |  22 +++
 migration/migration.c             |  45 +++++
 migration/migration.h             |   7 +
 migration/multifd-nocomp.c        |  20 ++
 migration/options.c               |  33 ++++
 migration/qemu-file.c             | 110 +++++++++++
 migration/ram.c                   | 298 ++++++++++++++++++++++++++++
 migration/rdma.c                  |  25 +++
 migration/savevm.c                | 100 ++++++++++
 net/dump.c                        |  18 ++
 net/filter.c                      |  16 ++
 net/net.c                         |  36 ++++
 system/cpus.c                     |  31 +++
 system/dma-helpers.c              |  17 ++
 system/memory.c                   |  21 ++
 system/physmem.c                  | 178 +++++++++++++++++
 target/arm/arm-qmp-cmds.c         |  18 ++
 target/arm/cpu64.c                |   4 +
 target/arm/kvm.c                  |  46 +++++
 target/i386/cpu.c                 |  47 +++++
 target/i386/cpu.h                 |   6 +
 target/i386/kvm/kvm.c             |  64 +++++++
 target/i386/machine.c             |  18 ++
 target/i386/tcg/misc_helper.c     |   3 +
 util/iov.c                        |  67 +++++++
 util/main-loop.c                  |  13 ++
 util/qemu-coroutine-io.c          |   5 +
 util/qemu-coroutine-lock.c        |   7 +
 util/qemu-coroutine.c             | 140 ++++++++++++++
 52 files changed, 2501 insertions(+)

diff --git a/accel/kvm/kvm-accel-ops.c b/accel/kvm/kvm-accel-ops.c
index c239dfc87..3e812dc88 100644
--- a/accel/kvm/kvm-accel-ops.c
+++ b/accel/kvm/kvm-accel-ops.c
@@ -26,6 +26,10 @@
 #include <linux/kvm.h>
 #include "kvm-cpus.h"
 
+/*
+ * 在以下使用kvm_vcpu_thread_fn():
+ *   - accel/kvm/kvm-accel-ops.c|71| <<kvm_start_vcpu_thread>> qemu_thread_create(cpu->thread, thread_name, kvm_vcpu_thread_fn,
+ */
 static void *kvm_vcpu_thread_fn(void *arg)
 {
     CPUState *cpu = arg;
diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index 801cff16a..bc809c494 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -437,6 +437,11 @@ int kvm_unpark_vcpu(KVMState *s, unsigned long vcpu_id)
     return kvm_fd;
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|472| <<kvm_create_and_park_vcpu>> ret = kvm_create_vcpu(cpu);
+ *   - accel/kvm/kvm-all.c|533| <<kvm_init_vcpu>> ret = kvm_create_vcpu(cpu);
+ */
 int kvm_create_vcpu(CPUState *cpu)
 {
     unsigned long vcpu_id = kvm_arch_vcpu_id(cpu);
@@ -465,6 +470,10 @@ int kvm_create_vcpu(CPUState *cpu)
     return 0;
 }
 
+/*
+ * called by:
+ *   - target/ppc/kvm.c|2361| <<kvmppc_cpu_realize>> ret = kvm_create_and_park_vcpu(cs);
+ */
 int kvm_create_and_park_vcpu(CPUState *cpu)
 {
     int ret = 0;
@@ -522,6 +531,10 @@ void kvm_destroy_vcpu(CPUState *cpu)
     }
 }
 
+/*
+ * called by:
+ *   - accel/kvm/kvm-accel-ops.c|41| <<kvm_vcpu_thread_fn>> r = kvm_init_vcpu(cpu, &error_fatal);
+ */
 int kvm_init_vcpu(CPUState *cpu, Error **errp)
 {
     KVMState *s = kvm_state;
@@ -530,6 +543,11 @@ int kvm_init_vcpu(CPUState *cpu, Error **errp)
 
     trace_kvm_init_vcpu(cpu->cpu_index, kvm_arch_vcpu_id(cpu));
 
+    /*
+     * called by:
+     *   - accel/kvm/kvm-all.c|472| <<kvm_create_and_park_vcpu>> ret = kvm_create_vcpu(cpu);
+     *   - accel/kvm/kvm-all.c|533| <<kvm_init_vcpu>> ret = kvm_create_vcpu(cpu);
+     */
     ret = kvm_create_vcpu(cpu);
     if (ret < 0) {
         error_setg_errno(errp, -ret,
@@ -1531,6 +1549,19 @@ static void kvm_set_phys_mem(KVMMemoryListener *kml,
         mem->ram_start_offset = ram_start_offset;
         mem->ram = ram;
         mem->flags = kvm_mem_flags(mr);
+        /*
+	 * 在以下使用RAMBlock->guest_memfd:
+         *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+         *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+         *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+         *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+         *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+         *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+         *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+         *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+         *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+         *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+	 */
         mem->guest_memfd = mr->ram_block->guest_memfd;
         mem->guest_memfd_offset = (uint8_t*)ram - mr->ram_block->host;
 
diff --git a/accel/tcg/plugin-gen.c b/accel/tcg/plugin-gen.c
index 0f47bfbb4..1f55ea684 100644
--- a/accel/tcg/plugin-gen.c
+++ b/accel/tcg/plugin-gen.c
@@ -45,6 +45,10 @@ void plugin_gen_disable_mem_helpers(void)
     }
 }
 
+/*
+ * called by:
+ *   - accel/tcg/plugin-gen.c|326| <<plugin_gen_inject>> gen_enable_mem_helper(plugin_tb, insn);
+ */
 static void gen_enable_mem_helper(struct qemu_plugin_tb *ptb,
                                   struct qemu_plugin_insn *insn)
 {
diff --git a/backends/hostmem-ram.c b/backends/hostmem-ram.c
index f7d81af78..bccb6b43e 100644
--- a/backends/hostmem-ram.c
+++ b/backends/hostmem-ram.c
@@ -16,6 +16,10 @@
 #include "qemu/module.h"
 #include "qom/object_interfaces.h"
 
+/*
+ * 在以下使用ram_backend_memory_alloc():
+ *   - backends/hostmem-ram.c|44| <<ram_backend_class_init>> bc->alloc = ram_backend_memory_alloc;
+ */
 static bool
 ram_backend_memory_alloc(HostMemoryBackend *backend, Error **errp)
 {
diff --git a/block/block-backend.c b/block/block-backend.c
index 85bcdedce..8e915f9da 100644
--- a/block/block-backend.c
+++ b/block/block-backend.c
@@ -2363,6 +2363,28 @@ bool blk_op_is_blocked(BlockBackend *blk, BlockOpType op, Error **errp)
  * which creates a drained section.  Therefore, incrementing such a BB's
  * in-flight counter will prevent its context from changing.
  */
+/*
+ * 非unittest的调用:
+ *   - block/block-backend.c|318| <<blk_root_get_parent_aio_context>> return blk_get_aio_context(blk);
+ *   - block/block-backend.c|2070| <<blk_drain>> AIO_WAIT_WHILE(blk_get_aio_context(blk), qatomic_read(&blk->in_flight) > 0);
+ *   - block/block-backend.c|2702| <<blk_io_limits_enable>> throttle_group_register_tgm(&blk->public.throttle_group_member, group, blk_get_aio_context(blk));
+ *   - block/export/fuse.c|97| <<fuse_export_drained_end>> exp->common.ctx = blk_get_aio_context(exp->common.blk);
+ *   - hw/core/qdev-properties-system.c|139| <<set_drive_helper>> ctx = blk_get_aio_context(blk);
+ *   - hw/ide/core.c|971| <<ide_dma_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk),
+ *   - hw/ide/macio.c|190| <<pmac_ide_transfer_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk), &s->sg,
+ *   - hw/scsi/scsi-bus.c|128| <<scsi_device_for_each_req_async_bh>> ctx = blk_get_aio_context(s->conf.blk);
+ *   - hw/scsi/scsi-bus.c|169| <<scsi_device_for_each_req_async_bh>> aio_bh_schedule_oneshot(blk_get_aio_context(s->conf.blk),
+ *   - hw/scsi/scsi-disk.c|333| <<scsi_aio_complete>> assert(blk_get_aio_context(s->qdev.conf.blk) ==
+ *   - hw/scsi/scsi-disk.c|438| <<scsi_read_complete_noio>> assert(blk_get_aio_context(s->qdev.conf.blk) ==
+ *   - hw/scsi/scsi-disk.c|491| <<scsi_do_read>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *   - hw/scsi/scsi-disk.c|571| <<scsi_write_complete_noio>> assert(blk_get_aio_context(s->qdev.conf.blk) ==
+ *   - hw/scsi/scsi-disk.c|654| <<scsi_write_data>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *   - hw/scsi/scsi-disk.c|2532| <<scsi_realize>> if (blk_get_aio_context(s->qdev.conf.blk) != qemu_get_aio_context() &&
+ *   - hw/scsi/virtio-scsi.c|361| <<virtio_scsi_ctx_check>> assert(blk_get_aio_context(d->conf.blk) == s->ctx);
+ *   - qemu-io-cmds.c|2579| <<wait_break_f>> aio_poll(blk_get_aio_context(blk), true);
+ *   - system/dma-helpers.c|263| <<wait_break_f>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ *   - system/dma-helpers.c|281| <<wait_break_f>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ */
 AioContext *blk_get_aio_context(BlockBackend *blk)
 {
     IO_CODE();
diff --git a/cpu-common.c b/cpu-common.c
index 0d607bbe4..bc6ea2a4e 100644
--- a/cpu-common.c
+++ b/cpu-common.c
@@ -131,9 +131,28 @@ struct qemu_work_item {
     bool free, exclusive, done;
 };
 
+/*
+ * called by:
+ *   - cpu-common.c|173| <<do_run_on_cpu>> queue_work_on_cpu(cpu, &wi);
+ *   - cpu-common.c|191| <<async_run_on_cpu>> queue_work_on_cpu(cpu, wi);
+ *   - cpu-common.c|348| <<async_safe_run_on_cpu>> queue_work_on_cpu(cpu, wi);
+ */
 static void queue_work_on_cpu(CPUState *cpu, struct qemu_work_item *wi)
 {
     qemu_mutex_lock(&cpu->work_mutex);
+    /*
+     * 在以下使用CPUState->work_list:
+     *   - cpu-common.c|137| <<queue_work_on_cpu>> QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
+     *   - cpu-common.c|340| <<free_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|341| <<free_queued_cpu_work>> struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|342| <<free_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - cpu-common.c|354| <<process_queued_cpu_work>> if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|358| <<process_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|359| <<process_queued_cpu_work>> wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|360| <<process_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - hw/core/cpu-common.c|257| <<cpu_common_initfn>> QSIMPLEQ_INIT(&cpu->work_list);
+     *   - system/cpus.c|83| <<cpu_work_list_empty>> return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
+     */
     QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
     wi->done = false;
     qemu_mutex_unlock(&cpu->work_mutex);
@@ -337,6 +356,19 @@ void async_safe_run_on_cpu(CPUState *cpu, run_on_cpu_func func,
 
 void free_queued_cpu_work(CPUState *cpu)
 {
+    /*
+     * 在以下使用CPUState->work_list:
+     *   - cpu-common.c|137| <<queue_work_on_cpu>> QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
+     *   - cpu-common.c|340| <<free_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|341| <<free_queued_cpu_work>> struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|342| <<free_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - cpu-common.c|354| <<process_queued_cpu_work>> if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|358| <<process_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|359| <<process_queued_cpu_work>> wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|360| <<process_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - hw/core/cpu-common.c|257| <<cpu_common_initfn>> QSIMPLEQ_INIT(&cpu->work_list);
+     *   - system/cpus.c|83| <<cpu_work_list_empty>> return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
+     */
     while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
         struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
         QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
@@ -346,11 +378,50 @@ void free_queued_cpu_work(CPUState *cpu)
     }
 }
 
+/*
+ * called by:
+ *   - bsd-user/aarch64/target_arch_cpu.h|57| <<target_cpu_loop>> process_queued_cpu_work(cs);
+ *   - bsd-user/arm/target_arch_cpu.h|49| <<target_cpu_loop>> process_queued_cpu_work(cs);
+ *   - bsd-user/i386/target_arch_cpu.h|116| <<target_cpu_loop>> process_queued_cpu_work(cs);
+ *   - bsd-user/riscv/target_arch_cpu.h|52| <<target_cpu_loop>> process_queued_cpu_work(cs);
+ *   - bsd-user/x86_64/target_arch_cpu.h|124| <<target_cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/aarch64/cpu_loop.c|89| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/alpha/cpu_loop.c|38| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/arm/cpu_loop.c|331| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/hexagon/cpu_loop.c|39| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/hppa/cpu_loop.c|122| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/i386/cpu_loop.c|217| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/loongarch64/cpu_loop.c|24| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/m68k/cpu_loop.c|36| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/microblaze/cpu_loop.c|35| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/mips/cpu_loop.c|77| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/openrisc/cpu_loop.c|36| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/ppc/cpu_loop.c|80| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/riscv/cpu_loop.c|39| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/s390x/cpu_loop.c|67| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/sh4/cpu_loop.c|37| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/sparc/cpu_loop.c|223| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - linux-user/xtensa/cpu_loop.c|136| <<cpu_loop>> process_queued_cpu_work(cs);
+ *   - system/cpus.c|453| <<qemu_wait_io_event_common>> process_queued_cpu_work(cpu);
+ */
 void process_queued_cpu_work(CPUState *cpu)
 {
     struct qemu_work_item *wi;
 
     qemu_mutex_lock(&cpu->work_mutex);
+    /*
+     * 在以下使用CPUState->work_list:
+     *   - cpu-common.c|137| <<queue_work_on_cpu>> QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
+     *   - cpu-common.c|340| <<free_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|341| <<free_queued_cpu_work>> struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|342| <<free_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - cpu-common.c|354| <<process_queued_cpu_work>> if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|358| <<process_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|359| <<process_queued_cpu_work>> wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|360| <<process_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - hw/core/cpu-common.c|257| <<cpu_common_initfn>> QSIMPLEQ_INIT(&cpu->work_list);
+     *   - system/cpus.c|83| <<cpu_work_list_empty>> return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
+     */
     if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
         qemu_mutex_unlock(&cpu->work_mutex);
         return;
diff --git a/dump/dump.c b/dump/dump.c
index 45e84428a..b1da6187d 100644
--- a/dump/dump.c
+++ b/dump/dump.c
@@ -279,11 +279,38 @@ static inline int cpu_index(CPUState *cpu)
     return cpu->cpu_index + 1;
 }
 
+/*
+ * called by:
+ *   - dump/dump.c|319| <<write_elf64_notes>> write_guest_note(f, s, errp);
+ *   - dump/dump.c|357| <<write_elf32_notes>> write_guest_note(f, s, errp);
+ */
 static void write_guest_note(WriteCoreDumpFunction f, DumpState *s,
                              Error **errp)
 {
     int ret;
 
+    /*
+     * 在以下使用guest_note:
+     *   - dump/dump.c|107| <<dump_cleanup>> g_free(s->guest_note);
+     *   - dump/dump.c|109| <<dump_cleanup>> s->guest_note = NULL;
+     *   - dump/dump.c|287| <<write_guest_note>> if (s->guest_note) {
+     *   - dump/dump.c|288| <<write_guest_note>> ret = f(s->guest_note, s->guest_note_size, s);
+     *   - dump/dump.c|1024| <<create_header32>> if (s->guest_note &&
+     *   - dump/dump.c|1025| <<create_header32>> note_name_equal(s, s->guest_note, "VMCOREINFO")) {
+     *   - dump/dump.c|1028| <<create_header32>> get_note_sizes(s, s->guest_note,
+     *   - dump/dump.c|1135| <<create_header64>> if (s->guest_note &&
+     *   - dump/dump.c|1136| <<create_header64>> note_name_equal(s, s->guest_note, "VMCOREINFO")) {
+     *   - dump/dump.c|1139| <<create_header64>> get_note_sizes(s, s->guest_note,
+     *   - dump/dump.c|1746| <<vmcoreinfo_update_phys_base>> if (!note_name_equal(s, s->guest_note, "VMCOREINFO")) {
+     *   - dump/dump.c|1750| <<vmcoreinfo_update_phys_base>> get_note_sizes(s, s->guest_note, &note_head_size, &name_size, &size);
+     *   - dump/dump.c|1753| <<vmcoreinfo_update_phys_base>> vmci = s->guest_note + note_head_size + ROUND_UP(name_size, 4);
+     *   - dump/dump.c|1890| <<dump_init>> s->guest_note = g_malloc(size + 1);
+     *   - dump/dump.c|1891| <<dump_init>> cpu_physical_memory_read(addr, s->guest_note, size);
+     *   - dump/dump.c|1893| <<dump_init>> get_note_sizes(s, s->guest_note, NULL, &name_size, &desc_size);
+     *   - dump/dump.c|1900| <<dump_init>> g_free(s->guest_note);
+     *   - dump/dump.c|1901| <<dump_init>> s->guest_note = NULL;
+     *   - dump/win_dump.c|410| <<create_win_dump>> WinDumpHeader *h = (void *)(s->guest_note + VMCOREINFO_ELF_NOTE_HDR_SIZE);
+     */
     if (s->guest_note) {
         ret = f(s->guest_note, s->guest_note_size, s);
         if (ret < 0) {
@@ -292,6 +319,11 @@ static void write_guest_note(WriteCoreDumpFunction f, DumpState *s,
     }
 }
 
+/*
+ * called by:
+ *   - dump/dump.c|651| <<write_elf_notes>> write_elf64_notes(fd_write_vmcore, s, errp);
+ *   - dump/dump.c|1161| <<create_header64>> write_elf64_notes(buf_write_note, s, errp);
+ */
 static void write_elf64_notes(WriteCoreDumpFunction f, DumpState *s,
                               Error **errp)
 {
@@ -645,6 +677,10 @@ static void write_elf_phdr_loads(DumpState *s, Error **errp)
     }
 }
 
+/*
+ * called by:
+ *   - dump/dump.c|711| <<dump_begin>> write_elf_notes(s, errp);
+ */
 static void write_elf_notes(DumpState *s, Error **errp)
 {
     if (dump_is_64bit(s)) {
diff --git a/hw/core/cpu-common.c b/hw/core/cpu-common.c
index 09c790359..ac3e1429c 100644
--- a/hw/core/cpu-common.c
+++ b/hw/core/cpu-common.c
@@ -254,6 +254,19 @@ static void cpu_common_initfn(Object *obj)
 
     qemu_mutex_init(&cpu->work_mutex);
     qemu_lockcnt_init(&cpu->in_ioctl_lock);
+    /*
+     * 在以下使用CPUState->work_list:
+     *   - cpu-common.c|137| <<queue_work_on_cpu>> QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
+     *   - cpu-common.c|340| <<free_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|341| <<free_queued_cpu_work>> struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|342| <<free_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - cpu-common.c|354| <<process_queued_cpu_work>> if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|358| <<process_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|359| <<process_queued_cpu_work>> wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|360| <<process_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - hw/core/cpu-common.c|257| <<cpu_common_initfn>> QSIMPLEQ_INIT(&cpu->work_list);
+     *   - system/cpus.c|83| <<cpu_work_list_empty>> return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
+     */
     QSIMPLEQ_INIT(&cpu->work_list);
     QTAILQ_INIT(&cpu->breakpoints);
     QTAILQ_INIT(&cpu->watchpoints);
diff --git a/hw/net/virtio-net.c b/hw/net/virtio-net.c
index 6e8c51a2d..bdae06685 100644
--- a/hw/net/virtio-net.c
+++ b/hw/net/virtio-net.c
@@ -222,6 +222,11 @@ static bool virtio_net_started(VirtIONet *n, uint8_t status)
         (n->status & VIRTIO_NET_S_LINK_UP) && vdev->vm_running;
 }
 
+/*
+ * called by:
+ *   - hw/net/virtio-net.c|240| <<virtio_net_announce_timer>> virtio_net_announce_notify(n);
+ *   - hw/net/virtio-net.c|259| <<virtio_net_announce>> virtio_net_announce_notify(n);
+ */
 static void virtio_net_announce_notify(VirtIONet *net)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(net);
@@ -240,6 +245,9 @@ static void virtio_net_announce_timer(void *opaque)
     virtio_net_announce_notify(n);
 }
 
+/*
+ * NetClientInfo net_virtio_info.announce = virtio_net_announce()
+ */
 static void virtio_net_announce(NetClientState *nc)
 {
     VirtIONet *n = qemu_get_nic_opaque(nc);
@@ -439,6 +447,9 @@ static void virtio_net_set_status(struct VirtIODevice *vdev, uint8_t status)
     }
 }
 
+/*
+ * NetClientInfo net_virtio_inf.link_status_changed = virtio_net_set_link_status()
+ */
 static void virtio_net_set_link_status(NetClientState *nc)
 {
     VirtIONet *n = qemu_get_nic_opaque(nc);
diff --git a/hw/pci/msix.c b/hw/pci/msix.c
index 487e49834..8e5316a24 100644
--- a/hw/pci/msix.c
+++ b/hw/pci/msix.c
@@ -615,6 +615,15 @@ static void msix_unset_notifier_for_vector(PCIDevice *dev, unsigned int vector)
     dev->msix_vector_release_notifier(dev, vector);
 }
 
+/*
+ * called by:
+ *   - hw/misc/ivshmem.c|775| <<ivshmem_enable_irqfd>> if (msix_set_vector_notifiers(pdev,
+ *            ivshmem_vector_unmask, ivshmem_vector_mask, ivshmem_vector_poll)) {
+ *   - hw/vfio/pci.c|686| <<vfio_msix_enable>> if (msix_set_vector_notifiers(&vdev->pdev,
+ *            vfio_msix_vector_use, vfio_msix_vector_release, NULL)) {
+ *   - hw/virtio/virtio-pci.c|1290| <<virtio_pci_set_guest_notifiers>> r = msix_set_vector_notifiers(&proxy->pci_dev,
+ *            virtio_pci_vector_unmask, virtio_pci_vector_mask, virtio_pci_vector_poll);
+ */
 int msix_set_vector_notifiers(PCIDevice *dev,
                               MSIVectorUseNotifier use_notifier,
                               MSIVectorReleaseNotifier release_notifier,
diff --git a/hw/scsi/vhost-scsi-common.c b/hw/scsi/vhost-scsi-common.c
index 4c8637045..652390edb 100644
--- a/hw/scsi/vhost-scsi-common.c
+++ b/hw/scsi/vhost-scsi-common.c
@@ -78,6 +78,9 @@ int vhost_scsi_common_start(VHostSCSICommon *vsc, Error **errp)
         }
     }
 
+    /*
+     * 里面会调用vhost_dev_set_features()
+     */
     ret = vhost_dev_start(&vsc->dev, vdev, true);
     if (ret < 0) {
         error_setg_errno(errp, -ret, "Error starting vhost dev");
diff --git a/hw/scsi/virtio-scsi.c b/hw/scsi/virtio-scsi.c
index 6637cfeaf..27e0c9377 100644
--- a/hw/scsi/virtio-scsi.c
+++ b/hw/scsi/virtio-scsi.c
@@ -82,6 +82,11 @@ static inline SCSIDevice *virtio_scsi_device_get(VirtIOSCSI *s, uint8_t *lun)
     return scsi_device_get(&s->bus, 0, lun[1], virtio_scsi_get_lun(lun));
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|247| <<virtio_scsi_pop_req>> virtio_scsi_init_req(s, vq, req);
+ *   - hw/scsi/virtio-scsi.c|276| <<virtio_scsi_load_request>> virtio_scsi_init_req(s, vs->cmd_vqs[n], req);
+ */
 static void virtio_scsi_init_req(VirtIOSCSI *s, VirtQueue *vq, VirtIOSCSIReq *req)
 {
     VirtIODevice *vdev = VIRTIO_DEVICE(s);
@@ -91,6 +96,12 @@ static void virtio_scsi_init_req(VirtIOSCSI *s, VirtQueue *vq, VirtIOSCSIReq *re
     req->vq = vq;
     req->dev = s;
     qemu_sglist_init(&req->qsgl, DEVICE(s), 8, vdev->dma_as);
+    /*
+     * VirtIOSCSIReq *req:
+     * -> VirtQueueElement elem;
+     * -> QEMUSGList qsgl;
+     * -> QEMUIOVector resp_iov;
+     */
     qemu_iovec_init(&req->resp_iov, 1);
     memset((uint8_t *)req + zero_skip, 0, sizeof(*req) - zero_skip);
 }
@@ -177,12 +188,31 @@ static size_t qemu_sgl_concat(VirtIOSCSIReq *req, struct iovec *iov,
     return copied;
 }
 
+/*
+ * called by:
+ */
 static int virtio_scsi_parse_req(VirtIOSCSIReq *req,
                                  unsigned req_size, unsigned resp_size)
 {
     VirtIODevice *vdev = (VirtIODevice *) req->dev;
     size_t in_size, out_size;
 
+    /*
+     * VirtIOSCSIReq *req:
+     *     union {
+     *         VirtIOSCSICmdResp     cmd;
+     *         VirtIOSCSICtrlTMFResp tmf;
+     *         VirtIOSCSICtrlANResp  an;
+     *         VirtIOSCSIEvent       event;
+     *     } resp;
+     *     union {
+     *         VirtIOSCSICmdReq      cmd;
+     *         VirtIOSCSICtrlTMFReq  tmf;
+     *         VirtIOSCSICtrlANReq   an;
+     *     } req;
+     *
+     * 把req header拷贝到req->req
+     */
     if (iov_to_buf(req->elem.out_sg, req->elem.out_num, 0,
                    &req->req, req_size) < req_size) {
         return -EINVAL;
@@ -235,15 +265,30 @@ static int virtio_scsi_parse_req(VirtIOSCSIReq *req,
     return 0;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|627| <<virtio_scsi_handle_ctrl_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+ *   - hw/scsi/virtio-scsi.c|860| <<virtio_scsi_handle_cmd_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+ *   - hw/scsi/virtio-scsi.c|990| <<virtio_scsi_push_event>> req = virtio_scsi_pop_req(s, vs->event_vq);
+ */
 static VirtIOSCSIReq *virtio_scsi_pop_req(VirtIOSCSI *s, VirtQueue *vq)
 {
     VirtIOSCSICommon *vs = (VirtIOSCSICommon *)s;
     VirtIOSCSIReq *req;
 
+    /*
+     * VirtIOSCSIReq *req:
+     * -> VirtQueueElement elem;
+     */
     req = virtqueue_pop(vq, sizeof(VirtIOSCSIReq) + vs->cdb_size);
     if (!req) {
         return NULL;
     }
+    /*
+     * called by:
+     *   - hw/scsi/virtio-scsi.c|247| <<virtio_scsi_pop_req>> virtio_scsi_init_req(s, vq, req);
+     *   - hw/scsi/virtio-scsi.c|276| <<virtio_scsi_load_request>> virtio_scsi_init_req(s, vs->cmd_vqs[n], req);
+     */
     virtio_scsi_init_req(s, vq, req);
     return req;
 }
@@ -554,6 +599,10 @@ fail:
     return ret;
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|614| <<virtio_scsi_handle_ctrl_vq>> virtio_scsi_handle_ctrl_req(s, req);
+ */
 static void virtio_scsi_handle_ctrl_req(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     VirtIODevice *vdev = (VirtIODevice *)s;
@@ -610,6 +659,12 @@ static void virtio_scsi_handle_ctrl_vq(VirtIOSCSI *s, VirtQueue *vq)
 {
     VirtIOSCSIReq *req;
 
+    /*
+     * called by:
+     *   - hw/scsi/virtio-scsi.c|627| <<virtio_scsi_handle_ctrl_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+     *   - hw/scsi/virtio-scsi.c|860| <<virtio_scsi_handle_cmd_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+     *   - hw/scsi/virtio-scsi.c|990| <<virtio_scsi_push_event>> req = virtio_scsi_pop_req(s, vs->event_vq);
+     */
     while ((req = virtio_scsi_pop_req(s, vq))) {
         virtio_scsi_handle_ctrl_req(s, req);
     }
@@ -775,6 +830,10 @@ static void virtio_scsi_fail_cmd_req(VirtIOSCSIReq *req)
     virtio_scsi_complete_cmd_req(req);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|861| <<virtio_scsi_handle_cmd_vq>> ret = virtio_scsi_handle_cmd_req_prepare(s, req);
+ */
 static int virtio_scsi_handle_cmd_req_prepare(VirtIOSCSI *s, VirtIOSCSIReq *req)
 {
     VirtIOSCSICommon *vs = VIRTIO_SCSI_COMMON(s);
@@ -843,6 +902,12 @@ static void virtio_scsi_handle_cmd_vq(VirtIOSCSI *s, VirtQueue *vq)
             virtio_queue_set_notification(vq, 0);
         }
 
+        /*
+	 * called by:
+	 *   - hw/scsi/virtio-scsi.c|627| <<virtio_scsi_handle_ctrl_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+	 *   - hw/scsi/virtio-scsi.c|860| <<virtio_scsi_handle_cmd_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+	 *   - hw/scsi/virtio-scsi.c|990| <<virtio_scsi_push_event>> req = virtio_scsi_pop_req(s, vs->event_vq);
+	 */
         while ((req = virtio_scsi_pop_req(s, vq))) {
             ret = virtio_scsi_handle_cmd_req_prepare(s, req);
             if (!ret) {
@@ -973,6 +1038,12 @@ static void virtio_scsi_push_event(VirtIOSCSI *s,
         return;
     }
 
+    /*
+     * called by:
+     *   - hw/scsi/virtio-scsi.c|627| <<virtio_scsi_handle_ctrl_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+     *   - hw/scsi/virtio-scsi.c|860| <<virtio_scsi_handle_cmd_vq>> while ((req = virtio_scsi_pop_req(s, vq))) {
+     *   - hw/scsi/virtio-scsi.c|990| <<virtio_scsi_push_event>> req = virtio_scsi_pop_req(s, vs->event_vq);
+     */
     req = virtio_scsi_pop_req(s, vs->event_vq);
     if (!req) {
         s->events_dropped = true;
@@ -1008,6 +1079,10 @@ static void virtio_scsi_push_event(VirtIOSCSI *s,
     virtio_scsi_complete_req(req);
 }
 
+/*
+ * called by:
+ *   - hw/scsi/virtio-scsi.c|1100| <<virtio_scsi_handle_event>> virtio_scsi_handle_event_vq(s, vq);
+ */
 static void virtio_scsi_handle_event_vq(VirtIOSCSI *s, VirtQueue *vq)
 {
     if (s->events_dropped) {
diff --git a/hw/virtio/vhost-user.c b/hw/virtio/vhost-user.c
index f170f0b25..21a7dae8e 100644
--- a/hw/virtio/vhost-user.c
+++ b/hw/virtio/vhost-user.c
@@ -502,6 +502,11 @@ static void vhost_user_fill_msg_region(VhostUserMemoryRegion *dst,
     dst->mmap_offset = mmap_offset;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost-user.c|923| <<vhost_user_set_mem_table_postcopy>> ret = vhost_user_fill_set_mem_table_msg(u, dev, &msg, fds, &fd_num,
+ *   - hw/virtio/vhost-user.c|1041| <<vhost_user_set_mem_table>> ret = vhost_user_fill_set_mem_table_msg(u, dev, &msg, fds, &fd_num,
+ */
 static int vhost_user_fill_set_mem_table_msg(struct vhost_user *u,
                                              struct vhost_dev *dev,
                                              VhostUserMsg *msg,
@@ -1038,6 +1043,11 @@ static int vhost_user_set_mem_table(struct vhost_dev *dev,
             return ret;
         }
     } else {
+	/*
+	 * called by:
+	 *   - hw/virtio/vhost-user.c|923| <<vhost_user_set_mem_table_postcopy>> ret = vhost_user_fill_set_mem_table_msg(u, dev, &msg, fds, &fd_num,
+	 *   - hw/virtio/vhost-user.c|1041| <<vhost_user_set_mem_table>> ret = vhost_user_fill_set_mem_table_msg(u, dev, &msg, fds, &fd_num,
+	 */
         ret = vhost_user_fill_set_mem_table_msg(u, dev, &msg, fds, &fd_num,
                                                 false);
         if (ret < 0) {
diff --git a/hw/virtio/vhost.c b/hw/virtio/vhost.c
index c40f48ac4..410176b64 100644
--- a/hw/virtio/vhost.c
+++ b/hw/virtio/vhost.c
@@ -338,6 +338,86 @@ static int vhost_set_backend_type(struct vhost_dev *dev,
     return r;
 }
 
+/*
+ * 关于cancel migration.
+ *
+ * vhost_net启动的时候.
+ *
+ * (gdb) bt
+ * #0  vhost_dev_set_features (dev=0x555557686cc0, enable_log=false) at ../hw/virtio/vhost.c:989
+ * #1  0x0000555555b94909 in vhost_dev_start (hdev=0x555557686cc0, vdev=0x555558428ba0, vrings=false) at ../hw/virtio/vhost.c:2082
+ * #2  0x0000555555a47c24 in vhost_net_start_one (net=0x555557686cc0, dev=0x555558428ba0) at ../hw/net/vhost_net.c:280
+ * #3  0x0000555555a48278 in vhost_net_start (dev=0x555558428ba0, ncs=0x555558459418, data_queue_pairs=2, cvq=0) at ../hw/net/vhost_net.c:421
+ * #4  0x0000555555df0a1b in virtio_net_vhost_status (n=0x555558428ba0, status=15 '\017') at ../hw/net/virtio-net.c:311
+ * #5  0x0000555555df0d2b in virtio_net_set_status (vdev=0x555558428ba0, status=15 '\017') at ../hw/net/virtio-net.c:393
+ * #6  0x0000555555e33c08 in virtio_set_status (vdev=0x555558428ba0, val=15 '\017') at ../hw/virtio/virtio.c:2242
+ * #7  0x0000555555b7b531 in virtio_pci_common_write (opaque=0x555558420720, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1608
+ * #8  0x0000555555e63c32 in memory_region_write_accessor (mr=0x555558421310, addr=20, value=0x7fffe9ed43f8, size=1, shift=0, mask=255, attrs=...)
+ *     at ../system/memory.c:497
+ * #9  0x0000555555e63f79 in access_with_adjusted_size (addr=20, value=0x7fffe9ed43f8, size=1, access_size_min=1, access_size_max=4,
+ *     access_fn=0x555555e63b3c <memory_region_write_accessor>, mr=0x555558421310, attrs=...) at ../system/memory.c:573
+ * #10 0x0000555555e674ae in memory_region_dispatch_write (mr=0x555558421310, addr=20, data=15, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #11 0x0000555555e76bdc in flatview_write_continue_step (attrs=..., buf=0x7ffff0036028 "\017", len=1, mr_addr=20, l=0x7fffe9ed44e0, mr=0x555558421310)
+ *     at ../system/physmem.c:2803
+ * #12 0x0000555555e76cae in flatview_write_continue (fv=0x7ffe4c000fc0, addr=4261412884, attrs=..., ptr=0x7ffff0036028, len=1, mr_addr=20, l=1,
+ *     mr=0x555558421310) at ../system/physmem.c:2833
+ * #13 0x0000555555e76dd6 in flatview_write (fv=0x7ffe4c000fc0, addr=4261412884, attrs=..., buf=0x7ffff0036028, len=1) at ../system/physmem.c:2864
+ * #14 0x0000555555e77250 in address_space_write (as=0x555557328200 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7ffff0036028, len=1)
+ *     at ../system/physmem.c:2984
+ * #15 0x0000555555e772c8 in address_space_rw (as=0x555557328200 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7ffff0036028, len=1,
+ *     is_write=true) at ../system/physmem.c:2994
+ * #16 0x0000555555ed4d63 in kvm_cpu_exec (cpu=0x5555577413a0) at ../accel/kvm/kvm-all.c:3075
+ * #17 0x0000555555ed8469 in kvm_vcpu_thread_fn (arg=0x5555577413a0) at ../accel/kvm/kvm-accel-ops.c:50
+ * #18 0x000055555616b040 in qemu_thread_start (args=0x55555774a370) at ../util/qemu-thread-posix.c:541
+ * #19 0x00007ffff68aa1da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #20 0x00007ffff525ae73 in clone () from /lib/../lib64/libc.so.6
+ *
+ * vhost_net开始迁移的时候.
+ *
+ * (gdb) bt
+ * #0  vhost_dev_set_features (dev=0x555557686cc0, enable_log=true) at ../hw/virtio/vhost.c:989
+ * #1  0x0000555555b919e5 in vhost_dev_set_log (dev=0x555557686cc0, enable_log=true) at ../hw/virtio/vhost.c:1024
+#2  0x0000555555b91c5a in vhost_migration_log (listener=0x555557686cc8, enable=true) at ../hw/virtio/vhost.c:1093
+#3  0x0000555555b91ccf in vhost_log_global_start (listener=0x555557686cc8, errp=0x7ffe2fbfa628) at ../hw/virtio/vhost.c:1125
+#4  0x0000555555e6b859 in memory_global_dirty_log_do_start (errp=0x7ffe2fbfa628) at ../system/memory.c:2902
+#5  0x0000555555e6b98d in memory_global_dirty_log_start (flags=1, errp=0x7ffe2fbfa628) at ../system/memory.c:2941
+#6  0x0000555555e84d5d in ram_init_bitmaps (rs=0x7ffe20007d40, errp=0x7ffe2fbfa628) at ../migration/ram.c:2782
+#7  0x0000555555e84e6e in ram_init_all (rsp=0x555557328500 <ram_state>, errp=0x7ffe2fbfa628) at ../migration/ram.c:2816
+#8  0x0000555555e854fc in ram_save_setup (f=0x555557674610, opaque=0x555557328500 <ram_state>, errp=0x7ffe2fbfa628) at ../migration/ram.c:3010
+#9  0x0000555555c2fe16 in qemu_savevm_state_setup (f=0x555557674610, errp=0x7ffe2fbfa628) at ../migration/savevm.c:1346
+#10 0x0000555555c19305 in migration_thread (opaque=0x5555573fe000) at ../migration/migration.c:3507
+#11 0x000055555616b040 in qemu_thread_start (args=0x555557753b70) at ../util/qemu-thread-posix.c:541
+#12 0x00007ffff68aa1da in start_thread () from /lib/../lib64/libpthread.so.0
+#13 0x00007ffff525ae73 in clone () from /lib/../lib64/libc.so.6
+
+vhost_net cancel迁移的时候.
+
+(gdb) bt
+#0  vhost_dev_set_features (dev=0x5555573f9850, enable_log=false) at ../hw/virtio/vhost.c:989
+#1  0x0000555555b919e5 in vhost_dev_set_log (dev=0x5555573f9850, enable_log=false) at ../hw/virtio/vhost.c:1024
+#2  0x0000555555b91c0f in vhost_migration_log (listener=0x5555573f9858, enable=false) at ../hw/virtio/vhost.c:1086
+#3  0x0000555555b91d0c in vhost_log_global_stop (listener=0x5555573f9858) at ../hw/virtio/vhost.c:1136
+#4  0x0000555555e6bacd in memory_global_dirty_log_do_stop (flags=1) at ../system/memory.c:2966
+#5  0x0000555555e6bc06 in memory_global_dirty_log_stop (flags=1) at ../system/memory.c:3012
+#6  0x0000555555e84004 in ram_save_cleanup (opaque=0x555557328500 <ram_state>) at ../migration/ram.c:2383
+#7  0x0000555555c30c66 in qemu_savevm_state_cleanup () at ../migration/savevm.c:1699
+#8  0x0000555555c1494a in migrate_fd_cleanup (s=0x5555573fe000) at ../migration/migration.c:1409
+#9  0x0000555555c14b5e in migrate_fd_cleanup_bh (opaque=0x5555573fe000) at ../migration/migration.c:1456
+#10 0x0000555555c120bb in migration_bh_dispatch_bh (opaque=0x7ffe20002cf0) at ../migration/migration.c:284
+#11 0x0000555556186520 in aio_bh_call (bh=0x7ffe2011c7d0) at ../util/async.c:171
+#12 0x000055555618666e in aio_bh_poll (ctx=0x5555573fd770) at ../util/async.c:218
+#13 0x0000555556165595 in aio_dispatch (ctx=0x5555573fd770) at ../util/aio-posix.c:423
+#14 0x0000555556186b3d in aio_ctx_dispatch (source=0x5555573fd770, callback=0x0, user_data=0x0) at ../util/async.c:360
+#15 0x00007ffff6fd494b in g_main_dispatch (context=0x5555573fdcb0) at ../glib/gmain.c:3325
+#16 g_main_context_dispatch (context=0x5555573fdcb0) at ../glib/gmain.c:4043
+#17 0x0000555556188215 in glib_pollfds_poll () at ../util/main-loop.c:287
+#18 0x00005555561882a3 in os_host_main_loop_wait (timeout=942440) at ../util/main-loop.c:310
+#19 0x00005555561883d2 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+#20 0x0000555555be0f17 in qemu_main_loop () at ../system/runstate.c:826
+#21 0x0000555556093994 in qemu_default_main () at ../system/main.c:37
+#22 0x00005555560939d1 in main (argc=16, argv=0x7fffffffdc68) at ../system/main.c:48
+ */
+
 static struct vhost_log *vhost_log_alloc(uint64_t size, bool share)
 {
     Error *err = NULL;
@@ -367,6 +447,11 @@ static struct vhost_log *vhost_log_alloc(uint64_t size, bool share)
     return log;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|443| <<vhost_dev_log_resize>> struct vhost_log *log = vhost_log_get(dev->vhost_ops->backend_type,
+ *   - hw/virtio/vhost.c|2114| <<vhost_dev_start>> hdev->log = vhost_log_get(hdev->vhost_ops->backend_type,
+ */
 static struct vhost_log *vhost_log_get(VhostBackendType backend_type,
                                        uint64_t size, bool share)
 {
@@ -391,6 +476,13 @@ static struct vhost_log *vhost_log_get(VhostBackendType backend_type,
     return log;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|455| <<vhost_dev_log_resize>> vhost_log_put(dev, true);
+ *   - hw/virtio/vhost.c|1139| <<vhost_migration_log>> vhost_log_put(dev, false);
+ *   - hw/virtio/vhost.c|2168| <<vhost_dev_start>> vhost_log_put(hdev, false);
+ *   - hw/virtio/vhost.c|2225| <<vhost_dev_stop>> vhost_log_put(hdev, true);
+ */
 static void vhost_log_put(struct vhost_dev *dev, bool sync)
 {
     struct vhost_log *log = dev->log;
@@ -438,8 +530,19 @@ static bool vhost_dev_log_is_shared(struct vhost_dev *dev)
            dev->vhost_ops->vhost_requires_shm_log(dev);
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|715| <<vhost_commit>> vhost_dev_log_resize(dev, log_size + VHOST_LOG_BUFFER);
+ *   - hw/virtio/vhost.c|723| <<vhost_commit>> vhost_dev_log_resize(dev, log_size);
+ *   - hw/virtio/vhost.c|1141| <<vhost_migration_log>> vhost_dev_log_resize(dev, vhost_get_log_size(dev));
+ */
 static inline void vhost_dev_log_resize(struct vhost_dev *dev, uint64_t size)
 {
+    /*
+     * called by:
+     *   - hw/virtio/vhost.c|443| <<vhost_dev_log_resize>> struct vhost_log *log = vhost_log_get(dev->vhost_ops->backend_type,
+     *   - hw/virtio/vhost.c|2114| <<vhost_dev_start>> hdev->log = vhost_log_get(hdev->vhost_ops->backend_type,
+     */
     struct vhost_log *log = vhost_log_get(dev->vhost_ops->backend_type,
                                           size, vhost_dev_log_is_shared(dev));
     uint64_t log_base = (uintptr_t)log->log;
@@ -447,11 +550,23 @@ static inline void vhost_dev_log_resize(struct vhost_dev *dev, uint64_t size)
 
     /* inform backend of log switching, this must be done before
        releasing the current log, to ensure no logging is lost */
+    /*
+     * vhost_kernel_set_log_base() -> VHOST_SET_LOG_BASE
+     * vhost_user_set_log_base()
+     * vhost_vdpa_set_log_base()
+     */
     r = dev->vhost_ops->vhost_set_log_base(dev, log_base, log);
     if (r < 0) {
         VHOST_OPS_DEBUG(r, "vhost_set_log_base failed");
     }
 
+    /*
+     * called by:
+     *   - hw/virtio/vhost.c|455| <<vhost_dev_log_resize>> vhost_log_put(dev, true);
+     *   - hw/virtio/vhost.c|1139| <<vhost_migration_log>> vhost_log_put(dev, false);
+     *   - hw/virtio/vhost.c|2168| <<vhost_dev_start>> vhost_log_put(hdev, false);
+     *   - hw/virtio/vhost.c|2225| <<vhost_dev_stop>> vhost_log_put(hdev, true);
+     */
     vhost_log_put(dev, true);
     dev->log = log;
     dev->log_size = size;
@@ -954,6 +1069,55 @@ void vhost_toggle_device_iotlb(VirtIODevice *vdev)
     }
 }
 
+/*
+ * (gdb) bt
+ * #0  vhost_kernel_set_vring_addr (dev=0x5555573f9c50, addr=0x7fffe3dfefd0) at ../hw/virtio/vhost-backend.c:110
+ * #1  0x0000555555b917d9 in vhost_virtqueue_set_addr (dev=0x5555573f9c50, vq=0x5555573f9f60, idx=1, enable_log=false) at ../hw/virtio/vhost.c:979
+ * #2  0x0000555555b92541 in vhost_virtqueue_start (dev=0x5555573f9c50, vdev=0x555558419170, vq=0x5555573f9f60, idx=3) at ../hw/virtio/vhost.c:1321
+ * #3  0x0000555555b949e8 in vhost_dev_start (hdev=0x5555573f9c50, vdev=0x555558419170, vrings=false) at ../hw/virtio/vhost.c:2097
+ * #4  0x0000555555a47c24 in vhost_net_start_one (net=0x5555573f9c50, dev=0x555558419170) at ../hw/net/vhost_net.c:280
+ * #5  0x0000555555a48278 in vhost_net_start (dev=0x555558419170, ncs=0x555558424898, data_queue_pairs=2, cvq=0) at ../hw/net/vhost_net.c:421
+ * #6  0x0000555555df0a1b in virtio_net_vhost_status (n=0x555558419170, status=15 '\017') at ../hw/net/virtio-net.c:311
+ * #7  0x0000555555df0d2b in virtio_net_set_status (vdev=0x555558419170, status=15 '\017') at ../hw/net/virtio-net.c:393
+ * #8  0x0000555555e33c08 in virtio_set_status (vdev=0x555558419170, val=15 '\017') at ../hw/virtio/virtio.c:2242
+ * #9  0x0000555555b7b531 in virtio_pci_common_write (opaque=0x555558410cf0, addr=20, val=15, size=1) at ../hw/virtio/virtio-pci.c:1608
+ * #10 0x0000555555e63c32 in memory_region_write_accessor (mr=0x5555584118e0, addr=20, value=0x7fffe3dff3f8, size=1, shift=0, mask=255, attrs=...)
+ *                           at ../system/memory.c:497
+ * #11 0x0000555555e63f79 in access_with_adjusted_size (addr=20, value=0x7fffe3dff3f8, size=1, access_size_min=1, access_size_max=4,
+ *                           access_fn=0x555555e63b3c <memory_region_write_accessor>, mr=0x5555584118e0, attrs=...) at ../system/memory.c:573
+ * #12 0x0000555555e674ae in memory_region_dispatch_write (mr=0x5555584118e0, addr=20, data=15, op=MO_8, attrs=...) at ../system/memory.c:1521
+ * #13 0x0000555555e76bdc in flatview_write_continue_step (attrs=..., buf=0x7ffff7e39028 "\017", len=1, mr_addr=20,
+ *                           l=0x7fffe3dff4e0, mr=0x5555584118e0) at ../system/physmem.c:2803
+ * #14 0x0000555555e76cae in flatview_write_continue (fv=0x7ffe542b6060, addr=4261412884, attrs=..., ptr=0x7ffff7e39028, len=1, mr_addr=20,
+ *                           l=1, mr=0x5555584118e0) at ../system/physmem.c:2833
+ * #15 0x0000555555e76dd6 in flatview_write (fv=0x7ffe542b6060, addr=4261412884, attrs=..., buf=0x7ffff7e39028, len=1) at ../system/physmem.c:2864
+ * #16 0x0000555555e77250 in address_space_write (as=0x555557328200 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7ffff7e39028, len=1)
+ *                           at ../system/physmem.c:2984
+ * #17 0x0000555555e772c8 in address_space_rw (as=0x555557328200 <address_space_memory>, addr=4261412884, attrs=..., buf=0x7ffff7e39028, len=1,
+ *                           is_write=true) at ../system/physmem.c:2994
+ * #18 0x0000555555ed4d63 in kvm_cpu_exec (cpu=0x555557748910) at ../accel/kvm/kvm-all.c:3075
+ * #19 0x0000555555ed8469 in kvm_vcpu_thread_fn (arg=0x555557748910) at ../accel/kvm/kvm-accel-ops.c:50
+ * #20 0x000055555616b040 in qemu_thread_start (args=0x555557752230) at ../util/qemu-thread-posix.c:541
+ * #21 0x00007ffff5c081da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #22 0x00007ffff3839e73 in clone () from /lib/../lib64/libc.so.6
+ *
+ * (gdb) bt
+ * #0  vhost_kernel_set_vring_addr (dev=0x5555573f9c50, addr=0x7ffe253ff350) at ../hw/virtio/vhost-backend.c:110
+ * #1  0x0000555555b917d9 in vhost_virtqueue_set_addr (dev=0x5555573f9c50, vq=0x5555573f9f60, idx=1, enable_log=true) at ../hw/virtio/vhost.c:979
+ * #2  0x0000555555b91a76 in vhost_dev_set_log (dev=0x5555573f9c50, enable_log=true) at ../hw/virtio/vhost.c:1040
+ * #3  0x0000555555b91c5a in vhost_migration_log (listener=0x5555573f9c58, enable=true) at ../hw/virtio/vhost.c:1093
+ * #4  0x0000555555b91ccf in vhost_log_global_start (listener=0x5555573f9c58, errp=0x7ffe253ff628) at ../hw/virtio/vhost.c:1125
+ * #5  0x0000555555e6b859 in memory_global_dirty_log_do_start (errp=0x7ffe253ff628) at ../system/memory.c:2902
+ * #6  0x0000555555e6b98d in memory_global_dirty_log_start (flags=1, errp=0x7ffe253ff628) at ../system/memory.c:2941
+ * #7  0x0000555555e84d5d in ram_init_bitmaps (rs=0x7fffdc008420, errp=0x7ffe253ff628) at ../migration/ram.c:2782
+ * #8  0x0000555555e84e6e in ram_init_all (rsp=0x555557328500 <ram_state>, errp=0x7ffe253ff628) at ../migration/ram.c:2816
+ * #9  0x0000555555e854fc in ram_save_setup (f=0x555557671f70, opaque=0x555557328500 <ram_state>, errp=0x7ffe253ff628) at ../migration/ram.c:3010
+ * #10 0x0000555555c2fe16 in qemu_savevm_state_setup (f=0x555557671f70, errp=0x7ffe253ff628) at ../migration/savevm.c:1346
+ * #11 0x0000555555c19305 in migration_thread (opaque=0x5555573fe6a0) at ../migration/migration.c:3507
+ * #12 0x000055555616b040 in qemu_thread_start (args=0x555558115c40) at ../util/qemu-thread-posix.c:541
+ * #13 0x00007ffff5c081da in start_thread () from /lib/../lib64/libpthread.so.0
+ * #14 0x00007ffff3839e73 in clone () from /lib/../lib64/libc.so.6
+ */
 static int vhost_virtqueue_set_addr(struct vhost_dev *dev,
                                     struct vhost_virtqueue *vq,
                                     unsigned idx, bool enable_log)
@@ -983,6 +1147,13 @@ static int vhost_virtqueue_set_addr(struct vhost_dev *dev,
     return r;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|1188| <<vhost_dev_set_log>> r = vhost_dev_set_features(dev, enable_log);
+ *   - hw/virtio/vhost.c|1230| <<vhost_dev_set_log>> vhost_dev_set_features(dev, dev->log_enabled);
+ *   - hw/virtio/vhost.c|2118| <<vhost_dev_prepare_inflight>> r = vhost_dev_set_features(hdev, hdev->log_enabled);
+ *   - hw/virtio/vhost.c|2200| <<vhost_dev_start>> r = vhost_dev_set_features(hdev, hdev->log_enabled);
+ */
 static int vhost_dev_set_features(struct vhost_dev *dev,
                                   bool enable_log)
 {
@@ -1016,6 +1187,11 @@ out:
     return r;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|1153| <<vhost_migration_log>> r = vhost_dev_set_log(dev, false);
+ *   - hw/virtio/vhost.c|1160| <<vhost_migration_log>> r = vhost_dev_set_log(dev, true);
+ */
 static int vhost_dev_set_log(struct vhost_dev *dev, bool enable_log)
 {
     int r, i, idx;
@@ -1068,6 +1244,11 @@ err_features:
     return r;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/vhost.c|1192| <<vhost_log_global_start>> r = vhost_migration_log(listener, true);
+ *   - hw/virtio/vhost.c|1203| <<vhost_log_global_stop>> r = vhost_migration_log(listener, false);
+ */
 static int vhost_migration_log(MemoryListener *listener, bool enable)
 {
     struct vhost_dev *dev = container_of(listener, struct vhost_dev,
@@ -1083,12 +1264,23 @@ static int vhost_migration_log(MemoryListener *listener, bool enable)
 
     r = 0;
     if (!enable) {
+        /*
+	 * called by:
+	 *   - hw/virtio/vhost.c|1153| <<vhost_migration_log>> r = vhost_dev_set_log(dev, false);
+	 *   - hw/virtio/vhost.c|1160| <<vhost_migration_log>> r = vhost_dev_set_log(dev, true);
+	 */
         r = vhost_dev_set_log(dev, false);
         if (r < 0) {
             goto check_dev_state;
         }
         vhost_log_put(dev, false);
     } else {
+        /*
+	 * called by:
+	 *   - hw/virtio/vhost.c|715| <<vhost_commit>> vhost_dev_log_resize(dev, log_size + VHOST_LOG_BUFFER);
+	 *   - hw/virtio/vhost.c|723| <<vhost_commit>> vhost_dev_log_resize(dev, log_size);
+	 *   - hw/virtio/vhost.c|1141| <<vhost_migration_log>> vhost_dev_log_resize(dev, vhost_get_log_size(dev));
+	 */
         vhost_dev_log_resize(dev, vhost_get_log_size(dev));
         r = vhost_dev_set_log(dev, true);
         if (r < 0) {
@@ -1097,6 +1289,25 @@ static int vhost_migration_log(MemoryListener *listener, bool enable)
     }
 
 check_dev_state:
+    /*
+     * 在以下设置vhost_dev->log_enabled;
+     *   - hw/virtio/vhost.c|1261| <<vhost_migration_log>> dev->log_enabled = enable;
+     *   - hw/virtio/vhost.c|1301| <<vhost_migration_log>> dev->log_enabled = enable;
+     *   - hw/virtio/vhost.c|1316| <<vhost_migration_log>> dev->log_enabled = false;
+     *   - hw/virtio/vhost.c|1817| <<vhost_dev_init>> hdev->log_enabled = false;
+     * 在以下使用vhost_dev->log_enabled;
+     *   - hw/virtio/vhost.c|203| <<vhost_sync_dirty_bitmap>> if (!dev->log_enabled || !dev->started) {
+     *   - hw/virtio/vhost.c|817| <<vhost_commit>> if (!dev->log_enabled) {
+     *   - hw/virtio/vhost.c|1240| <<vhost_dev_set_log>> vhost_virtqueue_set_addr(dev, dev->vqs + i, idx, dev->log_enabled);
+     *   - hw/virtio/vhost.c|1242| <<vhost_dev_set_log>> vhost_dev_set_features(dev, dev->log_enabled);
+     *   - hw/virtio/vhost.c|1257| <<vhost_migration_log>> if (enable == dev->log_enabled) {
+     *   - hw/virtio/vhost.c|1522| <<vhost_virtqueue_start>> r = vhost_virtqueue_set_addr(dev, vq, vhost_vq_index, dev->log_enabled);
+     *   - hw/virtio/vhost.c|2145| <<vhost_dev_prepare_inflight>> r = vhost_dev_set_features(hdev, hdev->log_enabled);
+     *   - hw/virtio/vhost.c|2227| <<vhost_dev_start>> r = vhost_dev_set_features(hdev, hdev->log_enabled);
+     *   - hw/virtio/vhost.c|2262| <<vhost_dev_start>> if (hdev->log_enabled) {
+     *   - hw/virtio/virtio-hmp-cmds.c|171| <<hmp_virtio_status>> s->vhost_dev->log_enabled ? "true" : "false");
+     *   - hw/virtio/virtio-qmp.c|791| <<qmp_x_query_virtio_status>> status->vhost_dev->log_enabled = hdev->log_enabled;
+     */
     dev->log_enabled = enable;
     /*
      * vhost-user-* devices could change their state during log
@@ -1118,6 +1329,24 @@ check_dev_state:
     return r;
 }
 
+/*
+ * 在以下使用vhost_log_global_start():
+ *   - hw/virtio/vhost.c|1814| <<vhost_dev_init>> .log_global_start = vhost_log_global_start,
+ *
+ * 1809     hdev->memory_listener = (MemoryListener) {
+ * 1810         .name = "vhost",
+ * 1811         .begin = vhost_begin,
+ * 1812         .commit = vhost_commit,
+ * 1813         .region_add = vhost_region_addnop,
+ * 1814         .region_nop = vhost_region_addnop,
+ * 1815         .log_start = vhost_log_start,
+ * 1816         .log_stop = vhost_log_stop,
+ * 1817         .log_sync = vhost_log_sync,
+ * 1818         .log_global_start = vhost_log_global_start,
+ * 1819         .log_global_stop = vhost_log_global_stop,
+ * 1820         .priority = MEMORY_LISTENER_PRIORITY_DEV_BACKEND
+ * 1821     };
+ */
 static bool vhost_log_global_start(MemoryListener *listener, Error **errp)
 {
     int r;
@@ -1248,6 +1477,11 @@ out:
     return ret;
 }
 
+/*
+ * called by:
+ *   - hw/net/vhost_net.c|766| <<vhost_net_virtqueue_restart>> r = vhost_virtqueue_start(&net->dev,
+ *   - hw/virtio/vhost.c|2252| <<vhost_dev_start>> r = vhost_virtqueue_start(hdev,
+ */
 int vhost_virtqueue_start(struct vhost_dev *dev,
                           struct VirtIODevice *vdev,
                           struct vhost_virtqueue *vq,
@@ -1506,6 +1740,21 @@ static void vhost_virtqueue_cleanup(struct vhost_virtqueue *vq)
     }
 }
 
+/*
+ * called by:
+ *   - backends/cryptodev-vhost.c|69| <<cryptodev_vhost_init>> r = vhost_dev_init(&crypto->dev, options->opaque, options->backend_type, 0,
+ *   - backends/vhost-user.c|39| <<vhost_user_backend_dev_init>> ret = vhost_dev_init(&b->dev, &b->vhost_user, VHOST_BACKEND_TYPE_USER, 0,
+ *   - hw/block/vhost-user-blk.c|348| <<vhost_user_blk_connect>> ret = vhost_dev_init(&s->dev, &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0,
+ *   - hw/net/vhost_net.c|354| <<vhost_net_init>> r = vhost_dev_init(&net->dev, options->opaque,
+ *   - hw/scsi/vhost-scsi.c|280| <<vhost_scsi_realize>> ret = vhost_dev_init(&vsc->dev, (void *)(uintptr_t)vhostfd,
+ *   - hw/scsi/vhost-user-scsi.c|160| <<vhost_user_scsi_connect>> ret = vhost_dev_init(&vsc->dev, &s->vhost_user, VHOST_BACKEND_TYPE_USER, 0,
+ *   - hw/virtio/vdpa-dev.c|120| <<vhost_vdpa_device_realize>> ret = vhost_dev_init(&v->dev, &v->vdpa, VHOST_BACKEND_TYPE_VDPA, 0, NULL);
+ *   - hw/virtio/vhost-user-base.c|327| <<vub_device_realize>> ret = vhost_dev_init(&vub->vhost_dev, &vub->vhost_user,
+ *   - hw/virtio/vhost-user-fs.c|253| <<vuf_device_realize>> ret = vhost_dev_init(&fs->vhost_dev, &fs->vhost_user,
+ *   - hw/virtio/vhost-user-scmi.c|250| <<vu_scmi_device_realize>> ret = vhost_dev_init(&scmi->vhost_dev, &scmi->vhost_user,
+ *   - hw/virtio/vhost-user-vsock.c|112| <<vuv_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev, &vsock->vhost_user,
+ *   - hw/virtio/vhost-vsock.c|171| <<vhost_vsock_device_realize>> ret = vhost_dev_init(&vvc->vhost_dev, (void *)(uintptr_t)vhostfd,
+ */
 int vhost_dev_init(struct vhost_dev *hdev, void *opaque,
                    VhostBackendType backend_type, uint32_t busyloop_timeout,
                    Error **errp)
diff --git a/hw/virtio/virtio.c b/hw/virtio/virtio.c
index f12c4aa81..6369ef121 100644
--- a/hw/virtio/virtio.c
+++ b/hw/virtio/virtio.c
@@ -300,6 +300,16 @@ void virtio_queue_update_rings(VirtIODevice *vdev, int n)
     virtio_init_region_cache(vdev, n);
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1286| <<virtqueue_split_read_next_desc>> vring_split_desc_read(vdev, desc, desc_cache, desc->next);
+ *   - hw/virtio/virtio.c|1335| <<virtqueue_split_get_avail_bytes>> vring_split_desc_read(vdev, &desc, desc_cache, i);
+ *   - hw/virtio/virtio.c|1361| <<virtqueue_split_get_avail_bytes>> vring_split_desc_read(vdev, &desc, desc_cache, i);
+ *   - hw/virtio/virtio.c|1825| <<virtqueue_split_pop>> vring_split_desc_read(vdev, &desc, desc_cache, i);
+ *   - hw/virtio/virtio.c|1843| <<virtqueue_split_pop>> vring_split_desc_read(vdev, &desc, desc_cache, i);
+ *   - hw/virtio/virtio.c|4479| <<qmp_x_query_virtio_queue_element>> vring_split_desc_read(vdev, &desc, desc_cache, i);
+ *   - hw/virtio/virtio.c|4492| <<qmp_x_query_virtio_queue_element>> vring_split_desc_read(vdev, &desc, desc_cache, i);
+ */
 /* Called within rcu_read_lock().  */
 static void vring_split_desc_read(VirtIODevice *vdev, VRingDesc *desc,
                                   MemoryRegionCache *cache, int i)
@@ -344,6 +354,29 @@ static void vring_packed_flags_write(VirtIODevice *vdev,
     address_space_cache_invalidate(cache, off, sizeof(flags));
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|356| <<vring_avail_flags>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|369| <<vring_avail_idx>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|407| <<vring_avail_ring>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|434| <<vring_used_write>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|450| <<vring_used_flags>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|463| <<vring_used_idx>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|476| <<vring_used_idx_set>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|490| <<vring_used_flags_set_bit>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|507| <<vring_used_flags_unset_bit>> VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|530| <<vring_set_avail_event>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|564| <<virtio_queue_packed_set_notification>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|752| <<virtio_queue_packed_empty_rcu>> cache = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|796| <<virtio_queue_packed_poll>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|1030| <<virtqueue_packed_fill_desc>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|1570| <<virtqueue_get_avail_bytes>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|1813| <<virtqueue_split_pop>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|1949| <<virtqueue_packed_pop>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|2129| <<virtqueue_packed_drop_all>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|2776| <<virtio_packed_should_notify>> caches = vring_get_region_caches(vq);
+ *   - hw/virtio/virtio.c|4468| <<qmp_x_query_virtio_queue_element>> caches = vring_get_region_caches(vq);
+ */
 /* Called within rcu_read_lock().  */
 static VRingMemoryRegionCaches *vring_get_region_caches(struct VirtQueue *vq)
 {
@@ -377,9 +410,33 @@ static inline uint16_t vring_avail_idx(VirtQueue *vq)
     return vq->shadow_avail_idx;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|396| <<vring_get_used_event>> return vring_avail_ring(vq, vq->vring.num);
+ *   - hw/virtio/virtio.c|1209| <<virtqueue_get_head>> *head = vring_avail_ring(vq, idx % vq->vring.num);
+ *   - hw/virtio/virtio.c|4319| <<qmp_x_query_virtio_queue_element>> head = vring_avail_ring(vq, vq->last_avail_idx % vq->vring.num);
+ *   - hw/virtio/virtio.c|4321| <<qmp_x_query_virtio_queue_element>> head = vring_avail_ring(vq, index % vq->vring.num);
+ */
 /* Called within rcu_read_lock().  */
 static inline uint16_t vring_avail_ring(VirtQueue *vq, int i)
 {
+    /*
+     * struct MemoryRegionCache {
+     *     uint8_t *ptr;
+     *     hwaddr xlat;
+     *     hwaddr len;
+     *     FlatView *fv;
+     *     MemoryRegionSection mrs;
+     *     bool is_write;
+     * };
+     *
+     * typedef struct VRingMemoryRegionCaches {
+     *     struct rcu_head rcu;
+     *     MemoryRegionCache desc;
+     *     MemoryRegionCache avail;
+     *     MemoryRegionCache used;
+     * } VRingMemoryRegionCaches;
+     */
     VRingMemoryRegionCaches *caches = vring_get_region_caches(vq);
     hwaddr pa = offsetof(VRingAvail, ring[i]);
 
@@ -387,12 +444,32 @@ static inline uint16_t vring_avail_ring(VirtQueue *vq, int i)
         return 0;
     }
 
+    /*
+     * called by:
+     *   - hw/virtio/virtio.c|322| <<vring_packed_event_read>> e->flags = virtio_lduw_phys_cached(vdev, cache, off_flags);
+     *   - hw/virtio/virtio.c|325| <<vring_packed_event_read>> e->off_wrap = virtio_lduw_phys_cached(vdev, cache, off_off);
+     *   - hw/virtio/virtio.c|363| <<vring_avail_flags>> return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+     *   - hw/virtio/virtio.c|376| <<vring_avail_idx>> vq->shadow_avail_idx = virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+     *   - hw/virtio/virtio.c|414| <<vring_avail_ring>> return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+     *   - hw/virtio/virtio.c|457| <<vring_used_flags>> return virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+     *   - hw/virtio/virtio.c|470| <<vring_used_idx>> return virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+     *   - hw/virtio/virtio.c|499| <<vring_used_flags_set_bit>> flags = virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+     *   - hw/virtio/virtio.c|516| <<vring_used_flags_unset_bit>> flags = virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+     *   - hw/virtio/virtio.c|622| <<vring_packed_desc_read_flags>> *flags = virtio_lduw_phys_cached(vdev, cache, off);
+     */
     return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
 }
 
 /* Called within rcu_read_lock().  */
 static inline uint16_t vring_get_used_event(VirtQueue *vq)
 {
+    /*
+     * called by:
+     *   - hw/virtio/virtio.c|396| <<vring_get_used_event>> return vring_avail_ring(vq, vq->vring.num);
+     *   - hw/virtio/virtio.c|1209| <<virtqueue_get_head>> *head = vring_avail_ring(vq, idx % vq->vring.num);
+     *   - hw/virtio/virtio.c|4319| <<qmp_x_query_virtio_queue_element>> head = vring_avail_ring(vq, vq->last_avail_idx % vq->vring.num);
+     *   - hw/virtio/virtio.c|4321| <<qmp_x_query_virtio_queue_element>> head = vring_avail_ring(vq, index % vq->vring.num);
+     */
     return vring_avail_ring(vq, vq->vring.num);
 }
 
@@ -1200,10 +1277,23 @@ static int virtqueue_num_heads(VirtQueue *vq, unsigned int idx)
     return num_heads;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1273| <<virtqueue_split_get_avail_bytes>> if (!virtqueue_get_head(vq, idx++, &i)) {
+ *   - hw/virtio/virtio.c|1708| <<virtqueue_split_pop>> if (!virtqueue_get_head(vq, vq->last_avail_idx++, &head)) {
+ *   - hw/virtio/virtio.c|2045| <<virtqueue_split_drop_all>> if (!virtqueue_get_head(vq, vq->last_avail_idx, &elem.index)) {
+ */
 /* Called within rcu_read_lock().  */
 static bool virtqueue_get_head(VirtQueue *vq, unsigned int idx,
                                unsigned int *head)
 {
+    /*
+     * called by:
+     *   - hw/virtio/virtio.c|396| <<vring_get_used_event>> return vring_avail_ring(vq, vq->vring.num);
+     *   - hw/virtio/virtio.c|1209| <<virtqueue_get_head>> *head = vring_avail_ring(vq, idx % vq->vring.num);
+     *   - hw/virtio/virtio.c|4319| <<qmp_x_query_virtio_queue_element>> head = vring_avail_ring(vq, vq->last_avail_idx % vq->vring.num);
+     *   - hw/virtio/virtio.c|4321| <<qmp_x_query_virtio_queue_element>> head = vring_avail_ring(vq, index % vq->vring.num);
+     */
     /* Grab the next descriptor number they're advertising, and increment
      * the index we've seen. */
     *head = vring_avail_ring(vq, idx % vq->vring.num);
@@ -1256,6 +1346,14 @@ static void virtqueue_split_get_avail_bytes(VirtQueue *vq,
     int64_t len = 0;
     int rc;
 
+    /*
+     * called by:
+     *   - hw/virtio/virtio.c|1259| <<virtqueue_split_get_avail_bytes>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|1399| <<virtqueue_packed_get_avail_bytes>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|1688| <<virtqueue_split_pop>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|1835| <<virtqueue_packed_pop>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|4312| <<qmp_x_query_virtio_queue_element>> address_space_cache_init_empty(&indirect_desc_cache);
+     */
     address_space_cache_init_empty(&indirect_desc_cache);
 
     idx = vq->last_avail_idx;
@@ -1270,6 +1368,12 @@ static void virtqueue_split_get_avail_bytes(VirtQueue *vq,
 
         num_bufs = total_bufs;
 
+	/*
+	 * called by:
+	 *   - hw/virtio/virtio.c|1273| <<virtqueue_split_get_avail_bytes>> if (!virtqueue_get_head(vq, idx++, &i)) {
+	 *   - hw/virtio/virtio.c|1708| <<virtqueue_split_pop>> if (!virtqueue_get_head(vq, vq->last_avail_idx++, &head)) {
+	 *   - hw/virtio/virtio.c|2045| <<virtqueue_split_drop_all>> if (!virtqueue_get_head(vq, vq->last_avail_idx, &elem.index)) {
+	 */
         if (!virtqueue_get_head(vq, idx++, &i)) {
             goto err;
         }
@@ -1552,6 +1656,23 @@ int virtqueue_avail_bytes(VirtQueue *vq, unsigned int in_bytes,
     return in_bytes <= in_total && out_bytes <= out_total;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1851| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+ *               &in_num, addr + out_num, iov + out_num,
+ *               VIRTQUEUE_MAX_SIZE - out_num, true,
+ *               desc.addr, desc.len);
+ *   - hw/virtio/virtio.c|1860| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+ *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+ *               desc.addr, desc.len);
+ * hw/virtio/virtio.c|1988| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+ *               &in_num, addr + out_num, iov + out_num,
+ *               VIRTQUEUE_MAX_SIZE - out_num, true,
+ *               desc.addr, desc.len);
+ * hw/virtio/virtio.c|1997| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+ *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+ *               desc.addr, desc.len);
+ */
 static bool virtqueue_map_desc(VirtIODevice *vdev, unsigned int *p_num_sg,
                                hwaddr *addr, struct iovec *iov,
                                unsigned int max_num_sg, bool is_write,
@@ -1670,6 +1791,10 @@ static void *virtqueue_alloc_element(size_t sz, unsigned out_num, unsigned in_nu
     return elem;
 }
 
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1977| <<virtqueue_pop>> return virtqueue_split_pop(vq, sz);
+ */
 static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
 {
     unsigned int i, head, max, idx;
@@ -1680,11 +1805,22 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
     VirtIODevice *vdev = vq->vdev;
     VirtQueueElement *elem = NULL;
     unsigned out_num, in_num, elem_entries;
+    /*
+     * #define VIRTQUEUE_MAX_SIZE 1024
+     */
     hwaddr addr[VIRTQUEUE_MAX_SIZE];
     struct iovec iov[VIRTQUEUE_MAX_SIZE];
     VRingDesc desc;
     int rc;
 
+    /*
+     * called by:
+     *   - hw/virtio/virtio.c|1259| <<virtqueue_split_get_avail_bytes>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|1399| <<virtqueue_packed_get_avail_bytes>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|1688| <<virtqueue_split_pop>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|1835| <<virtqueue_packed_pop>> address_space_cache_init_empty(&indirect_desc_cache);
+     *   - hw/virtio/virtio.c|4312| <<qmp_x_query_virtio_queue_element>> address_space_cache_init_empty(&indirect_desc_cache);
+     */
     address_space_cache_init_empty(&indirect_desc_cache);
 
     RCU_READ_LOCK_GUARD();
@@ -1698,6 +1834,21 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
     /* When we start there are none of either input nor output. */
     out_num = in_num = elem_entries = 0;
 
+    /*
+     * typedef struct VRing
+     * {   
+     *     unsigned int num;
+     *     unsigned int num_default;
+     *     unsigned int align;
+     *     hwaddr desc;
+     *     hwaddr avail;
+     *     hwaddr used;
+     *     VRingMemoryRegionCaches *caches;
+     * } VRing;
+     *
+     * VirtQueue *vq:
+     * -> VRing vring;
+     */
     max = vq->vring.num;
 
     if (vq->inuse >= vq->vring.num) {
@@ -1705,6 +1856,13 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         goto done;
     }
 
+    /*
+     * called by:
+     *   - called by:
+     *   - hw/virtio/virtio.c|1273| <<virtqueue_split_get_avail_bytes>> if (!virtqueue_get_head(vq, idx++, &i)) {
+     *   - hw/virtio/virtio.c|1708| <<virtqueue_split_pop>> if (!virtqueue_get_head(vq, vq->last_avail_idx++, &head)) {
+     *   - hw/virtio/virtio.c|2045| <<virtqueue_split_drop_all>> if (!virtqueue_get_head(vq, vq->last_avail_idx, &elem.index)) {
+     */
     if (!virtqueue_get_head(vq, vq->last_avail_idx++, &head)) {
         goto done;
     }
@@ -1753,6 +1911,23 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         bool map_ok;
 
         if (desc.flags & VRING_DESC_F_WRITE) {
+            /*
+	     * called by:
+             *   - hw/virtio/virtio.c|1851| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             *   - hw/virtio/virtio.c|1860| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1988| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1997| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+	     */
             map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
                                         iov + out_num,
                                         VIRTQUEUE_MAX_SIZE - out_num, true,
@@ -1762,6 +1937,23 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
                 virtio_error(vdev, "Incorrect order for descriptors");
                 goto err_undo_map;
             }
+            /*
+	     * called by:
+             *   - hw/virtio/virtio.c|1851| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             *   - hw/virtio/virtio.c|1860| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1988| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1997| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+	     */
             map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
                                         VIRTQUEUE_MAX_SIZE, false,
                                         desc.addr, desc.len);
@@ -1783,6 +1975,22 @@ static void *virtqueue_split_pop(VirtQueue *vq, size_t sz)
         goto err_undo_map;
     }
 
+    /*
+     * typedef struct VirtQueueElement
+     * {
+     *     unsigned int index;
+     *     unsigned int len;
+     *     unsigned int ndescs;
+     *     unsigned int out_num;
+     *     unsigned int in_num;
+     *     // Element has been processed (VIRTIO_F_IN_ORDER)
+     *     bool in_order_filled;
+     *     hwaddr *in_addr;
+     *     hwaddr *out_addr;
+     *     struct iovec *in_sg;
+     *     struct iovec *out_sg;
+     * } VirtQueueElement;
+     */
     /* Now copy what we have collected and mapped */
     elem = virtqueue_alloc_element(sz, out_num, in_num);
     elem->index = head;
@@ -1890,6 +2098,23 @@ static void *virtqueue_packed_pop(VirtQueue *vq, size_t sz)
         bool map_ok;
 
         if (desc.flags & VRING_DESC_F_WRITE) {
+            /*
+	     * called by:
+             *   - hw/virtio/virtio.c|1851| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             *   - hw/virtio/virtio.c|1860| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1988| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1997| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+	     */
             map_ok = virtqueue_map_desc(vdev, &in_num, addr + out_num,
                                         iov + out_num,
                                         VIRTQUEUE_MAX_SIZE - out_num, true,
@@ -1899,6 +2124,23 @@ static void *virtqueue_packed_pop(VirtQueue *vq, size_t sz)
                 virtio_error(vdev, "Incorrect order for descriptors");
                 goto err_undo_map;
             }
+            /*
+	     * called by:
+             *   - hw/virtio/virtio.c|1851| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             *   - hw/virtio/virtio.c|1860| <<virtqueue_split_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1988| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &in_num, addr + out_num, iov + out_num,
+             *               VIRTQUEUE_MAX_SIZE - out_num, true,
+             *               desc.addr, desc.len);
+             * hw/virtio/virtio.c|1997| <<virtqueue_packed_pop>> map_ok = virtqueue_map_desc(vdev,
+             *               &out_num, addr, iov, VIRTQUEUE_MAX_SIZE, false,
+             *               desc.addr, desc.len);
+	     */
             map_ok = virtqueue_map_desc(vdev, &out_num, addr, iov,
                                         VIRTQUEUE_MAX_SIZE, false,
                                         desc.addr, desc.len);
@@ -1965,6 +2207,48 @@ err_undo_map:
     goto done;
 }
 
+/*
+ * called by:
+ *   - hw/9pfs/virtio-9p-device.c|57| <<handle_9p_output>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/audio/virtio-snd.c|781| <<virtio_snd_handle_ctrl>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/audio/virtio-snd.c|789| <<virtio_snd_handle_ctrl>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/audio/virtio-snd.c|869| <<virtio_snd_handle_tx_xfer>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/audio/virtio-snd.c|950| <<virtio_snd_handle_rx_xfer>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/block/virtio-blk.c|177| <<virtio_blk_get_request>> VirtIOBlockReq *req = virtqueue_pop(vq, sizeof(VirtIOBlockReq));
+ *   - hw/char/virtio-serial-bus.c|116| <<write_to_port>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/char/virtio-serial-bus.c|141| <<discard_vq_data>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/char/virtio-serial-bus.c|175| <<do_flush_queued_data>> port->elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/char/virtio-serial-bus.c|234| <<send_control_msg>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/char/virtio-serial-bus.c|472| <<control_out>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/display/virtio-gpu-gl.c|91| <<virtio_gpu_gl_handle_ctrl>> cmd = virtqueue_pop(vq, sizeof(struct virtio_gpu_ctrl_command));
+ *   - hw/display/virtio-gpu-gl.c|97| <<virtio_gpu_gl_handle_ctrl>> cmd = virtqueue_pop(vq, sizeof(struct virtio_gpu_ctrl_command));
+ *   - hw/display/virtio-gpu-rutabaga.c|1059| <<virtio_gpu_rutabaga_handle_ctrl>> cmd = virtqueue_pop(vq, sizeof(struct virtio_gpu_ctrl_command));
+ *   - hw/display/virtio-gpu-rutabaga.c|1065| <<virtio_gpu_rutabaga_handle_ctrl>> cmd = virtqueue_pop(vq, sizeof(struct virtio_gpu_ctrl_command));
+ *   - hw/display/virtio-gpu.c|1110| <<virtio_gpu_handle_ctrl>> cmd = virtqueue_pop(vq, sizeof(struct virtio_gpu_ctrl_command));
+ *   - hw/display/virtio-gpu.c|1116| <<virtio_gpu_handle_ctrl>> cmd = virtqueue_pop(vq, sizeof(struct virtio_gpu_ctrl_command));
+ *   - hw/display/virtio-gpu.c|1141| <<virtio_gpu_handle_cursor>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/input/virtio-input.c|48| <<virtio_input_send>> elem = virtqueue_pop(vinput->evt, sizeof(VirtQueueElement));
+ *   - hw/input/virtio-input.c|86| <<virtio_input_handle_sts>> elem = virtqueue_pop(vinput->sts, sizeof(VirtQueueElement));
+ *   - hw/net/virtio-net.c|1608| <<virtio_net_handle_ctrl>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/net/virtio-net.c|1973| <<virtio_net_receive_rcu>> elem = virtqueue_pop(q->rx_vq, sizeof(VirtQueueElement));
+ *   - hw/net/virtio-net.c|2751| <<virtio_net_flush_tx>> elem = virtqueue_pop(q->tx_vq, sizeof(VirtQueueElement));
+ *   - hw/scsi/virtio-scsi.c|243| <<virtio_scsi_pop_req>> req = virtqueue_pop(vq, sizeof(VirtIOSCSIReq) + vs->cdb_size);
+ *   - hw/virtio/vhost-shadow-virtqueue.c|310| <<vhost_handle_guest_kick>> elem = virtqueue_pop(svq->vq, sizeof(*elem));
+ *   - hw/virtio/vhost-vsock-common.c|168| <<vhost_vsock_common_send_transport_reset>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-balloon.c|345| <<virtio_balloon_handle_report>> while ((elem = virtqueue_pop(vq, sizeof(VirtQueueElement)))) {
+ *   - hw/virtio/virtio-balloon.c|412| <<virtio_balloon_handle_output>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-balloon.c|467| <<virtio_balloon_receive_stats>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-balloon.c|524| <<get_free_page_hints>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-crypto.c|357| <<virtio_crypto_handle_ctrl>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-crypto.c|590| <<virtio_crypto_get_request>> VirtIOCryptoReq *req = virtqueue_pop(vq, sizeof(VirtIOCryptoReq));
+ *   - hw/virtio/virtio-iommu.c|1011| <<virtio_iommu_handle_command>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-iommu.c|1112| <<virtio_iommu_report_fault>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-mem.c|815| <<virtio_mem_handle_request>> elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-nsm.c|1541| <<handle_input>> out_elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-nsm.c|1554| <<handle_input>> in_elem = virtqueue_pop(vq, sizeof(VirtQueueElement));
+ *   - hw/virtio/virtio-pmem.c|75| <<virtio_pmem_flush>> req_data = virtqueue_pop(vq, sizeof(VirtIODeviceRequest));
+ *   - hw/virtio/virtio-rng.c|72| <<chr_read>> elem = virtqueue_pop(vrng->vq, sizeof(VirtQueueElement));
+ */
 void *virtqueue_pop(VirtQueue *vq, size_t sz)
 {
     if (virtio_device_disabled(vq->vdev)) {
@@ -2042,6 +2326,12 @@ static unsigned int virtqueue_split_drop_all(VirtQueue *vq)
         /* works similar to virtqueue_pop but does not map buffers
         * and does not allocate any memory */
         smp_rmb();
+        /*
+	 * called by:
+	 *   - hw/virtio/virtio.c|1273| <<virtqueue_split_get_avail_bytes>> if (!virtqueue_get_head(vq, idx++, &i)) {
+         *   - hw/virtio/virtio.c|1708| <<virtqueue_split_pop>> if (!virtqueue_get_head(vq, vq->last_avail_idx++, &head)) {
+         *   - hw/virtio/virtio.c|2045| <<virtqueue_split_drop_all>> if (!virtqueue_get_head(vq, vq->last_avail_idx, &elem.index)) {
+	 */
         if (!virtqueue_get_head(vq, vq->last_avail_idx, &elem.index)) {
             break;
         }
@@ -2719,6 +3009,25 @@ void virtio_notify(VirtIODevice *vdev, VirtQueue *vq)
     virtio_irq(vq);
 }
 
+/*
+ * called by:
+ *   - hw/block/vhost-user-blk.c|106| <<vhost_user_blk_sync_config>> virtio_notify_config(vdev);
+ *   - hw/block/virtio-blk.c|1360| <<virtio_resize_cb>> virtio_notify_config(vdev);
+ *   - hw/char/virtio-serial-bus.c|1004| <<virtser_port_device_plug>> virtio_notify_config(VIRTIO_DEVICE(hotplug_dev));
+ *   - hw/display/virtio-gpu-base.c|86| <<virtio_gpu_notify_event>> virtio_notify_config(&g->parent_obj);
+ *   - hw/input/virtio-input.c|183| <<virtio_input_set_config>> virtio_notify_config(vdev);
+ *   - hw/net/virtio-net.c|231| <<virtio_net_announce_notify>> virtio_notify_config(vdev);
+ *   - hw/net/virtio-net.c|454| <<virtio_net_set_link_status>> virtio_notify_config(vdev);
+ *   - hw/virtio/vhost-user-base.c|165| <<vub_config_notifier>> virtio_notify_config(dev->vdev);
+ *   - hw/virtio/vhost-user-vsock.c|48| <<vuv_handle_config_change>> virtio_notify_config(dev->vdev);
+ *   - hw/virtio/virtio-balloon.c|610| <<virtio_balloon_free_page_start>> virtio_notify_config(vdev);
+ *   - hw/virtio/virtio-balloon.c|630| <<virtio_balloon_free_page_stop>> virtio_notify_config(vdev);
+ *   - hw/virtio/virtio-balloon.c|643| <<virtio_balloon_free_page_done>> virtio_notify_config(vdev);
+ *   - hw/virtio/virtio-balloon.c|813| <<virtio_balloon_to_target>> virtio_notify_config(vdev);
+ *   - hw/virtio/virtio-mem.c|1598| <<virtio_mem_set_requested_size>> virtio_notify_config(VIRTIO_DEVICE(vmem));
+ *   - hw/virtio/virtio.c|3723| <<virtio_config_guest_notifier_read>> virtio_notify_config(vdev);
+ *   - hw/virtio/virtio.c|3917| <<virtio_error>> virtio_notify_config(vdev);
+ */
 void virtio_notify_config(VirtIODevice *vdev)
 {
     if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK))
diff --git a/include/exec/memory.h b/include/exec/memory.h
index 9458e2801..fc4c1255c 100644
--- a/include/exec/memory.h
+++ b/include/exec/memory.h
@@ -2833,6 +2833,14 @@ int64_t address_space_cache_init(MemoryRegionCache *cache,
  * Initializes #MemoryRegionCache structure without memory region attached.
  * Cache initialized this way can only be safely destroyed, but not used.
  */
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|1259| <<virtqueue_split_get_avail_bytes>> address_space_cache_init_empty(&indirect_desc_cache);
+ *   - hw/virtio/virtio.c|1399| <<virtqueue_packed_get_avail_bytes>> address_space_cache_init_empty(&indirect_desc_cache);
+ *   - hw/virtio/virtio.c|1688| <<virtqueue_split_pop>> address_space_cache_init_empty(&indirect_desc_cache);
+ *   - hw/virtio/virtio.c|1835| <<virtqueue_packed_pop>> address_space_cache_init_empty(&indirect_desc_cache);
+ *   - hw/virtio/virtio.c|4312| <<qmp_x_query_virtio_queue_element>> address_space_cache_init_empty(&indirect_desc_cache);
+ */
 static inline void address_space_cache_init_empty(MemoryRegionCache *cache)
 {
     cache->mrs.mr = NULL;
@@ -3049,6 +3057,13 @@ MemTxResult address_space_read(AddressSpace *as, hwaddr addr,
  * @buf: buffer with the data transferred
  * @len: length of the data transferred
  */
+/*
+ * called by:
+ *   - hw/virtio/virtio.c|307| <<vring_split_desc_read>> address_space_read_cached(cache, i * sizeof(VRingDesc),
+ *   - hw/virtio/virtio.c|639| <<vring_packed_desc_read>> address_space_read_cached(cache, off + offsetof(VRingPackedDesc, addr),
+ *   - hw/virtio/virtio.c|641| <<vring_packed_desc_read>> address_space_read_cached(cache, off + offsetof(VRingPackedDesc, id),
+ *   - hw/virtio/virtio.c|643| <<vring_packed_desc_read>> address_space_read_cached(cache, off + offsetof(VRingPackedDesc, len),
+ */
 static inline MemTxResult
 address_space_read_cached(MemoryRegionCache *cache, hwaddr addr,
                           void *buf, hwaddr len)
diff --git a/include/exec/ramblock.h b/include/exec/ramblock.h
index 0babd105c..a729537c9 100644
--- a/include/exec/ramblock.h
+++ b/include/exec/ramblock.h
@@ -24,6 +24,62 @@
 #include "qemu/rcu.h"
 #include "exec/ramlist.h"
 
+/*
+ * (gdb) p *(RAMBlock *)0x555557712270
+ * $4 = {rcu = {next = 0x0, func = 0x0}, mr = 0x5555575482e0, host = 0x7ffee3e00000 "S\377", colo_cache = 0x0, offset = 0, used_length = 4294967296, max_length = 4294967296,
+ *   resized = 0x0, flags = 16, idstr = "pc.ram", '\000' <repeats 249 times>, next = {le_next = 0x55555801c100, le_prev = 0x555557334880 <ram_list+64>},
+ *   ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0,
+ *   receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x55555801c100
+ * $5 = {rcu = {next = 0x0, func = 0x0}, mr = 0x55555800a0d0, host = 0x7ffee2c00000 "", colo_cache = 0x0, offset = 4295491584, used_length = 16777216, max_length = 16777216,
+ *   resized = 0x0, flags = 16, idstr = "0000:00:02.0/vga.vram", '\000' <repeats 234 times>, next = {le_next = 0x555557b2c820, le_prev = 0x5555577123c0},
+ *   ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0,
+ *   receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x555557b2c820
+ * $6 = {rcu = {next = 0x0, func = 0x0}, mr = 0x55555742c000, host = 0x7fffe8600000 "FACS@", colo_cache = 0x0, offset = 4312793088, used_length = 131072,
+ *   max_length = 2097152, resized = 0x5555559402c4 <fw_cfg_resized>, flags = 20, idstr = "/rom@etc/acpi/tables", '\000' <repeats 235 times>, next = {
+ *     le_next = 0x555557b19940, le_prev = 0x55555801c250}, ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0,
+ *   file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0, receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x555557b19940
+ * $7 = {rcu = {next = 0x0, func = 0x0}, mr = 0x5555576793a0, host = 0x7fffe9000000 "", colo_cache = 0x0, offset = 4294967296, used_length = 262144, max_length = 262144,
+ *   resized = 0x0, flags = 16, idstr = "pc.bios", '\000' <repeats 248 times>, next = {le_next = 0x5555585cf3a0, le_prev = 0x555557b2c970}, ramblock_notifiers = {
+ *     lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0, receivedmap = 0x0,
+ *   clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x5555585cf3a0
+ * $8 = {rcu = {next = 0x0, func = 0x0}, mr = 0x555558446d50, host = 0x7fffe8a00000 "U\252\207", <incomplete sequence \351\242>, colo_cache = 0x0, offset = 4312530944,
+ *   used_length = 262144, max_length = 262144, resized = 0x0, flags = 16, idstr = "0000:00:03.0/virtio-net-pci.rom", '\000' <repeats 224 times>, next = {
+ *     le_next = 0x555557680e90, le_prev = 0x555557b19a90}, ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0,
+ *   file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0, receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x555557680e90
+ * $9 = {rcu = {next = 0x0, func = 0x0}, mr = 0x555557565000, host = 0x7fffe8e00000 "", colo_cache = 0x0, offset = 4295229440, used_length = 131072, max_length = 131072,
+ *   resized = 0x0, flags = 16, idstr = "pc.rom", '\000' <repeats 249 times>, next = {le_next = 0x5555581e99c0, le_prev = 0x5555585cf4f0}, ramblock_notifiers = {
+ *     lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0, receivedmap = 0x0,
+ *   clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x5555581e99c0
+ * $10 = {rcu = {next = 0x0, func = 0x0}, mr = 0x555558009f20, host = 0x7fffe8c00000 "U\252M\351\220U\211", colo_cache = 0x0, offset = 4312268800, used_length = 65536,
+ *   max_length = 65536, resized = 0x0, flags = 16, idstr = "0000:00:02.0/vga.rom", '\000' <repeats 235 times>, next = {le_next = 0x555557b2c3f0, le_prev = 0x555557680fe0},
+ *   ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0, bitmap_offset = 0, pages_offset = 0,
+ *   receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x555557b2c3f0
+ * $11 = {rcu = {next = 0x0, func = 0x0}, mr = 0x5555578920d0, host = 0x7fffe8400000 "\001", colo_cache = 0x0, offset = 4314890240, used_length = 4096, max_length = 65536,
+ *   resized = 0x5555559402c4 <fw_cfg_resized>, flags = 20, idstr = "/rom@etc/table-loader", '\000' <repeats 234 times>, next = {le_next = 0x55555780b600,
+ *     le_prev = 0x5555581e9b10}, ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0,
+ *   bitmap_offset = 0, pages_offset = 0, receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ *
+ * (gdb) p *(RAMBlock *)0x55555780b600
+ * $12 = {rcu = {next = 0x0, func = 0x0}, mr = 0x555557b67eb0, host = 0x7fffe8200000 "RSD PTR ", colo_cache = 0x0, offset = 4315152384, used_length = 4096,
+ *   max_length = 4096, resized = 0x5555559402c4 <fw_cfg_resized>, flags = 20, idstr = "/rom@etc/acpi/rsdp", '\000' <repeats 237 times>, next = {le_next = 0x0,
+ *     le_prev = 0x555557b2c540}, ramblock_notifiers = {lh_first = 0x0}, fd = -1, fd_offset = 0, guest_memfd = -1, page_size = 4096, bmap = 0x0, file_bmap = 0x0,
+ *   bitmap_offset = 0, pages_offset = 0, receivedmap = 0x0, clear_bmap = 0x0, clear_bmap_shift = 0 '\000', postcopy_length = 0}
+ */
+
 struct RAMBlock {
     struct rcu_head rcu;
     struct MemoryRegion *mr;
@@ -41,6 +97,19 @@ struct RAMBlock {
     QLIST_HEAD(, RAMBlockNotifier) ramblock_notifiers;
     int fd;
     uint64_t fd_offset;
+    /*
+     * 在以下使用RAMBlock->guest_memfd:
+     *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+     *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+     *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+     *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+     *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+     *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+     *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+     *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+     *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+     *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+     */
     int guest_memfd;
     size_t page_size;
     /* dirty bitmap used during migration */
diff --git a/include/hw/core/cpu.h b/include/hw/core/cpu.h
index c3ca0babc..370f28660 100644
--- a/include/hw/core/cpu.h
+++ b/include/hw/core/cpu.h
@@ -498,6 +498,19 @@ struct CPUState {
     sigjmp_buf jmp_env;
 
     QemuMutex work_mutex;
+    /*
+     * 在以下使用CPUState->work_list:
+     *   - cpu-common.c|137| <<queue_work_on_cpu>> QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
+     *   - cpu-common.c|340| <<free_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|341| <<free_queued_cpu_work>> struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|342| <<free_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - cpu-common.c|354| <<process_queued_cpu_work>> if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|358| <<process_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|359| <<process_queued_cpu_work>> wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|360| <<process_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - hw/core/cpu-common.c|257| <<cpu_common_initfn>> QSIMPLEQ_INIT(&cpu->work_list);
+     *   - system/cpus.c|83| <<cpu_work_list_empty>> return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
+     */
     QSIMPLEQ_HEAD(, qemu_work_item) work_list;
 
     struct CPUAddressSpace *cpu_ases;
@@ -824,6 +837,15 @@ const char *parse_cpu_option(const char *cpu_option);
  *
  * Returns: %true if the CPU has work, %false otherwise.
  */
+/*
+ * called by:
+ *   - hw/ppc/spapr_hcall.c|506| <<h_cede>> if (!cpu_has_work(cs)) {
+ *   - system/cpus.c|94| <<cpu_thread_is_idle>> if (!cpu->halted || cpu_has_work(cpu)) {
+ *   - target/arm/cpu.c|1147| <<arm_cpu_exec_halt>> bool leave_halt = cpu_has_work(cs);
+ *   - target/arm/tcg/op_helper.c|388| <<HELPER>> if (cpu_has_work(cs)) {
+ *   - target/arm/tcg/op_helper.c|434| <<HELPER>> if (cpu_has_work(cs) || cntvct >= timeout) {
+ *   - target/i386/tcg/sysemu/seg_helper.c|143| <<x86_cpu_exec_halt>> if (!cpu_has_work(cpu)) {
+ */
 static inline bool cpu_has_work(CPUState *cpu)
 {
     CPUClass *cc = CPU_GET_CLASS(cpu);
diff --git a/include/hw/pci/pci_device.h b/include/hw/pci/pci_device.h
index 8eaf0d58b..1c286ab7f 100644
--- a/include/hw/pci/pci_device.h
+++ b/include/hw/pci/pci_device.h
@@ -156,6 +156,16 @@ struct PCIDevice {
     /* MSI-X notifiers */
     MSIVectorUseNotifier msix_vector_use_notifier;
     MSIVectorReleaseNotifier msix_vector_release_notifier;
+    /*
+     * 在以下使用PCIDevice->msix_vector_poll_notifier:
+     *   - hw/pci/msix.c|251| <<msix_pba_mmio_read>> if (dev->msix_vector_poll_notifier) {
+     *   - hw/pci/msix.c|254| <<msix_pba_mmio_read>> dev->msix_vector_poll_notifier(dev, vector_start, vector_end);
+     *   - hw/pci/msix.c|629| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier = poll_notifier;
+     *   - hw/pci/msix.c|640| <<msix_set_vector_notifiers>> if (dev->msix_vector_poll_notifier) {
+     *   - hw/pci/msix.c|641| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier(dev, 0, dev->msix_entries_nr);
+     *   - hw/pci/msix.c|651| <<msix_set_vector_notifiers>> dev->msix_vector_poll_notifier = NULL;
+     *   - hw/pci/msix.c|670| <<msix_unset_vector_notifiers>> dev->msix_vector_poll_notifier = NULL;
+     */
     MSIVectorPollNotifier msix_vector_poll_notifier;
 
     /* SPDM */
diff --git a/include/hw/virtio/vhost.h b/include/hw/virtio/vhost.h
index 461c168c3..ff4f012bd 100644
--- a/include/hw/virtio/vhost.h
+++ b/include/hw/virtio/vhost.h
@@ -122,6 +122,25 @@ struct vhost_dev {
     uint64_t backend_cap;
     /* @started: is the vhost device started? */
     bool started;
+    /*
+     * 在以下设置vhost_dev->log_enabled;
+     *   - hw/virtio/vhost.c|1261| <<vhost_migration_log>> dev->log_enabled = enable;
+     *   - hw/virtio/vhost.c|1301| <<vhost_migration_log>> dev->log_enabled = enable;
+     *   - hw/virtio/vhost.c|1316| <<vhost_migration_log>> dev->log_enabled = false;
+     *   - hw/virtio/vhost.c|1817| <<vhost_dev_init>> hdev->log_enabled = false;
+     * 在以下使用vhost_dev->log_enabled;
+     *   - hw/virtio/vhost.c|203| <<vhost_sync_dirty_bitmap>> if (!dev->log_enabled || !dev->started) {
+     *   - hw/virtio/vhost.c|817| <<vhost_commit>> if (!dev->log_enabled) {
+     *   - hw/virtio/vhost.c|1240| <<vhost_dev_set_log>> vhost_virtqueue_set_addr(dev, dev->vqs + i, idx, dev->log_enabled);
+     *   - hw/virtio/vhost.c|1242| <<vhost_dev_set_log>> vhost_dev_set_features(dev, dev->log_enabled);
+     *   - hw/virtio/vhost.c|1257| <<vhost_migration_log>> if (enable == dev->log_enabled) {
+     *   - hw/virtio/vhost.c|1522| <<vhost_virtqueue_start>> r = vhost_virtqueue_set_addr(dev, vq, vhost_vq_index, dev->log_enabled);
+     *   - hw/virtio/vhost.c|2145| <<vhost_dev_prepare_inflight>> r = vhost_dev_set_features(hdev, hdev->log_enabled);
+     *   - hw/virtio/vhost.c|2227| <<vhost_dev_start>> r = vhost_dev_set_features(hdev, hdev->log_enabled);
+     *   - hw/virtio/vhost.c|2262| <<vhost_dev_start>> if (hdev->log_enabled) {
+     *   - hw/virtio/virtio-hmp-cmds.c|171| <<hmp_virtio_status>> s->vhost_dev->log_enabled ? "true" : "false");
+     *   - hw/virtio/virtio-qmp.c|791| <<qmp_x_query_virtio_status>> status->vhost_dev->log_enabled = hdev->log_enabled;
+     */
     bool log_enabled;
     uint64_t log_size;
     Error *migration_blocker;
diff --git a/include/hw/virtio/virtio-access.h b/include/hw/virtio/virtio-access.h
index 07aae6904..c4086a3a5 100644
--- a/include/hw/virtio/virtio-access.h
+++ b/include/hw/virtio/virtio-access.h
@@ -156,6 +156,50 @@ static inline uint16_t virtio_tswap16(VirtIODevice *vdev, uint16_t s)
 #endif
 }
 
+/*
+ * (gdb) bt
+ * #0  lduw_he_p (ptr=0x7fb12d3b100a) at /home/zhang/kvm/qemu-9.2.0/include/qemu/bswap.h:252
+ * #1  0x0000557c152e15ef in lduw_le_p (ptr=0x7fb12d3b100a) at /home/zhang/kvm/qemu-9.2.0/include/qemu/bswap.h:301
+ * #2  0x0000557c152e197d in address_space_lduw_le_cached (cache=0x7fb00c064df0, addr=10, attrs=..., result=0x0)
+ *     at /home/zhang/kvm/qemu-9.2.0/include/exec/memory_ldst_cached.h.inc:33
+ * #3  0x0000557c152e1c2c in lduw_le_phys_cached (cache=0x7fb00c064df0, addr=10)
+ *     at /home/zhang/kvm/qemu-9.2.0/include/exec/memory_ldst_phys.h.inc:67
+ * #4  0x0000557c152e23c2 in virtio_lduw_phys_cached (vdev=0x557c50ebdc30, cache=0x7fb00c064df0, pa=10)
+ *     at /home/zhang/kvm/qemu-9.2.0/include/hw/virtio/virtio-access.h:166
+ * #5  0x0000557c152e2e83 in vring_avail_ring (vq=0x557c50f19a40, i=3) at ../hw/virtio/virtio.c:390
+ * #6  0x0000557c152e4da8 in virtqueue_get_head (vq=0x557c50f19a40, idx=3, head=0x7ffe4d5e0bb0) at ../hw/virtio/virtio.c:1209
+ * #7  0x0000557c152e6256 in virtqueue_split_pop (vq=0x557c50f19a40, sz=240) at ../hw/virtio/virtio.c:1708
+ * #8  0x0000557c152e72a4 in virtqueue_pop (vq=0x557c50f19a40, sz=240) at ../hw/virtio/virtio.c:1977
+ * #9  0x0000557c1527d3a9 in virtio_blk_get_request (s=0x557c50ebdc30, vq=0x557c50f19a40) at ../hw/block/virtio-blk.c:177
+ * #10 0x0000557c1527f68a in virtio_blk_handle_vq (s=0x557c50ebdc30, vq=0x557c50f19a40) at ../hw/block/virtio-blk.c:988
+ * #11 0x0000557c1527f782 in virtio_blk_handle_output (vdev=0x557c50ebdc30, vq=0x557c50f19a40) at ../hw/block/virtio-blk.c:1022
+ * #12 0x0000557c152e89fb in virtio_queue_notify_vq (vq=0x557c50f19a40) at ../hw/virtio/virtio.c:2484
+ * #13 0x0000557c152ec235 in virtio_queue_host_notifier_read (n=0x557c50f19ab4) at ../hw/virtio/virtio.c:3869
+ * #14 0x0000557c1561dc88 in aio_dispatch_handler (ctx=0x557c4fe7ec90, node=0x7fb014006010) at ../util/aio-posix.c:373
+ * #15 0x0000557c1561de59 in aio_dispatch_handlers (ctx=0x557c4fe7ec90) at ../util/aio-posix.c:415
+ * #16 0x0000557c1561deb5 in aio_dispatch (ctx=0x557c4fe7ec90) at ../util/aio-posix.c:425
+ * #17 0x0000557c1563f552 in aio_ctx_dispatch (source=0x557c4fe7ec90, callback=0x0, user_data=0x0) at ../util/async.c:361
+ * #18 0x00007fb44d92494b in g_main_dispatch (context=0x557c4fe7f180) at ../glib/gmain.c:3325
+ * #19 g_main_context_dispatch (context=0x557c4fe7f180) at ../glib/gmain.c:4043
+ * #20 0x0000557c15640c2a in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #21 0x0000557c15640cb8 in os_host_main_loop_wait (timeout=947384) at ../util/main-loop.c:310
+ * #22 0x0000557c15640de7 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #23 0x0000557c150914f9 in qemu_main_loop () at ../system/runstate.c:835
+ * #24 0x0000557c1554b052 in qemu_default_main () at ../system/main.c:37
+ * #25 0x0000557c1554b08f in main (argc=19, argv=0x7ffe4d5e71e8) at ../system/main.c:48
+ *
+ * called by:
+ *   - hw/virtio/virtio.c|322| <<vring_packed_event_read>> e->flags = virtio_lduw_phys_cached(vdev, cache, off_flags);
+ *   - hw/virtio/virtio.c|325| <<vring_packed_event_read>> e->off_wrap = virtio_lduw_phys_cached(vdev, cache, off_off);
+ *   - hw/virtio/virtio.c|363| <<vring_avail_flags>> return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+ *   - hw/virtio/virtio.c|376| <<vring_avail_idx>> vq->shadow_avail_idx = virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+ *   - hw/virtio/virtio.c|414| <<vring_avail_ring>> return virtio_lduw_phys_cached(vq->vdev, &caches->avail, pa);
+ *   - hw/virtio/virtio.c|457| <<vring_used_flags>> return virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+ *   - hw/virtio/virtio.c|470| <<vring_used_idx>> return virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+ *   - hw/virtio/virtio.c|499| <<vring_used_flags_set_bit>> flags = virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+ *   - hw/virtio/virtio.c|516| <<vring_used_flags_unset_bit>> flags = virtio_lduw_phys_cached(vq->vdev, &caches->used, pa);
+ *   - hw/virtio/virtio.c|622| <<vring_packed_desc_read_flags>> *flags = virtio_lduw_phys_cached(vdev, cache, off);
+ */
 static inline uint16_t virtio_lduw_phys_cached(VirtIODevice *vdev,
                                                MemoryRegionCache *cache,
                                                hwaddr pa)
@@ -163,6 +207,44 @@ static inline uint16_t virtio_lduw_phys_cached(VirtIODevice *vdev,
     if (virtio_access_is_big_endian(vdev)) {
         return lduw_be_phys_cached(cache, pa);
     }
+    /*
+     * include/exec/memory_ldst_phys.h.inc:
+     *
+     * 65	static inline uint16_t glue(lduw_le_phys, SUFFIX)(ARG1_DECL, hwaddr addr)
+     * 66	{
+     * 67	    return glue(address_space_lduw_le, SUFFIX)(ARG1, addr,
+     * 68	                                               MEMTXATTRS_UNSPECIFIED, NULL);
+     * 69	}
+     * 70
+     * 71	static inline uint16_t glue(lduw_be_phys, SUFFIX)(ARG1_DECL, hwaddr addr)
+     * 72	{
+     * 73	    return glue(address_space_lduw_be, SUFFIX)(ARG1, addr,
+     * 74	                                               MEMTXATTRS_UNSPECIFIED, NULL);
+     * 75	}
+     *
+     * include/exec/memory_ldst_cached.h.inc:
+     *
+     * 20 #define ADDRESS_SPACE_LD_CACHED(size) \
+     * 21     glue(glue(address_space_ld, size), glue(ENDIANNESS, _cached))
+     * 22 #define ADDRESS_SPACE_LD_CACHED_SLOW(size) \
+     * 23     glue(glue(address_space_ld, size), glue(ENDIANNESS, _cached_slow))
+     * 24 #define LD_P(size) \
+     * 25     glue(glue(ld, size), glue(ENDIANNESS, _p))
+     * 26
+     * 27 static inline uint16_t ADDRESS_SPACE_LD_CACHED(uw)(MemoryRegionCache *cache,
+     * 28     hwaddr addr, MemTxAttrs attrs, MemTxResult *result)
+     * 29 {
+     * 30     assert(addr < cache->len && 2 <= cache->len - addr);
+     * 31     fuzz_dma_read_cb(cache->xlat + addr, 2, cache->mrs.mr);
+     * 32     if (likely(cache->ptr)) {
+     * 33         return LD_P(uw)(cache->ptr + addr);
+     * 34     } else {
+     * 35         return ADDRESS_SPACE_LD_CACHED_SLOW(uw)(cache, addr, attrs, result);
+     * 36     }
+     * 37 }
+     *
+     * 只在此处调用
+     */
     return lduw_le_phys_cached(cache, pa);
 }
 
diff --git a/include/hw/virtio/virtio-scsi.h b/include/hw/virtio/virtio-scsi.h
index 7be010591..9fcf89541 100644
--- a/include/hw/virtio/virtio-scsi.h
+++ b/include/hw/virtio/virtio-scsi.h
@@ -82,6 +82,14 @@ struct VirtIOSCSI {
 
     SCSIBus bus;
     int resetting; /* written from main loop thread, read from any thread */
+    /*
+     * 在以下使用VirtIOSCSI->events_dropped:
+     *   - hw/scsi/virtio-scsi.c|1012| <<virtio_scsi_reset>> s->events_dropped = false;
+     *   - hw/scsi/virtio-scsi.c|1049| <<virtio_scsi_push_event>> s->events_dropped = true;
+     *   - hw/scsi/virtio-scsi.c|1053| <<virtio_scsi_push_event>> if (s->events_dropped) {
+     *   - hw/scsi/virtio-scsi.c|1055| <<virtio_scsi_push_event>> s->events_dropped = false;
+     *   - hw/scsi/virtio-scsi.c|1084| <<virtio_scsi_handle_event_vq>> if (s->events_dropped) {
+     */
     bool events_dropped;
 
     /*
diff --git a/include/net/filter.h b/include/net/filter.h
index f15f7932b..91fd24c59 100644
--- a/include/net/filter.h
+++ b/include/net/filter.h
@@ -43,6 +43,16 @@ struct NetFilterClass {
     FilterCleanup *cleanup;
     FilterStatusChanged *status_changed;
     FilterHandleEvent *handle_event;
+    /*
+     * 在以下使用NetFilterClass->receive_iov:
+     *   - net/dump.c|251| <<filter_dump_class_init>> nfc->receive_iov = filter_dump_receive_iov;
+     *   - net/filter-buffer.c|185| <<filter_buffer_class_init>> nfc->receive_iov = filter_buffer_receive_iov;
+     *   - net/filter-mirror.c|425| <<filter_mirror_class_init>> nfc->receive_iov = filter_mirror_receive_iov;
+     *   - net/filter-mirror.c|442| <<filter_redirector_class_init>> nfc->receive_iov = filter_redirector_receive_iov;
+     *   - net/filter-replay.c|72| <<filter_replay_class_init>> nfc->receive_iov = filter_replay_receive_iov;
+     *   - net/filter-rewriter.c|424| <<colo_rewriter_class_init>> nfc->receive_iov = colo_rewriter_receive_iov;
+     *   - net/filter.c|41| <<qemu_netfilter_receive>> return NETFILTER_GET_CLASS(OBJECT(nf))->receive_iov(
+     */
     /* mandatory */
     FilterReceiveIOV *receive_iov;
 };
diff --git a/include/sysemu/dump.h b/include/sysemu/dump.h
index d70285485..f41c395d2 100644
--- a/include/sysemu/dump.h
+++ b/include/sysemu/dump.h
@@ -210,6 +210,28 @@ typedef struct DumpState {
                                   * this could be used to calculate
                                   * how much work we have
                                   * finished. */
+    /*
+     * 在以下使用guest_note:
+     *   - dump/dump.c|107| <<dump_cleanup>> g_free(s->guest_note);
+     *   - dump/dump.c|109| <<dump_cleanup>> s->guest_note = NULL;
+     *   - dump/dump.c|287| <<write_guest_note>> if (s->guest_note) {
+     *   - dump/dump.c|288| <<write_guest_note>> ret = f(s->guest_note, s->guest_note_size, s);
+     *   - dump/dump.c|1024| <<create_header32>> if (s->guest_note &&
+     *   - dump/dump.c|1025| <<create_header32>> note_name_equal(s, s->guest_note, "VMCOREINFO")) {
+     *   - dump/dump.c|1028| <<create_header32>> get_note_sizes(s, s->guest_note,
+     *   - dump/dump.c|1135| <<create_header64>> if (s->guest_note &&
+     *   - dump/dump.c|1136| <<create_header64>> note_name_equal(s, s->guest_note, "VMCOREINFO")) {
+     *   - dump/dump.c|1139| <<create_header64>> get_note_sizes(s, s->guest_note,
+     *   - dump/dump.c|1746| <<vmcoreinfo_update_phys_base>> if (!note_name_equal(s, s->guest_note, "VMCOREINFO")) {
+     *   - dump/dump.c|1750| <<vmcoreinfo_update_phys_base>> get_note_sizes(s, s->guest_note, &note_head_size, &name_size, &size);
+     *   - dump/dump.c|1753| <<vmcoreinfo_update_phys_base>> vmci = s->guest_note + note_head_size + ROUND_UP(name_size, 4);
+     *   - dump/dump.c|1890| <<dump_init>> s->guest_note = g_malloc(size + 1);
+     *   - dump/dump.c|1891| <<dump_init>> cpu_physical_memory_read(addr, s->guest_note, size);
+     *   - dump/dump.c|1893| <<dump_init>> get_note_sizes(s, s->guest_note, NULL, &name_size, &desc_size);
+     *   - dump/dump.c|1900| <<dump_init>> g_free(s->guest_note);
+     *   - dump/dump.c|1901| <<dump_init>> s->guest_note = NULL;
+     *   - dump/win_dump.c|410| <<create_win_dump>> WinDumpHeader *h = (void *)(s->guest_note + VMCOREINFO_ELF_NOTE_HDR_SIZE);
+     */
     uint8_t *guest_note;         /* ELF note content */
     size_t guest_note_size;
 } DumpState;
diff --git a/migration/migration.c b/migration/migration.c
index 8c5bd0a75..4425bfd75 100644
--- a/migration/migration.c
+++ b/migration/migration.c
@@ -783,6 +783,26 @@ static void process_incoming_migration_bh(void *opaque)
     migration_incoming_state_destroy();
 }
 
+/*
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ *
+ * 在以下使用process_incoming_migration_co():
+ *   - migration/migration.c|892| <<migration_incoming_process>> Coroutine *co = qemu_coroutine_create(process_incoming_migration_co, NULL);
+ */
 static void coroutine_fn
 process_incoming_migration_co(void *opaque)
 {
@@ -870,6 +890,27 @@ static void migration_incoming_setup(QEMUFile *f)
     qemu_file_set_blocking(f, false);
 }
 
+/*
+ * (gdb) bt
+ * #0  migration_incoming_process () at ../migration/migration.c:875
+ * #1  0x0000555555c13d04 in migration_ioc_process_incoming (ioc=0x5555574b3a00, errp=0x7fffffffd8f8) at ../migration/migration.c:1008
+ * #2  0x0000555555c05c0d in migration_channel_process_incoming (ioc=0x5555574b3a00) at ../migration/channel.c:45
+ * #3  0x0000555555c0a722 in exec_accept_incoming_migration (ioc=0x5555574b3a00, condition=G_IO_IN, opaque=0x0) at ../migration/exec.c:66
+ * #4  0x0000555555f61bcf in qio_channel_fd_pair_source_dispatch (source=0x555557a05410, callback=0x555555c0a703 <exec_accept_incoming_migration>, user_data=0x0)
+ *     at ../io/channel-watch.c:217
+ * #5  0x00007ffff6fd3854 in g_main_dispatch (context=0x55555740af10) at ../glib/gmain.c:3325
+ * #6  g_main_context_dispatch (context=0x55555740af10) at ../glib/gmain.c:4043
+ * #7  0x000055555618fc2a in glib_pollfds_poll () at ../util/main-loop.c:287
+ * #8  0x000055555618fcb8 in os_host_main_loop_wait (timeout=2224360793) at ../util/main-loop.c:310
+ * #9  0x000055555618fde7 in main_loop_wait (nonblocking=0) at ../util/main-loop.c:589
+ * #10 0x0000555555be04f9 in qemu_main_loop () at ../system/runstate.c:835
+ * #11 0x000055555609a052 in qemu_default_main () at ../system/main.c:37
+ * #12 0x000055555609a08f in main (argc=20, argv=0x7fffffffdbd8) at ../system/main.c:48
+ *
+ * called by:
+ *   - migration/migration.c|935| <<migration_fd_process_incoming>> migration_incoming_process();
+ *   - migration/migration.c|1025| <<migration_ioc_process_incoming>> migration_incoming_process();
+ */
 void migration_incoming_process(void)
 {
     Coroutine *co = qemu_coroutine_create(process_incoming_migration_co, NULL);
@@ -3213,6 +3254,10 @@ typedef enum {
  * Return true if continue to the next iteration directly, false
  * otherwise.
  */
+/*
+ * called by:
+ *   migration/migration.c|3513| <<migration_thread>> MigIterateState iter_state = migration_iteration_run(s);
+ */
 static MigIterateState migration_iteration_run(MigrationState *s)
 {
     uint64_t must_precopy, can_postcopy, pending_size;
diff --git a/migration/migration.h b/migration/migration.h
index 3857905c0..1b1743d1b 100644
--- a/migration/migration.h
+++ b/migration/migration.h
@@ -470,6 +470,13 @@ struct MigrationState {
      */
     bool switchover_acked;
     /* Is this a rdma migration */
+    /*
+     * 在以下使用MigrationState->rdma_migration:
+     *   - migration/migration.c|1703| <<migrate_init>> s->rdma_migration = false;
+     *   - migration/options.c|367| <<migrate_rdma>> return s->rdma_migration;
+     *   - migration/rdma.c|4101| <<rdma_start_incoming_migration>> s->rdma_migration = true;
+     *   - migration/rdma.c|4176| <<rdma_start_outgoing_migration>> s->rdma_migration = true;
+     */
     bool rdma_migration;
 };
 
diff --git a/migration/multifd-nocomp.c b/migration/multifd-nocomp.c
index 55191152f..46daea58d 100644
--- a/migration/multifd-nocomp.c
+++ b/migration/multifd-nocomp.c
@@ -298,11 +298,31 @@ static inline bool multifd_queue_full(MultiFDPages_t *pages)
     return pages->num == multifd_ram_page_count();
 }
 
+/*
+ * called by:
+ *   - migration/multifd-nocomp.c|322| <<multifd_queue_page>> multifd_enqueue(pages, offset);
+ *   - migration/multifd-nocomp.c|342| <<multifd_queue_page>> multifd_enqueue(pages, offset);
+ */
 static inline void multifd_enqueue(MultiFDPages_t *pages, ram_addr_t offset)
 {
+    /*
+     * 76 typedef struct {
+     * 77     // number of used pages
+     * 78     uint32_t num;
+     * 79     // number of normal pages
+     * 80     uint32_t normal_num;
+     * 81     RAMBlock *block;
+     * 82     // offset of each page
+     * 83     ram_addr_t offset[];
+     * 84 } MultiFDPages_t;
+     */
     pages->offset[pages->num++] = offset;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1384| <<ram_save_multifd_page>> if (!multifd_queue_page(block, offset)) {
+ */
 /* Returns true if enqueue successful, false otherwise */
 bool multifd_queue_page(RAMBlock *block, ram_addr_t offset)
 {
diff --git a/migration/options.c b/migration/options.c
index ad8d6989a..2a29dbefe 100644
--- a/migration/options.c
+++ b/migration/options.c
@@ -304,6 +304,32 @@ bool migrate_rdma_pin_all(void)
     return s->capabilities[MIGRATION_CAPABILITY_RDMA_PIN_ALL];
 }
 
+/*
+ * 默认:
+ * (qemu) info migrate_capabilities
+ * xbzrle: off
+ * rdma-pin-all: off
+ * auto-converge: off
+ * zero-blocks: off
+ * events: off
+ * postcopy-ram: off
+ * x-colo: off
+ * release-ram: off
+ * return-path: off
+ * pause-before-switchover: off
+ * multifd: off
+ * dirty-bitmaps: off
+ * postcopy-blocktime: off
+ * late-block-activate: off
+ * x-ignore-shared: off
+ * validate-uuid: off
+ * background-snapshot: off
+ * zero-copy-send: off
+ * postcopy-preempt: off
+ * switchover-ack: off
+ * dirty-limit: off
+ * mapped-ram: off
+ */
 bool migrate_release_ram(void)
 {
     MigrationState *s = migrate_get_current();
@@ -364,6 +390,13 @@ bool migrate_rdma(void)
 {
     MigrationState *s = migrate_get_current();
 
+    /*
+     * 在以下使用MigrationState->rdma_migration:
+     *   - migration/migration.c|1703| <<migrate_init>> s->rdma_migration = false;
+     *   - migration/options.c|367| <<migrate_rdma>> return s->rdma_migration;
+     *   - migration/rdma.c|4101| <<rdma_start_incoming_migration>> s->rdma_migration = true;
+     *   - migration/rdma.c|4176| <<rdma_start_outgoing_migration>> s->rdma_migration = true;
+     */
     return s->rdma_migration;
 }
 
diff --git a/migration/qemu-file.c b/migration/qemu-file.c
index b6d2f588b..d01dcfa94 100644
--- a/migration/qemu-file.c
+++ b/migration/qemu-file.c
@@ -421,6 +421,10 @@ static void add_buf_to_iovec(QEMUFile *f, size_t len)
     }
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|1306| <<save_normal_page>> qemu_put_buffer_async(file, buf, TARGET_PAGE_SIZE,
+ */
 void qemu_put_buffer_async(QEMUFile *f, const uint8_t *buf, size_t size,
                            bool may_free)
 {
@@ -431,6 +435,55 @@ void qemu_put_buffer_async(QEMUFile *f, const uint8_t *buf, size_t size,
     add_to_iovec(f, buf, size, may_free);
 }
 
+/*
+ * called by:
+ *   - hw/display/virtio-gpu.c|1244| <<virtio_gpu_save>> qemu_put_buffer(f, (void *)pixman_image_get_data(res->image),
+ *   - hw/intc/s390_flic_kvm.c|446| <<kvm_flic_save>> qemu_put_buffer(f, (uint8_t *) buf,
+ *   - hw/pci/msix.c|486| <<msix_save>> qemu_put_buffer(f, dev->msix_table, n * PCI_MSIX_ENTRY_SIZE);
+ *   - hw/pci/msix.c|487| <<msix_save>> qemu_put_buffer(f, dev->msix_pba, DIV_ROUND_UP(n, 8));
+ *   - hw/pci/pci.c|692| <<put_pci_config_device>> qemu_put_buffer(f, *v, size);
+ *   - hw/pci/shpc.c|766| <<shpc_save>> qemu_put_buffer(f, d->shpc->config, SHPC_SIZEOF(d));
+ *   - hw/ppc/spapr.c|2175| <<htab_save_chunk>> qemu_put_buffer(f, HPTE(spapr->htab, chunkstart),
+ *   - hw/s390x/s390-skeys.c|375| <<s390_storage_keys_save>> qemu_put_buffer(f, buf, cur_pages);
+ *   - hw/scsi/mptsas.c|1221| <<mptsas_save_request>> qemu_put_buffer(f, (unsigned char *)&req->scsi_io, sizeof(req->scsi_io));
+ *   - hw/scsi/scsi-bus.c|1828| <<put_scsi_req>> qemu_put_buffer(f, req->cmd.buf, sizeof(req->cmd.buf));
+ *   - hw/scsi/scsi-disk.c|166| <<scsi_disk_save_request>> qemu_put_buffer(f, r->iov.iov_base, r->iov.iov_len);
+ *   - hw/scsi/scsi-disk.c|170| <<scsi_disk_save_request>> qemu_put_buffer(f, r->iov.iov_base, r->iov.iov_len);
+ *   - hw/scsi/scsi-generic.c|51| <<scsi_generic_save_request>> qemu_put_buffer(f, r->buf, r->req.cmd.xfer);
+ *   - hw/usb/redirect.c|2254| <<usbredir_put_parser>> qemu_put_buffer(f, data, len);
+ *   - hw/usb/redirect.c|2320| <<usbredir_put_bufpq>> qemu_put_buffer(f, bufp->data + bufp->offset, len);
+ *   - hw/vfio/migration.c|391| <<vfio_save_block>> qemu_put_buffer(f, migration->data_buffer, data_size);
+ *   - hw/virtio/vhost.c|2520| <<vhost_save_backend_state>> qemu_put_buffer(f, transfer_buf, read_ret);
+ *   - hw/virtio/virtio.c|2181| <<qemu_put_virtqueue_element>> qemu_put_buffer(f, (uint8_t *)&data, sizeof(VirtQueueElementOld));
+ *   - hw/virtio/virtio.c|3042| <<virtio_save>> qemu_put_buffer(f, vdev->config, vdev->config_len);
+ *   - migration/block-dirty-bitmap.c|461| <<send_bitmap_bits>> qemu_put_buffer(f, buf, buf_size);
+ *   - migration/colo.c|485| <<colo_do_checkpoint_transaction>> qemu_put_buffer(s->to_dst_file, bioc->data, bioc->usage);
+ *   - migration/migration.c|455| <<migrate_send_rp_message>> qemu_put_buffer(mis->to_src_file, data, len);
+ *   - migration/migration.c|2869| <<bg_migration_completion>> qemu_put_buffer(s->to_dst_file, s->bioc->data, s->bioc->usage);
+ *   - migration/qemu-file.c|807| <<qemu_put_counted_string>> qemu_put_buffer(f, (const uint8_t *)str, len);
+ *   - migration/ram.c|327| <<ramblock_recv_bitmap_send>> qemu_put_buffer(file, (const uint8_t *)le_bitmap, size);
+ *   - migration/ram.c|534| <<save_page_header>> qemu_put_buffer(f, (uint8_t *)block->idstr, len);
+ *   - migration/ram.c|699| <<save_xbzrle_page>> qemu_put_buffer(file, XBZRLE.encoded_buf, encoded_len);
+ *   - migration/ram.c|1261| <<save_normal_page>> qemu_put_buffer(file, buf, TARGET_PAGE_SIZE);
+ *   - migration/ram.c|2995| <<mapped_ram_setup_ramblock>> qemu_put_buffer(file, (uint8_t *) header, header_size);
+ *   - migration/ram.c|3072| <<ram_save_setup>> qemu_put_buffer(f, (uint8_t *)block->idstr, strlen(block->idstr));
+ *   - migration/savevm.c|413| <<put_capability>> qemu_put_buffer(f, (uint8_t *)capability_str, len);
+ *   - migration/savevm.c|975| <<save_section_header>> qemu_put_buffer(f, (uint8_t *)se->idstr, len);
+ *   - migration/savevm.c|1051| <<qemu_savevm_command_send>> qemu_put_buffer(f, data, len);
+ *   - migration/savevm.c|1104| <<qemu_savevm_send_packaged>> qemu_put_buffer(f, buf, len);
+ *   - migration/savevm.c|1574| <<qemu_savevm_state_complete_precopy_non_iterable>> qemu_put_buffer(f, (uint8_t *)json_writer_get(vmdesc), vmdesc_len);
+ *   - migration/vmstate-types.c|464| <<put_buffer>> qemu_put_buffer(f, v, size);
+ *   - migration/vmstate-types.c|500| <<put_unused_buffer>> qemu_put_buffer(f, buf, block_len);
+ *   - migration/vmstate.c|550| <<vmstate_subsection_save>> qemu_put_buffer(f, (uint8_t *)vmsdsub->name, len);
+ *   - net/slirp.c|392| <<net_slirp_stream_write>> qemu_put_buffer(f, buf, size);
+ *   - target/ppc/kvm.c|2730| <<kvmppc_save_htab>> qemu_put_buffer(f, (void *)(head + 1),
+ *   - tests/unit/test-vmstate.c|81| <<save_buffer>> qemu_put_buffer(fsave, buf, buf_size);
+ *   - tests/unit/test-vmstate.c|113| <<load_vmstate_one>> qemu_put_buffer(f, wire, size);
+ *   - tests/unit/test-vmstate.c|768| <<test_load_q>> qemu_put_buffer(fsave, wire_q, sizeof(wire_q));
+ *   - tests/unit/test-vmstate.c|1124| <<test_gtree_load_domain>> qemu_put_buffer(fsave, first_domain_dump, sizeof(first_domain_dump));
+ *   - tests/unit/test-vmstate.c|1239| <<test_gtree_load_iommu>> qemu_put_buffer(fsave, iommu_dump, sizeof(iommu_dump));
+ *   - tests/unit/test-vmstate.c|1374| <<test_load_qlist>> qemu_put_buffer(fsave, qlist_dump, sizeof(qlist_dump));
+ */
 void qemu_put_buffer(QEMUFile *f, const uint8_t *buf, size_t size)
 {
     size_t l;
@@ -628,6 +681,63 @@ size_t coroutine_mixed_fn qemu_peek_buffer(QEMUFile *f, uint8_t **buf, size_t si
  * return as many as it managed to read (assuming blocking fd's which
  * all current QEMUFile are)
  */
+/*
+ * called by:
+ *   - hw/display/virtio-gpu.c|1333| <<virtio_gpu_load>> qemu_get_buffer(f, (void *)pixman_image_get_data(res->image),
+ *   - hw/intc/s390_flic_kvm.c|487| <<kvm_flic_load>> if (qemu_get_buffer(f, (uint8_t *) buf, len) != len) {
+ *   - hw/pci/msix.c|501| <<msix_load>> qemu_get_buffer(f, dev->msix_table, n * PCI_MSIX_ENTRY_SIZE);
+ *   - hw/pci/msix.c|502| <<msix_load>> qemu_get_buffer(f, dev->msix_pba, DIV_ROUND_UP(n, 8));
+ *   - hw/pci/pci.c|659| <<get_pci_config_device>> qemu_get_buffer(f, config, size);
+ *   - hw/pci/shpc.c|775| <<shpc_load>> int ret = qemu_get_buffer(f, d->shpc->config, SHPC_SIZEOF(d));
+ *   - hw/ppc/spapr.c|2452| <<htab_load>> qemu_get_buffer(f, HPTE(spapr->htab, index),
+ *   - hw/s390x/s390-skeys.c|433| <<s390_storage_keys_load>> qemu_get_buffer(f, buf, cur_count);
+ *   - hw/s390x/s390-stattrib.c|138| <<cmma_load>> qemu_get_buffer(f, buf, count);
+ *   - hw/scsi/mptsas.c|1238| <<mptsas_load_request>> qemu_get_buffer(f, (unsigned char *)&req->scsi_io, sizeof(req->scsi_io));
+ *   - hw/scsi/scsi-bus.c|1862| <<get_scsi_requests>> qemu_get_buffer(f, buf, sizeof(buf));
+ *   - hw/scsi/scsi-disk.c|194| <<scsi_disk_load_request>> qemu_get_buffer(f, r->iov.iov_base, r->iov.iov_len);
+ *   - hw/scsi/scsi-disk.c|200| <<scsi_disk_load_request>> qemu_get_buffer(f, r->iov.iov_base, r->iov.iov_len);
+ *   - hw/scsi/scsi-generic.c|62| <<scsi_generic_load_request>> qemu_get_buffer(f, r->buf, r->req.cmd.xfer);
+ *   - hw/usb/redirect.c|2288| <<usbredir_get_parser>> qemu_get_buffer(f, data, len);
+ *   - hw/usb/redirect.c|2348| <<usbredir_get_bufpq>> qemu_get_buffer(f, bufp->data, bufp->len);
+ *   - hw/virtio/vhost.c|2607| <<vhost_load_backend_state>> if (qemu_get_buffer(f, transfer_buf, this_chunk_size) <
+ *   - hw/virtio/virtio.c|2106| <<qemu_get_virtqueue_element>> qemu_get_buffer(f, (uint8_t *)&data, sizeof(VirtQueueElementOld));
+ *   - hw/virtio/virtio.c|3270| <<virtio_load>> qemu_get_buffer(f, vdev->config, MIN(config_len, vdev->config_len));
+ *   - migration/block-dirty-bitmap.c|1015| <<dirty_bitmap_load_bits>> ret = qemu_get_buffer(f, buf, buf_size);
+ *   - migration/colo.c|710| <<colo_incoming_process_checkpoint>> total_size = qemu_get_buffer(mis->from_src_file, bioc->data, value);
+ *   - migration/migration.c|2320| <<source_return_path_thread>> res = qemu_get_buffer(rp, buf, header_len);
+ *   - migration/qemu-file.c|740| <<qemu_get_buffer_in_place>> return qemu_get_buffer(f, *buf, size);
+ *   - migration/qemu-file.c|843| <<qemu_get_counted_string>> size_t res = qemu_get_buffer(f, (uint8_t *)buf, len);
+ *   - migration/ram.c|3097| <<mapped_ram_read_header>> ret = qemu_get_buffer(file, (uint8_t *)header, header_size);
+ *   - migration/ram.c|3546| <<ram_block_from_stream>> qemu_get_buffer(f, (uint8_t *)id, len);
+ *   - migration/ram.c|3920| <<ram_load_postcopy>> qemu_get_buffer(f, page_buffer, TARGET_PAGE_SIZE);
+ *   - migration/ram.c|4236| <<parse_ramblocks>> qemu_get_buffer(f, (uint8_t *)id, len);
+ *   - migration/ram.c|4375| <<ram_load_precopy>> qemu_get_buffer(f, host, TARGET_PAGE_SIZE);
+ *   - migration/ram.c|4548| <<ram_dirty_bitmap_reload>> size = qemu_get_buffer(file, (uint8_t *)le_bitmap, local_size);
+ *   - migration/savevm.c|392| <<get_capability>> qemu_get_buffer(f, (uint8_t *)capability_str, len);
+ *   - migration/savevm.c|2316| <<loadvm_handle_cmd_packaged>> ret = qemu_get_buffer(mis->from_src_file,
+ *   - migration/savevm.c|3014| <<qemu_loadvm_state>> qemu_get_buffer(f, buf, read_chunk);
+ *   - migration/vmstate-types.c|456| <<get_buffer>> qemu_get_buffer(f, v, size);
+ *   - migration/vmstate-types.c|486| <<get_unused_buffer>> qemu_get_buffer(f, buf, block_len);
+ *   - net/slirp.c|384| <<net_slirp_stream_read>> return qemu_get_buffer(f, buf, size);
+ *   - target/ppc/kvm.c|2756| <<kvmppc_load_htab_chunk>> qemu_get_buffer(f, (void *)(buf + 1), HASH_PTE_SIZE_64 * n_valid);
+ *   - tests/unit/test-vmstate.c|92| <<compare_vmstate>> g_assert_cmpint(qemu_get_buffer(f, result, size), ==, size);
+ *
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ */
 size_t coroutine_mixed_fn qemu_get_buffer(QEMUFile *f, uint8_t *buf, size_t size)
 {
     size_t pending = size;
diff --git a/migration/ram.c b/migration/ram.c
index 05ff9eb32..5c7a6e862 100644
--- a/migration/ram.c
+++ b/migration/ram.c
@@ -362,6 +362,16 @@ struct RAMState {
     int uffdio_fd;
     /* total ram size in bytes */
     uint64_t ram_bytes_total;
+    /*
+     * 在以下使用RAMState->last_seen_block:
+     *   - migration/ram.c|1312| <<find_dirty_block>> if (pss->complete_round && pss->block == rs->last_seen_block &&
+     *   - migration/ram.c|2260| <<ram_find_and_save_block>> if (!rs->last_seen_block) {
+     *   - migration/ram.c|2261| <<ram_find_and_save_block>> rs->last_seen_block = QLIST_FIRST_RCU(&ram_list.blocks);
+     *   - migration/ram.c|2265| <<ram_find_and_save_block>> pss_init(pss, rs->last_seen_block, rs->last_page);
+     *   - migration/ram.c|2288| <<ram_find_and_save_block>> rs->last_seen_block = pss->block;
+     *   - migration/ram.c|2408| <<ram_state_reset>> rs->last_seen_block = NULL;
+     *   - migration/ram.c|2601| <<ram_postcopy_send_discard_bitmap>> rs->last_seen_block = NULL;
+     */
     /* Last block that we have visited searching for dirty pages */
     RAMBlock *last_seen_block;
     /* Last dirty target page we have sent */
@@ -383,6 +393,13 @@ struct RAMState {
     uint64_t xbzrle_pages_prev;
     /* Amount of xbzrle encoded bytes since the beginning of the period */
     uint64_t xbzrle_bytes_prev;
+    /*
+     * 在以下使用RAMState->xbzrle_started:
+     *   - migration/ram.c|1240| <<save_zero_page>> if (rs->xbzrle_started) {
+     *   - migration/ram.c|1340| <<ram_save_page>> if (rs->xbzrle_started && !migration_in_postcopy()) {
+     *   - migration/ram.c|1428| <<find_dirty_block>> rs->xbzrle_started = true;
+     *   - migration/ram.c|2516| <<ram_state_reset>> rs->xbzrle_started = false;
+     */
     /* Are we really using XBZRLE (e.g., after the first round). */
     bool xbzrle_started;
     /* Are we on the last stage of migration */
@@ -456,6 +473,16 @@ uint64_t ram_bytes_remaining(void)
                        0;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+ *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+ *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+ *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+ *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+ *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+ *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+ */
 void ram_transferred_add(uint64_t bytes)
 {
     if (runstate_is_running()) {
@@ -506,6 +533,12 @@ static bool pss_overlap(PageSearchStatus *pss1, PageSearchStatus *pss2)
  * @offset: offset inside the block for the page
  *          in the lower bits, it contains flags
  */
+/*
+ * called by:
+ *   - migration/ram.c|695| <<save_xbzrle_page>> bytes_xbzrle = save_page_header(pss, pss->pss_channel, block,
+ *   - migration/ram.c|1188| <<save_zero_page>> len += save_page_header(pss, file, pss->block, offset | RAM_SAVE_FLAG_ZERO);
+ *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+ */
 static size_t save_page_header(PageSearchStatus *pss, QEMUFile *f,
                                RAMBlock *block, ram_addr_t offset)
 {
@@ -693,6 +726,16 @@ static int save_xbzrle_page(RAMState *rs, PageSearchStatus *pss,
      * RAM_SAVE_FLAG_CONTINUE.
      */
     xbzrle_counters.bytes += bytes_xbzrle - 8;
+    /*
+     * called by:
+     *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+     *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+     *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+     *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+     *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+     *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+     *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+     */
     ram_transferred_add(bytes_xbzrle);
 
     return 1;
@@ -709,6 +752,12 @@ static int save_xbzrle_page(RAMState *rs, PageSearchStatus *pss,
  *
  * @pss: the current page search status
  */
+/*
+ * called by:
+ *   - migration/ram.c|1310| <<find_dirty_block>> pss_find_next_dirty(pss);
+ *   - migration/ram.c|2131| <<ram_save_host_page_urgent>> pss_find_next_dirty(pss);
+ *   - migration/ram.c|2216| <<ram_save_host_page>> pss_find_next_dirty(pss);
+ */
 static void pss_find_next_dirty(PageSearchStatus *pss)
 {
     RAMBlock *rb = pss->block;
@@ -730,6 +779,25 @@ static void pss_find_next_dirty(PageSearchStatus *pss)
         size = MIN(size, pss->host_page_end);
     }
 
+    /*
+     * 116 struct PageSearchStatus {
+     * 117     // The migration channel used for a specific host page
+     * 118     QEMUFile    *pss_channel;
+     * 119     // Last block from where we have sent data
+     * 120     RAMBlock *last_sent_block;
+     * 121     // Current block being searched
+     * 122     RAMBlock    *block;
+     * 123     // Current page to search from
+     * 124     unsigned long page;
+     * 125     // Set once we wrap around
+     * 126     bool         complete_round;
+     * 127     // Whether we're sending a host page
+     * 128     bool          host_page_sending;
+     * 129     // The start/end of current host page.  Invalid if host_page_sending==false
+     * 130     unsigned long host_page_start;
+     * 131     unsigned long host_page_end;
+     * 132 };
+     */
     pss->page = find_next_bit(bitmap, size, pss->page);
 }
 
@@ -1109,6 +1177,11 @@ void migration_bitmap_sync_precopy(bool last_stage)
     }
 }
 
+/*
+ * called by:
+ *   - migration/multifd-zero-page.c|73| <<multifd_send_zero_page_detect>> ram_release_page(rb->idstr, offset);
+ *   - migration/ram.c|1191| <<save_zero_page>> ram_release_page(pss->block->idstr, offset);
+ */
 void ram_release_page(const char *rbname, uint64_t offset)
 {
     if (!migrate_release_ram() || !migration_in_postcopy()) {
@@ -1127,9 +1200,20 @@ void ram_release_page(const char *rbname, uint64_t offset)
  * @pss: current PSS channel
  * @offset: offset inside the block for the page
  */
+/*
+ * called by:
+ *   - migration/ram.c|2040| <<ram_save_target_page_legacy>> if (save_zero_page(rs, pss, offset)) {
+ *   - migration/ram.c|2065| <<ram_save_target_page_multifd>> if (save_zero_page(rs, pss, offset)) {
+ */
 static int save_zero_page(RAMState *rs, PageSearchStatus *pss,
                           ram_addr_t offset)
 {
+    /*
+     * PageSearchStatus *pss:
+     * -> RAMBlock *block;
+     *    -> struct MemoryRegion *mr;
+     *    -> uint8_t *host;
+     */
     uint8_t *p = pss->block->host + offset;
     QEMUFile *file = pss->pss_channel;
     int len = 0;
@@ -1154,6 +1238,16 @@ static int save_zero_page(RAMState *rs, PageSearchStatus *pss,
     qemu_put_byte(file, 0);
     len += 1;
     ram_release_page(pss->block->idstr, offset);
+    /*
+     * called by:
+     *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+     *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+     *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+     *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+     *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+     *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+     *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+     */
     ram_transferred_add(len);
 
     /*
@@ -1206,6 +1300,10 @@ static bool control_save_page(PageSearchStatus *pss,
  * @buf: the page to be sent
  * @async: send to page asyncly
  */
+/*
+ * called by:
+ *   - migration/ram.c|1360| <<ram_save_page>> pages = save_normal_page(pss, block, offset, p, send_async);
+ */
 static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
                             ram_addr_t offset, uint8_t *buf, bool async)
 {
@@ -1216,6 +1314,26 @@ static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
                            block->pages_offset + offset);
         set_bit(offset >> TARGET_PAGE_BITS, block->file_bmap);
     } else {
+        /*
+	 * called by:                        
+	 *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+	 *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+	 *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+	 *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+	 *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+	 *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+	 *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+	 *
+	 * 在以下使用RAM_SAVE_FLAG_PAGE:
+	 *   - migration/ram.c|87| <<global>> #define RAM_SAVE_FLAG_PAGE 0x08
+	 *   - migration/ram.c|1308| <<save_normal_page>> ram_transferred_add(save_page_header(pss,
+	 *                      pss->pss_channel, block, offset | RAM_SAVE_FLAG_PAGE));
+	 *   - migration/ram.c|3839| <<ram_load_postcopy>> if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE)) {
+	 *   - migration/ram.c|3916| <<ram_load_postcopy>> case RAM_SAVE_FLAG_PAGE:
+	 *   - migration/ram.c|4292| <<ram_load_precopy>> RAM_SAVE_FLAG_PAGE | RAM_SAVE_FLAG_XBZRLE |
+	 *   - migration/ram.c|4329| <<ram_load_precopy>> if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE |
+	 *   - migration/ram.c|4395| <<ram_load_precopy>> case RAM_SAVE_FLAG_PAGE:
+	 */
         ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
                                              offset | RAM_SAVE_FLAG_PAGE));
         if (async) {
@@ -1226,6 +1344,16 @@ static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
             qemu_put_buffer(file, buf, TARGET_PAGE_SIZE);
         }
     }
+    /*
+     * called by:
+     *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+     *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+     *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+     *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+     *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+     *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+     *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+     */
     ram_transferred_add(TARGET_PAGE_SIZE);
     stat64_add(&mig_stats.normal_pages, 1);
     return 1;
@@ -1243,6 +1371,10 @@ static int save_normal_page(PageSearchStatus *pss, RAMBlock *block,
  * @block: block that contains the page we want to send
  * @offset: offset inside the block for the page
  */
+/*
+ * called by:
+ *   - migration/ram.c|2098| <<ram_save_target_page_legacy>> return ram_save_page(rs, pss);
+ */
 static int ram_save_page(RAMState *rs, PageSearchStatus *pss)
 {
     int pages = -1;
@@ -1256,6 +1388,13 @@ static int ram_save_page(RAMState *rs, PageSearchStatus *pss)
     trace_ram_save_page(block->idstr, (uint64_t)offset, p);
 
     XBZRLE_cache_lock();
+    /*
+     * 在以下使用RAMState->xbzrle_started:
+     *   - migration/ram.c|1240| <<save_zero_page>> if (rs->xbzrle_started) {
+     *   - migration/ram.c|1340| <<ram_save_page>> if (rs->xbzrle_started && !migration_in_postcopy()) {
+     *   - migration/ram.c|1428| <<find_dirty_block>> rs->xbzrle_started = true;
+     *   - migration/ram.c|2516| <<ram_state_reset>> rs->xbzrle_started = false;
+     */
     if (rs->xbzrle_started && !migration_in_postcopy()) {
         pages = save_xbzrle_page(rs, pss, &p, current_addr,
                                  block, offset);
@@ -1269,6 +1408,9 @@ static int ram_save_page(RAMState *rs, PageSearchStatus *pss)
 
     /* XBZRLE overflow or normal page */
     if (pages == -1) {
+        /*
+	 * 只在此处调用
+	 */
         pages = save_normal_page(pss, block, offset, p, send_async);
     }
 
@@ -1993,19 +2135,43 @@ int ram_save_queue_pages(const char *rbname, ram_addr_t start, ram_addr_t len,
  * @rs: current RAM state
  * @pss: data about the page we want to send
  */
+/*
+ * 3156     migration_ops = g_malloc0(sizeof(MigrationOps));
+ * 3157 
+ * 3158     if (migrate_multifd()) {
+ * 3159         multifd_ram_save_setup();
+ * 3160         migration_ops->ram_save_target_page = ram_save_target_page_multifd;
+ * 3161     } else {
+ * 3162         migration_ops->ram_save_target_page = ram_save_target_page_legacy;
+ * 3163     }
+ *
+ * 在以下使用ram_save_target_page_legacy():
+ *   - migration/ram.c|3108| <<ram_save_setup>> migration_ops->ram_save_target_page = ram_save_target_page_legacy;
+ */
 static int ram_save_target_page_legacy(RAMState *rs, PageSearchStatus *pss)
 {
     ram_addr_t offset = ((ram_addr_t)pss->page) << TARGET_PAGE_BITS;
     int res;
 
+    /*
+     * 不支持rdma不用
+     */
     if (control_save_page(pss, offset, &res)) {
         return res;
     }
 
+    /*
+     * called by:
+     *   - migration/ram.c|2040| <<ram_save_target_page_legacy>> if (save_zero_page(rs, pss, offset)) {
+     *   - migration/ram.c|2065| <<ram_save_target_page_multifd>> if (save_zero_page(rs, pss, offset)) {
+     */
     if (save_zero_page(rs, pss, offset)) {
         return 1;
     }
 
+    /*
+     * 只在此处调用
+     */
     return ram_save_page(rs, pss);
 }
 
@@ -2235,6 +2401,11 @@ static int ram_save_host_page(RAMState *rs, PageSearchStatus *pss)
  * On systems where host-page-size > target-page-size it will send all the
  * pages in a host page that are dirty.
  */
+/*
+ * called by:
+ *   - migration/ram.c|3168| <<ram_save_iterate>> pages = ram_find_and_save_block(rs);
+ *   - migration/ram.c|3267| <<ram_save_complete>> pages = ram_find_and_save_block(rs);
+ */
 static int ram_find_and_save_block(RAMState *rs)
 {
     PageSearchStatus *pss = &rs->pss[RAM_CHANNEL_PRECOPY];
@@ -2769,6 +2940,10 @@ static void migration_bitmap_clear_discarded_pages(RAMState *rs)
     }
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|2816| <<ram_init_all>> if (!ram_init_bitmaps(*rsp, errp)) {
+ */
 static bool ram_init_bitmaps(RAMState *rs, Error **errp)
 {
     bool ret = true;
@@ -3095,6 +3270,16 @@ static void ram_save_file_bmap(QEMUFile *f)
 
         qemu_put_buffer_at(f, (uint8_t *)block->file_bmap, bitmap_size,
                            block->bitmap_offset);
+	/*
+	 * called by:
+	 *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+	 *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+	 *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+	 *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+	 *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+	 *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+	 *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+	 */
         ram_transferred_add(bitmap_size);
 
         /*
@@ -3218,6 +3403,16 @@ out:
         }
 
         qemu_put_be64(f, RAM_SAVE_FLAG_EOS);
+	/*
+	 * called by:
+	 *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+	 *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+	 *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+	 *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+	 *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+	 *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+	 *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+	 */
         ram_transferred_add(8);
         ret = qemu_fflush(f);
     }
@@ -3779,6 +3974,17 @@ int ram_load_postcopy(QEMUFile *f, int channel)
             break;
 
         case RAM_SAVE_FLAG_PAGE:
+            /*
+	     * 在以下使用RAM_SAVE_FLAG_PAGE:
+	     *   - migration/ram.c|87| <<global>> #define RAM_SAVE_FLAG_PAGE 0x08
+	     *   - migration/ram.c|1308| <<save_normal_page>> ram_transferred_add(save_page_header(pss,
+	     *                      pss->pss_channel, block, offset | RAM_SAVE_FLAG_PAGE));
+	     *   - migration/ram.c|3839| <<ram_load_postcopy>> if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE)) {
+	     *   - migration/ram.c|3916| <<ram_load_postcopy>> case RAM_SAVE_FLAG_PAGE:
+	     *   - migration/ram.c|4292| <<ram_load_precopy>> RAM_SAVE_FLAG_PAGE | RAM_SAVE_FLAG_XBZRLE |
+	     *   - migration/ram.c|4329| <<ram_load_precopy>> if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE |
+	     *   - migration/ram.c|4395| <<ram_load_precopy>> case RAM_SAVE_FLAG_PAGE:
+	     */
             tmp_page->all_zero = false;
             if (!matches_target_page_size) {
                 /* For huge pages, we always use temporary buffer */
@@ -3906,6 +4112,10 @@ static size_t ram_load_multifd_pages(void *host_addr, size_t size,
     return size;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|4010| <<parse_ramblock_mapped_ram>> if (!read_ramblock_mapped_ram(f, block, num_pages, bitmap, errp)) {
+ */
 static bool read_ramblock_mapped_ram(QEMUFile *f, RAMBlock *block,
                                      long num_pages, unsigned long *bitmap,
                                      Error **errp)
@@ -3961,6 +4171,10 @@ err:
     return false;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|4031| <<parse_ramblock>> parse_ramblock_mapped_ram(f, block, length, &local_err);
+ */
 static void parse_ramblock_mapped_ram(QEMUFile *f, RAMBlock *block,
                                       ram_addr_t length, Error **errp)
 {
@@ -4008,6 +4222,10 @@ static void parse_ramblock_mapped_ram(QEMUFile *f, RAMBlock *block,
     return;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|4104| <<parse_ramblocks>> ret = parse_ramblock(f, block, length);
+ */
 static int parse_ramblock(QEMUFile *f, RAMBlock *block, ram_addr_t length)
 {
     int ret = 0;
@@ -4075,6 +4293,10 @@ static int parse_ramblock(QEMUFile *f, RAMBlock *block, ram_addr_t length)
     return ret;
 }
 
+/*
+ * called by:
+ *   - migration/ram.c|4351| <<ram_load_precopy>> ret = parse_ramblocks(f, addr);
+ */
 static int parse_ramblocks(QEMUFile *f, ram_addr_t total_ram_bytes)
 {
     int ret = 0;
@@ -4114,6 +4336,39 @@ static int parse_ramblocks(QEMUFile *f, ram_addr_t total_ram_bytes)
  *
  * @f: QEMUFile where to send the data
  */
+/*
+ * (gdb) bt
+ * #0  ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4119
+ * #1  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #2  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #3  0x0000555555c32a8e in qemu_loadvm_section_start_full (f=0x555557804800, type=1 '\001') at ../migration/savevm.c:2619
+ * #4  0x0000555555c33498 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2873
+ * #5  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #6  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #7  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #8  0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #9  0x00007fffffffc7d0 in ?? ()
+ * #10 0x0000000000000000 in ?? ()
+ *
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ *
+ * called by:
+ *   - migration/ram.c|4454| <<ram_load>> ret = ram_load_precopy(f);
+ */
 static int ram_load_precopy(QEMUFile *f)
 {
     MigrationIncomingState *mis = migration_incoming_get_current();
@@ -4158,6 +4413,17 @@ static int ram_load_precopy(QEMUFile *f)
             break;
         }
 
+	/*
+	 * 在以下使用RAM_SAVE_FLAG_PAGE:
+	 *   - migration/ram.c|87| <<global>> #define RAM_SAVE_FLAG_PAGE 0x08
+	 *   - migration/ram.c|1308| <<save_normal_page>> ram_transferred_add(save_page_header(pss,
+	 *                      pss->pss_channel, block, offset | RAM_SAVE_FLAG_PAGE));
+	 *   - migration/ram.c|3839| <<ram_load_postcopy>> if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE)) {
+	 *   - migration/ram.c|3916| <<ram_load_postcopy>> case RAM_SAVE_FLAG_PAGE:
+	 *   - migration/ram.c|4292| <<ram_load_precopy>> RAM_SAVE_FLAG_PAGE | RAM_SAVE_FLAG_XBZRLE |
+	 *   - migration/ram.c|4329| <<ram_load_precopy>> if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE |
+	 *   - migration/ram.c|4395| <<ram_load_precopy>> case RAM_SAVE_FLAG_PAGE:
+	 */
         if (flags & (RAM_SAVE_FLAG_ZERO | RAM_SAVE_FLAG_PAGE |
                      RAM_SAVE_FLAG_XBZRLE)) {
             RAMBlock *block = ram_block_from_stream(mis, f, flags,
@@ -4272,6 +4538,38 @@ static int ram_load_precopy(QEMUFile *f)
     return ret;
 }
 
+/*
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ *
+ * 4704 static SaveVMHandlers savevm_ram_handlers = {
+ * 4705     .save_setup = ram_save_setup,
+ * 4706     .save_live_iterate = ram_save_iterate,
+ * 4707     .save_live_complete_postcopy = ram_save_complete,
+ * 4708     .save_live_complete_precopy = ram_save_complete,
+ * 4709     .has_postcopy = ram_has_postcopy,
+ * 4710     .state_pending_exact = ram_state_pending_exact,
+ * 4711     .state_pending_estimate = ram_state_pending_estimate,
+ * 4712     .load_state = ram_load,
+ * 4713     .save_cleanup = ram_save_cleanup,
+ * 4714     .load_setup = ram_load_setup,
+ * 4715     .load_cleanup = ram_load_cleanup,
+ * 4716     .resume_prepare = ram_resume_prepare,
+ * 4717 };
+ */
 static int ram_load(QEMUFile *f, void *opaque, int version_id)
 {
     int ret = 0;
diff --git a/migration/rdma.c b/migration/rdma.c
index 855753c67..281bc6f0b 100644
--- a/migration/rdma.c
+++ b/migration/rdma.c
@@ -1734,6 +1734,11 @@ static int qemu_rdma_post_send_control(RDMAContext *rdma, uint8_t *buf,
     }
 
 
+    /*
+     * 在以下调用ibv_post_send():
+     *   - migration/rdma.c|1737| <<qemu_rdma_post_send_control>> ret = ibv_post_send(rdma->qp, &send_wr, &bad_wr);
+     *   - migration/rdma.c|2188| <<qemu_rdma_write_one>> ret = ibv_post_send(rdma->qp, &send_wr, &bad_wr);
+     */
     ret = ibv_post_send(rdma->qp, &send_wr, &bad_wr);
 
     if (ret > 0) {
@@ -1991,6 +1996,11 @@ static int qemu_rdma_exchange_recv(RDMAContext *rdma, RDMAControlHeader *head,
  * If we're using dynamic registration on the dest-side, we have to
  * send a registration command first.
  */
+/*
+ * called by:
+ *   - migration/rdma.c|2239| <<qemu_rdma_write_flush>> ret = qemu_rdma_write_one(rdma,
+ *                  rdma->current_index, rdma->current_addr, rdma->current_length, errp);
+ */
 static int qemu_rdma_write_one(RDMAContext *rdma,
                                int current_index, uint64_t current_addr,
                                uint64_t length, Error **errp)
@@ -2185,6 +2195,11 @@ retry:
      * ibv_post_send() does not return negative error numbers,
      * per the specification they are positive - no idea why.
      */
+    /*
+     * 在以下调用ibv_post_send():
+     *   - migration/rdma.c|1737| <<qemu_rdma_post_send_control>> ret = ibv_post_send(rdma->qp, &send_wr, &bad_wr);
+     *   - migration/rdma.c|2188| <<qemu_rdma_write_one>> ret = ibv_post_send(rdma->qp, &send_wr, &bad_wr);
+     */
     ret = ibv_post_send(rdma->qp, &send_wr, &bad_wr);
 
     if (ret == ENOMEM) {
@@ -2216,6 +2231,16 @@ retry:
      * but this being RDMA, who knows.
      */
     stat64_add(&mig_stats.rdma_bytes, sge.length);
+    /*
+     * called by:
+     *   - migration/ram.c|706| <<save_xbzrle_page>> ram_transferred_add(bytes_xbzrle);
+     *   - migration/ram.c|1192| <<save_zero_page>> ram_transferred_add(len);
+     *   - migration/ram.c|1254| <<save_normal_page>> ram_transferred_add(save_page_header(pss, pss->pss_channel, block,
+     *   - migration/ram.c|1264| <<save_normal_page>> ram_transferred_add(TARGET_PAGE_SIZE);
+     *   - migration/ram.c|3142| <<ram_save_file_bmap>> ram_transferred_add(bitmap_size);
+     *   - migration/ram.c|3265| <<ram_save_iterate>> ram_transferred_add(8);
+     *   - migration/rdma.c|2219| <<qemu_rdma_write_one>> ram_transferred_add(sge.length);
+     */
     ram_transferred_add(sge.length);
     rdma->total_writes++;
 
diff --git a/migration/savevm.c b/migration/savevm.c
index 98821c812..244ce8f69 100644
--- a/migration/savevm.c
+++ b/migration/savevm.c
@@ -930,6 +930,23 @@ void vmstate_unregister(VMStateIf *obj, const VMStateDescription *vmsd,
     }
 }
 
+/*
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ */
 static int vmstate_load(QEMUFile *f, SaveStateEntry *se)
 {
     trace_vmstate_load(se->idstr, se->vmsd ? se->vmsd->name : "(old)");
@@ -1513,6 +1530,11 @@ int qemu_savevm_state_complete_precopy_iterable(QEMUFile *f, bool in_postcopy)
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|3667| <<bg_migration_thread>> if (qemu_savevm_state_complete_precopy_non_iterable(fb, false, false)) {
+ *   - migration/savevm.c|1629| <<qemu_savevm_state_complete_precopy>> ret = qemu_savevm_state_complete_precopy_non_iterable(f, in_postcopy,
+ */
 int qemu_savevm_state_complete_precopy_non_iterable(QEMUFile *f,
                                                     bool in_postcopy,
                                                     bool inactivate_disks)
@@ -1583,6 +1605,14 @@ int qemu_savevm_state_complete_precopy_non_iterable(QEMUFile *f,
     return 0;
 }
 
+/*
+ * called by:
+ *   - migration/migration.c|2567| <<postcopy_start>> qemu_savevm_state_complete_precopy(ms->to_dst_file, true, false);
+ *   - migration/migration.c|2612| <<postcopy_start>> qemu_savevm_state_complete_precopy(fb, false, false);
+ *   - migration/migration.c|2762| <<migration_completion_precopy>> ret = qemu_savevm_state_complete_precopy(s->to_dst_file, false,
+ *   - migration/savevm.c|1733| <<qemu_savevm_state>> qemu_savevm_state_complete_precopy(f, false, false);
+ *   - migration/savevm.c|1759| <<qemu_savevm_live_state>> qemu_savevm_state_complete_precopy(f, true, false);
+ */
 int qemu_savevm_state_complete_precopy(QEMUFile *f, bool iterable_only,
                                        bool inactivate_disks)
 {
@@ -2558,6 +2588,10 @@ static bool check_section_footer(QEMUFile *f, SaveStateEntry *se)
     return true;
 }
 
+/*
+ * called by:
+ *   - migration/savevm.c|2873| <<qemu_loadvm_state_main>> ret = qemu_loadvm_section_start_full(f, section_type);
+ */
 static int
 qemu_loadvm_section_start_full(QEMUFile *f, uint8_t type)
 {
@@ -2636,6 +2670,23 @@ qemu_loadvm_section_start_full(QEMUFile *f, uint8_t type)
     return 0;
 }
 
+/*
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ */
 static int
 qemu_loadvm_section_part_end(QEMUFile *f, uint8_t type)
 {
@@ -2852,6 +2903,30 @@ static bool postcopy_pause_incoming(MigrationIncomingState *mis)
     return true;
 }
 
+/*
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ *
+ * called by:
+ *   - migration/colo.c|686| <<colo_incoming_process_checkpoint>> ret = qemu_loadvm_state_main(mis->from_src_file, mis);
+ *   - migration/savevm.c|1999| <<postcopy_ram_listen_thread>> load_res = qemu_loadvm_state_main(f, mis);
+ *   - migration/savevm.c|2351| <<loadvm_handle_cmd_packaged>> ret = qemu_loadvm_state_main(packf, mis);
+ *   - migration/savevm.c|2957| <<qemu_loadvm_state>> ret = qemu_loadvm_state_main(f, mis);
+ *   - migration/savevm.c|3020| <<qemu_load_device_state>> ret = qemu_loadvm_state_main(f, mis);
+ */
 int qemu_loadvm_state_main(QEMUFile *f, MigrationIncomingState *mis)
 {
     uint8_t section_type;
@@ -2927,6 +3002,23 @@ out:
     return ret;
 }
 
+/*
+ * Target QEMU.
+ * (gdb) bt
+ * #0  0x00007ffff52bf413 in __memmove_avx_unaligned_erms_rtm () from /lib64/libc.so.6
+ * #1  0x0000555555f59607 in qemu_get_buffer (f=0x555557804800, buf=0x7ffee8000000 "\002", size=4096) at ../migration/qemu-file.c:644
+ * #2  0x0000555555e8adc4 in ram_load_precopy (f=0x555557804800) at ../migration/ram.c:4228
+ * #3  0x0000555555e8af76 in ram_load (f=0x555557804800, opaque=0x555557334c60 <ram_state>, version_id=4) at ../migration/ram.c:4307
+ * #4  0x0000555555c2ee96 in vmstate_load (f=0x555557804800, se=0x555557711b10) at ../migration/savevm.c:937
+ * #5  0x0000555555c32c88 in qemu_loadvm_section_part_end (f=0x555557804800, type=2 '\002') at ../migration/savevm.c:2672
+ * #6  0x0000555555c334b8 in qemu_loadvm_state_main (f=0x555557804800, mis=0x5555574093d0) at ../migration/savevm.c:2880
+ * #7  0x0000555555c33652 in qemu_loadvm_state (f=0x555557804800) at ../migration/savevm.c:2957
+ * #8  0x0000555555c13760 in process_incoming_migration_co (opaque=0x0) at ../migration/migration.c:803
+ * #9  0x0000555556192d6f in coroutine_trampoline (i0=1466557760, i1=21845) at ../util/coroutine-ucontext.c:175
+ * #10 0x00007ffff5233120 in ?? () from /lib64/libc.so.6
+ * #11 0x00007fffffffc7d0 in ?? ()
+ * #12 0x0000000000000000 in ?? ()
+ */
 int qemu_loadvm_state(QEMUFile *f)
 {
     MigrationIncomingState *mis = migration_incoming_get_current();
@@ -2954,6 +3046,14 @@ int qemu_loadvm_state(QEMUFile *f)
 
     cpu_synchronize_all_pre_loadvm();
 
+    /*
+     * called by:
+     *   - migration/colo.c|686| <<colo_incoming_process_checkpoint>> ret = qemu_loadvm_state_main(mis->from_src_file, mis);
+     *   - migration/savevm.c|1999| <<postcopy_ram_listen_thread>> load_res = qemu_loadvm_state_main(f, mis);
+     *   - migration/savevm.c|2351| <<loadvm_handle_cmd_packaged>> ret = qemu_loadvm_state_main(packf, mis);
+     *   - migration/savevm.c|2957| <<qemu_loadvm_state>> ret = qemu_loadvm_state_main(f, mis);
+     *   - migration/savevm.c|3020| <<qemu_load_device_state>> ret = qemu_loadvm_state_main(f, mis);
+     */
     ret = qemu_loadvm_state_main(f, mis);
     qemu_event_set(&mis->main_thread_load_event);
 
diff --git a/net/dump.c b/net/dump.c
index 956e34a12..b8c6253d9 100644
--- a/net/dump.c
+++ b/net/dump.c
@@ -61,6 +61,10 @@ struct pcap_sf_pkthdr {
     uint32_t len;
 };
 
+/*
+ * called by:
+ *   - net/dump.c|162| <<filter_dump_receive_iov>> dump_receive_iov(&nfds->ds, iov, iovcnt, qemu_get_vnet_hdr_len(nf->netdev));
+ */
 static ssize_t dump_receive_iov(DumpState *s, const struct iovec *iov, int cnt,
                                 int offset)
 {
@@ -149,6 +153,10 @@ struct NetFilterDumpState {
     uint32_t maxlen;
 };
 
+/*
+ * 在以下使用filter_dump_receive_iov():
+ *   - net/dump.c|247| <<filter_dump_class_init>> nfc->receive_iov = filter_dump_receive_iov;
+ */
 static ssize_t filter_dump_receive_iov(NetFilterState *nf, NetClientState *sndr,
                                        unsigned flags, const struct iovec *iov,
                                        int iovcnt, NetPacketSent *sent_cb)
@@ -244,6 +252,16 @@ static void filter_dump_class_init(ObjectClass *oc, void *data)
 
     nfc->setup = filter_dump_setup;
     nfc->cleanup = filter_dump_cleanup;
+    /*
+     * 在以下使用NetFilterClass->receive_iov:
+     *   - net/dump.c|251| <<filter_dump_class_init>> nfc->receive_iov = filter_dump_receive_iov;
+     *   - net/filter-buffer.c|185| <<filter_buffer_class_init>> nfc->receive_iov = filter_buffer_receive_iov;
+     *   - net/filter-mirror.c|425| <<filter_mirror_class_init>> nfc->receive_iov = filter_mirror_receive_iov;
+     *   - net/filter-mirror.c|442| <<filter_redirector_class_init>> nfc->receive_iov = filter_redirector_receive_iov;
+     *   - net/filter-replay.c|72| <<filter_replay_class_init>> nfc->receive_iov = filter_replay_receive_iov;
+     *   - net/filter-rewriter.c|424| <<colo_rewriter_class_init>> nfc->receive_iov = colo_rewriter_receive_iov;
+     *   - net/filter.c|41| <<qemu_netfilter_receive>> return NETFILTER_GET_CLASS(OBJECT(nf))->receive_iov(
+     */
     nfc->receive_iov = filter_dump_receive_iov;
 }
 
diff --git a/net/filter.c b/net/filter.c
index 333590877..33995aafe 100644
--- a/net/filter.c
+++ b/net/filter.c
@@ -25,6 +25,12 @@ static inline bool qemu_can_skip_netfilter(NetFilterState *nf)
     return !nf->on;
 }
 
+/*
+ * called by:
+ *   - net/filter.c|99| <<qemu_netfilter_pass_to_next>> ret = qemu_netfilter_receive(next, direction, sender, flags, iov,
+ *   - net/net.c|631| <<filter_receive_iov>> ret = qemu_netfilter_receive(nf, direction, sender, flags, iov,
+ *   - net/net.c|639| <<filter_receive_iov>> ret = qemu_netfilter_receive(nf, direction, sender, flags, iov,
+ */
 ssize_t qemu_netfilter_receive(NetFilterState *nf,
                                NetFilterDirection direction,
                                NetClientState *sender,
@@ -38,6 +44,16 @@ ssize_t qemu_netfilter_receive(NetFilterState *nf,
     }
     if (nf->direction == direction ||
         nf->direction == NET_FILTER_DIRECTION_ALL) {
+        /*
+	 * 在以下使用NetFilterClass->receive_iov:
+	 *   - net/dump.c|251| <<filter_dump_class_init>> nfc->receive_iov = filter_dump_receive_iov;
+	 *   - net/filter-buffer.c|185| <<filter_buffer_class_init>> nfc->receive_iov = filter_buffer_receive_iov;
+	 *   - net/filter-mirror.c|425| <<filter_mirror_class_init>> nfc->receive_iov = filter_mirror_receive_iov;
+	 *   - net/filter-mirror.c|442| <<filter_redirector_class_init>> nfc->receive_iov = filter_redirector_receive_iov;
+	 *   - net/filter-replay.c|72| <<filter_replay_class_init>> nfc->receive_iov = filter_replay_receive_iov;
+	 *   - net/filter-rewriter.c|424| <<colo_rewriter_class_init>> nfc->receive_iov = colo_rewriter_receive_iov;
+	 *   - net/filter.c|41| <<qemu_netfilter_receive>> return NETFILTER_GET_CLASS(OBJECT(nf))->receive_iov(
+	 */
         return NETFILTER_GET_CLASS(OBJECT(nf))->receive_iov(
                                    nf, sender, flags, iov, iovcnt, sent_cb);
     }
diff --git a/net/net.c b/net/net.c
index 7ef688587..84a715859 100644
--- a/net/net.c
+++ b/net/net.c
@@ -615,6 +615,12 @@ int qemu_can_send_packet(NetClientState *sender)
     return qemu_can_receive_packet(sender->peer);
 }
 
+/*
+ * called by:
+ *   - net/net.c|663| <<filter_receive>> return filter_receive_iov(nc, direction, sender, flags, &iov, 1, sent_cb);
+ *   - net/net.c|861| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender, NET_FILTER_DIRECTION_TX, sender,
+ *   - net/net.c|867| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender->peer, NET_FILTER_DIRECTION_RX, sender,
+ */
 static ssize_t filter_receive_iov(NetClientState *nc,
                                   NetFilterDirection direction,
                                   NetClientState *sender,
@@ -647,6 +653,11 @@ static ssize_t filter_receive_iov(NetClientState *nc,
     return ret;
 }
 
+/*
+ * called by:
+ *   - net/net.c|718| <<qemu_send_packet_async_with_flags>> ret = filter_receive(sender, NET_FILTER_DIRECTION_TX,
+ *   - net/net.c|724| <<qemu_send_packet_async_with_flags>> ret = filter_receive(sender->peer, NET_FILTER_DIRECTION_RX,
+ */
 static ssize_t filter_receive(NetClientState *nc,
                               NetFilterDirection direction,
                               NetClientState *sender,
@@ -660,6 +671,12 @@ static ssize_t filter_receive(NetClientState *nc,
         .iov_len = size
     };
 
+    /*
+     * called by:
+     *   - net/net.c|663| <<filter_receive>> return filter_receive_iov(nc, direction, sender, flags, &iov, 1, sent_cb);
+     *   - net/net.c|861| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender, NET_FILTER_DIRECTION_TX, sender,
+     *   - net/net.c|867| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender->peer, NET_FILTER_DIRECTION_RX, sender,
+     */
     return filter_receive_iov(nc, direction, sender, flags, &iov, 1, sent_cb);
 }
 
@@ -841,6 +858,13 @@ static ssize_t qemu_deliver_packet_iov(NetClientState *sender,
     return ret;
 }
 
+/*
+ * called by:
+ *   - hw/net/virtio-net.c|2806| <<virtio_net_flush_tx>> ret = qemu_sendv_packet_async(qemu_get_subqueue(n->nic, queue_index),
+ *   - net/af-xdp.c|240| <<af_xdp_send>> if (!qemu_sendv_packet_async(&s->nc, &iov, 1,
+ *   - net/net.c|883| <<qemu_sendv_packet>> return qemu_sendv_packet_async(nc, iov, iovcnt, NULL);
+ *   - net/netmap.c|283| <<netmap_send>> iovsize = qemu_sendv_packet_async(&s->nc, s->iov, iovcnt,
+ */
 ssize_t qemu_sendv_packet_async(NetClientState *sender,
                                 const struct iovec *iov, int iovcnt,
                                 NetPacketSent *sent_cb)
@@ -857,6 +881,12 @@ ssize_t qemu_sendv_packet_async(NetClientState *sender,
         return size;
     }
 
+    /*
+     * called by:
+     *   - net/net.c|663| <<filter_receive>> return filter_receive_iov(nc, direction, sender, flags, &iov, 1, sent_cb);
+     *   - net/net.c|861| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender, NET_FILTER_DIRECTION_TX, sender,
+     *   - net/net.c|867| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender->peer, NET_FILTER_DIRECTION_RX, sender,
+     */
     /* Let filters handle the packet first */
     ret = filter_receive_iov(sender, NET_FILTER_DIRECTION_TX, sender,
                              QEMU_NET_PACKET_FLAG_NONE, iov, iovcnt, sent_cb);
@@ -864,6 +894,12 @@ ssize_t qemu_sendv_packet_async(NetClientState *sender,
         return ret;
     }
 
+    /*
+     * called by:
+     *   - net/net.c|663| <<filter_receive>> return filter_receive_iov(nc, direction, sender, flags, &iov, 1, sent_cb);
+     *   - net/net.c|861| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender, NET_FILTER_DIRECTION_TX, sender,
+     *   - net/net.c|867| <<qemu_sendv_packet_async>> ret = filter_receive_iov(sender->peer, NET_FILTER_DIRECTION_RX, sender,
+     */
     ret = filter_receive_iov(sender->peer, NET_FILTER_DIRECTION_RX, sender,
                              QEMU_NET_PACKET_FLAG_NONE, iov, iovcnt, sent_cb);
     if (ret) {
diff --git a/system/cpus.c b/system/cpus.c
index 1c818ff68..2daacdaf0 100644
--- a/system/cpus.c
+++ b/system/cpus.c
@@ -80,6 +80,19 @@ bool cpu_is_stopped(CPUState *cpu)
 
 bool cpu_work_list_empty(CPUState *cpu)
 {
+    /*
+     * 在以下使用CPUState->work_list:
+     *   - cpu-common.c|137| <<queue_work_on_cpu>> QSIMPLEQ_INSERT_TAIL(&cpu->work_list, wi, node);
+     *   - cpu-common.c|340| <<free_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|341| <<free_queued_cpu_work>> struct qemu_work_item *wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|342| <<free_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - cpu-common.c|354| <<process_queued_cpu_work>> if (QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|358| <<process_queued_cpu_work>> while (!QSIMPLEQ_EMPTY(&cpu->work_list)) {
+     *   - cpu-common.c|359| <<process_queued_cpu_work>> wi = QSIMPLEQ_FIRST(&cpu->work_list);
+     *   - cpu-common.c|360| <<process_queued_cpu_work>> QSIMPLEQ_REMOVE_HEAD(&cpu->work_list, node);
+     *   - hw/core/cpu-common.c|257| <<cpu_common_initfn>> QSIMPLEQ_INIT(&cpu->work_list);
+     *   - system/cpus.c|83| <<cpu_work_list_empty>> return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
+     */
     return QSIMPLEQ_EMPTY_ATOMIC(&cpu->work_list);
 }
 
@@ -182,6 +195,9 @@ void cpu_synchronize_post_reset(CPUState *cpu)
 
 void cpu_synchronize_post_init(CPUState *cpu)
 {
+    /*
+     * kvm_cpu_synchronize_post_init()
+     */
     if (cpus_accel->synchronize_post_init) {
         cpus_accel->synchronize_post_init(cpu);
     }
@@ -441,6 +457,14 @@ static void qemu_cpu_stop(CPUState *cpu, bool exit)
     qemu_cond_broadcast(&qemu_pause_cond);
 }
 
+/*
+ * called by:
+ *   - accel/tcg/tcg-accel-ops-rr.c|120| <<rr_wait_io_event>> qemu_wait_io_event_common(cpu);
+ *   - accel/tcg/tcg-accel-ops-rr.c|206| <<rr_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+ *   - system/cpus.c|471| <<qemu_wait_io_event>> qemu_wait_io_event_common(cpu);
+ *   - target/i386/nvmm/nvmm-accel-ops.c|53| <<qemu_nvmm_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+ *   - target/i386/whpx/whpx-accel-ops.c|53| <<whpx_cpu_thread_fn>> qemu_wait_io_event_common(cpu);
+ */
 void qemu_wait_io_event_common(CPUState *cpu)
 {
     qatomic_set_mb(&cpu->thread_kicked, false);
@@ -450,6 +474,13 @@ void qemu_wait_io_event_common(CPUState *cpu)
     process_queued_cpu_work(cpu);
 }
 
+/*
+ * called by:
+ *   - accel/dummy-cpus.c|59| <<dummy_cpu_thread_fn>> qemu_wait_io_event(cpu);
+ *   - accel/hvf/hvf-accel-ops.c|452| <<hvf_cpu_thread_fn>> qemu_wait_io_event(cpu);
+ *   - accel/kvm/kvm-accel-ops.c|55| <<kvm_vcpu_thread_fn>> qemu_wait_io_event(cpu);
+ *   - accel/tcg/tcg-accel-ops-mttcg.c|118| <<mttcg_cpu_thread_fn>> qemu_wait_io_event(cpu);
+ */
 void qemu_wait_io_event(CPUState *cpu)
 {
     bool slept = false;
diff --git a/system/dma-helpers.c b/system/dma-helpers.c
index cbcd89dfa..dc8f29eed 100644
--- a/system/dma-helpers.c
+++ b/system/dma-helpers.c
@@ -211,6 +211,23 @@ static const AIOCBInfo dma_aiocb_info = {
     .cancel_async       = dma_aio_cancel,
 };
 
+/*
+ * called by:
+ *   - hw/ide/core.c|971| <<ide_dma_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk),
+ *     &s->sg, offset, BDRV_SECTOR_SIZE, ide_issue_trim, s, ide_dma_cb, s, DMA_DIRECTION_TO_DEVICE);
+ *   - hw/ide/macio.c|190| <<pmac_ide_transfer_cb>> s->bus->dma->aiocb = dma_blk_io(blk_get_aio_context(s->blk),
+ *     &s->sg, offset, 0x1, ide_issue_trim, s, pmac_ide_transfer_cb, io, DMA_DIRECTION_TO_DEVICE);
+ *   - hw/scsi/scsi-disk.c|491| <<scsi_do_read>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *     r->req.sg, r->sector << BDRV_SECTOR_BITS, BDRV_SECTOR_SIZE, sdc->dma_readv, r, scsi_dma_complete, r,
+ *     DMA_DIRECTION_FROM_DEVICE);
+ *   - hw/scsi/scsi-disk.c|654| <<scsi_write_data>> r->req.aiocb = dma_blk_io(blk_get_aio_context(s->qdev.conf.blk),
+ *     r->req.sg, r->sector << BDRV_SECTOR_BITS, BDRV_SECTOR_SIZE, sdc->dma_writev, r, scsi_dma_complete, r,
+ *     DMA_DIRECTION_TO_DEVICE);
+ *   - system/dma-helpers.c|254| <<dma_blk_io>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ *     dma_blk_read_io_func, blk, cb, opaque, DMA_DIRECTION_FROM_DEVICE);
+ *   - system/dma-helpers.c|272| <<dma_blk_io>> return dma_blk_io(blk_get_aio_context(blk), sg, offset, align,
+ *     dma_blk_write_io_func, blk, cb, opaque, DMA_DIRECTION_TO_DEVICE);
+ */
 BlockAIOCB *dma_blk_io(AioContext *ctx,
     QEMUSGList *sg, uint64_t offset, uint32_t align,
     DMAIOFunc *io_func, void *io_func_opaque,
diff --git a/system/memory.c b/system/memory.c
index 85f6834cb..2adeece51 100644
--- a/system/memory.c
+++ b/system/memory.c
@@ -1588,6 +1588,14 @@ bool memory_region_init_ram_nomigrate(MemoryRegion *mr,
                                                   size, 0, errp);
 }
 
+/*
+ * clled by:
+ *   - backends/hostmem-ram.c|34| <<ram_backend_memory_alloc>> return memory_region_init_ram_flags_nomigrate(&backend->mr, OBJECT(backend),
+ *   - hw/m68k/next-cube.c|1013| <<next_cube_init>> memory_region_init_ram_flags_nomigrate(&m->bmapm1, NULL, "next.bmapmem",
+ *   - system/memory.c|1587| <<memory_region_init_ram_nomigrate>> return memory_region_init_ram_flags_nomigrate(mr, owner, name,
+ *   - system/memory.c|1748| <<memory_region_init_rom_nomigrate>> if (!memory_region_init_ram_flags_nomigrate(mr, owner, name,
+ *   - system/memory.c|3718| <<memory_region_init_ram_guest_memfd>> if (!memory_region_init_ram_flags_nomigrate(mr, owner, name, size,
+ */
 bool memory_region_init_ram_flags_nomigrate(MemoryRegion *mr,
                                             Object *owner,
                                             const char *name,
@@ -1884,6 +1892,19 @@ bool memory_region_is_protected(MemoryRegion *mr)
 
 bool memory_region_has_guest_memfd(MemoryRegion *mr)
 {
+    /*
+     * 在以下使用RAMBlock->guest_memfd:
+     *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+     *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+     *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+     *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+     *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+     *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+     *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+     *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+     *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+     *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+     */
     return mr->ram_block && mr->ram_block->guest_memfd >= 0;
 }
 
diff --git a/system/physmem.c b/system/physmem.c
index dc1db3a38..91b899706 100644
--- a/system/physmem.c
+++ b/system/physmem.c
@@ -84,6 +84,43 @@
 
 //#define DEBUG_SUBPAGE
 
+/*
+ * 在以下使用ram_list:
+ *   - system/physmem.c|949| <<global>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - hw/core/numa.c|846| <<ram_block_notifier_add>> QLIST_INSERT_HEAD(&ram_list.ramblock_notifiers, n, next);
+ *   - hw/core/numa.c|868| <<ram_block_notify_add>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - hw/core/numa.c|880| <<ram_block_notify_remove>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - hw/core/numa.c|892| <<ram_block_notify_resize>> QLIST_FOREACH_SAFE(notifier, &ram_list.ramblock_notifiers, next, next) {
+ *   - include/exec/ram_addr.h|169| <<cpu_physical_memory_get_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|210| <<cpu_physical_memory_all_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|283| <<cpu_physical_memory_set_dirty_flag>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - include/exec/ram_addr.h|306| <<cpu_physical_memory_set_dirty_range>> blocks[i] = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+ *   - include/exec/ram_addr.h|376| <<cpu_physical_memory_set_dirty_lebitmap>> qatomic_rcu_read(&ram_list.dirty_memory[i])->blocks;
+ *   - include/exec/ram_addr.h|500| <<cpu_physical_memory_sync_dirty_bitmap>> &ram_list.dirty_memory[DIRTY_MEMORY_MIGRATION])->blocks;
+ *   - include/exec/ramlist.h|61| <<INTERNAL_RAMBLOCK_FOREACH>> QLIST_FOREACH_RCU(block, &ram_list.blocks, next)
+ *   - migration/ram.c|1342| <<find_dirty_block>> pss->block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - migration/ram.c|2256| <<ram_find_and_save_block>> rs->last_seen_block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - migration/ram.c|2405| <<ram_state_reset>> rs->last_version = ram_list.version;
+ *   - migration/ram.c|3145| <<ram_save_iterate>> if (ram_list.version != rs->last_version) {
+ *   - migration/ram.c|3863| <<colo_flush_ram_cache>> block = QLIST_FIRST_RCU(&ram_list.blocks);
+ *   - system/physmem.c|819| <<qemu_get_ram_block>> block = qatomic_rcu_read(&ram_list.mru_block);
+ *   - system/physmem.c|849| <<qemu_get_ram_block>> ram_list.mru_block = block;
+ *   - system/physmem.c|893| <<cpu_physical_memory_test_and_clear_dirty>> blocks = qatomic_rcu_read(&ram_list.dirty_memory[client]);
+ *   - system/physmem.c|1148| <<qemu_mutex_lock_ramlist>> qemu_mutex_lock(&ram_list.mutex);
+ *   - system/physmem.c|1153| <<qemu_mutex_unlock_ramlist>> qemu_mutex_unlock(&ram_list.mutex);
+ *   - system/physmem.c|1492| <<find_ram_offset>> if (QLIST_EMPTY_RCU(&ram_list.blocks)) {
+ *   - system/physmem.c|1792| <<dirty_memory_extend>> unsigned int old_num_blocks = ram_list.num_dirty_blocks;
+ *   - system/physmem.c|1807| <<dirty_memory_extend>> old_blocks = qatomic_rcu_read(&ram_list.dirty_memory[i]);
+ *   - system/physmem.c|1820| <<dirty_memory_extend>> qatomic_rcu_set(&ram_list.dirty_memory[i], new_blocks);
+ *   - system/physmem.c|1827| <<dirty_memory_extend>> ram_list.num_dirty_blocks = new_num_blocks;
+ *   - system/physmem.c|1907| <<ram_block_add>> QLIST_INSERT_HEAD_RCU(&ram_list.blocks, new_block, next);
+ *   - system/physmem.c|1909| <<ram_block_add>> ram_list.mru_block = NULL;
+ *   - system/physmem.c|1913| <<ram_block_add>> ram_list.version++;
+ *   - system/physmem.c|2161| <<qemu_ram_free>> ram_list.mru_block = NULL;
+ *   - system/physmem.c|2164| <<qemu_ram_free>> ram_list.version++;
+ *   - system/physmem.c|2313| <<qemu_ram_block_from_host>> block = qatomic_rcu_read(&ram_list.mru_block);
+ *   - system/physmem.c|3129| <<cpu_exec_init_all>> qemu_mutex_init(&ram_list.mutex);
+ */
 /* ram_list is read under rcu_read_lock()/rcu_read_unlock().  Writes
  * are protected by the ramlist lock.
  */
@@ -1827,6 +1864,51 @@ static void dirty_memory_extend(ram_addr_t new_ram_size)
     ram_list.num_dirty_blocks = new_num_blocks;
 }
 
+/*
+ * 如果只是普通的"-m xxxx"
+ *
+ * (gdb) bt
+ * #0  ram_block_add (new_block=0x555557712270, errp=0x7fffffffd688) at ../system/physmem.c:1831
+ * #1  0x0000555555e77d15 in qemu_ram_alloc_internal (size=4294967296, max_size=4294967296, resized=0x0, host=0x0, ram_flags=0, mr=0x5555575482e0, errp=0x7fffffffd720)
+ *     at ../system/physmem.c:2092
+ * #2  0x0000555555e77e41 in qemu_ram_alloc (size=4294967296, ram_flags=0, mr=0x5555575482e0, errp=0x7fffffffd720) at ../system/physmem.c:2112
+ * #3  0x0000555555e69caa in memory_region_init_ram_flags_nomigrate (mr=0x5555575482e0, owner=0x555557548270, name=0x555557407540 "pc.ram", size=4294967296, ram_flags=0,
+ *     errp=0x7fffffffd848) at ../system/memory.c:1603
+ * #4  0x0000555555bf0197 in ram_backend_memory_alloc (backend=0x555557548270, errp=0x7fffffffd848) at ../backends/hostmem-ram.c:34
+ * #5  0x0000555555bf0fee in host_memory_backend_memory_complete (uc=0x555557548270, errp=0x7fffffffd848) at ../backends/hostmem.c:345
+ * #6  0x0000555555efe460 in user_creatable_complete (uc=0x555557548270, errp=0x7fffffffd900) at ../qom/object_interfaces.c:28
+ * #7  0x00005555559475a1 in create_default_memdev (ms=0x5555576791f0, path=0x0, errp=0x7fffffffd900) at ../hw/core/machine.c:1046
+ * #8  0x0000555555949399 in machine_run_board_init (machine=0x5555576791f0, mem_path=0x0, errp=0x7fffffffd900) at ../hw/core/machine.c:1584
+ * #9  0x0000555555be76cb in qemu_init_board () at ../system/vl.c:2632
+ * #10 0x0000555555be7a2b in qmp_x_exit_preconfig (errp=0x555557350860 <error_fatal>) at ../system/vl.c:2718
+ * #11 0x0000555555bea569 in qemu_init (argc=18, argv=0x7fffffffdbf8) at ../system/vl.c:3753
+ * #12 0x000055555609a086 in main (argc=18, argv=0x7fffffffdbf8) at ../system/main.c:47
+ *
+ * (gdb) bt
+ * #0  ram_block_add (new_block=0x555557680e90, errp=0x7fffffffd558) at ../system/physmem.c:1831
+ * #1  0x0000555555e77d15 in qemu_ram_alloc_internal (size=131072, max_size=131072, resized=0x0, host=0x0, ram_flags=0, mr=0x555557565000, errp=0x7fffffffd5f0)
+ *     at ../system/physmem.c:2092
+ * #2  0x0000555555e77e41 in qemu_ram_alloc (size=131072, ram_flags=0, mr=0x555557565000, errp=0x7fffffffd5f0) at ../system/physmem.c:2112
+ * #3  0x0000555555e69caa in memory_region_init_ram_flags_nomigrate (mr=0x555557565000, owner=0x0, name=0x55555635dacf "pc.rom", size=131072, ram_flags=0,
+ *     errp=0x555557350860 <error_fatal>) at ../system/memory.c:1603
+ * #4  0x0000555555e69c0f in memory_region_init_ram_nomigrate (mr=0x555557565000, owner=0x0, name=0x55555635dacf "pc.rom", size=131072, errp=0x555557350860 <error_fatal>)
+ *     at ../system/memory.c:1587
+ * #5  0x0000555555e705f4 in memory_region_init_ram (mr=0x555557565000, owner=0x0, name=0x55555635dacf "pc.rom", size=131072, errp=0x555557350860 <error_fatal>)
+ *     at ../system/memory.c:3695
+ * #6  0x0000555555d1d904 in pc_memory_init (pcms=0x5555576791f0, system_memory=0x5555575feae0, rom_memory=0x555557407190, pci_hole64_size=2147483648) at ../hw/i386/pc.c:975
+ * #7  0x0000555555cf82d9 in pc_init1 (machine=0x5555576791f0, pci_type=0x5555563555ea "i440FX") at ../hw/i386/pc_piix.c:225
+ * #8  0x0000555555cf8c82 in pc_i440fx_init (machine=0x5555576791f0) at ../hw/i386/pc_piix.c:445
+ * #9  0x0000555555cf8e7a in pc_i440fx_machine_9_2_init (machine=0x5555576791f0) at ../hw/i386/pc_piix.c:484
+ * #10 0x0000555555949537 in machine_run_board_init (machine=0x5555576791f0, mem_path=0x0, errp=0x7fffffffd900) at ../hw/core/machine.c:1630
+ * #11 0x0000555555be76cb in qemu_init_board () at ../system/vl.c:2632
+ * #12 0x0000555555be7a2b in qmp_x_exit_preconfig (errp=0x555557350860 <error_fatal>) at ../system/vl.c:2718
+ * #13 0x0000555555bea569 in qemu_init (argc=18, argv=0x7fffffffdbf8) at ../system/vl.c:3753
+ * #14 0x000055555609a086 in main (argc=18, argv=0x7fffffffdbf8) at ../system/main.c:47
+ *
+ * called by:
+ *   - system/physmem.c|2001| <<qemu_ram_alloc_from_fd>> ram_block_add(new_block, &local_err);
+ *   - system/physmem.c|2092| <<qemu_ram_alloc_from_fd>> ram_block_add(new_block, &local_err);
+ */
 static void ram_block_add(RAMBlock *new_block, Error **errp)
 {
     const bool noreserve = qemu_ram_is_noreserve(new_block);
@@ -1879,6 +1961,19 @@ static void ram_block_add(RAMBlock *new_block, Error **errp)
             goto out_free;
         }
 
+        /*
+	 * 在以下使用RAMBlock->guest_memfd:
+         *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+         *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+         *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+         *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+         *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+         *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+         *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+         *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+         *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+         *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+	 */
         new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
                                                         0, errp);
         if (new_block->guest_memfd < 0) {
@@ -1990,6 +2085,19 @@ RAMBlock *qemu_ram_alloc_from_fd(ram_addr_t size, MemoryRegion *mr,
     new_block->used_length = size;
     new_block->max_length = size;
     new_block->flags = ram_flags;
+    /*
+     * 在以下使用RAMBlock->guest_memfd:
+     *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+     *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+     *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+     *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+     *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+     *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+     *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+     *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+     *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+     *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+     */
     new_block->guest_memfd = -1;
     new_block->host = file_ram_alloc(new_block, size, fd, !file_size, offset,
                                      errp);
@@ -2085,6 +2193,19 @@ RAMBlock *qemu_ram_alloc_internal(ram_addr_t size, ram_addr_t max_size,
     new_block->max_length = max_size;
     assert(max_size >= size);
     new_block->fd = -1;
+    /*
+     * 在以下使用RAMBlock->guest_memfd: 
+     *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+     *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+     *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+     *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+     *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+     *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+     *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+     *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+     *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+     *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+     */
     new_block->guest_memfd = -1;
     new_block->page_size = qemu_real_host_page_size();
     new_block->host = host;
@@ -2105,6 +2226,21 @@ RAMBlock *qemu_ram_alloc_from_ptr(ram_addr_t size, void *host,
                                    errp);
 }
 
+/*
+ * (gdb) bt
+ * #0  qemu_ram_alloc (size=4294967296, ram_flags=0, mr=0x5555575482e0, errp=0x7fffffffd720) at ../system/physmem.c:2111
+ * #1  0x0000555555e69caa in memory_region_init_ram_flags_nomigrate (mr=0x5555575482e0, owner=0x555557548270, name=0x555557407540 "pc.ram", size=4294967296, ram_flags=0,
+ *     errp=0x7fffffffd848) at ../system/memory.c:1603
+ * #2  0x0000555555bf0197 in ram_backend_memory_alloc (backend=0x555557548270, errp=0x7fffffffd848) at ../backends/hostmem-ram.c:34
+ * #3  0x0000555555bf0fee in host_memory_backend_memory_complete (uc=0x555557548270, errp=0x7fffffffd848) at ../backends/hostmem.c:345
+ * #4  0x0000555555efe460 in user_creatable_complete (uc=0x555557548270, errp=0x7fffffffd900) at ../qom/object_interfaces.c:28
+ * #5  0x00005555559475a1 in create_default_memdev (ms=0x5555576791f0, path=0x0, errp=0x7fffffffd900) at ../hw/core/machine.c:1046
+ * #6  0x0000555555949399 in machine_run_board_init (machine=0x5555576791f0, mem_path=0x0, errp=0x7fffffffd900) at ../hw/core/machine.c:1584
+ * #7  0x0000555555be76cb in qemu_init_board () at ../system/vl.c:2632
+ * #8  0x0000555555be7a2b in qmp_x_exit_preconfig (errp=0x555557350860 <error_fatal>) at ../system/vl.c:2718
+ * #9  0x0000555555bea569 in qemu_init (argc=18, argv=0x7fffffffdbf8) at ../system/vl.c:3753
+ * #10 0x000055555609a086 in main (argc=18, argv=0x7fffffffdbf8) at ../system/main.c:47
+ */
 RAMBlock *qemu_ram_alloc(ram_addr_t size, uint32_t ram_flags,
                          MemoryRegion *mr, Error **errp)
 {
@@ -2137,6 +2273,19 @@ static void reclaim_ramblock(RAMBlock *block)
         qemu_anon_ram_free(block->host, block->max_length);
     }
 
+    /*
+     * 在以下使用RAMBlock->guest_memfd:
+     *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+     *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+     *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+     *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+     *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+     *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+     *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+     *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+     *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+     *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+     */
     if (block->guest_memfd >= 0) {
         close(block->guest_memfd);
         ram_block_discard_require(false);
@@ -3626,6 +3775,22 @@ int qemu_ram_foreach_block(RAMBlockIterFunc func, void *opaque)
  * Returns: 0 on success, none-0 on failure
  *
  */
+/*
+ * called by:
+ *   - accel/kvm/kvm-all.c|3086| <<kvm_convert_memory>> ret = ram_block_discard_range(rb, offset, size);
+ *   - hw/hyperv/hv-balloon-our_range_memslots.c|153| <<hvb_our_range_memslots_free>> ram_block_discard_range(rb, 0, qemu_ram_get_used_length(rb));
+ *   - hw/hyperv/hv-balloon.c|866| <<hv_balloon_handle_remove_host_addr_node>> if (ram_block_discard_range(rb, rb_offset, discard_size *
+ *   - hw/virtio/virtio-balloon.c|96| <<balloon_inflate_page>> ram_block_discard_range(rb, rb_offset, rb_page_size);
+ *   - hw/virtio/virtio-balloon.c|135| <<balloon_inflate_page>> ram_block_discard_range(rb, rb_aligned_offset, rb_page_size);
+ *   - hw/virtio/virtio-balloon.c|391| <<virtio_balloon_handle_report>> ram_block_discard_range(rb, ram_offset, size);
+ *   - hw/virtio/virtio-mem.c|598| <<virtio_mem_set_block_state>> if (ram_block_discard_range(rb, offset, size)) {
+ *   - hw/virtio/virtio-mem.c|652| <<virtio_mem_set_block_state>> ram_block_discard_range(vmem->memdev->mr.ram_block, offset, size);
+ *   - hw/virtio/virtio-mem.c|748| <<virtio_mem_unplug_all>> if (ram_block_discard_range(rb, 0, qemu_ram_get_used_length(rb))) {
+ *   - hw/virtio/virtio-mem.c|1082| <<virtio_mem_device_realize>> ret = ram_block_discard_range(rb, 0, qemu_ram_get_used_length(rb));
+ *   - hw/virtio/virtio-mem.c|1162| <<virtio_mem_discard_range_cb>> return ram_block_discard_range(rb, offset, size) ? -EINVAL : 0;
+ *   - hw/virtio/virtio-mem.c|1300| <<virtio_mem_post_load_early>> if (ram_block_discard_range(rb, 0, qemu_ram_get_used_length(rb))) {
+ *   - migration/ram.c|2777| <<ram_discard_range>> return ram_block_discard_range(rb, start, length);
+ */
 int ram_block_discard_range(RAMBlock *rb, uint64_t start, size_t length)
 {
     int ret = -1;
@@ -3747,6 +3912,19 @@ int ram_block_discard_guest_memfd_range(RAMBlock *rb, uint64_t start,
     int ret = -1;
 
 #ifdef CONFIG_FALLOCATE_PUNCH_HOLE
+    /*
+     * 在以下使用RAMBlock->guest_memfd:
+     *   - system/physmem.c|2170| <<qemu_ram_alloc_internal>> new_block->guest_memfd = -1;
+     *   - accel/kvm/kvm-all.c|1534| <<kvm_set_phys_mem>> mem->guest_memfd = mr->ram_block->guest_memfd;
+     *   - system/memory.c|1895| <<memory_region_has_guest_memfd>> return mr->ram_block && mr->ram_block->guest_memfd >= 0;
+     *   - system/physmem.c|1954| <<ram_block_add>> assert(new_block->guest_memfd < 0);
+     *   - system/physmem.c|1964| <<ram_block_add>> new_block->guest_memfd = kvm_create_guest_memfd(new_block->max_length,
+     *   - system/physmem.c|1966| <<ram_block_add>> if (new_block->guest_memfd < 0) {
+     *   - system/physmem.c|2075| <<qemu_ram_alloc_from_fd>> new_block->guest_memfd = -1;
+     *   - system/physmem.c|2237| <<reclaim_ramblock>> if (block->guest_memfd >= 0) {
+     *   - system/physmem.c|2238| <<reclaim_ramblock>> close(block->guest_memfd);
+     *   - system/physmem.c|3847| <<ram_block_discard_guest_memfd_range>> ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
+     */
     ret = fallocate(rb->guest_memfd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
                     start, length);
 
diff --git a/target/arm/arm-qmp-cmds.c b/target/arm/arm-qmp-cmds.c
index 3cc8cc738..d457d55d7 100644
--- a/target/arm/arm-qmp-cmds.c
+++ b/target/arm/arm-qmp-cmds.c
@@ -41,11 +41,29 @@ static GICCapability *gic_cap_new(int version)
     return cap;
 }
 
+/*
+ * called by:
+ *   - target/arm/arm-qmp-cmds.c|89| <<qmp_query_gic_capabilities>> gic_cap_kvm_probe(v2, v3);
+ */
 static inline void gic_cap_kvm_probe(GICCapability *v2, GICCapability *v3)
 {
 #ifdef CONFIG_KVM
     int fdarray[3];
 
+    /*
+     * 在以下调用kvm_arm_create_scratch_host_vcpu():
+     *   - target/arm/arm-qmp-cmds.c|49| <<gic_cap_kvm_probe>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, NULL)) {
+     *   - target/arm/kvm.c|303| <<kvm_arm_get_host_cpu_features>> if (!kvm_arm_create_scratch_host_vcpu(cpus_to_try, fdarray, &init)) {
+     *   - target/arm/kvm.c|1838| <<kvm_arm_sve_get_vls>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, &init)) {
+     *
+     * 注释:
+     * Create a scratch vcpu in its own VM of the type preferred by the host
+     * kernel (as would be used for '-cpu host'), for purposes of probing it
+     * for capabilities.
+     *
+     * Returns: true on success (and fdarray and init are filled in),
+     * false on failure (and fdarray and init are not valid).
+     */
     if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, NULL)) {
         return;
     }
diff --git a/target/arm/cpu64.c b/target/arm/cpu64.c
index 458d1cee0..4abf06e95 100644
--- a/target/arm/cpu64.c
+++ b/target/arm/cpu64.c
@@ -36,6 +36,10 @@
 #include "cpu-features.h"
 #include "cpregs.h"
 
+/*
+ * called by:
+ *   - target/arm/cpu.c|1930| <<arm_cpu_finalize_features>> arm_cpu_sve_finalize(cpu, &local_err);
+ */
 void arm_cpu_sve_finalize(ARMCPU *cpu, Error **errp)
 {
     /*
diff --git a/target/arm/kvm.c b/target/arm/kvm.c
index 7b6812c0d..65423887e 100644
--- a/target/arm/kvm.c
+++ b/target/arm/kvm.c
@@ -100,6 +100,20 @@ static int kvm_arm_vcpu_finalize(ARMCPU *cpu, int feature)
     return kvm_vcpu_ioctl(CPU(cpu), KVM_ARM_VCPU_FINALIZE, &feature);
 }
 
+/*
+ * 在以下调用kvm_arm_create_scratch_host_vcpu():
+ *   - target/arm/arm-qmp-cmds.c|49| <<gic_cap_kvm_probe>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, NULL)) {
+ *   - target/arm/kvm.c|303| <<kvm_arm_get_host_cpu_features>> if (!kvm_arm_create_scratch_host_vcpu(cpus_to_try, fdarray, &init)) {
+ *   - target/arm/kvm.c|1838| <<kvm_arm_sve_get_vls>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, &init)) {
+ *
+ * 注释:
+ * Create a scratch vcpu in its own VM of the type preferred by the host
+ * kernel (as would be used for '-cpu host'), for purposes of probing it
+ * for capabilities.
+ *
+ * Returns: true on success (and fdarray and init are filled in),
+ * false on failure (and fdarray and init are not valid).
+ */
 bool kvm_arm_create_scratch_host_vcpu(const uint32_t *cpus_to_try,
                                       int *fdarray,
                                       struct kvm_vcpu_init *init)
@@ -300,6 +314,20 @@ static bool kvm_arm_get_host_cpu_features(ARMHostCPUFeatures *ahcf)
         features |= 1ULL << ARM_FEATURE_PMU;
     }
 
+    /*
+     * 在以下调用kvm_arm_create_scratch_host_vcpu():
+     *   - target/arm/arm-qmp-cmds.c|49| <<gic_cap_kvm_probe>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, NULL)) {
+     *   - target/arm/kvm.c|303| <<kvm_arm_get_host_cpu_features>> if (!kvm_arm_create_scratch_host_vcpu(cpus_to_try, fdarray, &init)) {
+     *   - target/arm/kvm.c|1838| <<kvm_arm_sve_get_vls>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, &init)) {
+     *
+     * 注释:
+     * Create a scratch vcpu in its own VM of the type preferred by the host
+     * kernel (as would be used for '-cpu host'), for purposes of probing it
+     * for capabilities.
+     *
+     * Returns: true on success (and fdarray and init are filled in),
+     * false on failure (and fdarray and init are not valid).
+     */
     if (!kvm_arm_create_scratch_host_vcpu(cpus_to_try, fdarray, &init)) {
         return false;
     }
@@ -1809,6 +1837,10 @@ bool kvm_arm_mte_supported(void)
 
 QEMU_BUILD_BUG_ON(KVM_ARM64_SVE_VQ_MIN != 1);
 
+/*
+ * called by:
+ *   - target/arm/cpu64.c|70| <<arm_cpu_sve_finalize>> cpu->sve_vq.supported = kvm_arm_sve_get_vls(cpu);
+ */
 uint32_t kvm_arm_sve_get_vls(ARMCPU *cpu)
 {
     /* Only call this function if kvm_arm_sve_supported() returns true. */
@@ -1835,6 +1867,20 @@ uint32_t kvm_arm_sve_get_vls(ARMCPU *cpu)
 
         probed = true;
 
+	/*
+	 * 在以下调用kvm_arm_create_scratch_host_vcpu():
+	 *   - target/arm/arm-qmp-cmds.c|49| <<gic_cap_kvm_probe>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, NULL)) {
+	 *   - target/arm/kvm.c|303| <<kvm_arm_get_host_cpu_features>> if (!kvm_arm_create_scratch_host_vcpu(cpus_to_try, fdarray, &init)) {
+	 *   - target/arm/kvm.c|1838| <<kvm_arm_sve_get_vls>> if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, &init)) {
+	 *
+	 * 注释:
+	 * Create a scratch vcpu in its own VM of the type preferred by the host
+	 * kernel (as would be used for '-cpu host'), for purposes of probing it
+	 * for capabilities.
+	 *
+	 * Returns: true on success (and fdarray and init are filled in),
+	 * false on failure (and fdarray and init are not valid).
+	 */
         if (!kvm_arm_create_scratch_host_vcpu(NULL, fdarray, &init)) {
             error_report("failed to create scratch VCPU with SVE enabled");
             abort();
diff --git a/target/i386/cpu.c b/target/i386/cpu.c
index 3725dbbc4..6f84294f1 100644
--- a/target/i386/cpu.c
+++ b/target/i386/cpu.c
@@ -6210,6 +6210,17 @@ uint64_t x86_cpu_get_supported_feature_word(X86CPU *cpu, FeatureWord w)
     return r;
 }
 
+/*
+ * called by:
+ *   - target/i386/cpu.c|6718| <<cpu_x86_cpuid>> x86_cpu_get_supported_cpuid(0xA, count, eax, ebx, ecx, edx);
+ *   - target/i386/cpu.c|6758| <<cpu_x86_cpuid>> x86_cpu_get_supported_cpuid(0x1C, 0, eax, ebx, ecx, edx);
+ *   - target/i386/cpu.c|6809| <<cpu_x86_cpuid>> x86_cpu_get_supported_cpuid(0xD, count, eax, ebx, ecx, edx);
+ *   - target/i386/cpu.c|6861| <<cpu_x86_cpuid>> x86_cpu_get_supported_cpuid(0x12, count, eax, ebx, ecx, edx);
+ *   - target/i386/cpu.c|7564| <<x86_cpu_expand_features>> x86_cpu_get_supported_cpuid(0x24, 0, &eax, &ebx, &ecx, &edx);
+ *   - target/i386/cpu.c|7710| <<x86_cpu_filter_features>> x86_cpu_get_supported_cpuid(0x14, 0, &eax_0, &ebx_0, &ecx_0, &edx_0);
+ *   - target/i386/cpu.c|7712| <<x86_cpu_filter_features>> x86_cpu_get_supported_cpuid(0x14, 1, &eax_1, &ebx_1, &ecx_1, &edx_1);
+ *   - target/i386/cpu.c|7737| <<x86_cpu_filter_features>> x86_cpu_get_supported_cpuid(0x24, 0, &eax_0, &ebx_0, &ecx_0, &edx_0);
+ */
 static void x86_cpu_get_supported_cpuid(uint32_t func, uint32_t index,
                                         uint32_t *eax, uint32_t *ebx,
                                         uint32_t *ecx, uint32_t *edx)
@@ -6489,6 +6500,34 @@ uint32_t cpu_x86_virtual_addr_width(CPUX86State *env)
     }
 }
 
+/*
+ * called by:
+ *   - hw/i386/fw_cfg.c|183| <<fw_cfg_build_feature_control>> cpu_x86_cpuid(env, 1, 0, &unused, &unused, &ecx, &edx);
+ *   - hw/i386/fw_cfg.c|195| <<fw_cfg_build_feature_control>> cpu_x86_cpuid(env, 0x7, 0, &unused, &ebx, &ecx, &unused);
+ *   - target/i386/hvf/hvf.c|411| <<hvf_cpu_x86_cpuid>> cpu_x86_cpuid(env, index, count, eax, ebx, ecx, edx);
+ *   - target/i386/kvm/kvm.c|1836| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, 0, 0, &limit, &unused, &unused, &unused);
+ *   - target/i386/kvm/kvm.c|1850| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, 0, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1864| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, 0, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1881| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, j, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1911| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, j, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1933| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, 0, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1944| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, j, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1951| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, 0, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|1966| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, 0x0a, 0, &eax, &unused, &unused, &edx);
+ *   - target/i386/kvm/kvm.c|1990| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, 0x80000000, 0, &limit, &unused, &unused, &unused);
+ *   - target/i386/kvm/kvm.c|2006| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, j, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|2020| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, 0, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/kvm/kvm.c|2034| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, 0xC0000000, 0, &limit, &unused, &unused, &unused);
+ *   - target/i386/kvm/kvm.c|2045| <<kvm_x86_build_cpuid>> cpu_x86_cpuid(env, i, 0, &c->eax, &c->ebx, &c->ecx, &c->edx);
+ *   - target/i386/tcg/fpu_helper.c|3191| <<helper_xsetbv>> cpu_x86_cpuid(env, 0x0d, 0, &ena_lo, &dummy, &dummy, &ena_hi);
+ *   - target/i386/tcg/misc_helper.c|56| <<helper_cpuid>> cpu_x86_cpuid(env, (uint32_t)env->regs[R_EAX], (uint32_t)env->regs[R_ECX],
+ *   - target/i386/whpx/whpx-all.c|1946| <<whpx_vcpu_run>> cpu_x86_cpuid(env, cpuid_fn, 0, (UINT32 *)&rax, (UINT32 *)&rbx,
+ *
+ * typedef uint64_t FeatureWordArray[FEATURE_WORDS];
+ *
+ * CPUX86State *env:
+ * -> FeatureWordArray features;
+ */
 void cpu_x86_cpuid(CPUX86State *env, uint32_t index, uint32_t count,
                    uint32_t *eax, uint32_t *ebx,
                    uint32_t *ecx, uint32_t *edx)
@@ -8062,6 +8101,14 @@ static void x86_cpu_get_bit_prop(Object *obj, Visitor *v, const char *name,
     visit_type_bool(v, name, &value, errp);
 }
 
+/*
+ * 在以下使用x86_cpu_set_bit_prop():
+ *   -  target/i386/cpu.c|8156| <<x86_cpu_register_bit_prop>> object_class_property_add(oc,
+ *                                     prop_name, "bool",
+ *                                     x86_cpu_get_bit_prop,
+ *                                     x86_cpu_set_bit_prop,
+ *                                     NULL, fp);
+ */
 static void x86_cpu_set_bit_prop(Object *obj, Visitor *v, const char *name,
                                  void *opaque, Error **errp)
 {
diff --git a/target/i386/cpu.h b/target/i386/cpu.h
index 4c239a697..67510e4b7 100644
--- a/target/i386/cpu.h
+++ b/target/i386/cpu.h
@@ -1863,6 +1863,12 @@ typedef struct CPUArchState {
     struct {} end_init_save;
 
     uint64_t system_time_msr;
+    /*
+     * 在以下使用CPUX86State->wall_clock_msr:
+     *   - target/i386/machine.c|1740| <<global>> VMSTATE_UINT64(env.wall_clock_msr, X86CPU),
+     *   - target/i386/kvm/kvm.c|3965| <<kvm_put_msrs>> kvm_msr_entry_add(cpu, MSR_KVM_WALL_CLOCK, env->wall_clock_msr);
+     *   - target/i386/kvm/kvm.c|4708| <<kvm_get_msrs>> case MSR_KVM_SYSTEM_TIME: env->wall_clock_msr = msrs[i].data;
+     */
     uint64_t wall_clock_msr;
     uint64_t steal_time_msr;
     uint64_t async_pf_en_msr;
diff --git a/target/i386/kvm/kvm.c b/target/i386/kvm/kvm.c
index 8e17942c3..1cc7d27b5 100644
--- a/target/i386/kvm/kvm.c
+++ b/target/i386/kvm/kvm.c
@@ -363,6 +363,10 @@ static struct kvm_cpuid2 *try_get_cpuid(KVMState *s, int max)
 /* Run KVM_GET_SUPPORTED_CPUID ioctl(), allocating a buffer large enough
  * for all entries.
  */
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|442| <<kvm_arch_get_supported_cpuid>> cpuid = get_supported_cpuid(s);
+ */
 static struct kvm_cpuid2 *get_supported_cpuid(KVMState *s)
 {
     struct kvm_cpuid2 *cpuid;
@@ -431,6 +435,31 @@ static struct kvm_cpuid_entry2 *cpuid_find_entry(struct kvm_cpuid2 *cpuid,
     return NULL;
 }
 
+/*
+ * called by:
+ *   - target/i386/cpu.c|6144| <<x86_cpu_get_supported_feature_word>> r = kvm_arch_get_supported_cpuid(kvm_state, wi->cpuid.eax,
+ *                                wi->cpuid.ecx, wi->cpuid.reg);
+ *   - target/i386/cpu.c|6218| <<x86_cpu_get_supported_cpuid>> *eax = kvm_arch_get_supported_cpuid(kvm_state, func, index, R_EAX);
+ *   - target/i386/cpu.c|6219| <<x86_cpu_get_supported_cpuid>> *ebx = kvm_arch_get_supported_cpuid(kvm_state, func, index, R_EBX);
+ *   - target/i386/cpu.c|6220| <<x86_cpu_get_supported_cpuid>> *ecx = kvm_arch_get_supported_cpuid(kvm_state, func, index, R_ECX);
+ *   - target/i386/cpu.c|6221| <<x86_cpu_get_supported_cpuid>> *edx = kvm_arch_get_supported_cpuid(kvm_state, func, index, R_EDX);
+ *   - target/i386/cpu.c|7139| <<cpu_x86_cpuid>> *ebx |= kvm_arch_get_supported_cpuid(cs->kvm_state, index, count, R_EBX) & 0xf;
+ *   - target/i386/kvm/kvm-cpu.c|25| <<kvm_set_guest_phys_bits>> eax = kvm_arch_get_supported_cpuid(cs->kvm_state, 0x80000008, 0, R_EAX);
+ *   - target/i386/kvm/kvm-cpu.c|118| <<kvm_cpu_max_instance_init>> kvm_arch_get_supported_cpuid(s, 0x0, 0, R_EAX);
+ *   - target/i386/kvm/kvm-cpu.c|120| <<kvm_cpu_max_instance_init>> kvm_arch_get_supported_cpuid(s, 0x80000000, 0, R_EAX);
+ *   - target/i386/kvm/kvm-cpu.c|122| <<kvm_cpu_max_instance_init>> kvm_arch_get_supported_cpuid(s, 0xC0000000, 0, R_EAX);
+ *   - target/i386/kvm/kvm.c|558| <<kvm_arch_get_supported_cpuid>> cpuid_1_edx = kvm_arch_get_supported_cpuid(s, 1, 0, R_EDX);
+ *   - target/i386/kvm/kvm.c|622| <<kvm_arch_get_supported_msr_feature>> if (kvm_arch_get_supported_cpuid(s, 0xD, 1, R_ECX) &
+ *   - target/i386/kvm/kvm.c|626| <<kvm_arch_get_supported_msr_feature>> if (kvm_arch_get_supported_cpuid(s, 1, 0, R_ECX) &
+ *   - target/i386/kvm/kvm.c|630| <<kvm_arch_get_supported_msr_feature>> if (kvm_arch_get_supported_cpuid(s, 7, 0, R_EBX) &
+ *   - target/i386/kvm/kvm.c|634| <<kvm_arch_get_supported_msr_feature>> if (kvm_arch_get_supported_cpuid(s, 7, 0, R_EBX) &
+ *   - target/i386/kvm/kvm.c|638| <<kvm_arch_get_supported_msr_feature>> if (kvm_arch_get_supported_cpuid(s, 0x80000001, 0, R_EDX) &
+ *   - target/i386/kvm/kvm.c|1800| <<kvm_init_xsave>> assert(kvm_arch_get_supported_cpuid(kvm_state, 0xd, 0, R_ECX) <=
+ *   - target/i386/kvm/kvm.c|4131| <<kvm_put_msrs>> int addr_num = kvm_arch_get_supported_cpuid(kvm_state, 0x14, 1, R_EAX) & 0x7;
+ *   - target/i386/kvm/kvm.c|4562| <<kvm_get_msrs>> kvm_arch_get_supported_cpuid(kvm_state, 0x14, 1, R_EAX) & 0x7;
+ *   - target/i386/kvm/kvm.c|6398| <<kvm_request_xsave_components>> supported = kvm_arch_get_supported_cpuid(s, 0xd, 0, R_EAX);
+ *   - target/i386/kvm/kvm.c|6399| <<kvm_request_xsave_components>> supported |= (uint64_t)kvm_arch_get_supported_cpuid(s, 0xd, 0, R_EDX) << 32;
+ */
 uint32_t kvm_arch_get_supported_cpuid(KVMState *s, uint32_t function,
                                       uint32_t index, int reg)
 {
@@ -439,6 +468,9 @@ uint32_t kvm_arch_get_supported_cpuid(KVMState *s, uint32_t function,
     uint32_t cpuid_1_edx, unused;
     uint64_t bitmask;
 
+    /*
+     * 只在此处调用
+     */
     cpuid = get_supported_cpuid(s);
 
     struct kvm_cpuid_entry2 *entry = cpuid_find_entry(cpuid, function, index);
@@ -1825,6 +1857,10 @@ static void kvm_init_nested_state(CPUX86State *env)
     }
 }
 
+/*
+ * called by:
+ *   - target/i386/kvm/kvm.c|2268| <<kvm_arch_init_vcpu>> cpuid_i = kvm_x86_build_cpuid(env, cpuid_data.entries, cpuid_i);
+ */
 static uint32_t kvm_x86_build_cpuid(CPUX86State *env,
                                     struct kvm_cpuid_entry2 *entries,
                                     uint32_t cpuid_i)
@@ -2068,6 +2104,13 @@ int kvm_arch_init_vcpu(CPUState *cs)
                       sizeof(struct kvm_cpuid2) +
                       sizeof(struct kvm_cpuid_entry2) * KVM_MAX_CPUID_ENTRIES);
 
+    /*
+     * struct ArchCPU {
+     *     CPUState parent_obj;
+     *
+     *     CPUX86State env;
+     *     VMChangeStateEntry *vmsentry;
+     */
     X86CPU *cpu = X86_CPU(cs);
     CPUX86State *env = &cpu->env;
     uint32_t cpuid_i;
@@ -3959,9 +4002,18 @@ static int kvm_put_msrs(X86CPU *cpu, int level)
      * The following MSRs have side effects on the guest or are too heavy
      * for normal writeback. Limit them to reset or full state updates.
      */
+    /*
+     * 迁移的时候是KVM_PUT_FULL_STATE
+     */
     if (level >= KVM_PUT_RESET_STATE) {
         kvm_msr_entry_add(cpu, MSR_IA32_TSC, env->tsc);
         kvm_msr_entry_add(cpu, MSR_KVM_SYSTEM_TIME, env->system_time_msr);
+        /*
+	 * 在以下使用CPUX86State->wall_clock_msr:
+	 *   - target/i386/machine.c|1740| <<global>> VMSTATE_UINT64(env.wall_clock_msr, X86CPU),
+	 *   - target/i386/kvm/kvm.c|3965| <<kvm_put_msrs>> kvm_msr_entry_add(cpu, MSR_KVM_WALL_CLOCK, env->wall_clock_msr);
+	 *   - target/i386/kvm/kvm.c|4708| <<kvm_get_msrs>> case MSR_KVM_SYSTEM_TIME: env->wall_clock_msr = msrs[i].data;
+	 */
         kvm_msr_entry_add(cpu, MSR_KVM_WALL_CLOCK, env->wall_clock_msr);
         if (env->features[FEAT_KVM] & (1 << KVM_FEATURE_ASYNC_PF_INT)) {
             kvm_msr_entry_add(cpu, MSR_KVM_ASYNC_PF_INT, env->async_pf_int_msr);
@@ -5232,6 +5284,18 @@ static int kvm_get_nested_state(X86CPU *cpu)
     return ret;
 }
 
+/*
+ * (gdb) bt
+ * #0  kvm_arch_put_registers (cpu=0x555557721960, level=3, errp=0x7fffedb02580) at ../target/i386/kvm/kvm.c:5237
+ * #1  0x0000555555edb672 in do_kvm_cpu_synchronize_post_init (cpu=0x555557721960, arg=...) at ../accel/kvm/kvm-all.c:2905
+ * #2  0x00005555558841ea in process_queued_cpu_work (cpu=0x555557721960) at ../cpu-common.c:375
+ * #3  0x0000555555bd2d4f in qemu_wait_io_event_common (cpu=0x555557721960) at ../system/cpus.c:456
+ * #4  0x0000555555bd2de8 in qemu_wait_io_event (cpu=0x555557721960) at ../system/cpus.c:474
+ * #5  0x0000555555edf7e1 in kvm_vcpu_thread_fn (arg=0x555557721960) at ../accel/kvm/kvm-accel-ops.c:55
+ * #6  0x000055555617e61f in qemu_thread_start (args=0x55555772bbe0) at ../util/qemu-thread-posix.c:541
+ * #7  0x00007ffff68a91da in start_thread () from /lib64/libpthread.so.0
+ * #8  0x00007ffff4bda8d3 in clone () from /lib64/libc.so.6
+ */
 int kvm_arch_put_registers(CPUState *cpu, int level, Error **errp)
 {
     X86CPU *x86_cpu = X86_CPU(cpu);
diff --git a/target/i386/machine.c b/target/i386/machine.c
index b4610325a..3030da012 100644
--- a/target/i386/machine.c
+++ b/target/i386/machine.c
@@ -641,6 +641,24 @@ static const VMStateDescription vmstate_msr_ia32_feature_control = {
     }
 };
 
+/*
+ * 668 static const VMStateDescription vmstate_msr_architectural_pmu = {
+ * 669     .name = "cpu/msr_architectural_pmu",
+ * 670     .version_id = 1,
+ * 671     .minimum_version_id = 1,
+ * 672     .needed = pmu_enable_needed,
+ * 673     .fields = (const VMStateField[]) {
+ * 674         VMSTATE_UINT64(env.msr_fixed_ctr_ctrl, X86CPU),
+ * 675         VMSTATE_UINT64(env.msr_global_ctrl, X86CPU),
+ * 676         VMSTATE_UINT64(env.msr_global_status, X86CPU),
+ * 677         VMSTATE_UINT64(env.msr_global_ovf_ctrl, X86CPU),
+ * 678         VMSTATE_UINT64_ARRAY(env.msr_fixed_counters, X86CPU, MAX_FIXED_COUNTERS),
+ * 679         VMSTATE_UINT64_ARRAY(env.msr_gp_counters, X86CPU, MAX_GP_COUNTERS),
+ * 680         VMSTATE_UINT64_ARRAY(env.msr_gp_evtsel, X86CPU, MAX_GP_COUNTERS),
+ * 681         VMSTATE_END_OF_LIST()
+ * 682     }
+ * 683 };
+ */
 static bool pmu_enable_needed(void *opaque)
 {
     X86CPU *cpu = opaque;
diff --git a/target/i386/tcg/misc_helper.c b/target/i386/tcg/misc_helper.c
index ed4cda800..ce611e652 100644
--- a/target/i386/tcg/misc_helper.c
+++ b/target/i386/tcg/misc_helper.c
@@ -47,6 +47,9 @@ void helper_into(CPUX86State *env, int next_eip_addend)
     }
 }
 
+/*
+ * 没人调用helper_cpuid()
+ */
 void helper_cpuid(CPUX86State *env)
 {
     uint32_t eax, ebx, ecx, edx;
diff --git a/util/iov.c b/util/iov.c
index 777711612..2df71d9c0 100644
--- a/util/iov.c
+++ b/util/iov.c
@@ -283,6 +283,63 @@ unsigned iov_copy(struct iovec *dst_iov, unsigned int dst_iov_cnt,
 
 /* io vectors */
 
+/*
+ * called by:
+ *   - block/blklogwrites.c|504| <<global>> qemu_iovec_init(&log_qiov, niov + 2);
+ *   - block/blklogwrites.c|442| <<blk_log_writes_co_do_log>> qemu_iovec_init(&qiov, 2);
+ *   - block/blkverify.c|242| <<blkverify_co_preadv>> qemu_iovec_init(&raw_qiov, qiov->niov);
+ *   - block/bochs.c|261| <<bochs_co_preadv>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - block/crypto.c|486| <<block_crypto_co_preadv>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/crypto.c|550| <<block_crypto_co_pwritev>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/io.c|1630| <<bdrv_create_padded_qiov>> qemu_iovec_init(&pad->local_qiov, MIN(padded_niov, IOV_MAX));
+ *   - block/io.c|1656| <<bdrv_create_padded_qiov>> qemu_iovec_init(&pad->pre_collapse_qiov, collapse_count);
+ *   - block/io_uring.c|96| <<luring_resubmit_short_read>> qemu_iovec_init(resubmit_qiov, luringcb->qiov->niov);
+ *   - block/mirror.c|385| <<mirror_co_read>> qemu_iovec_init(&op->qiov, nb_chunks);
+ *   - block/mirror.c|1597| <<bdrv_mirror_top_pwritev>> qemu_iovec_init(&bounce_qiov, 1);
+ *   - block/nbd.c|791| <<nbd_co_receive_offset_data_payload>> qemu_iovec_init(&sub_qiov, qiov->niov);
+ *   - block/nvme.c|1280| <<nvme_co_prw>> qemu_iovec_init(&local_qiov, 1);
+ *   - block/nvme.c|1447| <<nvme_co_pdiscard>> qemu_iovec_init(&local_qiov, 1);
+ *   - block/parallels.c|451| <<parallels_co_writev>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/parallels.c|494| <<parallels_co_readv>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/qcow2-cluster.c|934| <<perform_cow>> qemu_iovec_init(&qiov, 2 + (m->data_qiov ?
+ *   - block/qed.c|1403| <<qed_co_request>> qemu_iovec_init(&acb.cur_qiov, qiov->niov);
+ *   - block/quorum.c|618| <<read_quorum_children>> qemu_iovec_init(&acb->qcrs[i].qiov, acb->qiov->niov);
+ *   - block/raw-format.c|261| <<raw_co_pwritev>> qemu_iovec_init(&local_qiov, qiov->niov + 1);
+ *   - block/replication.c|275| <<replication_co_writev>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/vdi.c|565| <<vdi_co_preadv>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - block/vdi.c|625| <<vdi_co_pwritev>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - block/vhdx.c|1191| <<vhdx_co_readv>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/vhdx.c|1353| <<vhdx_co_writev>> qemu_iovec_init(&hd_qiov, qiov->niov);
+ *   - block/vmdk.c|1880| <<vmdk_write_extent>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - block/vmdk.c|1989| <<vmdk_co_preadv>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - block/vpc.c|631| <<vpc_co_preadv>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - block/vpc.c|681| <<vpc_co_pwritev>> qemu_iovec_init(&local_qiov, qiov->niov);
+ *   - hw/9pfs/9p.c|2231| <<v9fs_init_qiov_from_pdu>> qemu_iovec_init(qiov, niov);
+ *   - hw/9pfs/9p.c|2395| <<v9fs_read>> qemu_iovec_init(&qiov, qiov_full.niov);
+ *   - hw/9pfs/9p.c|2712| <<v9fs_write>> qemu_iovec_init(&qiov, qiov_full.niov);
+ *   - hw/block/dataplane/xen-block.c|110| <<xen_block_start_request>> qemu_iovec_init(&request->v, 1);
+ *   - hw/block/m25p80.c|578| <<flash_sync_page>> qemu_iovec_init(iov, 1);
+ *   - hw/block/m25p80.c|595| <<flash_sync_area>> qemu_iovec_init(iov, 1);
+ *   - hw/block/virtio-blk.c|237| <<submit_requests>> qemu_iovec_init(qiov, niov);
+ *   - hw/nvme/ctrl.c|721| <<nvme_sg_init>> qemu_iovec_init(&sg->iov, 0);
+ *   - hw/nvme/ctrl.c|2479| <<nvme_compare_data_cb>> qemu_iovec_init(&ctx->mdata.iov, 1);
+ *   - hw/nvme/ctrl.c|2718| <<nvme_verify>> qemu_iovec_init(&ctx->data.iov, 1);
+ *   - hw/nvme/ctrl.c|3398| <<nvme_copy>> qemu_iovec_init(&iocb->iov, 1);
+ *   - hw/nvme/ctrl.c|3469| <<nvme_compare>> qemu_iovec_init(&ctx->data.iov, 1);
+ *   - hw/nvme/dif.c|582| <<nvme_dif_rw>> qemu_iovec_init(&ctx->mdata.iov, 1);
+ *   - hw/nvme/dif.c|643| <<nvme_dif_rw>> qemu_iovec_init(&ctx->data.iov, 1);
+ *   - hw/nvme/dif.c|663| <<nvme_dif_rw>> qemu_iovec_init(&ctx->mdata.iov, 1);
+ *   - hw/scsi/virtio-scsi.c|94| <<virtio_scsi_init_req>> qemu_iovec_init(&req->resp_iov, 1);
+ *   - hw/usb/combined-packet.c|161| <<usb_ep_combine_input_packets>> qemu_iovec_init(&combined->iov, 2);
+ *   - hw/usb/core.c|531| <<usb_packet_init>> qemu_iovec_init(&p->iov, 1);
+ *   - hw/usb/xen-usb.c|233| <<usbback_init_packet>> qemu_iovec_init(&packet->iov, USBIF_MAX_SEGMENTS_PER_REQUEST);
+ *   - qemu-img.c|4730| <<img_bench>> qemu_iovec_init(&data.qiov[i], 1);
+ *   - qemu-io-cmds.c|531| <<create_iovec>> qemu_iovec_init(qiov, nr_iov);
+ *   - system/dma-helpers.c|235| <<dma_blk_io>> qemu_iovec_init(&dbs->iov, sg->nsg);
+ *   - tests/unit/test-replication.c|73| <<test_blk_read>> qemu_iovec_init(&qiov, 1);
+ *   - tests/unit/test-replication.c|110| <<test_blk_write>> qemu_iovec_init(&qiov, 1);
+ *   - util/iov.c|473| <<qemu_iovec_init_slice>> qemu_iovec_init(qiov, slice_niov);
+ */
 void qemu_iovec_init(QEMUIOVector *qiov, int alloc_hint)
 {
     qiov->iov = g_new(struct iovec, alloc_hint);
@@ -327,6 +384,16 @@ void qemu_iovec_add(QEMUIOVector *qiov, void *base, size_t len)
  * of src".
  * Only vector pointers are processed, not the actual data buffers.
  */
+/*
+ * called by:
+ *   - block/io.c|1657| <<bdrv_create_padded_qiov>> qemu_iovec_concat_iov(&pad->pre_collapse_qiov, iov,
+  3 block/io.c|1679| <<bdrv_create_padded_qiov>> qemu_iovec_concat_iov(&pad->local_qiov, iov, niov, iov_offset, bytes);
+  4 block/vhdx.c|1418| <<vhdx_co_writev>> qemu_iovec_concat_iov(&hd_qiov, &iov1, 1, 0,
+  5 block/vhdx.c|1434| <<vhdx_co_writev>> qemu_iovec_concat_iov(&hd_qiov, &iov2, 1, 0,
+  6 hw/scsi/virtio-scsi.c|202| <<virtio_scsi_parse_req>> if (qemu_iovec_concat_iov(&req->resp_iov,
+ *   - util/iov.c|426| <<qemu_iovec_concat>> qemu_iovec_concat_iov(dst, src->iov, src->niov, soffset, sbytes);
+  9 util/iov.c|531| <<qemu_iovec_init_slice>> qemu_iovec_concat_iov(qiov, slice_iov, slice_niov, slice_head, len);
+ */
 size_t qemu_iovec_concat_iov(QEMUIOVector *dst,
                              struct iovec *src_iov, unsigned int src_cnt,
                              size_t soffset, size_t sbytes)
diff --git a/util/main-loop.c b/util/main-loop.c
index a0386cfeb..0124be33c 100644
--- a/util/main-loop.c
+++ b/util/main-loop.c
@@ -128,6 +128,19 @@ static int qemu_signal_init(Error **errp)
 }
 #endif
 
+/*
+ * 在以下使用qemu_aio_context:
+ *   - util/main-loop.c|143| <<qemu_get_aio_context>> return qemu_aio_context;
+ *   - util/main-loop.c|148| <<qemu_notify_event>> if (!qemu_aio_context) {
+ *   - util/main-loop.c|168| <<qemu_init_main_loop>> qemu_aio_context = aio_context_new(errp);
+ *   - util/main-loop.c|169| <<qemu_init_main_loop>> if (!qemu_aio_context) {
+ *   - util/main-loop.c|172| <<qemu_init_main_loop>> qemu_set_current_aio_context(qemu_aio_context);
+ *   - util/main-loop.c|175| <<qemu_init_main_loop>> src = aio_get_g_source(qemu_aio_context);
+ *   - util/main-loop.c|190| <<main_loop_update_params>> if (!qemu_aio_context) {
+ *   - util/main-loop.c|195| <<main_loop_update_params>> aio_context_set_aio_params(qemu_aio_context, base->aio_max_batch);
+ *   - util/main-loop.c|197| <<main_loop_update_params>> aio_context_set_thread_pool_params(qemu_aio_context, base->thread_pool_min,
+ *   - util/main-loop.c|608| <<qemu_bh_new_full>> return aio_bh_new_full(qemu_aio_context, cb, opaque, name,
+ */
 static AioContext *qemu_aio_context;
 static QEMUBH *qemu_notify_bh;
 
diff --git a/util/qemu-coroutine-io.c b/util/qemu-coroutine-io.c
index 364f4d5ab..2649a2a7a 100644
--- a/util/qemu-coroutine-io.c
+++ b/util/qemu-coroutine-io.c
@@ -58,6 +58,11 @@ qemu_co_sendv_recvv(int sockfd, struct iovec *iov, unsigned iov_cnt,
     return done;
 }
 
+/*
+ * called by:
+ *   - include/qemu/coroutine.h|307| <<qemu_co_recv>> qemu_co_send_recv(sockfd, buf, bytes, false)
+ *   - include/qemu/coroutine.h|309| <<qemu_co_send>> qemu_co_send_recv(sockfd, buf, bytes, true)
+ */
 ssize_t coroutine_fn
 qemu_co_send_recv(int sockfd, void *buf, size_t bytes, bool do_send)
 {
diff --git a/util/qemu-coroutine-lock.c b/util/qemu-coroutine-lock.c
index 253443538..d76bbcc69 100644
--- a/util/qemu-coroutine-lock.c
+++ b/util/qemu-coroutine-lock.c
@@ -429,6 +429,13 @@ void coroutine_fn qemu_co_rwlock_downgrade(CoRwlock *lock)
     qemu_co_rwlock_maybe_wake_one(lock);
 }
 
+/*
+ * called by:
+ *   - hw/9pfs/9p.h|430| <<v9fs_path_write_lock>> qemu_co_rwlock_wrlock(&s->rename_lock);
+ *   - tests/unit/test-coroutine.c|297| <<rwlock_wrlock_yield>> qemu_co_rwlock_wrlock(&rwlock);
+ *   - tests/unit/test-coroutine.c|339| <<rwlock_wrlock_downgrade>> qemu_co_rwlock_wrlock(&rwlock);
+ *   - tests/unit/test-coroutine.c|356| <<rwlock_wrlock>> qemu_co_rwlock_wrlock(&rwlock);
+ */
 void coroutine_fn qemu_co_rwlock_wrlock(CoRwlock *lock)
 {
     Coroutine *self = qemu_coroutine_self();
diff --git a/util/qemu-coroutine.c b/util/qemu-coroutine.c
index 64d6264fc..823d39020 100644
--- a/util/qemu-coroutine.c
+++ b/util/qemu-coroutine.c
@@ -21,6 +21,53 @@
 #include "qemu/cutils.h"
 #include "block/aio.h"
 
+/*
+ * 注释:
+ *
+ * Switch to a new coroutine pool implementation with a global pool that
+ * grows to a maximum number of coroutines and per-thread local pools that
+ * are capped at hardcoded small number of coroutines.
+ *
+ * This approach does not leave large numbers of coroutines pooled in a
+ * thread that may not use them again. In order to perform well it
+ * amortizes the cost of global pool accesses by working in batches of
+ * coroutines instead of individual coroutines.
+ *
+ * The global pool is a list. Threads donate batches of coroutines to when
+ * they have too many and take batches from when they have too few:
+ *
+ * .-----------------------------------.
+ * | Batch 1 | Batch 2 | Batch 3 | ... | global_pool
+ * `-----------------------------------'
+ *
+ * Each thread has up to 2 batches of coroutines:
+ *
+ * .-------------------.
+ * | Batch 1 | Batch 2 | per-thread local_pool (maximum 2 batches)
+ * `-------------------'
+ *
+ * The goal of this change is to reduce the excessive number of pooled
+ * coroutines that cause QEMU to abort when vm.max_map_count is reached
+ * without losing the performance of an adequately sized coroutine pool.
+ *
+ * Here are virtio-blk disk I/O benchmark results:
+ *
+ *       RW BLKSIZE IODEPTH    OLD    NEW CHANGE
+ * randread      4k       1 113725 117451 +3.3%
+ * randread      4k       8 192968 198510 +2.9%
+ * randread      4k      16 207138 209429 +1.1%
+ * randread      4k      32 212399 215145 +1.3%
+ * randread      4k      64 218319 221277 +1.4%
+ * randread    128k       1  17587  17535 -0.3%
+ * randread    128k       8  17614  17616 +0.0%
+ * randread    128k      16  17608  17609 +0.0%
+ * randread    128k      32  17552  17553 +0.0%
+ * randread    128k      64  17484  17484 +0.0%
+ *
+ * See files/{fio.sh,test.xml.j2} for the benchmark configuration:
+ * https://gitlab.com/stefanha/virt-playbooks/-/tree/coroutine-pool-fix-sizing
+ */
+
 enum {
     COROUTINE_POOL_BATCH_MAX_SIZE = 128,
 };
@@ -61,6 +108,13 @@ static unsigned int global_pool_hard_max_size;
 
 static QemuMutex global_pool_lock; /* protects the following variables */
 static CoroutinePool global_pool = QSLIST_HEAD_INITIALIZER(global_pool);
+/*
+ * 在以下使用global_pool_size:
+ *   - util/qemu-coroutine.c|64| <<QSLIST_HEAD>> static unsigned int global_pool_size;
+ *   - util/qemu-coroutine.c|146| <<coroutine_pool_refill_local>> global_pool_size -= batch->size;
+ *   - util/qemu-coroutine.c|163| <<coroutine_pool_put_global>> if (global_pool_size < max) {
+ *   - util/qemu-coroutine.c|167| <<coroutine_pool_put_global>> global_pool_size += batch->size;
+ */
 static unsigned int global_pool_size;
 static unsigned int global_pool_max_size = COROUTINE_POOL_BATCH_MAX_SIZE;
 
@@ -173,6 +227,10 @@ static void coroutine_pool_put_global(CoroutinePoolBatch *batch)
     coroutine_pool_batch_delete(batch);
 }
 
+/*
+ * called by:
+ *   - util/qemu-coroutine.c|222| <<qemu_coroutine_create>> co = coroutine_pool_get();
+ */
 /* Get the next unused coroutine from the pool or return NULL */
 static Coroutine *coroutine_pool_get(void)
 {
@@ -311,6 +369,40 @@ void qemu_aio_coroutine_enter(AioContext *ctx, Coroutine *co)
     }
 }
 
+/*
+ * 除了unittest的使用:
+ *   - block/aio_task.c|94| <<aio_task_pool_start_task>> qemu_coroutine_enter(qemu_coroutine_create(aio_task_co, task));
+ *   - block/blkdebug.c|915| <<resume_req_by_tag>> qemu_coroutine_enter(co);
+ *   - block/blkverify.c|217| <<blkverify_co_prwv>> qemu_coroutine_enter(co_a);
+ *   - block/blkverify.c|218| <<blkverify_co_prwv>> qemu_coroutine_enter(co_b);
+ *   - block/block-copy.c|992| <<block_copy_async>> qemu_coroutine_enter(call_state->co);
+ *   - block/export/vduse-blk.c|112| <<vduse_blk_vq_handler>> qemu_coroutine_enter(co);
+ *   - block/export/vhost-user-blk-server.c|102| <<vu_blk_process_vq>> qemu_coroutine_enter(co);
+ *   - block/mirror.c|469| <<mirror_perform>> qemu_coroutine_enter(co);
+ *   - block/nvme.c|1169| <<nvme_rw_cb_bh>> qemu_coroutine_enter(data->co);
+ *   - block/qed.c|329| <<qed_need_check_timer_cb>> qemu_coroutine_enter(co);
+ *   - block/qed.c|593| <<bdrv_qed_open>> qemu_coroutine_enter(qemu_coroutine_create(bdrv_qed_open_entry, &qoc));
+ *   - block/quorum.c|332| <<quorum_rewrite_bad_versions>> qemu_coroutine_enter(co);
+ *   - block/quorum.c|630| <<read_quorum_children>> qemu_coroutine_enter(co);
+ *   - block/quorum.c|744| <<quorum_co_pwritev>> qemu_coroutine_enter(co);
+ *   - hw/9pfs/9p.c|4188| <<pdu_submit>> qemu_coroutine_enter(co);
+ *   - hw/9pfs/9p.c|4345| <<v9fs_reset>> qemu_coroutine_enter(co);
+ *   - hw/9pfs/coth.c|30| <<coroutine_enter_cb>> qemu_coroutine_enter(co);
+ *   - hw/9pfs/coth.c|37| <<coroutine_enter_func>> qemu_coroutine_enter(co);
+ *   - hw/remote/remote-obj.c|125| <<remote_object_machine_done>> qemu_coroutine_enter(co);
+ *   - migration/colo.c|153| <<secondary_vm_do_failover>> qemu_coroutine_enter(mis->colo_incoming_co);
+ *   - migration/migration.c|917| <<migration_incoming_process>> qemu_coroutine_enter(co);
+ *   - migration/rdma.c|3328| <<rdma_cm_poll_handler>> qemu_coroutine_enter(mis->loadvm_co);
+ *   - nbd/server.c|3278| <<rdma_cm_poll_handler>> qemu_coroutine_enter(co);
+ *   - net/colo-compare.c|824| <<compare_chr_send>> qemu_coroutine_enter(sendco->co);
+ *   - net/filter-mirror.c|128| <<filter_send>> qemu_coroutine_enter(co);
+ *   - qemu-img.c|2079| <<convert_co_do_copy>> qemu_coroutine_enter(s->co[i]);
+ *   - qemu-img.c|2140| <<convert_do_copy>> qemu_coroutine_enter(s->co[i]);
+ *   - scsi/qemu-pr-helper.c|817| <<accept_client>> qemu_coroutine_enter(prh->co);
+ *   - tools/i386/qemu-vmsr-helper.c|296| <<accept_client>> qemu_coroutine_enter(vmsrh->co);
+ *   - util/qemu-coroutine-io.c|78| <<fd_coroutine_enter>> qemu_coroutine_enter(data->co);
+ *   - util/qemu-coroutine.c|322| <<qemu_coroutine_enter_if_inactive>> qemu_coroutine_enter(co);
+ */
 void qemu_coroutine_enter(Coroutine *co)
 {
     qemu_aio_coroutine_enter(qemu_get_current_aio_context(), co);
@@ -361,6 +453,54 @@ void qemu_coroutine_dec_pool_size(unsigned int removing_pool_size)
     global_pool_max_size -= removing_pool_size;
 }
 
+/*
+ * 在以下调用g_file_get_contents():
+ *   - authz/listfile.c|62| <<qauthz_list_file_load>> if (!g_file_get_contents(fauthz->filename, &content, &len, &err)) {
+ *   - block/file-posix.c|1233| <<get_sysfs_str_val>> if (!g_file_get_contents(sysfspath, val, &len, NULL)) {
+ *   - crypto/secret.c|50| <<qcrypto_secret_load_data>> if (!g_file_get_contents(secret->file, &data, &length, &gerr)) {
+ *   - crypto/tlscreds.c|61| <<qcrypto_tls_creds_get_dh_params_file>> if (!g_file_get_contents(filename,
+ *   - crypto/tlscredspsk.c|45| <<lookup_key>> if (!g_file_get_contents(pskfile, &content, &clen, &gerr)) {
+ *   - crypto/tlscredsx509.c|395| <<qcrypto_tls_creds_load_cert>> if (!g_file_get_contents(certFile, &buf, &buflen, &gerr)) {
+ *   - crypto/tlscredsx509.c|442| <<qcrypto_tls_creds_load_ca_cert_list>> if (!g_file_get_contents(certFile, &buf, &buflen, &gerr)) {
+ *   - hw/arm/boot.c|870| <<load_aarch64_image>> if (!g_file_get_contents(filename, (char **)&buffer, &len, NULL)) {
+ *   - hw/core/loader.c|812| <<load_image_gzipped_buffer>> if (!g_file_get_contents(filename, (char **) &compressed_data, &len,
+ *   - hw/core/loader.c|1085| <<rom_add_file>> if (!g_file_get_contents(rom->path, (gchar **) &rom->data,
+ *   - hw/core/loader.c|1925| <<load_targphys_hex_as>> if (!g_file_get_contents(filename, &hex_blob, &hex_blob_size, NULL)) {
+ *   - hw/cxl/cxl-cdat.c|127| <<ct3_load_cdat>> if (!g_file_get_contents(cdat->filename, (gchar **)&buf,
+ *   - hw/ipmi/ipmi_bmc_sim.c|2074| <<ipmi_sdr_init>> !g_file_get_contents(ibs->sdr_filename, (gchar **) &sdrs, &sdrs_size,
+ *   - hw/nvram/fw_cfg.c|139| <<read_splashfile>> if (!g_file_get_contents(filename, &content, file_sizep, &err)) {
+ *   - hw/nvram/fw_cfg.c|1242| <<load_image_to_fw_cfg>> if (!g_file_get_contents(image_name, &contents, &length, NULL)) {
+ *   - hw/ppc/mac_newworld.c|520| <<ppc_core99_init>> if (g_file_get_contents(filename, &ndrv_file, &ndrv_size, NULL)) {
+ *   - hw/ppc/mac_oldworld.c|359| <<ppc_heathrow_init>> if (g_file_get_contents(filename, &ndrv_file, &ndrv_size, NULL)) {
+ *   - hw/ppc/spapr_pci.c|801| <<spapr_phb_vfio_get_loc_code>> if (!g_file_get_contents(path, &devspec, NULL, NULL)) {
+ *   - hw/ppc/spapr_pci.c|808| <<spapr_phb_vfio_get_loc_code>> if (!g_file_get_contents(path, &buf, NULL, NULL)) {
+ *   - hw/vfio/iommufd.c|209| <<iommufd_cdev_getfd>> if (!g_file_get_contents(vfio_dev_path, &contents, &length, NULL)) {
+ *   - hw/vfio/platform.c|595| <<vfio_platform_realize>> if (!g_file_get_contents(path, &contents, &length, &gerr)) {
+ *   - hw/virtio/vhost-backend.c|58| <<vhost_kernel_memslots_limit>> if (g_file_get_contents("/sys/module/vhost/parameters/max_mem_regions",
+ *   - hw/virtio/virtio-mem.c|112| <<virtio_mem_thp_size>> if (g_file_get_contents(HPAGE_PMD_SIZE_PATH, &content, NULL, NULL) &&
+ *   - qga/commands-common-ssh.c|15| <<read_authkeys>> if (!g_file_get_contents(path, &contents, NULL, &err)) {
+ *   - qga/commands-linux.c|1002| <<qmp_guest_get_disks>> if (!g_file_get_contents(size_path, &line, NULL, NULL)) {
+ *   - qga/commands-posix-ssh.c|286| <<test_authorized_keys_equal>> g_file_get_contents(path, &contents, NULL, &err);
+ *   - qga/commands-posix.c|1257| <<ga_parse_osrelease>> if (!g_file_get_contents(fname, &content, NULL, &err)) {
+ *   - system/device_tree.c|184| <<read_fstree>> if (!g_file_get_contents(tmpnam, &val, &len, NULL)) {
+ *   - system/physmem.c|1302| <<get_file_size>> if (g_file_get_contents(size_path, &size_str, NULL, NULL)) {
+ *   - system/vl.c|1193| <<parse_fw_cfg>> if (!g_file_get_contents(file, &buf, &size, &err)) {
+ *   - target/i386/kvm/vmsr_energy.c|219| <<vmsr_count_cpus_per_package>> if (!g_file_get_contents(path, &file_contents, &length, NULL)) {
+ *   - target/i386/kvm/vmsr_energy.c|247| <<vmsr_get_physical_package_id>> if (!g_file_get_contents(file_path, &file_contents, &length, NULL)) {
+ *   - target/i386/sev.c|752| <<sev_read_file_base64>> if (!g_file_get_contents(filename, &base64, &sz, &error)) {
+ *   - target/ppc/kvm.c|1863| <<kvmppc_get_host_serial>> return g_file_get_contents("/proc/device-tree/system-id", value, NULL,
+ *   - target/ppc/kvm.c|1869| <<kvmppc_get_host_model>> return g_file_get_contents("/proc/device-tree/model", value, NULL, NULL);
+ *   - tests/qtest/bios-tables-test.c|338| <<load_asl>> ret = g_file_get_contents(sdt->asl_file, &sdt->asl,
+ *   - tests/qtest/bios-tables-test.c|421| <<load_expected_aml>> ret = g_file_get_contents(aml_file, (gchar **)&exp_sdt.aml,
+ *   - tests/qtest/migration-test.c|1829| <<file_check_offset_region>> g_assert(g_file_get_contents(path, &actual, NULL, NULL));
+ *   - tests/unit/test-char.c|1335| <<char_file_test_internal>> ret = g_file_get_contents(out, &contents, &length, NULL);
+ *   - tests/unit/test-seccomp.c|201| <<can_play_with_seccomp>> if (!g_file_get_contents("/proc/self/status", &status, NULL, NULL)) {
+ *   - util/mmap-alloc.c|125| <<map_noreserve_effective>> if (g_file_get_contents(OVERCOMMIT_MEMORY_PATH, &content, NULL, NULL) &&
+ *   - util/oslib-posix.c|652| <<qemu_get_pid_name>> g_file_get_contents(pid_path, &name, &len, NULL);
+ *   - util/qemu-coroutine.c|376| <<get_global_pool_hard_max_size>> if (g_file_get_contents("/proc/sys/vm/max_map_count", &contents, NULL,
+ *   - util/selfmap.c|19| <<read_self_maps>> if (!g_file_get_contents("/proc/self/maps", &maps, NULL, NULL)) {
+ */
+
 static unsigned int get_global_pool_hard_max_size(void)
 {
 #ifdef __linux__
-- 
2.39.5 (Apple Git-154)

