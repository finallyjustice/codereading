From 0e12cc3457fd42c751f286486751503bff8f7e94 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 12 Mar 2019 05:39:33 +0800
Subject: [PATCH 1/1] block comment for block and drivers for
 linux-block:for-5.1/block-20190302

This is for linux-block: for-5.1/block-20190302

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-core.c          |   1 +
 block/blk-merge.c         |   4 +
 block/blk-mq-cpumap.c     |  15 ++
 block/blk-mq-pci.c        |   7 +
 block/blk-mq-sched.c      |  79 +++++++
 block/blk-mq-sched.h      |   3 +
 block/blk-mq-tag.c        |  14 ++
 block/blk-mq-tag.h        |  49 +++++
 block/blk-mq.c            | 545 ++++++++++++++++++++++++++++++++++++++++++++++
 block/blk-mq.h            |  95 +++++++-
 block/blk-settings.c      |  39 ++++
 block/blk-softirq.c       |  10 +
 block/blk-stat.c          |  49 +++++
 block/blk-timeout.c       |  14 ++
 drivers/nvme/host/core.c  |   5 +
 drivers/nvme/host/pci.c   |  25 +++
 include/linux/blk-mq.h    | 256 ++++++++++++++++++++++
 include/linux/blk_types.h |   7 +
 include/linux/blkdev.h    | 166 ++++++++++++++
 19 files changed, 1382 insertions(+), 1 deletion(-)

diff --git a/block/blk-core.c b/block/blk-core.c
index 6b78ec5..4c72e4e 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -496,6 +496,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	if (!q->backing_dev_info)
 		goto fail_split;
 
+	/* struct blk_queue_stats */
 	q->stats = blk_alloc_queue_stats();
 	if (!q->stats)
 		goto fail_stats;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 22467f4..9fcb5d5 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -318,6 +318,10 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return do_split ? new : NULL;
 }
 
+/*
+ * 根据块设备请求队列的limits.max_sectors和limits.max_segmetns
+ * 来拆分bio,适应设备缓存.会在函数blk_set_default_limits中设置
+ */
 void blk_queue_split(struct request_queue *q, struct bio **bio)
 {
 	struct bio *split, *res;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 03a5348..4b4c4da 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -31,6 +31,21 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|50| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|52| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3486| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3782| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|506| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1814| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2080| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2081| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6940| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1818| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[0]);
+ *
+ * 初始化blk_mq_queue_map->map
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index 1dce185..de49b40 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -31,6 +31,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|504| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6942| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5792| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[0],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
@@ -42,6 +48,7 @@ int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 		if (!mask)
 			goto fallback;
 
+		/* 猜测是每一个sw queue对应的hw queue??? */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 4090553..c4f55a9 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -84,6 +84,15 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|211| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|216| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *
+ * 对于参数的hctx, 如果没有调度器就返回
+ * 否则用调度器的dispatch_request()取出下一个request
+ * 用blk_mq_dispatch_rq_list()下发
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -111,6 +120,7 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 		 * in blk_mq_dispatch_rq_list().
 		 */
 		list_add(&rq->queuelist, &rq_list);
+	/* 为参数list中的每一个request调用queue_rq() */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 }
 
@@ -130,6 +140,15 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|213| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *
+ * dequeue request one by one from sw queue if queue is busy
+ * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ * 然后为这个request调用queue_rq()
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -139,12 +158,18 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	do {
 		struct request *rq;
 
+		/*
+		 * hctx->ctx_map在以下被设置:
+		 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+		 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+		 */
 		if (!sbitmap_any_bit_set(&hctx->ctx_map))
 			break;
 
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/* 从start代表的第一个hctx->ctx_map开始, 取出一个request */
 		rq = blk_mq_dequeue_from_ctx(hctx, ctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -161,11 +186,16 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		/* round robin for fair dispatch */
 		ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
 
+	/* 为参数list中的每一个request调用queue_rq() */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1495| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -205,19 +235,46 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	 */
 	if (!list_empty(&rq_list)) {
 		blk_mq_sched_mark_restart_hctx(hctx);
+		/* 为参数list中的每一个request调用queue_rq() */
 		if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+			/*
+			 * blk_mq_do_dispatch_sched():
+			 * 对于参数的hctx, 如果没有调度器就返回
+			 * 否则用调度器的dispatch_request()取出下一个request
+			 * 用blk_mq_dispatch_rq_list()下发
+			 *
+			 * blk_mq_do_dispatch_ctx()
+			 * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+			 * 然后为这个request调用queue_rq()
+			 */
 			if (has_sched_dispatch)
 				blk_mq_do_dispatch_sched(hctx);
 			else
 				blk_mq_do_dispatch_ctx(hctx);
 		}
 	} else if (has_sched_dispatch) {
+		/*
+		 * 对于参数的hctx, 如果没有调度器就返回
+		 * 否则用调度器的dispatch_request()取出下一个request
+		 * 用blk_mq_dispatch_rq_list()下发
+		 */
 		blk_mq_do_dispatch_sched(hctx);
 	} else if (hctx->dispatch_busy) {
 		/* dequeue request one by one from sw queue if queue is busy */
+		/*
+		 * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+		 * 然后为这个request调用queue_rq()
+		 */
 		blk_mq_do_dispatch_ctx(hctx);
 	} else {
+		/*
+		 * 对于hctx->ctx_map中每个设置的bit对应的ctx
+		 * 把ctx->rq_list[type]的request们拼接到参数的list
+		 * 把ctx在hctx->ctx_map清空
+		 * 最后参数list中的就是这个hctx->ctx_map中每一个ctx的rq_list的总和拼
+		 */
 		blk_mq_flush_busy_ctxs(hctx, &rq_list);
+		/* 为参数list中的每一个request调用queue_rq() */
 		blk_mq_dispatch_rq_list(q, &rq_list, false);
 	}
 }
@@ -301,6 +358,10 @@ EXPORT_SYMBOL_GPL(blk_mq_bio_list_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-sched.c|395| <<__blk_mq_sched_bio_merge>> ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_hw_ctx *hctx,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
@@ -309,6 +370,10 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/*
+	 * Iterate list of requests and see if we can merge this bio with any
+	 * of them.
+	 */
 	if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
 		ctx->rq_merged++;
 		return true;
@@ -423,6 +488,11 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 		 * busy in case of 'none' scheduler, and this way may save
 		 * us one extra enqueue & dequeue to sw queue.
 		 */
+		/*
+		 * blk_mq_insert_requests():
+		 * 把list中的request们放入ctx->rq_lists[type]
+		 * 把ctx在hctx->ctx_map对应的bit设置
+		 */
 		if (!hctx->dispatch_busy && !e && !run_queue_async)
 			blk_mq_try_issue_list_directly(hctx, list);
 		else
@@ -443,6 +513,10 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|497| <<blk_mq_init_sched>> ret = blk_mq_sched_alloc_tags(q, hctx, i);
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -472,6 +546,11 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 		blk_mq_sched_free_tags(set, hctx, i);
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|577| <<elevator_switch_mq>> ret = blk_mq_init_sched(q, new_e);
+ *   - block/elevator.c|623| <<elevator_init_mq>> err = blk_mq_init_sched(q, e);
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index c7bdb52..f7ce452 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -76,6 +76,9 @@ static inline void blk_mq_sched_requeue_request(struct request *rq)
 		e->type->ops.requeue_request(rq);
 }
 
+/*
+ * 如果支持elevator, 用elevator的has_work查看是否有work
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index a4931fc..b84d0f2 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -27,8 +27,13 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|72| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/* test_and_set_bit(): Set a bit and return its old value */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -107,6 +112,11 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则返回使用data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
@@ -435,6 +445,10 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2068| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..f8d3a1a 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -8,15 +8,45 @@
  * Tag address space map.
  */
 struct blk_mq_tags {
+	/*
+	 * 只在一处设置:
+	 */
 	unsigned int nr_tags;
+	/*
+	 * 只在一处设置:
+	 *   - block/blk-mq-tag.c|454| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * active_queues使用的地方:
+	 *   - block/blk-mq-debugfs.c|477| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|60| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|85| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
+	/*
+	 * 分配指针数组的地方:
+	 *   - block/blk-mq.c|2073| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *
+	 * 设置指针内容的地方:
+	 *   - block/blk-mq-tag.h|92| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|307| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1056| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 */
 	struct request **rqs;
+	/*
+	 * 分配指针数组的地方:
+	 *   - block/blk-mq.c|2081| <<blk_mq_alloc_rq_map>> tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *
+	 * 设置指针内容的地方 (指向的都是下面page_list的内容):
+	 *   - block/blk-mq.c|2173| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -36,6 +66,12 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|147| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|198| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1484| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(&hctx->tags->bitmap_tags, hctx)->wait;
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -53,6 +89,14 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|394| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1078| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ *
+ * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+ * 则不会调用__blk_mq_tag_busy()
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,6 +119,11 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|220| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|303| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
diff --git a/block/blk-mq.c b/block/blk-mq.c
index fa024bc..f150453 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -38,9 +38,118 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 一共两处对queue_rq()的调用:
+ *   - blk_mq_try_issue_directly()-->__blk_mq_issue_directly()
+ *   - blk_mq_dispatch_rq_list()
+ */
+
+/*
+ * 一共三处缓存request的地方:
+ * 1. hctx->dispatch
+ * 2. ctx->rq_lists
+ * 3. scheduler自己的数据
+ *
+ *
+ * 下发hctx->dispatch的地方:
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+ *
+ * 添加到hctx->dispatch的地方:
+ *   - block/blk-mq-sched.c|422| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+ *   - block/blk-mq.c|1535| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+ *   - block/blk-mq.c|1935| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+ *   - block/blk-mq.c|2537| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+ *
+ *
+ * 下发ctx->rq_lists的地方:
+ *   - block/blk-mq.c|1148| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+ *   - block/blk-mq.c|1204| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+ *   - block/blk-mq.c|2528| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+ *
+ * 添加到ctx->rq_lists的地方:
+ *   - block/blk-mq-sched.c|377| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/blk-mq.c|1906| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+ *   - block/blk-mq.c|1908| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+ *   - block/blk-mq.c|1966| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+ *
+ *
+ * 调用scheduler的insert_requests()和requeue_request()缓存request的地方:
+ *   - block/blk-mq-sched.c|464| <<blk_mq_sched_insert_request>> e->type->ops.insert_requests(hctx, &list, at_head);
+ *   - block/blk-mq-sched.c|484| <<blk_mq_sched_insert_requests>> e->type->ops.insert_requests(hctx, list, false);
+ *   - block/blk-mq-sched.h|76| <<blk_mq_sched_requeue_request>> e->type->ops.requeue_request(rq);
+ *
+ * 调用scheduler的dispatch_request()准备下发request的地方:
+ *   - block/blk-mq-sched.c|111| <<blk_mq_do_dispatch_sched>> rq = e->type->ops.dispatch_request(hctx);
+ *
+ *
+ * blk_mq_sched_dispatch_requests()
+ * blk_mq_sched_bypass_insert()
+ * blk_mq_request_bypass_insert()
+ * blk_mq_hctx_notify_dead()
+ * flush_busy_ctx()
+ * dispatch_rq_from_ctx()
+ * blk_mq_attempt_merge()
+ * __blk_mq_insert_req_list()
+ * blk_mq_insert_requests()
+ * blk_mq_sched_insert_requests()
+ * blk_mq_sched_requeue_request()
+ * blk_mq_do_dispatch_sched()
+ */
+ 
+/*
+ * BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+ * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+ *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+ *
+ * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+ *
+ * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+ *
+ * RQF_xxx用在request->rq_flags
+ *
+ * MQ_RQ_xxx用在request->state
+ *
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+
+/*
+ * 关于下发request到硬件驱动时的timeout:
+ *
+ * 比如virtio_queue_rq()调用blk_mq_start_request()
+ * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+ *
+ * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+ * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+ *
+ * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+ * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+ *
+ * - nvme的例子是nvme_timeout()
+ * - scsi的例子是scsi_timeout()
+ */
+
+/*
+ * 关于blk-mq complete的例子:
+ * 
+ * virtblk_done()调用blk_mq_complete_request()
+ * 调用__blk_mq_complete_request()
+ * 根据情况调用__blk_complete_request()或者q->mq_ops->complete()
+ *
+ * q->mq_ops->complete()的几个例子:
+ * - nvme_pci_complete_rq()
+ * - blkif_complete_rq()
+ * - virtblk_request_done()
+ * - scsi_softirq_done()
+ */
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2888| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3442| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, bytes, bucket;
@@ -63,6 +172,21 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * hctx->dispatch在以下插入新元素:
+	 *   - block/blk-mq-sched.c|189| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1303| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1663| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * hctx->ctx_map在以下被修改:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * blk_mq_sched_has_work():
+	 * 如果支持elevator, 用elevator的has_work查看是否有work
+	 */
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
 			blk_mq_sched_has_work(hctx);
@@ -71,6 +195,13 @@ static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1693| <<__blk_mq_insert_request>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *   - block/blk-mq.c|1730| <<blk_mq_insert_requests>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *
+ * 把ctx在hctx->ctx_map对应的bit设置
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
@@ -80,6 +211,9 @@ static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 		sbitmap_set_bit(&hctx->ctx_map, bit);
 }
 
+/*
+ * 把ctx在hctx->ctx_map对应的bit清空
+ */
 static void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,
 				      struct blk_mq_ctx *ctx)
 {
@@ -108,6 +242,10 @@ static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|74| <<part_in_flight>> return blk_mq_in_flight(q, part);
+ */
 unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	unsigned inflight[2];
@@ -119,6 +257,10 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight[0];
 }
 
+/*
+ * used only by:
+ *   - block/blk-mq.c|196| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ */
 static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 				     struct request *rq, void *priv,
 				     bool reserved)
@@ -140,10 +282,24 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|283| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|245| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3903| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|317| <<pmem_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3852| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	int freeze_depth;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 */
 	freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
 	if (freeze_depth == 1) {
 		percpu_ref_kill(&q->q_usage_counter);
@@ -153,6 +309,11 @@ void blk_freeze_queue_start(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|246| <<blk_freeze_queue>> blk_mq_freeze_queue_wait(q);
+ *   - drivers/nvme/host/core.c|3841| <<nvme_wait_freeze>> blk_mq_freeze_queue_wait(ns->queue);
+ */
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
@@ -212,6 +373,13 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  * FIXME: replace the scsi_internal_device_*block_nowait() calls in the
  * mpt3sas driver such that this function can be removed.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|297| <<blk_mq_quiesce_queue>> blk_mq_quiesce_queue_nowait(q);
+ *   - drivers/scsi/scsi_lib.c|2667| <<scsi_internal_device_block_nowait>> blk_mq_quiesce_queue_nowait(q);
+ *
+ * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+ */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
@@ -227,20 +395,34 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
  * sure no dispatch can happen until the queue is unquiesced via
  * blk_mq_unquiesce_queue().
  */
+/*
+ * called by:
+ *   - block/blk-core.c|360| <<blk_cleanup_queue>> blk_mq_quiesce_queue(q);
+ *   - block/blk-mq.c|3352| <<blk_mq_update_nr_requests>> blk_mq_quiesce_queue(q);
+ *   - block/elevator.c|645| <<elevator_switch>> blk_mq_quiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|958| <<virtblk_freeze>> blk_mq_quiesce_queue(vblk->disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2699| <<scsi_internal_device_block>> blk_mq_quiesce_queue(q)
+ *
+ * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+ * 然后根据hctx->flags & BLK_MQ_F_BLOCKING的情况使用synchronize_srcu()或者synchronize_rcu()
+ */
 void blk_mq_quiesce_queue(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 	bool rcu = false;
 
+	/* 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED */
 	blk_mq_quiesce_queue_nowait(q);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 		if (hctx->flags & BLK_MQ_F_BLOCKING)
 			synchronize_srcu(hctx->srcu);
 		else
 			rcu = true;
 	}
+	/* wait until a grace period has elapsed */
 	if (rcu)
 		synchronize_rcu();
 }
@@ -253,6 +435,19 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  * This function recovers queue into the state before quiescing
  * which is done by blk_mq_quiesce_queue.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3453| <<blk_mq_update_nr_requests>> blk_mq_unquiesce_queue(q);
+ *   - block/blk-sysfs.c|485| <<queue_wb_lat_store>> blk_mq_unquiesce_queue(q);
+ *   - block/elevator.c|649| <<elevator_switch>> blk_mq_unquiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|975| <<virtblk_restore>> blk_mq_unquiesce_queue(vblk->disk->queue);
+ *   - drivers/nvme/host/pci.c|1627| <<nvme_dev_remove_admin>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1661| <<nvme_alloc_admin_tags>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2709| <<scsi_start_queue>> blk_mq_unquiesce_queue(q);
+ *
+ * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+ * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+ */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
 	blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
@@ -262,6 +457,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * 只被以下调用:
+ *   - block/blk-core.c|286| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -282,11 +481,20 @@ EXPORT_SYMBOL(blk_mq_can_queue);
  * Only need start/end time stamping if we have stats enabled, or using
  * an IO scheduler.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|423| <<blk_mq_rq_ctx_init>> if (blk_mq_need_time_stamp(rq))
+ *   - block/blk-mq.c|640| <<__blk_mq_end_request>> if (blk_mq_need_time_stamp(rq))
+ */
 static inline bool blk_mq_need_time_stamp(struct request *rq)
 {
 	return (rq->rq_flags & RQF_IO_STAT) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|397| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
@@ -347,6 +555,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|423| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|474| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|1932| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -481,6 +695,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|755| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|1187| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -498,6 +717,19 @@ static void __blk_mq_free_request(struct request *rq)
 	blk_queue_exit(q);
 }
 
+/*
+ * 主要调用的几个地方:
+ *   - block/bfq-iosched.c|1893| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|594| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|768| <<__blk_mq_end_request>> blk_mq_free_request(rq->next_rq);
+ *   - block/blk-mq.c|769| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|483| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/nvme/host/core.c|801| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|902| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|912| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1220| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|2241| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -557,6 +789,19 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|197| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|379| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1632| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|2294| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/loop.c|485| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/null_blk_main.c|620| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|225| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/nvme/host/core.c|281| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -623,6 +868,7 @@ static void __blk_mq_complete_request(struct request *rq)
 static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 	__releases(hctx->srcu)
 {
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING))
 		rcu_read_unlock();
 	else
@@ -632,6 +878,7 @@ static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
 	__acquires(hctx->srcu)
 {
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		/* shut up gcc false positive */
 		*srcu_idx = 0;
@@ -657,12 +904,48 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|296| <<bt_tags_iter>> if (rq && blk_mq_request_started(rq))
+ *   - block/blk-mq.c|718| <<__blk_mq_requeue_request>> if (blk_mq_request_started(rq)) {
+ *   - drivers/block/nbd.c|648| <<nbd_read_stat>> if (!req || !blk_mq_request_started(req)) {
+ */
 int blk_mq_request_started(struct request *rq)
 {
+	/*
+	 * MQ_RQ_IDLE
+	 * MQ_RQ_IN_FLIGHT
+	 * MQ_RQ_COMPLETE
+	 */
 	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
 
+/*
+ * 被以下的例子调用:
+ *   - drivers/block/null_blk_main.c|1332| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|318| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/nvme/host/pci.c|942| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1638| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1722| <<scsi_queue_rq>> blk_mq_start_request(req);
+ *
+ * 关于下发request到硬件驱动时的timeout:
+ *
+ * 比如virtio_queue_rq()调用blk_mq_start_request()
+ * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+ *
+ * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+ * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+ *
+ * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+ * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+ *
+ * - nvme的例子是nvme_timeout()
+ * - scsi的例子是scsi_timeout()
+ *
+ * 设置request->state为MQ_RQ_IN_FLIGHT
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -685,6 +968,9 @@ void blk_mq_start_request(struct request *rq)
 	blk_add_timer(rq);
 	WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
 
+	/*
+	 * q->dma_drain_size似乎目前只被libata-scsi使用
+	 */
 	if (q->dma_drain_size && blk_rq_bytes(rq)) {
 		/*
 		 * Make sure space for the drain appears.  We know we can do
@@ -754,6 +1040,11 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|137| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|878| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -831,9 +1122,17 @@ bool blk_mq_queue_inflight(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_queue_inflight);
 
+/*
+ * called only by:
+ *  - block/blk-mq.c|1054| <<blk_mq_check_expired>> blk_mq_rq_timed_out(rq, reserved);
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	req->rq_flags |= RQF_TIMED_OUT;
+	/*
+	 * nvme的例子是nvme_timeout()
+	 * scsi的例子是scsi_timeout()
+	 */
 	if (req->q->mq_ops->timeout) {
 		enum blk_eh_timer_return ret;
 
@@ -866,6 +1165,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|1113| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -904,6 +1207,20 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * timeout_work可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+ *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+ *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ *
+ * timeout_work使用的地方:
+ *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+ *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+ *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+ *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+ *
+ * 在以下使用:
+ *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -953,6 +1270,13 @@ struct flush_busy_ctx_data {
 	struct list_head *list;
 };
 
+/*
+ * 只在以下被调用:
+ *   - block/blk-mq.c|1063| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+ *
+ * 把ctx->rq_list[type]的request们拼接到flush_busy_ctx_data->list
+ * 把ctx在hctx->ctx_map清空
+ */
 static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
 {
 	struct flush_busy_ctx_data *flush_data = data;
@@ -971,6 +1295,12 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
  * Process software queues that have been marked busy, splicing them
  * to the for-dispatch
  */
+/*
+ * 对于hctx->ctx_map中每个设置的bit对应的ctx
+ * 把ctx->rq_list[type]的request们拼接到参数的list
+ * 把ctx在hctx->ctx_map清空
+ * 最后参数list中的就是这个hctx->ctx_map中每一个ctx的rq_list的总和拼接
+ */
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 {
 	struct flush_busy_ctx_data data = {
@@ -978,6 +1308,15 @@ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 		.list = list,
 	};
 
+	/*
+	 * hctx->ctx_map在以下被设置:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * flush_busy_ctx():
+	 * 把ctx->rq_list[type]的request们拼接到flush_busy_ctx_data->list
+	 * 把ctx在hctx->ctx_map清空
+	 */
 	sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
 }
 EXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);
@@ -987,6 +1326,11 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * 从bitnr对应的hctx->ctx_map的ctx中的ctx->rq_lists[type].next取出一个request
+ * 放入dispatch_data->rq
+ * 如果取完空了就把hctx->ctx_map对应的bit清掉
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1007,6 +1351,9 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * 从start代表的第一个hctx->ctx_map开始, 取出一个request
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1016,6 +1363,13 @@ struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 		.rq   = NULL,
 	};
 
+	/*
+	 * 从bitnr对应的hctx->ctx_map的ctx中的ctx->rq_lists[type].next取出一个request (就一个!!)
+	 * 放入dispatch_data->rq
+	 * 如果取完空了就把hctx->ctx_map对应的bit清掉
+	 *
+	 * 因为dispatch_rq_from_ctx()成功返回非0, 所以实际就取出一个的
+	 */
 	__sbitmap_for_each_set(&hctx->ctx_map, off,
 			       dispatch_rq_from_ctx, &data);
 
@@ -1030,8 +1384,24 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1103| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1128| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1207| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1239| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|1822| <<blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ *
+ * 如果request->tag不为-1, 直接返回
+ * 否则分配一个给driver用的(不是internal的)tag!
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 */
 	struct blk_mq_alloc_data data = {
 		.q = rq->q,
 		.hctx = rq->mq_hctx,
@@ -1046,6 +1416,10 @@ bool blk_mq_get_driver_tag(struct request *rq)
 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
 		data.flags |= BLK_MQ_REQ_RESERVED;
 
+	/*
+	 * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+	 * 则不会调用__blk_mq_tag_busy()
+	 */
 	shared = blk_mq_tag_busy(data.hctx);
 	rq->tag = blk_mq_get_tag(&data);
 	if (rq->tag >= 0) {
@@ -1177,6 +1551,15 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|114| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|164| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|208| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 为参数list中的每一个request调用queue_rq()
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1189,6 +1572,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	if (list_empty(list))
 		return false;
 
+	/* list_is_singular(): tests whether a list has just one entry */
 	WARN_ON(!list_is_singular(list) && got_budget);
 
 	/*
@@ -1200,6 +1584,12 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 
 		rq = list_first_entry(list, struct request, queuelist);
 
+		/*
+		 * 在以下设置:
+		 *   - block/blk-flush.c|298| <<blk_kick_flush>> flush_rq->mq_hctx = first_rq->mq_hctx;
+		 *   - block/blk-mq.c|317| <<blk_mq_rq_ctx_init>> rq->mq_hctx = data->hctx;
+		 *   - block/blk-mq.c|502| <<__blk_mq_free_request>> rq->mq_hctx = NULL;
+		 */
 		hctx = rq->mq_hctx;
 		if (!got_budget && !blk_mq_get_dispatch_budget(hctx))
 			break;
@@ -1331,6 +1721,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1530| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1699| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1366,9 +1761,13 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * __blk_mq_run_hw_queue()是唯一调用blk_mq_sched_dispatch_requests()的地方
+	 */
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
 }
@@ -1434,6 +1833,7 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 	if (unlikely(blk_mq_hctx_stopped(hctx)))
 		return;
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		int cpu = get_cpu();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
@@ -1455,6 +1855,22 @@ void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|79| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|452| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|476| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|157| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1198| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1455| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1639| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1704| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1724| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1798| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2106| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|709| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1590,6 +2006,10 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2327| <<blk_mq_init_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1605,6 +2025,12 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 	__blk_mq_run_hw_queue(hctx);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|1820| <<__blk_mq_insert_request>> __blk_mq_insert_req_list(hctx, rq, at_head);
+ *
+ * 把request添加到对应的request->mq_ctx的ctx->rq_lists[type]
+ */
 static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    bool at_head)
@@ -1622,6 +2048,10 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * 把request放入request->mq_ctx的rq_lists
+ * 然后把ctx在hctx->ctx_map对应的bit设置
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1649,6 +2079,13 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|429| <<blk_mq_sched_insert_requests>> blk_mq_insert_requests(hctx, ctx, list);
+ *
+ * 把list中的request们放入ctx->rq_lists[type]
+ * 把ctx在hctx->ctx_map对应的bit设置
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1667,6 +2104,7 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 
 	spin_lock(&ctx->lock);
 	list_splice_tail_init(list, &ctx->rq_lists[type]);
+	/* 把ctx在hctx->ctx_map对应的bit设置 */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 	spin_unlock(&ctx->lock);
 }
@@ -1749,6 +2187,12 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2046| <<blk_mq_try_issue_directly>> ret = __blk_mq_issue_directly(hctx, rq, cookie, last);
+ *
+ * 为参数的request调用queue_rq()!
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1788,6 +2232,15 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1267| <<blk_insert_cloned_request>> return blk_mq_try_issue_directly(rq->mq_hctx, rq, &unused, true, true);
+ *   - block/blk-mq.c|2089| <<blk_mq_try_issue_list_directly>> ret = blk_mq_try_issue_directly(hctx, rq, &unused,
+ *   - block/blk-mq.c|2213| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2220| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie, false, true);
+ *
+ * 核心思想是为参数的request调用queue_rq()!
+ */
 blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1829,6 +2282,9 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	 *.queue_rq() to the hardware dispatch list.
 	 */
 	force = true;
+	/*
+	 * 为参数的request调用queue_rq()!
+	 */
 	ret = __blk_mq_issue_directly(hctx, rq, cookie, last);
 out_unlock:
 	hctx_unlock(hctx, srcu_idx);
@@ -1860,6 +2316,10 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|484| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -2314,6 +2774,7 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx, node))
 		goto free_fq;
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (hctx->flags & BLK_MQ_F_BLOCKING)
 		init_srcu_struct(hctx->srcu);
 
@@ -2333,6 +2794,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|2941| <<blk_mq_init_allocated_queue>> blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+ */
 static void blk_mq_init_cpu_queues(struct request_queue *q,
 				   unsigned int nr_hw_queues)
 {
@@ -2392,6 +2857,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2915| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3295| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2404,6 +2874,7 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/* 遍历每一个request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2430,6 +2901,14 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		}
 
 		ctx = per_cpu_ptr(q->queue_ctx, i);
+		/*
+		 * 设置nr_maps的地方:
+		 *   - block/blk-mq.c|2713| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3058| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3069| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - drivers/nvme/host/pci.c|2322| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2324| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 */
 		for (j = 0; j < set->nr_maps; j++) {
 			if (!set->map[j].nr_queues) {
 				ctx->hctxs[j] = blk_mq_map_queue_type(q,
@@ -2459,6 +2938,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			BUG_ON(!hctx->nr_ctx);
 		}
 
+		/*
+		 * 这里很重要, 把剩下的的j网上的type都map到HCTX_TYPE_DEFAULT的hctx
+		 */
 		for (; j < HCTX_MAX_TYPES; j++)
 			ctx->hctxs[j] = blk_mq_map_queue_type(q,
 					HCTX_TYPE_DEFAULT, i);
@@ -2548,6 +3030,13 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2914| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ *
+ * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+ * 多个request_queue可能共享1个blk_mq_tag_set
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2570,6 +3059,12 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|3299| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ *
+ * 分配软件的request_queue->queue_ctx!
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
 	struct blk_mq_ctxs *ctxs;
@@ -2721,6 +3216,11 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return hctx;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2881| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3287| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
@@ -2805,6 +3305,9 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	/* mark the queue as mq asap */
 	q->mq_ops = set->ops;
 
+	/*
+	 * struct blk_stat_callback
+	 */
 	q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
 					     blk_mq_poll_stats_bkt,
 					     BLK_MQ_POLL_STATS_BKTS, q);
@@ -2817,7 +3320,15 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	/* init q->mq_kobj and sw queues' kobjects */
 	blk_mq_sysfs_init(q);
 
+	/*
+	 * Maximum number of hardware queues we support. For single sets, we'll never
+	 * have more than the CPUs (software queues). For multiple sets, the tag_set
+	 * user may have set ->nr_hw_queues larger.
+	 */
 	q->nr_queues = nr_hw_queues(set);
+	/*
+	 * 注意, 数量是上面的q->nr_queues = nr_hw_queues(set)!!!!!!!!!!
+	 */
 	q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
 						GFP_KERNEL, set->numa_node);
 	if (!q->queue_hw_ctx)
@@ -2856,6 +3367,10 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	q->poll_nsec = -1;
 
 	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+	/*
+	 * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+	 * 多个request_queue可能共享1个blk_mq_tag_set
+	 */
 	blk_mq_add_queue_tag_set(set, q);
 	blk_mq_map_swqueue(q);
 
@@ -2939,6 +3454,9 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * 初始化软件队列到硬件队列的mapping???
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2964,6 +3482,7 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		return set->ops->map_queues(set);
 	} else {
 		BUG_ON(set->nr_maps > 1);
+		/* 初始化blk_mq_queue_map->map */
 		return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
 	}
 }
@@ -2974,6 +3493,23 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * vblk->tag_set.ops = &virtio_mq_ops;
+ * vblk->tag_set.queue_depth = virtblk_queue_depth;
+ * vblk->tag_set.numa_node = NUMA_NO_NODE;
+ * vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+ * vblk->tag_set.cmd_size =
+ *                    sizeof(struct virtblk_req) +
+ *                    sizeof(struct scatterlist) * sg_elems;
+ * vblk->tag_set.driver_data = vblk;
+ * vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+ *
+ * err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ * if (err)
+ *	goto out_put_disk;
+ *
+ * q = blk_mq_init_queue(&vblk->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -2990,6 +3526,7 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (!set->ops->queue_rq)
 		return -EINVAL;
 
+	/* 要么都设置, 要么都不设置 */
 	if (!set->ops->get_budget ^ !set->ops->put_budget)
 		return -EINVAL;
 
@@ -3021,6 +3558,7 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)
 		set->nr_hw_queues = nr_cpu_ids;
 
+	/* 每个queue一个 struct blk_mq_tags */
 	set->tags = kcalloc_node(nr_hw_queues(set), sizeof(struct blk_mq_tags *),
 				 GFP_KERNEL, set->numa_node);
 	if (!set->tags)
@@ -3028,6 +3566,10 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * nr_hw_queues可能比cpu数量多,
+		 * 猜测是每一个sw queue对应的hw queue??? 所以用cpu的数量
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
@@ -3036,6 +3578,9 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 		set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
 	}
 
+	/*
+	 * 初始化软件队列到硬件队列的mapping???
+	 */
 	ret = blk_mq_update_queue_map(set);
 	if (ret)
 		goto out_free_mq_map;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index c11353a..a279531 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -7,6 +7,9 @@
 
 struct blk_mq_tag_set;
 
+/*
+ * 这个结构保存的真正的request_queue中每个ctx的实体!
+ */
 struct blk_mq_ctxs {
 	struct kobject kobj;
 	struct blk_mq_ctx __percpu	*queue_ctx;
@@ -18,10 +21,20 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 添加的地方:
+		 *   - block/blk-mq.c|1808| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1810| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1861| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|2922| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -99,10 +112,29 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *
  * @flags: request command flags
  * @cpu: cpu ctx
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|389| <<__blk_mq_sched_bio_merge>> struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, bio->bi_opf, ctx);
+ *   - block/blk-mq-tag.c|182| <<blk_mq_get_tag>> data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
+ *   - block/blk-mq.c|580| <<blk_mq_get_request>> data->hctx = blk_mq_map_queue(q, data->cmd_flags,
+ *   - block/blk.h|41| <<blk_get_flush_queue>> return blk_mq_map_queue(q, REQ_OP_FLUSH, ctx)->fq;
+ *
+ * 如果是REQ_HIPRI, 返回HCTX_TYPE_POLL
+ * 如果是REQ_OP_READ, 返回HCTX_TYPE_READ
+ * 其他的, 返回默认的HCTX_TYPE_DEFAULT
+ * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+ * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+ * 返回的仍然可以是HCTX_TYPE_DEFAULT
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 						     unsigned int flags,
 						     struct blk_mq_ctx *ctx)
 {
+	/*
+	 * HCTX_TYPE_DEFAULT = 0
+	 * HCTX_TYPE_READ    = 1
+	 * HCTX_TYPE_POLL    = 2
+	 */
 	enum hctx_type type = HCTX_TYPE_DEFAULT;
 
 	/*
@@ -112,7 +144,12 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		type = HCTX_TYPE_POLL;
 	else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
 		type = HCTX_TYPE_READ;
-	
+
+	/*
+	 * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+	 * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+	 * 返回的仍然可以是HCTX_TYPE_DEFAULT
+	 */
 	return ctx->hctxs[type];
 }
 
@@ -171,14 +208,37 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|115| <<blk_mq_get_tag>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq-tag.c|179| <<blk_mq_get_tag>> tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq.c|297| <<blk_mq_rq_ctx_init>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *
+ * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+ * 则返回使用data->hctx->sched_tags
+ * 否则返回data->hctx->tags
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * BLK_MQ_REQ_INTERNAL在以下被设置:
+	 *   - block/blk-mq.c|372| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
 		return data->hctx->sched_tags;
 
 	return data->hctx->tags;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|207| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1833| <<__blk_mq_delay_run_hw_queue>> if (unlikely(blk_mq_hctx_stopped(hctx)))
+ *   - block/blk-mq.c|1907| <<blk_mq_run_hw_queues>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1928| <<blk_mq_queue_stopped>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1991| <<blk_mq_start_stopped_hw_queue>> if (!blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|2263| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+ */
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -193,6 +253,9 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part);
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2]);
 
+/*
+ * 调用q->mq_ops->put_budget(hctx)
+ */
 static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -201,6 +264,9 @@ static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 		q->mq_ops->put_budget(hctx);
 }
 
+/*
+ * 调用q->mq_ops->get_budget(hctx)
+ */
 static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -210,18 +276,34 @@ static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 	return true;
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ */
 static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 					   struct request *rq)
 {
 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
 	rq->tag = -1;
 
+	/*
+	 * 是伴随着hctx->nr_active的, 每次设置的时候, hctx->nr_active就增长
+	 * 取消的时候, hctx->nr_active就减少
+	 *   - block/blk-mq.c|510| <<blk_mq_rq_ctx_init>> rq_flags = RQF_MQ_INFLIGHT;
+	 *   - block/blk-mq.c|750| <<blk_mq_free_request>> if (rq->rq_flags & RQF_MQ_INFLIGHT)
+	 *   - block/blk-mq.c|1427| <<blk_mq_get_driver_tag>> rq->rq_flags |= RQF_MQ_INFLIGHT;
+	 *   - block/blk-mq.h|288| <<__blk_mq_put_driver_tag>> if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+	 *   - block/blk-mq.h|289| <<__blk_mq_put_driver_tag>> rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+	 */
 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
 		atomic_dec(&hctx->nr_active);
 	}
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ * 如果rq->tag和rq->internal_tag有一个已经是-1了就不用了
+ */
 static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
 {
@@ -231,6 +313,10 @@ static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ * 如果rq->tag和rq->internal_tag有一个已经是-1了就不用了
+ */
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	if (rq->tag == -1 || rq->internal_tag == -1)
@@ -239,6 +325,13 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|60| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3480| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ *
+ * 把每个blk_mq_queue_map->mq_map[cpu]都设置成0
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 6375afa..a58b773 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -20,6 +20,10 @@ EXPORT_SYMBOL(blk_max_low_pfn);
 
 unsigned long blk_max_pfn;
 
+/*
+ * 被很多地方调用, 其中一个例子
+ *   - block/blk-mq.c|2849| <<blk_mq_init_allocated_queue>> blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);
+ */
 void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
 {
 	q->rq_timeout = timeout;
@@ -33,6 +37,11 @@ EXPORT_SYMBOL_GPL(blk_queue_rq_timeout);
  * Description:
  *   Returns a queue_limit struct to its default state.
  */
+/*
+ * called by:
+ *   - block/blk-settings.c|73| <<blk_set_stacking_limits>> blk_set_default_limits(lim);
+ *   - block/blk-settings.c|119| <<blk_queue_make_request>> blk_set_default_limits(&q->limits);
+ */
 void blk_set_default_limits(struct queue_limits *lim)
 {
 	lim->max_segments = BLK_MAX_SEGMENTS;
@@ -178,6 +187,13 @@ EXPORT_SYMBOL(blk_queue_bounce_limit);
  *    per-device basis in /sys/block/<device>/queue/max_sectors_kb.
  *    The soft limit can not exceed max_hw_sectors.
  **/
+/*
+ * 被很多调用, 这里是几个调用的例子:
+ *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+ *   - drivers/block/virtio_blk.c|827| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+ *   - drivers/nvme/host/core.c|1972| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+ *   - drivers/block/loop.c|1968| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+ */
 void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
 {
 	struct queue_limits *limits = &q->limits;
@@ -264,6 +280,14 @@ EXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);
  *    Enables a low level driver to set an upper limit on the number of
  *    hw data segments in a request.
  **/
+/*
+ * 选择的一些地方:
+ *   - drivers/block/virtio_blk.c|824| <<virtblk_probe>> blk_queue_max_segments(q, vblk->sg_elems-2);
+ *   - drivers/block/xen-blkfront.c|954| <<blkif_set_queue_limits>> blk_queue_max_segments(rq, segments / GRANTS_PER_PSEG);
+ *   - drivers/block/xen-blkfront.c|2027| <<blkif_recover>> blk_queue_max_segments(info->rq, segs / GRANTS_PER_PSEG);
+ *   - drivers/nvme/host/core.c|1973| <<nvme_set_queue_limits>> blk_queue_max_segments(q, min_t(u32, max_segments, USHRT_MAX));
+ *   - drivers/scsi/scsi_lib.c|1828| <<__scsi_init_queue>> blk_queue_max_segments(q, min_t(unsigned short , shost->sg_tablesize,
+ */
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
@@ -462,6 +486,11 @@ EXPORT_SYMBOL(blk_queue_io_opt);
  * @t:	the stacking driver (top)
  * @b:  the underlying device (bottom)
  **/
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_nl.c|1377| <<drbd_setup_queue_param>> blk_queue_stack_limits(q, b);
+ *   - drivers/nvme/host/core.c|1649| <<__nvme_revalidate_disk>> blk_queue_stack_limits(ns->head->disk->queue, ns->queue)
+ */
 void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b)
 {
 	blk_stack_limits(&t->limits, &b->limits, 0);
@@ -489,6 +518,12 @@ EXPORT_SYMBOL(blk_queue_stack_limits);
  *    queue_limits will have the misaligned flag set to indicate that
  *    the alignment_offset is undefined.
  */
+/*
+ * called by:
+ *   - block/blk-settings.c|483| <<blk_queue_stack_limits>> blk_stack_limits(&t->limits, &b->limits, 0);
+ *   - block/blk-settings.c|650| <<bdev_stack_limits>> return blk_stack_limits(t, &bq->limits, start);
+ *   - drivers/md/dm-table.c|1541| <<dm_calculate_queue_limits>> if (blk_stack_limits(limits, &ti_limits, 0) < 0)
+ */
 int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 		     sector_t start)
 {
@@ -716,6 +751,10 @@ EXPORT_SYMBOL(blk_queue_update_dma_pad);
  * this routine, you must set the limit to one fewer than your device
  * can support otherwise there won't be room for the drain buffer.
  */
+/*
+ * called only by:
+ *   - drivers/ata/libata-scsi.c|1290| <<ata_scsi_dev_config>> blk_queue_dma_drain(q, atapi_drain_needed, buf, ATAPI_MAX_DRAIN);
+ */
 int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 457d9ba..78878a1 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -95,6 +95,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|604| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -136,6 +140,12 @@ void __blk_complete_request(struct request *req)
 		 * entries there, someone already raised the irq but it
 		 * hasn't run yet.
 		 */
+		/*
+		 * block/blk-softirq.c|154| <<blk_softirq_init>> open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
+		 *
+		 * 对于blk_cpu_done链表上的每一个request调用rq->q->mq_ops->complete(rq)
+		 * 并且删除这个request
+		 */
 		if (list->next == &req->ipi_list)
 			raise_softirq_irqoff(BLOCK_SOFTIRQ);
 	} else if (raise_blk_irq(ccpu, req))
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 696a041..8ca76e2 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -17,6 +17,13 @@ struct blk_queue_stats {
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|197| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|85| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|93| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|145| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -25,6 +32,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|208| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|92| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -39,6 +51,11 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|220| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|72| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -47,6 +64,10 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|627| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -75,6 +96,10 @@ void blk_stat_add(struct request *rq, u64 now)
 	rcu_read_unlock();
 }
 
+/*
+ * used by:
+ *   - block/blk-stat.c|146| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -97,6 +122,11 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3075| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|828| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -131,6 +161,11 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3579| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|851| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -179,6 +214,11 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2450| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|442| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -188,6 +228,10 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|500| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -203,6 +247,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|546| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|857| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 124c261..bf1cef2 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -108,6 +108,11 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|839| <<blk_mq_start_request>> blk_add_timer(rq);
+ *   - block/blk-mq.c|1009| <<blk_mq_rq_timed_out>> blk_add_timer(req);
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -143,6 +148,15 @@ void blk_add_timer(struct request *req)
 		 * modifying the timer because expires for value X
 		 * will be X + something.
 		 */
+		/*
+		 * 注意! 这里的q是request_queue, q->timeout应该是
+		 *
+		 * q->timeout设置的地方:
+		 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+		 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+		 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+		 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+		 */
 		if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
 			mod_timer(&q->timeout, expiry);
 	}
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 07bf2bf..79916c9 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -3843,6 +3843,11 @@ void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1271| <<nvme_passthru_start>> nvme_start_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|2463| <<nvme_dev_disable>> nvme_start_freeze(&dev->ctrl);
+ */
 void nvme_start_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index f54718b..86bcf34 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -2435,6 +2435,20 @@ static void nvme_pci_disable(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1288| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1315| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1331| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2530| <<nvme_remove_dead_ctrl>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2552| <<nvme_reset_work>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2839| <<nvme_reset_prepare>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_shutdown>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2868| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2875| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2892| <<nvme_suspend>> nvme_dev_disable(ndev, true);
+ *   - drivers/nvme/host/pci.c|2924| <<nvme_error_detected>> nvme_dev_disable(dev, false);
+ */
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 {
 	bool dead = true;
@@ -2856,6 +2870,17 @@ static void nvme_shutdown(struct pci_dev *pdev)
  * state. This function must not have any dependencies on the device state in
  * order to proceed.
  */
+/*
+ * 调用的一个例子:
+ * [0] nvme_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] nvme_remove_dead_ctrl_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_remove(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b0c814b..34b5aeb 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,6 +15,14 @@ struct blk_flush_queue;
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 在以下插入新元素:
+		 *   - block/blk-mq-sched.c|189| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1303| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1663| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
@@ -26,28 +34,91 @@ struct blk_mq_hw_ctx {
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
+	/*
+	 * 设置sched_data的地方:
+	 *   - block/kyber-iosched.c|515| <<kyber_init_hctx>> hctx->sched_data = khd;
+	 */
 	void			*sched_data;
 	struct request_queue	*queue;
 	struct blk_flush_queue	*fq;
 
 	void			*driver_data;
 
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-debugfs.c|467| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|142| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|67| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|79| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|991| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1029| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2267| <<blk_mq_exit_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2313| <<blk_mq_init_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2346| <<blk_mq_init_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2512| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 *
+	 * 在以下被设置:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 */
 	struct sbitmap		ctx_map;
 
+	/*
+	 * 用到dispatch_from的地方:
+	 *   - block/blk-mq-sched.c|137| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+	 *   - block/blk-mq-sched.c|166| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+	 *   - block/blk-mq.c|2428| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+	 */
 	struct blk_mq_ctx	*dispatch_from;
+	/*
+	 * 修改dispatch_busy的地方:
+	 *   - block/blk-mq.c|1190| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 */
 	unsigned int		dispatch_busy;
 
 	unsigned short		type;
+	/*
+	 * 在以下被修改:
+	 *   - block/blk-mq.c|2471| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	unsigned short		nr_ctx;
+	/*
+	 * 分配ctxs数组指针的地方:
+	 *   - block/blk-mq.c|2308| <<blk_mq_init_hctx>> hctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),
+	 *
+	 * 设置每个指针元素的地方:
+	 *   - block/blk-mq.c|2471| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;
 	wait_queue_entry_t	dispatch_wait;
+	/*
+	 * used by only:
+	 *   - block/blk-mq-tag.h|80| <<bt_wait_ptr>> return sbq_wait_ptr(bt, &hctx->wait_index);
+	 */
 	atomic_t		wait_index;
 
+	/*
+	 * 在以下设置了tags:
+	 *   - block/blk-mq.c|2302| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+	 *   - block/blk-mq.c|2504| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+	 */
 	struct blk_mq_tags	*tags;
+	/*
+	 * 在以下分配:
+	 *   - block/blk-mq-sched.c|453| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+	 */
 	struct blk_mq_tags	*sched_tags;
 
+	/*
+	 * 在以下进行使用:
+	 *   - block/blk-mq-debugfs.c|607| <<hctx_queued_show>> seq_printf(m, "%lu\n", hctx->queued);
+	 *   - block/blk-mq-debugfs.c|616| <<hctx_queued_write>> hctx->queued = 0;
+	 *   - block/blk-mq.c|418| <<blk_mq_get_request>> data->hctx->queued++;
+	 */
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
@@ -56,14 +127,48 @@ struct blk_mq_hw_ctx {
 	unsigned int		numa_node;
 	unsigned int		queue_num;
 
+	/*
+	 * 是伴随着RQF_MQ_INFLIGHT的:
+	 *   - block/blk-mq-debugfs.c|641| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 *   - block/blk-mq-tag.c|98| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq.c|511| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|751| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|1428| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|3205| <<blk_mq_alloc_and_init_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq.h|290| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 */
 	atomic_t		nr_active;
 	unsigned int		nr_expired;
 
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|2662| <<blk_mq_hctx_notify_dead>> hctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);
+	 *   - block/blk-mq.c|2687| <<blk_mq_remove_cpuhp>> &hctx->cpuhp_dead);
+	 *   - block/blk-mq.c|2742| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
+	 */
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;
 
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|560| <<hctx_io_poll_show>> seq_printf(m, "considered=%lu\n", hctx->poll_considered);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3992| <<blk_poll>> hctx->poll_considered++;
+	 */
 	unsigned long		poll_considered;
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|561| <<hctx_io_poll_show>> seq_printf(m, "invoked=%lu\n", hctx->poll_invoked);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3998| <<blk_poll>> hctx->poll_invoked++;
+	 */
 	unsigned long		poll_invoked;
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|562| <<hctx_io_poll_show>> seq_printf(m, "success=%lu\n", hctx->poll_success);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|4002| <<blk_poll>> hctx->poll_success++;
+	 */
 	unsigned long		poll_success;
 
 #ifdef CONFIG_BLK_DEBUG_FS
@@ -76,6 +181,9 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_queue_map {
+	/*
+	 * 猜测是每一个sw queue对应的hw queue???
+	 */
 	unsigned int *mq_map;
 	unsigned int nr_queues;
 	unsigned int queue_offset;
@@ -97,6 +205,14 @@ struct blk_mq_tag_set {
 	 * share maps between types.
 	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|2713| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3058| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3069| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2322| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2324| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 */
 	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
@@ -216,24 +332,149 @@ struct blk_mq_ops {
 };
 
 enum {
+	/*
+	 * 下面的BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+	 * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+	 *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 */
+	/*
+	 * 主要使用的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - block/blk-mq-sched.c|334| <<__blk_mq_sched_bio_merge>> if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
+	 *
+	 * 某些设置的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - drivers/block/loop.c|1954| <<loop_add>> lo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/null_blk_main.c|1553| <<null_init_tag_set>> set->flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/virtio_blk.c|787| <<virtblk_probe>> vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/xen-blkfront.c|980| <<xlvbd_init_blk_queue>> info->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/nvme/host/pci.c|2334| <<nvme_dev_add>> dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/scsi/scsi_lib.c|1902| <<scsi_mq_setup_tags>> shost->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 */
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x2):
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|88| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|96| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq.c|305| <<blk_mq_rq_ctx_init>> if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
+	 *   - block/blk-mq.c|1109| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|1239| <<blk_mq_dispatch_rq_list>> if (hctx->flags & BLK_MQ_F_TAG_SHARED)
+	 *   - block/blk-mq.c|2298| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2533| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2535| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2561| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2578| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|2579| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2583| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_SHARED)
+	 *
+	 * 设置和删除的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - block/blk-mq.c|2544| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2590| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2309| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2546| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2572| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu
+	 *
+	 * 会设置到BLK_MQ_F_BLOCKING的地方, 非常少: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x20)
+	 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1559| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|910| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|795| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|785| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|413| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|448| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 使用BLK_MQ_F_NO_SCHED的地方, 主要是第一个: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40)
+	 *   - block/blk-mq.c|2891| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
+	 *   - block/elevator.c|691| <<elv_support_iosched>> if (q->tag_set && (q->tag_set->flags & BLK_MQ_F_NO_SCHED))
+	 *
+	 * 设置BLK_MQ_F_NO_SCHED的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40)
+	 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1555| <<null_init_tag_set>> set->flags |= BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/fc.c|3057| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/pci.c|1643| <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/rdma.c|728| <<nvme_rdma_alloc_tagset>> set->flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/target/loop.c|363| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|394| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
+	 *   - include/linux/blk-mq.h|398| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> << BLK_MQ_F_ALLOC_POLICY_START_BIT)
+	 */
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|395| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+	 *   - include/linux/blk-mq.h|397| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> ((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
+	 */
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
+	/*
+	 * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+	 */
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq.c|1550| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1575| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1596| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1620| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 */
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|33| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|57| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|76| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
+	/*
+	 * 使用的一个例子:
+	 *   - block/blk-mq.c|3051| <<blk_mq_alloc_tag_set>> if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
+	 */
 	BLK_MQ_MAX_DEPTH	= 10240,
 
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|2555| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|260| <<hctx_flags_show>> const int alloc_policy = BLK_MQ_FLAG_TO_ALLOC_POLICY(hctx->flags);
+ *   - block/blk-mq.c|2124| <<blk_mq_alloc_rq_map>> BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
+ */
 #define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
 	((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
 		((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|270| <<hctx_flags_show>> hctx->flags ^ BLK_ALLOC_POLICY_TO_MQ_FLAG(alloc_policy),
+ *   - drivers/block/skd_main.c|2846| <<skd_cons_disk>> BLK_ALLOC_POLICY_TO_MQ_FLAG(BLK_TAG_ALLOC_FIFO);
+ *   - drivers/scsi/scsi_lib.c|1904| <<scsi_mq_setup_tags>> BLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);
+ */
 #define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \
 	((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
 		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
@@ -259,11 +500,23 @@ bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 bool blk_mq_queue_inflight(struct request_queue *q);
 
 enum {
+	/*
+	 * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+	 */
 	/* return when out of requests */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 在以下被设置:
+	 *   - block/blk-mq.c|372| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *
+	 * 在以下被使用:
+	 *   - block/blk-mq-tag.c|104| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|297| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
@@ -348,6 +601,9 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/*
+ * 遍历每一个request_queue->queue_hw_ctx[i]
+ */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index d66bf5f..3e92106 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -347,6 +347,13 @@ enum req_flag_bits {
 #define REQ_BACKGROUND		(1ULL << __REQ_BACKGROUND)
 #define REQ_NOWAIT		(1ULL << __REQ_NOWAIT)
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 设置和取消REQ_HIPRI的地方:
+ *   - drivers/nvme/host/core.c|756| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1272| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|838| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|405| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index faed9d9..1509f69 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -62,6 +62,9 @@ typedef void (rq_end_io_fn)(struct request *, blk_status_t);
  * request flags */
 typedef __u32 __bitwise req_flags_t;
 
+/*
+ * RQF_xxx用在request->rq_flags
+ */
 /* elevator knows about this request */
 #define RQF_SORTED		((__force req_flags_t)(1 << 0))
 /* drive already may have started this one */
@@ -73,6 +76,15 @@ typedef __u32 __bitwise req_flags_t;
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
 /* track inflight for MQ */
+/*
+ * 是伴随着hctx->nr_active的, 每次设置的时候, hctx->nr_active就增长
+ * 取消的时候, hctx->nr_active就减少
+ *   - block/blk-mq.c|510| <<blk_mq_rq_ctx_init>> rq_flags = RQF_MQ_INFLIGHT;
+ *   - block/blk-mq.c|750| <<blk_mq_free_request>> if (rq->rq_flags & RQF_MQ_INFLIGHT)
+ *   - block/blk-mq.c|1427| <<blk_mq_get_driver_tag>> rq->rq_flags |= RQF_MQ_INFLIGHT;
+ *   - block/blk-mq.h|288| <<__blk_mq_put_driver_tag>> if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ *   - block/blk-mq.h|289| <<__blk_mq_put_driver_tag>> rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ */
 #define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
 /* don't call prep for this one */
 #define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
@@ -114,9 +126,37 @@ typedef __u32 __bitwise req_flags_t;
 /*
  * Request state for blk-mq.
  */
+/*
+ * MQ_RQ_xxx用在request->state
+ */
 enum mq_rq_state {
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|536| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|719| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2127| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|672| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *   - block/blk-mq.c|693| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|696| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|825| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|863| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|593| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|3389| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -128,12 +168,31 @@ enum mq_rq_state {
  */
 struct request {
 	struct request_queue *q;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-flush.c|297| <<blk_kick_flush>> flush_rq->mq_ctx = first_rq->mq_ctx;
+	 *   - block/blk-mq.c|316| <<blk_mq_rq_ctx_init>> rq->mq_ctx = data->ctx;
+	 */
 	struct blk_mq_ctx *mq_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-flush.c|298| <<blk_kick_flush>> flush_rq->mq_hctx = first_rq->mq_hctx;
+	 *   - block/blk-mq.c|317| <<blk_mq_rq_ctx_init>> rq->mq_hctx = data->hctx;
+	 *   - block/blk-mq.c|502| <<__blk_mq_free_request>> rq->mq_hctx = NULL;
+	 */
 	struct blk_mq_hw_ctx *mq_hctx;
 
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 
+	/*
+	 * 在以下修改:
+	 *   - block/blk-core.c|116| <<blk_rq_init>> rq->internal_tag = -1;
+	 *   - block/blk-flush.c|224| <<flush_end_io>> flush_rq->internal_tag = -1;
+	 *   - block/blk-flush.c|305| <<blk_kick_flush>> flush_rq->internal_tag = first_rq->internal_tag;
+	 *   - block/blk-mq.c|303| <<blk_mq_rq_ctx_init>> rq->internal_tag = tag;
+	 *   - block/blk-mq.c|310| <<blk_mq_rq_ctx_init>> rq->internal_tag = -1;
+	 */
 	int internal_tag;
 
 	/* the following two fields are internal, NEVER access directly */
@@ -317,9 +376,34 @@ struct queue_limits {
 	unsigned long		seg_boundary_mask;
 	unsigned long		virt_boundary_mask;
 
+	/*
+	 * max_hw_sectors和max_sectors设置的地方主要是blk_queue_max_hw_sectors()
+	 *
+	 * 调用blk_queue_max_hw_sectors()的地方很多, 例子是:
+	 *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+	 *   - drivers/block/virtio_blk.c|827| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+	 *   - drivers/nvme/host/core.c|1972| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+	 *   - drivers/block/loop.c|1968| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+	 */
 	unsigned int		max_hw_sectors;
+	/*
+	 * max_dev_sectors在以下被修改:
+	 *   - block/blk-settings.c|54| <<blk_set_default_limits>> lim->max_dev_sectors = 0;
+	 *   - block/blk-settings.c|90| <<blk_set_stacking_limits>> lim->max_dev_sectors = UINT_MAX;
+	 *   - block/blk-settings.c|526| <<blk_stack_limits>> t->max_dev_sectors = min_not_zero(t->max_dev_sectors, b->max_dev_sectors);
+	 *   - drivers/s390/block/dasd.c|3155| <<dasd_setup_queue>> q->limits.max_dev_sectors = max;
+	 *   - drivers/scsi/sd.c|3120| <<sd_revalidate_disk>> q->limits.max_dev_sectors = logical_to_sectors(sdp, dev_max);
+	 */
 	unsigned int		max_dev_sectors;
 	unsigned int		chunk_sectors;
+	/*
+	 * 设置max_sectors的地方:
+	 *   - block/blk-settings.c|53| <<blk_set_default_limits>> lim->max_sectors = lim->max_hw_sectors = BLK_SAFE_MAX_SECTORS;
+	 *   - block/blk-settings.c|89| <<blk_set_stacking_limits>> lim->max_sectors = UINT_MAX;
+	 *   - block/blk-settings.c|211| <<blk_queue_max_hw_sectors>> limits->max_sectors = max_sectors;
+	 *   - block/blk-settings.c|524| <<blk_stack_limits>> t->max_sectors = min_not_zero(t->max_sectors, b->max_sectors);
+	 *   - block/blk-sysfs.c|239| <<queue_max_sectors_store>> q->limits.max_sectors = max_sectors_kb << 1;
+	 */
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
 	unsigned int		physical_block_size;
@@ -470,6 +554,13 @@ struct request_queue {
 	 */
 	unsigned long		nr_requests;	/* Max # of requests */
 
+	/*
+	 * 设置dma_drain_size的地方:
+	 *   - block/blk-settings.c|764| <<blk_queue_dma_drain>> q->dma_drain_size = size;
+	 *   - drivers/ata/libata-scsi.c|1390| <<ata_scsi_slave_destroy>> q->dma_drain_size = 0;
+	 *
+	 * 似乎目前只被libata-scsi使用
+	 */
 	unsigned int		dma_drain_size;
 	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
@@ -481,7 +572,25 @@ struct request_queue {
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
+	/*
+	 * 使用的主要地方:
+	 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+	 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+	 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+	 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+	 */
 	struct timer_list	timeout;
+	/*
+	 * 可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+	 *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+	 *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+	 *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 */
 	struct work_struct	timeout_work;
 
 	struct list_head	icq_list;
@@ -537,6 +646,11 @@ struct request_queue {
 
 	struct mutex		sysfs_lock;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 */
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -548,7 +662,40 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|527| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_queue_head_t	mq_freeze_wq;
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|406| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-core.c|415| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|447| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|453| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|544| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-mq-tag.c|401| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|305| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *   - block/blk-mq.c|319| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|327| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-mq.c|366| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-mq.c|1245| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *   - block/blk-sysfs.c|924| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 *   - block/blk.h|66| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|617| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|627| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 */
 	struct percpu_ref	q_usage_counter;
 	struct list_head	all_q_node;
 
@@ -572,6 +719,13 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
+/*
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+/*
+ * used by:
+ *   - include/linux/blkdev.h|678| <<blk_queue_stopped>> #define blk_queue_stopped(q) test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_BIDI		2	/* queue supports bidi requests */
@@ -592,10 +746,22 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 使用的例子:
+ *   - block/blk-mq.c|791| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|185| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|195| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|226| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * used by:
+ *   - block/blk-mq.c|278| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|320| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
-- 
2.7.4

