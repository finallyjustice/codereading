From c218f11474748e2d58dd31cdc2805a2be779f747 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 1 Apr 2019 16:02:37 +0800
Subject: [PATCH 1/1] block comment for block and drivers for
 linux-block:for-5.1/block-20190302

This is for linux-block: for-5.1/block-20190302

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/bio.c                     |    3 +
 block/blk-core.c                |  156 ++++++
 block/blk-flush.c               |  426 ++++++++++++++++
 block/blk-merge.c               |    4 +
 block/blk-mq-cpumap.c           |   15 +
 block/blk-mq-debugfs.c          |   12 +
 block/blk-mq-pci.c              |    7 +
 block/blk-mq-sched.c            |  228 +++++++++
 block/blk-mq-sched.h            |   22 +
 block/blk-mq-sysfs.c            |   84 ++++
 block/blk-mq-tag.c              |  332 ++++++++++++
 block/blk-mq-tag.h              |   95 ++++
 block/blk-mq-virtio.c           |   11 +
 block/blk-mq.c                  | 1062 +++++++++++++++++++++++++++++++++++++++
 block/blk-mq.h                  |  104 +++-
 block/blk-settings.c            |   39 ++
 block/blk-softirq.c             |   10 +
 block/blk-stat.c                |   49 ++
 block/blk-sysfs.c               |   46 ++
 block/blk-timeout.c             |   14 +
 block/blk.h                     |   48 ++
 block/genhd.c                   |    5 +
 drivers/block/virtio_blk.c      |    3 +
 drivers/nvme/host/core.c        |  118 +++++
 drivers/nvme/host/nvme.h        |   11 +
 drivers/nvme/host/pci.c         |  359 +++++++++++++
 drivers/scsi/scsi_lib.c         |   21 +
 include/linux/blk-mq.h          |  342 +++++++++++++
 include/linux/blk_types.h       |   19 +
 include/linux/blkdev.h          |  302 +++++++++++
 include/linux/percpu-refcount.h |  223 ++++++++
 include/linux/sbitmap.h         |   45 ++
 lib/kobject.c                   |    5 +
 lib/percpu-refcount.c           |  123 +++++
 lib/sbitmap.c                   |   89 ++++
 35 files changed, 4431 insertions(+), 1 deletion(-)

diff --git a/block/bio.c b/block/bio.c
index 83a2dfa..2175b00 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -931,6 +931,9 @@ int submit_bio_wait(struct bio *bio)
 	DECLARE_COMPLETION_ONSTACK_MAP(done, bio->bi_disk->lockdep_map);
 
 	bio->bi_private = &done;
+	/*
+	 * submit_bio_wait_endio()在上面定义, 用来complete bi_private的done
+	 */
 	bio->bi_end_io = submit_bio_wait_endio;
 	bio->bi_opf |= REQ_SYNC;
 	submit_bio(bio);
diff --git a/block/blk-core.c b/block/blk-core.c
index 6b78ec5..2e2b38f 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -65,6 +65,13 @@ struct kmem_cache *blk_requestq_cachep;
 /*
  * Controlling structure to kblockd
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1696| <<kblockd_schedule_work>> return queue_work(kblockd_workqueue, work);
+ *   - block/blk-core.c|1702| <<kblockd_schedule_work_on>> return queue_work_on(cpu, kblockd_workqueue, work);
+ *   - block/blk-core.c|1709| <<kblockd_mod_delayed_work_on>> return mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);
+ *   - block/blk-core.c|1838| <<blk_dev_init>> kblockd_workqueue = alloc_workqueue("kblockd",
+ */
 static struct workqueue_struct *kblockd_workqueue;
 
 /**
@@ -103,6 +110,11 @@ bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_queue_flag_test_and_set);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|287| <<blk_kick_flush>> blk_rq_init(q, flush_rq);
+ *   - drivers/scsi/scsi_error.c|2328| <<scsi_ioctl_reset>> blk_rq_init(NULL, rq);    
+ */
 void blk_rq_init(struct request_queue *q, struct request *rq)
 {
 	memset(rq, 0, sizeof(*rq));
@@ -228,16 +240,55 @@ EXPORT_SYMBOL(blk_dump_rq_flags);
  *     and blkcg_exit_queue() to be called with queue lock initialized.
  *
  */
+/*
+ * called by:
+ *   - block/blk-core.c|409| <<blk_cleanup_queue>> blk_sync_queue(q);
+ *   - drivers/md/md.c|5878| <<mddev_detach>> blk_sync_queue(mddev->queue);
+ */
 void blk_sync_queue(struct request_queue *q)
 {
+	/*
+	 * 使用的主要地方:
+	 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+	 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+	 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+	 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+	 */
 	del_timer_sync(&q->timeout);
+	/*
+	 * 可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+	 *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+	 *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+	 *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 */
 	cancel_work_sync(&q->timeout_work);
 
 	if (queue_is_mq(q)) {
 		struct blk_mq_hw_ctx *hctx;
 		int i;
 
+		/*
+		 * q->requeue_work使用的地方:
+		 *   - block/blk-core.c|252| <<blk_sync_queue>> cancel_delayed_work_sync(&q->requeue_work);
+		 *   - block/blk-mq.c|1245| <<blk_mq_requeue_work>> container_of(work, struct request_queue, requeue_work.work);
+		 *   - block/blk-mq.c|1310| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+		 *   - block/blk-mq.c|1317| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+		 *   - block/blk-mq.c|3781| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+		 */
 		cancel_delayed_work_sync(&q->requeue_work);
+		/*
+		 * hctx->run_work使用的地方:
+		 *   - block/blk-core.c|254| <<blk_sync_queue>> cancel_delayed_work_sync(&hctx->run_work);
+		 *   - block/blk-mq.c|2171| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+		 *   - block/blk-mq.c|2273| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+		 *   - block/blk-mq.c|2344| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+		 *   - block/blk-mq.c|3106| <<blk_mq_init_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+		 */
 		queue_for_each_hw_ctx(q, hctx, i)
 			cancel_delayed_work_sync(&hctx->run_work);
 	}
@@ -265,12 +316,25 @@ void blk_clear_pm_only(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_clear_pm_only);
 
+/*
+ * 因为kobject_init(&q->kobj, &blk_queue_ktype)用的blk_queue_ktype,
+ * 所以当put这个q->kobj的时候间接调用blk_release_queue()
+ */
 void blk_put_queue(struct request_queue *q)
 {
 	kobject_put(&q->kobj);
 }
 EXPORT_SYMBOL(blk_put_queue);
 
+/*
+ * called by:
+ *   - block/blk-core.c|345| <<blk_cleanup_queue>> blk_set_queue_dying(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|4318| <<mtip_pci_remove>> blk_set_queue_dying(dd->queue);
+ *   - drivers/block/rbd.c|6036| <<do_rbd_remove>> blk_set_queue_dying(rbd_dev->disk->queue);
+ *   - drivers/md/dm.c|2346| <<__dm_destroy>> blk_set_queue_dying(md->queue);
+ *   - drivers/nvme/host/core.c|115| <<nvme_set_queue_dying>> blk_set_queue_dying(ns->queue);
+ *   - drivers/nvme/host/multipath.c|600| <<nvme_mpath_remove_disk>> blk_set_queue_dying(head->disk->queue);
+ */
 void blk_set_queue_dying(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_DYING, q);
@@ -280,6 +344,7 @@ void blk_set_queue_dying(struct request_queue *q)
 	 * entering queue, so we call blk_freeze_queue_start() to
 	 * prevent I/O from crossing blk_queue_enter().
 	 */
+	/* 正如上面注释写的, 这里防止其他越过blk_queue_enter() */
 	blk_freeze_queue_start(q);
 
 	if (queue_is_mq(q))
@@ -326,6 +391,23 @@ void blk_exit_queue(struct request_queue *q)
  * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and
  * put it.  All future requests will be failed immediately with -ENODEV.
  */
+/*
+ * 一些调用的例子:
+ *   - drivers/block/loop.c|2032| <<loop_remove>> blk_cleanup_queue(lo->lo_queue);
+ *   - drivers/block/null_blk_main.c|1391| <<null_del_dev>> blk_cleanup_queue(nullb->q);
+ *   - drivers/block/virtio_blk.c|929| <<virtblk_remove>> blk_cleanup_queue(vblk->disk->queue);
+ *   - drivers/block/xen-blkfront.c|1210| <<xlvbd_release_gendisk>> blk_cleanup_queue(info->rq);
+ *   - drivers/md/dm.c|1876| <<cleanup_mapped_device>> blk_cleanup_queue(md->queue);
+ *   - drivers/md/md.c|5215| <<md_free>> blk_cleanup_queue(mddev->queue);
+ *   - drivers/nvme/host/core.c|3374| <<nvme_ns_remove>> blk_cleanup_queue(ns->queue);
+ *   - drivers/nvme/host/pci.c|1809| <<nvme_dev_remove_admin>> blk_cleanup_queue(dev->ctrl.admin_q);
+ *   - drivers/scsi/scsi_sysfs.c|1400| <<__scsi_remove_device>> blk_cleanup_queue(sdev->request_queue);
+ *
+ * The blk_cleanup_queue() function is
+ * responsible for stopping all code that can run the queue after all
+ * requests have finished and before destruction of the data structures
+ * needed for request processing starts.
+ */
 void blk_cleanup_queue(struct request_queue *q)
 {
 	/* mark @q DYING, no new request or merges will be allowed afterwards */
@@ -341,6 +423,9 @@ void blk_cleanup_queue(struct request_queue *q)
 	 * Drain all requests queued before DYING marking. Set DEAD flag to
 	 * prevent that q->request_fn() gets invoked after draining finished.
 	 */
+	/*
+	 * 过了这里可以认为所有q_usage_counter的get都put了
+	 */
 	blk_freeze_queue(q);
 
 	rq_qos_exit(q);
@@ -374,6 +459,7 @@ void blk_cleanup_queue(struct request_queue *q)
 
 	blk_exit_queue(q);
 
+	/* 如果q->mq_ops存在 */
 	if (queue_is_mq(q))
 		blk_mq_free_queue(q);
 
@@ -395,6 +481,16 @@ EXPORT_SYMBOL(blk_alloc_queue);
  * @q: request queue pointer
  * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PREEMPT
  */
+/*
+ * called by:
+ *   - block/blk-core.c|1126| <<generic_make_request>> else if (blk_queue_enter(q, flags) < 0) {
+ *   - block/blk-core.c|1179| <<generic_make_request>> if (blk_queue_enter(q, flags) < 0) {
+ *   - block/blk-core.c|1244| <<direct_make_request>> if (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {
+ *   - block/blk-mq.c|778| <<blk_mq_alloc_request>> ret = blk_queue_enter(q, flags);
+ *   - block/blk-mq.c|821| <<blk_mq_alloc_request_hctx>> ret = blk_queue_enter(q, flags);
+ *   - fs/block_dev.c|720| <<bdev_read_page>> result = blk_queue_enter(bdev->bd_queue, 0);
+ *   - fs/block_dev.c|757| <<bdev_write_page>> result = blk_queue_enter(bdev->bd_queue, 0);
+ */
 int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 {
 	const bool pm = flags & BLK_MQ_REQ_PREEMPT;
@@ -403,6 +499,16 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 		bool success = false;
 
 		rcu_read_lock();
+		/*
+		 * 如果ref->percpu_count_ptr的最后两位有__PERCPU_REF_DEAD被设置
+		 * 函数返回false
+		 *
+		 * 如果ref->percpu_count_ptr的最后两位有__PERCPU_REF_ATOMIC被设置
+		 * 则用atomic_long_inc_not_zero(&ref->count)增加ref->count
+		 *
+		 * 否则用this_cpu_inc(*percpu_count)增加ref->percpu_count_ptr
+		 * 并且返回true
+		 */
 		if (percpu_ref_tryget_live(&q->q_usage_counter)) {
 			/*
 			 * The code that increments the pm_only counter is
@@ -432,6 +538,17 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 		 */
 		smp_rmb();
 
+		/*
+		 * 使用的例子:
+		 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+		 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|527| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+		 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+		 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+		 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+		 */
 		wait_event(q->mq_freeze_wq,
 			   (atomic_read(&q->mq_freeze_depth) == 0 &&
 			    (pm || (blk_pm_request_resume(q),
@@ -442,16 +559,50 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1154| <<generic_make_request>> blk_queue_exit(q);
+ *   - block/blk-core.c|1200| <<generic_make_request>> blk_queue_exit(q);
+ *   - block/blk-core.c|1234| <<direct_make_request>> blk_queue_exit(q);
+ *   - block/blk-mq-tag.c|692| <<blk_mq_queue_tag_busy_iter>> blk_queue_exit(q);
+ *   - block/blk-mq.c|730| <<blk_mq_get_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|765| <<blk_mq_alloc_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|813| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|820| <<blk_mq_alloc_request_hctx>> blk_queue_exit(q);
+ *   - block/blk-mq.c|853| <<__blk_mq_free_request>> blk_queue_exit(q);
+ *   - block/blk-mq.c|1442| <<blk_mq_timeout_work>> blk_queue_exit(q);
+ *   - fs/block_dev.c|725| <<bdev_read_page>> blk_queue_exit(bdev->bd_queue);
+ *   - fs/block_dev.c|770| <<bdev_write_page>> blk_queue_exit(bdev->bd_queue);
+ *
+ * 就是调用percpu_ref_put(&q->q_usage_counter);
+ */
 void blk_queue_exit(struct request_queue *q)
 {
 	percpu_ref_put(&q->q_usage_counter);
 }
 
+/*
+ * 用作q->q_usage_counter的release函数:
+ *   - block/blk-core.c|614| <<blk_alloc_queue_node>> blk_queue_usage_counter_release,
+ */
 static void blk_queue_usage_counter_release(struct percpu_ref *ref)
 {
 	struct request_queue *q =
 		container_of(ref, struct request_queue, q_usage_counter);
 
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|527| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 *
+	 * blk_queue_usage_counter_release()被调用说明所有的都put了
+	 */
 	wake_up_all(&q->mq_freeze_wq);
 }
 
@@ -496,6 +647,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	if (!q->backing_dev_info)
 		goto fail_split;
 
+	/* struct blk_queue_stats */
 	q->stats = blk_alloc_queue_stats();
 	if (!q->stats)
 		goto fail_stats;
@@ -515,6 +667,10 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	INIT_LIST_HEAD(&q->blkg_list);
 #endif
 
+	/*
+	 * 因为kobject_init(&q->kobj, &blk_queue_ktype)用的blk_queue_ktype,
+	 * 所以当put这个q->kobj的时候间接调用blk_release_queue()
+	 */
 	kobject_init(&q->kobj, &blk_queue_ktype);
 
 #ifdef CONFIG_BLK_DEV_IO_TRACE
diff --git a/block/blk-flush.c b/block/blk-flush.c
index 6e0f2d9..8934474 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -76,6 +76,120 @@
 #include "blk-mq-tag.h"
 #include "blk-mq-sched.h"
 
+/*
+ * 硬盘在控制器上的一块内存芯片,其类型一般以SDRAM为主,具有极快的存取速度,
+ * 它是硬盘内部存储和外界接口之间的缓冲器.由于硬盘的内部数据传输速度和外界
+ * 介面传输速度不同,缓存在其中起到一个缓冲的作用.缓存的大小与速度是直接关
+ * 系到硬盘的传输速度的重要因素,能够大幅度地提高硬盘整体性能.
+ *
+ * 如果硬盘的cache启用了,那么很有可能写入的数据是写到了硬盘的cache中,而没
+ * 有真正写到磁盘介质上.
+ *
+ * 在linux下,查看磁盘cache是否开启可通过hdparm命令:
+ *
+ * #hdparm -W /dev/sdx    //是否开启cache，1为enable
+ * #hdparm -W 0 /dev/sdx  //关闭cache
+ * #hdparm -W 1 /dev/sdx  //enable cache
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * blk_insert_flush()是非常重要的入口!
+ */
+
+/*
+ * 冲刷的过程中request_queue为什么要使用双缓冲队列来存放fs_request?
+ *
+ * 双缓冲队列可以做到只执行一次冲刷请求就可以完成多个fs_request的冲刷要求.队列自
+ * 带的冲刷request在执行的过程中,blk_insert_flush()可以被调用多次,来自上层的
+ * fs_request被添加到pending1队列,等待冲刷request的下一次执行,当冲刷requst可以再
+ * 次被执行时,pending1队列不再接收新的fs_request(fs_request被加入到pending2队列),
+ * 冲刷request执行完毕后,pending1队列所有的fs_request的PREFLUSH/POSTFLUSH执行完毕.
+ */
+
+/*
+ * 设置write back的一些地方:
+ *   - block/blk-settings.c|865| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-settings.c|867| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q);
+ *   - block/blk-sysfs.c|514| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-sysfs.c|516| <<queue_wc_store>> blk_queue_flag_clear(QUEUE_FLAG_WC, q);
+ *
+ *   - drivers/block/loop.c|964| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/null_blk_main.c|1680| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - drivers/block/virtio_blk.c|606| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/nvme/host/core.c|2031| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|315| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/scsi/sd.c|153| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ */
+
+/*
+ * 假设三个都要做:
+ *
+ * blk_insert_flush()
+ *
+ * rq->flush.saved_end_io保存原来的rq->end_io
+ * 把rq->end_io设置为mq_flush_data_end_io()
+ *
+ * spin_lock_irq(&fq->mq_flush_lock);
+ * 调用blk_flush_complete_seq()
+ * 一开始是REQ_FSEQ_PREFLUSH, 把request放入fq->flush_queue[fq->flush_pending_idx]
+ * 此时flush_pending_idx == flush_running_idx
+ * 调用blk_kick_flush(q, fq, cmd_flags)把request从fq->flush_queue[fq->flush_pending_idx]获得(不取出)
+ * 反转fq->flush_pending_idx ^= 1, 然后flush_pending_idx就不等于flush_running_idx了
+ * 设置对应的预留的flush_rq, cmd_flags设置上REQ_OP_FLUSH | REQ_PREFLUSH;
+ * flush_rq->end_io设置成flush_end_io
+ * 下发!!!
+ * spin_unlock_irq(&fq->mq_flush_lock);
+ *
+ *
+ * ---> possible race :)
+ *
+ *
+ * 下发成功后flush_end_io()被调用
+ * spin_lock_irqsave(&fq->mq_flush_lock, flags);
+ * 此时flush_pending_idx不等于flush_running_idx了
+ * 反转fq->flush_running_idx, 这样flush_pending_idx == flush_running_idx了
+ *
+ * 把fq->flush_queue[反转前的fq->flush_running_idx]的每一个获得(不取出), 先讨论一个的情况吧
+ * 调用blk_flush_complete_seq()
+ * 这次是REQ_FSEQ_DATA, 把request从刚才的fq->flush_queue[反转前的fq->flush_running_idx]放入fq->flush_data_in_flight
+ * 调用blk_flush_queue_rq()下发request 
+ * 再次进入blk_kick_flush(), 此时fq->flush_queue[fq->flush_pending_idx]是空的(第二个list了), 所以直接return
+ * spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+ *
+ *
+ * ---> possible race :)
+ *
+ *
+ * 下发成功后mq_flush_data_end_io()被调用
+ * spin_lock_irqsave(&fq->mq_flush_lock, flags);
+ * 调用blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);
+ * 这时是REQ_FSEQ_POSTFLUSH, 把request放入fq->flush_queue[fq->flush_pending_idx]
+ * 此时flush_pending_idx == flush_running_idx
+ * 调用blk_kick_flush(q, fq, cmd_flags)把request从fq->flush_queue[fq->flush_pending_idx]获得(不取出)
+ * 反转fq->flush_pending_idx ^= 1, 然后flush_pending_idx就不等于flush_running_idx了
+ * 设置对应的预留的flush_rq, cmd_flags设置上REQ_OP_FLUSH | REQ_PREFLUSH;
+ * flush_rq->end_io设置成flush_end_io
+ * 下发!!!
+ * spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+ *
+ *
+ * ---> possible race :)
+ *
+ *
+ * 下发成功后flush_end_io()被调用
+ * spin_lock_irqsave(&fq->mq_flush_lock, flags); 
+ * 此时flush_pending_idx不等于flush_running_idx了
+ * 反转fq->flush_running_idx, 这样flush_pending_idx == flush_running_idx了
+ *
+ * 把fq->flush_queue[反转前的fq->flush_running_idx]的每一个获得(不取出), 先讨论一个的情况吧
+ * 调用blk_flush_complete_seq() 
+ * 这次是REQ_FSEQ_DONE, 直接从所有list删掉, 然后blk_mq_end_request()!!
+ * spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+ */
+
 /* PREFLUSH/FUA sequences */
 enum {
 	REQ_FSEQ_PREFLUSH	= (1 << 0), /* pre-flushing in progress */
@@ -96,16 +210,61 @@ enum {
 static void blk_kick_flush(struct request_queue *q,
 			   struct blk_flush_queue *fq, unsigned int flags);
 
+/*
+ * called only by:
+ *   - block/blk-flush.c|422| <<blk_insert_flush>> unsigned int policy = blk_flush_policy(fflags, rq);
+ *
+ * 如果有数据policy要有REQ_FSEQ_DATA
+ * 如果request->cmd_flags有REQ_PREFLUSH, 则要在policy设置REQ_FSEQ_PREFLUSH
+ * 如果request->cmd_flags有REQ_FUA但是设备不支持QUEUE_FLAG_FUA, 还要在policy设置REQ_FSEQ_POSTFLUSH
+ */
 static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 {
+	/*
+	 * If a request doesn't have data, only REQ_PREFLUSH makes sense, which
+	 * indicates a simple flush request.  If there is data, REQ_PREFLUSH indicates
+	 * that the device cache should be flushed before the data is executed, and
+	 * REQ_FUA means that the data must be on non-volatile media on request
+	 * completion.
+	 *
+	 * If the device doesn't have writeback cache, PREFLUSH and FUA don't make any
+	 * difference.  The requests are either completed immediately if there's no data
+	 * or executed as normal requests otherwise.
+	 *
+	 * If the device has writeback cache and supports FUA, REQ_PREFLUSH is
+	 * translated to PREFLUSH but REQ_FUA is passed down directly with DATA.
+	 *
+	 * If the device has writeback cache and doesn't support FUA, REQ_PREFLUSH
+	 * is translated to PREFLUSH and REQ_FUA to POSTFLUSH.
+	 */
 	unsigned int policy = 0;
 
+	/* 如果有数据policy要有REQ_FSEQ_DATA */
 	if (blk_rq_sectors(rq))
 		policy |= REQ_FSEQ_DATA;
 
+	/*
+	 * Write back is a storage method in which data is written into the
+	 * cache every time a change occurs, but is written into the
+	 * corresponding location in main memory only at specified
+	 * intervals or under certain conditions.
+	 */
+
+	/*
+	 * 如果硬盘本身不支持WC (write back cache)
+	 * 就没有设置任何的意义
+	 */
+
 	if (fflags & (1UL << QUEUE_FLAG_WC)) {
 		if (rq->cmd_flags & REQ_PREFLUSH)
 			policy |= REQ_FSEQ_PREFLUSH;
+		/*
+		 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替
+		 *
+		 * 注意: 这里没有REQ_POSTFLUSH的定义
+		 *       下发的request的cmd_flags有REQ_PREFLUSH和REQ_FUA
+		 *       但是没有REQ_POSTFLUSH!
+		 */
 		if (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&
 		    (rq->cmd_flags & REQ_FUA))
 			policy |= REQ_FSEQ_POSTFLUSH;
@@ -113,11 +272,40 @@ static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 	return policy;
 }
 
+/*
+ * 定义struct request的地方
+ * include/linux/blkdev.h
+ *
+ * 238         union {
+ * 239                 struct {
+ * 240                         struct io_cq            *icq;
+ * 241                         void                    *priv[2];
+ * 242                 } elv;
+ * 243 
+ * 244                 struct {
+ * 245                         unsigned int            seq;
+ * 246                         struct list_head        list;
+ * 247                         rq_end_io_fn            *saved_end_io;
+ * 248                 } flush;
+ * 249         };
+ */
+/*
+ * called by:
+ *   - block/blk-flush.c|277| <<blk_flush_complete_seq>> seq = blk_flush_cur_seq(rq);
+ *   - block/blk-flush.c|376| <<flush_end_io>> unsigned int seq = blk_flush_cur_seq(rq);
+ *
+ * 返回1 << ffz(rq->flush.seq)
+ * find first zero bit in word!!!!
+ */
 static unsigned int blk_flush_cur_seq(struct request *rq)
 {
 	return 1 << ffz(rq->flush.seq);
 }
 
+/*
+ * called by only:
+ *   - block/blk-flush.c|345| <<blk_flush_complete_seq>> blk_flush_restore_request(rq);
+ */
 static void blk_flush_restore_request(struct request *rq)
 {
 	/*
@@ -132,6 +320,13 @@ static void blk_flush_restore_request(struct request *rq)
 	rq->end_io = rq->flush.saved_end_io;
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|209| <<blk_flush_complete_seq>> blk_flush_queue_rq(rq, true);
+ *   - block/blk-flush.c|339| <<blk_kick_flush>> blk_flush_queue_rq(flush_rq, false);
+ *
+ * 把request插入q->requeue_list, 调用blk_mq_kick_requeue_list(q)
+ */
 static void blk_flush_queue_rq(struct request *rq, bool add_front)
 {
 	blk_mq_add_to_requeue_list(rq, add_front, true);
@@ -153,18 +348,44 @@ static void blk_flush_queue_rq(struct request *rq, bool add_front)
  * RETURNS:
  * %true if requests were added to the dispatch queue, %false otherwise.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|270| <<flush_end_io>> blk_flush_complete_seq(rq, fq, seq, error);
+ *   - block/blk-flush.c|374| <<mq_flush_data_end_io>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);
+ *   - block/blk-flush.c|464| <<blk_insert_flush>> blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
+ *
+ * flush_end_io()用在REQ_FSEQ_PREFLUSH和REQ_FSEQ_POSTFLUSH
+ * mq_flush_data_end_io()用在REQ_FSEQ_DATA
+ * blk_insert_flush()是最重要的入口
+ *
+ * 参数的seq是可以跳过的步骤, 如果结果是0则说明没有可以跳过的步骤
+ */
 static void blk_flush_complete_seq(struct request *rq,
 				   struct blk_flush_queue *fq,
 				   unsigned int seq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
+	/*
+	 * fq->flush_pending_idx只在以下修改:
+	 *   - block/blk-flush.c|327| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
 	unsigned int cmd_flags;
 
 	BUG_ON(rq->flush.seq & seq);
+	/*
+	 * 参数的seq是可以跳过的步骤, 如果结果是0则说明没有可以跳过的步骤
+	 *
+	 * 将已完成步骤更新到rq->flush.seq中,后面调用blk_flush_cur_seq()看下一步是哪步
+	 */
 	rq->flush.seq |= seq;
 	cmd_flags = rq->cmd_flags;
 
+	/*
+	 * 调用blk_flush_cur_seq()看下一步是哪步
+	 * 返回1 << ffz(rq->flush.seq)
+	 * find first zero bit in word!!!!
+	 */
 	if (likely(!error))
 		seq = blk_flush_cur_seq(rq);
 	else
@@ -176,23 +397,53 @@ static void blk_flush_complete_seq(struct request *rq,
 		/* queue for flush */
 		if (list_empty(pending))
 			fq->flush_pending_since = jiffies;
+		/* 把前者从自己本身的队列删除掉, 加入后面的队列 */
 		list_move_tail(&rq->flush.list, pending);
 		break;
 
 	case REQ_FSEQ_DATA:
+		/* 把前者从自己本身的队列删除掉, 加入后面的队列 */
 		list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+		/* 把request插入q->requeue_list, 调用blk_mq_kick_requeue_list(q) */
 		blk_flush_queue_rq(rq, true);
 		break;
 
 	case REQ_FSEQ_DONE:
 		/*
+		 * [0] blk_flush_complete_seq+0x31a/0x330
+		 * [0] flush_end_io+0x116/0x1a0
+		 * [0] scsi_end_request+0x8c/0x150
+		 * [0] scsi_io_completion+0x51/0x540
+		 * [0] blk_done_softirq+0x7e/0xb0
+		 * [0] __do_softirq+0xf2/0x2c7
+		 * [0] irq_exit+0xa3/0xb0
+		 * [0] call_function_single_interrupt+0xf/0x20
+		 *
+		 * [0] blk_flush_complete_seq+0x2b1/0x340
+		 * [0] flush_end_io+0x116/0x1a0
+		 * [0] blk_mq_complete_request+0xdc/0xf0
+		 * [0] virtblk_done+0x67/0xf0
+		 * [0] vring_interrupt+0x55/0x80
+		 * [0] __handle_irq_event_percpu+0x76/0x180
+		 * [0] handle_irq_event_percpu+0x2b/0x70
+		 * [0] handle_irq_event
+		 * [0] handle_edge_irq
+		 * [0] handle_irq
+		 * [0] do_IRQ
+		 * [0] common_interrupt
+		 */
+		/*
 		 * @rq was previously adjusted by blk_flush_issue() for
 		 * flush sequencing and may already have gone through the
 		 * flush data request completion path.  Restore @rq for
 		 * normal completion and end it.
 		 */
 		BUG_ON(!list_empty(&rq->queuelist));
+		/* 从自己本身的队列删除掉 */
 		list_del_init(&rq->flush.list);
+		/*
+		 * 唯一调用blk_flush_restore_request()的地方
+		 */
 		blk_flush_restore_request(rq);
 		blk_mq_end_request(rq, error);
 		break;
@@ -204,6 +455,13 @@ static void blk_flush_complete_seq(struct request *rq,
 	blk_kick_flush(q, fq, cmd_flags);
 }
 
+/*
+ * used by:
+ *   - block/blk-flush.c|354| <<blk_kick_flush>> flush_rq->end_io = flush_end_io;
+ *
+ * .end_io()就一个地方调用:
+ *   - block/blk-mq.c|884| <<__blk_mq_end_request>> rq->end_io(rq, error);
+ */
 static void flush_end_io(struct request *flush_rq, blk_status_t error)
 {
 	struct request_queue *q = flush_rq->q;
@@ -224,6 +482,10 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
 		flush_rq->internal_tag = -1;
 	}
 
+	/*
+	 * flush_running_idx在以下修改:
+	 *   - block/blk-flush.c|294| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 */
 	running = &fq->flush_queue[fq->flush_running_idx];
 	BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
 
@@ -232,9 +494,16 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
 
 	/* and push the waiting requests to the next stage */
 	list_for_each_entry_safe(rq, n, running, flush.list) {
+		/*
+		 * 返回1 << ffz(rq->flush.seq)
+		 * find first zero bit in word!!!!
+		 */
 		unsigned int seq = blk_flush_cur_seq(rq);
 
 		BUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);
+		/*
+		 * 参数的seq是可以跳过的步骤, 如果结果是0则说明没有可以跳过的步骤
+		 */
 		blk_flush_complete_seq(rq, fq, seq, error);
 	}
 
@@ -255,15 +524,27 @@ static void flush_end_io(struct request *flush_rq, blk_status_t error)
  * spin_lock_irq(fq->mq_flush_lock)
  *
  */
+/*
+ * called by only:
+ *   - block/blk-flush.c|236| <<blk_flush_complete_seq>> blk_kick_flush(q, fq, cmd_flags);
+ */
 static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 			   unsigned int flags)
 {
+	/*
+	 * flush_pending_idx只在以下修改:
+	 *   - block/blk-flush.c|327| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 */
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
 	struct request *first_rq =
 		list_first_entry(pending, struct request, flush.list);
 	struct request *flush_rq = fq->flush_rq;
 
 	/* C1 described at the top of this file */
+	/*
+	 * C1. At any given time, only one flush shall be in progress.  This makes
+	 *     double buffering sufficient.
+	 */
 	if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
 		return;
 
@@ -273,6 +554,20 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 	 * assigned to empty flushes, and we deadlock if we are expecting
 	 * other requests to make progress. Don't defer for that case.
 	 */
+	/*
+	 * C2. Flush is deferred if any request is executing DATA of its sequence.
+	 *     This avoids issuing separate POSTFLUSHes for requests which shared
+	 *     PREFLUSH.
+	 *
+	 * C3. The second condition is ignored if there is a request which has
+	 *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid
+	 *     starvation in the unlikely case where there are continuous stream of
+	 *     FUA (without PREFLUSH) requests.
+	 */
+	/*
+	 * flush_data_in_flight在以下添加元素:
+	 *   - block/blk-flush.c|239| <<blk_flush_complete_seq>> list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+	 */
 	if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
 	    time_before(jiffies,
 			fq->flush_pending_since + FLUSH_PENDING_TIMEOUT))
@@ -305,15 +600,31 @@ static void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,
 		flush_rq->internal_tag = first_rq->internal_tag;
 	}
 
+	/*
+	 * 设置REQ_OP_FLUSH目的很明确
+	 * 不知道为什么设置REQ_PREFLUSH
+	 */
 	flush_rq->cmd_flags = REQ_OP_FLUSH | REQ_PREFLUSH;
 	flush_rq->cmd_flags |= (flags & REQ_DRV) | (flags & REQ_FAILFAST_MASK);
 	flush_rq->rq_flags |= RQF_FLUSH_SEQ;
 	flush_rq->rq_disk = first_rq->rq_disk;
+	/*
+	 * end_io调用就一个地方:
+	 *   - block/blk-mq.c|884| <<__blk_mq_end_request>> rq->end_io(rq, error);
+	 */
 	flush_rq->end_io = flush_end_io;
 
+	/*
+	 * 把request插入q->requeue_list, 调用blk_mq_kick_requeue_list(q)
+	 * 后者触发q->requeue_work, 也就是blk_mq_requeue_work()
+	 */
 	blk_flush_queue_rq(flush_rq, false);
 }
 
+/*
+ * used by only:
+ *   - block/blk-flush.c|648| <<blk_insert_flush>> rq->end_io = mq_flush_data_end_io;
+ */
 static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 {
 	struct request_queue *q = rq->q;
@@ -335,6 +646,10 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
 	blk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);
 	spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
 
+	/*
+	 * 如果之前设置了BLK_MQ_S_SCHED_RESTART到hctx->state
+	 * 取消这个bit, 触发blk_mq_run_hw_queue(hctx, true);
+	 */
 	blk_mq_sched_restart(hctx);
 }
 
@@ -347,10 +662,45 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|573| <<blk_mq_sched_insert_request>> blk_insert_flush(rq);
+ *   - block/blk-mq.c|2635| <<blk_mq_make_request>> blk_insert_flush(rq);
+ *
+ * REQ_FUA     : forced unit access,绕过磁盘cache,直接把数据写到磁盘介质中.
+ * REQ_PREFLUSH: request for cache flush, 表示把磁盘cache中的data刷新到磁盘介质中,防止掉电丢失
+ *
+ * 在blk_mq_make_request()中:
+ * 如果is_flush_fua = op_is_flush(bio->bi_opf)是true,
+ * 也就是说如果bi_opf的REQ_FUA | REQ_PREFLUSH设置了一个
+ * 就调用blk_insert_flush(rq)
+ *
+ * 在blk_mq_sched_insert_request()中:
+ * 如果(!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags))
+ * 就调用blk_insert_flush(rq)
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
+	/*
+	 * queue_flags的两个例子:
+	 *   - QUEUE_FLAG_WC      Write back caching
+	 *   - QUEUE_FLAG_FUA     device supports FUA writes
+	 */
 	unsigned long fflags = q->queue_flags;	/* may change, cache */
+	/*
+	 * 调用blk_flush_policy()判断需要做哪些冲刷的步骤,可能有3步:
+	 * REQ_FSEQ_PREFLUSH(在数据请求以前冲刷磁盘缓存),
+	 * REQ_FSEQ_DATA(写入数据请求),
+	 * REQ_FSEQ_POSTFLUSH(在数据请求之后冲刷磁盘缓存).
+	 *
+	 * 如果磁盘不支持FUA,那么可以使用REQ_FSEQ_POSTFLUSH代替.
+	 * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+	 *
+	 * 如果有数据policy要有REQ_FSEQ_DATA
+	 * 如果request->cmd_flags有REQ_PREFLUSH, 则要在policy设置REQ_FSEQ_PREFLUSH
+	 * 如果request->cmd_flags有REQ_FUA但是设备不支持QUEUE_FLAG_FUA, 还要在policy设置REQ_FSEQ_POSTFLUSH
+	 */
 	unsigned int policy = blk_flush_policy(fflags, rq);
 	struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
 
@@ -358,7 +708,16 @@ void blk_insert_flush(struct request *rq)
 	 * @policy now records what operations need to be done.  Adjust
 	 * REQ_PREFLUSH and FUA for the driver.
 	 */
+	/*
+	 * 猜测这里因为REQ_PREFLUSH需要用预留的flush_rq, 所以可以在data rq里删掉这个flag
+	 */
 	rq->cmd_flags &= ~REQ_PREFLUSH;
+	/*
+	 * 假定我们分析的场景中,磁盘不支持FUA,则最终我们的冲刷策略为3步都做(policy=111).
+	 *
+	 * 磁盘不支持FUA就按正常数据下发, 就不用REQ_FUA这个flag了
+	 * 反正最后还是要FLUSH一下!
+	 */
 	if (!(fflags & (1UL << QUEUE_FLAG_FUA)))
 		rq->cmd_flags &= ~REQ_FUA;
 
@@ -389,6 +748,10 @@ void blk_insert_flush(struct request *rq)
 	 */
 	if ((policy & REQ_FSEQ_DATA) &&
 	    !(policy & (REQ_FSEQ_PREFLUSH | REQ_FSEQ_POSTFLUSH))) {
+		/*
+		 * 直接把request插入hctx->dispatch,
+		 * 如果参数run_queue是true就调用blk_mq_run_hw_queue()
+		 */
 		blk_mq_request_bypass_insert(rq, false);
 		return;
 	}
@@ -399,12 +762,19 @@ void blk_insert_flush(struct request *rq)
 	 */
 	memset(&rq->flush, 0, sizeof(rq->flush));
 	INIT_LIST_HEAD(&rq->flush.list);
+	/* request for flush sequence */
 	rq->rq_flags |= RQF_FLUSH_SEQ;
 	rq->flush.saved_end_io = rq->end_io; /* Usually NULL */
 
 	rq->end_io = mq_flush_data_end_io;
 
 	spin_lock_irq(&fq->mq_flush_lock);
+	/*
+	 * REQ_FSEQ_ACTIONS & ~policy把可以跳过的步骤对应的位置填1
+	 * 如果结果是0则说明没有可以跳过的步骤
+	 *
+	 * 但是原始的policy没传进去??
+	 */
 	blk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);
 	spin_unlock_irq(&fq->mq_flush_lock);
 }
@@ -420,6 +790,32 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * called by:
+ *   - drivers/md/dm-zoned-metadata.c|647| <<dmz_write_sb>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|681| <<dmz_write_dirty_mblocks>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/dm-zoned-metadata.c|745| <<dmz_flush_metadata>> ret = blkdev_issue_flush(zmd->dev->bdev, GFP_NOIO, NULL);
+ *   - drivers/md/raid5-ppl.c|1045| <<ppl_recover>> ret = blkdev_issue_flush(rdev->bdev, GFP_KERNEL, NULL);
+ *   - drivers/nvme/target/io-cmd-bdev.c|182| <<nvmet_bdev_flush>> if (blkdev_issue_flush(req->ns->bdev, GFP_KERNEL, NULL))
+ *   - fs/block_dev.c|687| <<blkdev_fsync>> error = blkdev_issue_flush(bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/fsync.c|162| <<ext4_sync_file>> err = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/ext4/ialloc.c|1427| <<ext4_init_inode_table>> blkdev_issue_flush(sb->s_bdev, GFP_NOFS, NULL);
+ *   - fs/ext4/super.c|5052| <<ext4_sync_fs>> err = blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/inode.c|343| <<hfsplus_file_fsync>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/hfsplus/super.c|241| <<hfsplus_sync_fs>> blkdev_issue_flush(sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/jbd2/checkpoint.c|405| <<jbd2_cleanup_journal_tail>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|768| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_fs_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/commit.c|872| <<jbd2_journal_commit_transaction>> blkdev_issue_flush(journal->j_dev, GFP_NOFS, NULL);
+ *   - fs/jbd2/recovery.c|289| <<jbd2_journal_recover>> err2 = blkdev_issue_flush(journal->j_fs_dev, GFP_KERNEL, NULL);
+ *   - fs/libfs.c|1021| <<generic_file_fsync>> return blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/nilfs2/the_nilfs.h|378| <<nilfs_flush_device>> err = blkdev_issue_flush(nilfs->ns_bdev, GFP_KERNEL, NULL);
+ *   - fs/ocfs2/file.c|211| <<ocfs2_sync_file>> ret = blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/reiserfs/file.c|168| <<reiserfs_sync_file>> blkdev_issue_flush(inode->i_sb->s_bdev, GFP_KERNEL, NULL);
+ *   - fs/xfs/xfs_super.c|684| <<xfs_blkdev_issue_flush>> blkdev_issue_flush(buftarg->bt_bdev, GFP_NOFS, NULL);
+ *
+ * 分配一个bio, 设置bio->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
+ * 然后下发submit_bio_wait(bio)直到完成
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -430,6 +826,7 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 	if (bdev->bd_disk == NULL)
 		return -ENXIO;
 
+	/* bdev->bd_disk->queue; */
 	q = bdev_get_queue(bdev);
 	if (!q)
 		return -ENXIO;
@@ -447,6 +844,7 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 	bio_set_dev(bio, bdev);
 	bio->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
 
+	/* submit a bio, and wait until it completes */
 	ret = submit_bio_wait(bio);
 
 	/*
@@ -462,6 +860,14 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3018| <<blk_mq_init_hctx>> hctx->fq = blk_alloc_flush_queue(q, hctx->numa_node, set->cmd_size,
+ *
+ * 这个函数为每一个hctx返回一个blk_flush_queue
+ *
+ * 每一个hctx一个struct blk_flush_queue!!!
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size, gfp_t flags)
 {
@@ -474,13 +880,33 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 
 	spin_lock_init(&fq->mq_flush_lock);
 
+	/*
+	 * cmd_size的一些例子:
+	 *   - drivers/scsi/virtio_scsi.c|692| <<global>> .cmd_size = sizeof(struct virtio_scsi_cmd),
+	 *   - drivers/block/virtio_blk.c|788| <<virtblk_probe>> vblk->tag_set.cmd_size = sizeof(struct virtblk_req) + sizeof(struct scatterlist) * sg_elems;
+	 *   - drivers/block/xen-blkfront.c|981| <<xlvbd_init_blk_queue>> info->tag_set.cmd_size = sizeof(struct blkif_req);
+	 */
 	rq_sz = round_up(rq_sz + cmd_size, cache_line_size());
+	/*
+	 * flush_rq相当于一个request附着一个command的空间?
+	 */
 	fq->flush_rq = kzalloc_node(rq_sz, flags, node);
 	if (!fq->flush_rq)
 		goto fail_rq;
 
+	/*
+	 * flush_queue在以下使用:
+	 *   - block/blk-flush.c|193| <<blk_flush_complete_seq>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
+	 *   - block/blk-flush.c|259| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|293| <<blk_kick_flush>> struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx]
+	 */
 	INIT_LIST_HEAD(&fq->flush_queue[0]);
 	INIT_LIST_HEAD(&fq->flush_queue[1]);
+	/*
+	 * flush_data_in_flight在以下使用:
+	 *   - block/blk-flush.c|215| <<blk_flush_complete_seq>> list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+	 *   - block/blk-flush.c|308| <<blk_kick_flush>> if (!list_empty(&fq->flush_data_in_flight) && q->elevator &&
+	 */
 	INIT_LIST_HEAD(&fq->flush_data_in_flight);
 
 	return fq;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 22467f4..9fcb5d5 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -318,6 +318,10 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return do_split ? new : NULL;
 }
 
+/*
+ * 根据块设备请求队列的limits.max_sectors和limits.max_segmetns
+ * 来拆分bio,适应设备缓存.会在函数blk_set_default_limits中设置
+ */
 void blk_queue_split(struct request_queue *q, struct bio **bio)
 {
 	struct bio *split, *res;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 03a5348..4b4c4da 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -31,6 +31,21 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|50| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|52| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3486| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3782| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|506| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1814| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2080| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2081| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6940| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1818| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[0]);
+ *
+ * 初始化blk_mq_queue_map->map
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index bac34b7..d4e1bfe 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -1058,6 +1058,14 @@ void blk_mq_debugfs_unregister_queue_rqos(struct request_queue *q)
 	q->rqos_debugfs_dir = NULL;
 }
 
+/*
+ * echo kyber > /sys/block/nvme0n1/queue/scheduler后获得了
+ *
+ * !/sys/kernel/debug/block/nvme0n1/sched
+ *
+ * # ls /sys/kernel/debug/block/nvme0n1/sched --> kyber的例子
+ * async_depth  discard_tokens  other_tokens  read_tokens  write_tokens
+ */
 int blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx)
 {
@@ -1081,6 +1089,10 @@ int blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|760| <<blk_mq_exit_sched>> blk_mq_debugfs_unregister_sched_hctx(hctx);
+ */
 void blk_mq_debugfs_unregister_sched_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	debugfs_remove_recursive(hctx->sched_debugfs_dir);
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index 1dce185..de49b40 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -31,6 +31,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|504| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6942| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5792| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[0],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
@@ -42,6 +48,7 @@ int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 		if (!mask)
 			goto fallback;
 
+		/* 猜测是每一个sw queue对应的hw queue??? */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 4090553..e91b09d 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -16,6 +16,28 @@
 #include "blk-mq-tag.h"
 #include "blk-wbt.h"
 
+/*
+ * sched中下发的主要函数:
+ *
+ * - blk_mq_do_dispatch_sched()
+ *   对于参数的hctx, 如果没有调度器就返回
+ *   否则用调度器的dispatch_request()取出下一个request
+ *   用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ *
+ * - blk_mq_do_dispatch_ctx()
+ *   dequeue request one by one from sw queue if queue is busy
+ *   不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ *   然后为这个request调用queue_rq()
+ *   最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ *
+ *
+ * 插入的主要函数是blk_mq_sched_insert_request()
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
+
+/*
+ * 没有任何调用!!!!!!!!!!
+ */
 void blk_mq_sched_free_hctx_data(struct request_queue *q,
 				 void (*exit)(struct blk_mq_hw_ctx *))
 {
@@ -31,6 +53,10 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_free_hctx_data);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|633| <<blk_mq_get_request>> blk_mq_sched_assign_ioc(rq);
+ */
 void blk_mq_sched_assign_ioc(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -45,14 +71,17 @@ void blk_mq_sched_assign_ioc(struct request *rq)
 		return;
 
 	spin_lock_irq(&q->queue_lock);
+	/* lookup io_cq from ioc */
 	icq = ioc_lookup_icq(ioc, q);
 	spin_unlock_irq(&q->queue_lock);
 
 	if (!icq) {
+		/* create and link io_cq */
 		icq = ioc_create_icq(ioc, q, GFP_ATOMIC);
 		if (!icq)
 			return;
 	}
+	/* increment reference count to io_context */
 	get_io_context(icq->ioc);
 	rq->elv.icq = icq;
 }
@@ -61,8 +90,25 @@ void blk_mq_sched_assign_ioc(struct request *rq)
  * Mark a hardware queue as needing a restart. For shared queues, maintain
  * a count of how many hardware queues are marked for restart.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|237| <<blk_mq_sched_dispatch_requests>> blk_mq_sched_mark_restart_hctx(hctx);
+ *   - block/mq-deadline.c|396| <<dd_dispatch_request>> blk_mq_sched_mark_restart_hctx(hctx);
+ */
 void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 
@@ -70,8 +116,28 @@ void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);
 
+/*
+ * called by;
+ *   - block/blk-flush.c|338| <<mq_flush_data_end_io>> blk_mq_sched_restart(hctx);
+ *   - block/blk-mq.c|733| <<__blk_mq_free_request>> blk_mq_sched_restart(hctx);
+ *
+ * 如果之前设置了BLK_MQ_S_SCHED_RESTART到hctx->state
+ * 取消这个bit, 触发blk_mq_run_hw_queue(hctx, true);
+ */
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 	clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
@@ -84,6 +150,15 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|211| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|216| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *
+ * 对于参数的hctx, 如果没有调度器就返回
+ * 否则用调度器的dispatch_request()取出下一个request
+ * 用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -111,9 +186,14 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 		 * in blk_mq_dispatch_rq_list().
 		 */
 		list_add(&rq->queuelist, &rq_list);
+	/* 为参数list中的每一个request调用queue_rq() */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|231| <<blk_mq_do_dispatch_ctx>> ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
+ */
 static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
 					  struct blk_mq_ctx *ctx)
 {
@@ -130,6 +210,16 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|213| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *
+ * dequeue request one by one from sw queue if queue is busy
+ * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ * 然后为这个request调用queue_rq()
+ * 最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -139,12 +229,26 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	do {
 		struct request *rq;
 
+		/*
+		 * hctx->ctx_map在以下被设置:
+		 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+		 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+		 *
+		 * 这个函数的调用路径是blk_mq_do_dispatch_ctx()-->blk_mq_dequeue_from_ctx()-->对于每一个ctx调用dispatch_rq_from_ctx()
+		 * dispatch_rq_from_ctx()会把ctx_map对应的bit给clear掉
+		 */
 		if (!sbitmap_any_bit_set(&hctx->ctx_map))
 			break;
 
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * blk_mq_dequeue_from_ctx()只在这里被调用
+		 * blk_mq_do_dispatch_ctx()-->blk_mq_dequeue_from_ctx()-->对于每一个ctx调用dispatch_rq_from_ctx()
+		 *
+		 * 从start代表的第一个hctx->ctx_map开始, 取出一个request 就取出一个啊!!!
+		 */
 		rq = blk_mq_dequeue_from_ctx(hctx, ctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -161,11 +265,16 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		/* round robin for fair dispatch */
 		ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
 
+	/* 为参数list中的每一个request调用queue_rq() */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1495| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -205,23 +314,55 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	 */
 	if (!list_empty(&rq_list)) {
 		blk_mq_sched_mark_restart_hctx(hctx);
+		/* 为参数list中的每一个request调用queue_rq() */
 		if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+			/*
+			 * blk_mq_do_dispatch_sched():
+			 * 对于参数的hctx, 如果没有调度器就返回
+			 * 否则用调度器的dispatch_request()取出下一个request
+			 * 用blk_mq_dispatch_rq_list()下发
+			 *
+			 * blk_mq_do_dispatch_ctx()
+			 * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+			 * 然后为这个request调用queue_rq()
+			 */
 			if (has_sched_dispatch)
 				blk_mq_do_dispatch_sched(hctx);
 			else
 				blk_mq_do_dispatch_ctx(hctx);
 		}
 	} else if (has_sched_dispatch) {
+		/*
+		 * 对于参数的hctx, 如果没有调度器就返回
+		 * 否则用调度器的dispatch_request()取出下一个request
+		 * 用blk_mq_dispatch_rq_list()下发
+		 */
 		blk_mq_do_dispatch_sched(hctx);
 	} else if (hctx->dispatch_busy) {
 		/* dequeue request one by one from sw queue if queue is busy */
+		/*
+		 * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+		 * 然后为这个request调用queue_rq()
+		 */
 		blk_mq_do_dispatch_ctx(hctx);
 	} else {
+		/*
+		 * 对于hctx->ctx_map中每个设置的bit对应的ctx
+		 * 把ctx->rq_list[type]的request们拼接到参数的list
+		 * 把ctx在hctx->ctx_map清空
+		 * 最后参数list中的就是这个hctx->ctx_map中每一个ctx的rq_list的总和拼
+		 */
 		blk_mq_flush_busy_ctxs(hctx, &rq_list);
+		/* 为参数list中的每一个request调用queue_rq() */
 		blk_mq_dispatch_rq_list(q, &rq_list, false);
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|1890| <<bfq_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ *   - block/mq-deadline.c|479| <<dd_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ */
 bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 			    struct request **merged_request)
 {
@@ -258,6 +399,11 @@ EXPORT_SYMBOL_GPL(blk_mq_sched_try_merge);
  * Iterate list of requests and see if we can merge this bio with any
  * of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|438| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/kyber-iosched.c|586| <<kyber_bio_merge>> merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio);
+ */
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio)
 {
@@ -301,6 +447,10 @@ EXPORT_SYMBOL_GPL(blk_mq_bio_list_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-sched.c|395| <<__blk_mq_sched_bio_merge>> ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_hw_ctx *hctx,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
@@ -309,6 +459,10 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/*
+	 * Iterate list of requests and see if we can merge this bio with any
+	 * of them.
+	 */
 	if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
 		ctx->rq_merged++;
 		return true;
@@ -317,6 +471,10 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.h|38| <<blk_mq_sched_bio_merge>> return __blk_mq_sched_bio_merge(q, bio);
+ */
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
@@ -343,6 +501,11 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|4734| <<bfq_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq)) {
+ *   - block/mq-deadline.c|504| <<dd_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq))
+ */
 bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
 	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
@@ -355,6 +518,10 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|533| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -373,6 +540,23 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|61| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|1049| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|1055| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2328| <<blk_mq_try_issue_directly>> blk_mq_sched_insert_request(rq, false,
+ *   - block/blk-mq.c|2361| <<blk_mq_try_issue_list_directly>> blk_mq_sched_insert_request(rq, false, true, false);
+ *   - block/blk-mq.c|2505| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *
+ * 1. 如果有RQF_FLUSH_SEQ则blk_insert_flush()
+ * 2. 试试能否blk_mq_sched_bypass_insert(): 比如是否有RQF_FLUSH_SEQ
+ * 3. 如果支持IO调度则用scheduler的.insert_requests()
+ * 4. 否则把request放入request->mq_ctx的rq_lists, 然后把ctx在hctx->ctx_map对应的bit设置
+ * 根据参数是否blk_mq_run_hw_queue()!
+ *
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -383,10 +567,16 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 
 	/* flush rq in flush machinery need to be dispatched directly */
 	if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+		/*
+		 * insert a new PREFLUSH/FUA request
+		 */
 		blk_insert_flush(rq);
 		goto run;
 	}
 
+	/*
+	 * 如果又存在IO调度器, 又有driver tag是不正常的
+	 */
 	WARN_ON(e && (rq->tag != -1));
 
 	if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
@@ -399,6 +589,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		e->type->ops.insert_requests(hctx, &list, at_head);
 	} else {
 		spin_lock(&ctx->lock);
+		/*
+		 * 把request放入request->mq_ctx的rq_lists
+		 * 然后把ctx在hctx->ctx_map对应的bit设置
+		 */
 		__blk_mq_insert_request(hctx, rq, at_head);
 		spin_unlock(&ctx->lock);
 	}
@@ -408,6 +602,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2179| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|2200| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -423,6 +622,11 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 		 * busy in case of 'none' scheduler, and this way may save
 		 * us one extra enqueue & dequeue to sw queue.
 		 */
+		/*
+		 * blk_mq_insert_requests():
+		 * 把list中的request们放入ctx->rq_lists[type]
+		 * 把ctx在hctx->ctx_map对应的bit设置
+		 */
 		if (!hctx->dispatch_busy && !e && !run_queue_async)
 			blk_mq_try_issue_list_directly(hctx, list);
 		else
@@ -432,6 +636,11 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 	blk_mq_run_hw_queue(hctx, run_queue_async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|615| <<blk_mq_sched_alloc_tags>> blk_mq_sched_free_tags(set, hctx, hctx_idx);
+ *   - block/blk-mq-sched.c|627| <<blk_mq_sched_tags_teardown>> blk_mq_sched_free_tags(set, hctx, i);
+ */
 static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -443,6 +652,10 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|497| <<blk_mq_init_sched>> ret = blk_mq_sched_alloc_tags(q, hctx, i);
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -462,6 +675,11 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|684| <<blk_mq_init_sched>> blk_mq_sched_tags_teardown(q);
+ *   - block/blk-mq-sched.c|709| <<blk_mq_exit_sched>> blk_mq_sched_tags_teardown(q);
+ */
 static void blk_mq_sched_tags_teardown(struct request_queue *q)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -472,6 +690,11 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 		blk_mq_sched_free_tags(set, hctx, i);
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|577| <<elevator_switch_mq>> ret = blk_mq_init_sched(q, new_e);
+ *   - block/elevator.c|623| <<elevator_init_mq>> err = blk_mq_init_sched(q, e);
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -526,6 +749,11 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|592| <<blk_mq_init_sched>> blk_mq_exit_sched(q, eq);
+ *   - block/elevator.c|184| <<elevator_exit>> blk_mq_exit_sched(q, e);
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index c7bdb52..e66aae3 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -76,6 +76,12 @@ static inline void blk_mq_sched_requeue_request(struct request *rq)
 		e->type->ops.requeue_request(rq);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|192| <<blk_mq_hctx_has_pending>> blk_mq_sched_has_work(hctx);
+ *
+ * 如果支持elevator, 用elevator的has_work查看是否有work
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
@@ -86,8 +92,24 @@ static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 	return false;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|1724| <<blk_mq_dispatch_rq_list>> needs_restart = blk_mq_sched_needs_restart(hctx);
+ */
 static inline bool blk_mq_sched_needs_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 }
 
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 3f9c3f4..62463c9 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -143,23 +143,38 @@ static ssize_t blk_mq_hw_sysfs_store(struct kobject *kobj,
 	return res;
 }
 
+/*
+ * 打印/sys/block/nvme0n1/mq/0/nr_tags
+ */
 static ssize_t blk_mq_hw_sysfs_nr_tags_show(struct blk_mq_hw_ctx *hctx,
 					    char *page)
 {
 	return sprintf(page, "%u\n", hctx->tags->nr_tags);
 }
 
+/*
+ * 打印/sys/block/nvme0n1/mq/0/nr_reserved_tags
+ */
 static ssize_t blk_mq_hw_sysfs_nr_reserved_tags_show(struct blk_mq_hw_ctx *hctx,
 						     char *page)
 {
 	return sprintf(page, "%u\n", hctx->tags->nr_reserved_tags);
 }
 
+/*
+ * 打印/sys/block/nvme0n1/mq/1/cpu_list
+ * 也就是当前hctx(hw queue) map了哪些cpu
+ */
 static ssize_t blk_mq_hw_sysfs_cpus_show(struct blk_mq_hw_ctx *hctx, char *page)
 {
 	unsigned int i, first = 1;
 	ssize_t ret = 0;
 
+	/*
+	 * 只在以下修改数据cpumask:
+	 *   - block/blk-mq.c|2990| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *   - block/blk-mq.c|3040| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 */
 	for_each_cpu(i, hctx->cpumask) {
 		if (first)
 			ret += sprintf(ret + page, "%u", i);
@@ -177,6 +192,13 @@ static struct attribute *default_ctx_attrs[] = {
 	NULL,
 };
 
+/*
+ * ls /sys/block/nvme0n1/mq/0/
+ * cpu0  cpu1  cpu_list  nr_reserved_tags  nr_tags
+ *
+ * 下面添加的nr_tags, nr_reserved_tags和cpu_list
+ */
+
 static struct blk_mq_hw_ctx_sysfs_entry blk_mq_hw_sysfs_nr_tags = {
 	.attr = {.name = "nr_tags", .mode = 0444 },
 	.show = blk_mq_hw_sysfs_nr_tags_show,
@@ -190,6 +212,9 @@ static struct blk_mq_hw_ctx_sysfs_entry blk_mq_hw_sysfs_cpus = {
 	.show = blk_mq_hw_sysfs_cpus_show,
 };
 
+/*
+ * 用做struct kobj_type blk_mq_hw_ktype.default_attrs
+ */
 static struct attribute *default_hw_ctx_attrs[] = {
 	&blk_mq_hw_sysfs_nr_tags.attr,
 	&blk_mq_hw_sysfs_nr_reserved_tags.attr,
@@ -224,6 +249,12 @@ static struct kobj_type blk_mq_hw_ktype = {
 	.release	= blk_mq_hw_sysfs_release,
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|276| <<blk_mq_unregister_dev>> blk_mq_unregister_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|344| <<__blk_mq_register_dev>> blk_mq_unregister_hctx(q->queue_hw_ctx[i]);
+ *   - block/blk-mq-sysfs.c|373| <<blk_mq_sysfs_unregister>> blk_mq_unregister_hctx(hctx);
+ */
 static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_ctx *ctx;
@@ -238,6 +269,14 @@ static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 	kobject_del(&hctx->kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|332| <<__blk_mq_register_dev>> ret = blk_mq_register_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|393| <<blk_mq_sysfs_register>> ret = blk_mq_register_hctx(hctx);
+ *
+ * 比如/sys/block/nvme0n1/mq/0
+ * 比如/sys/block/nvme0n1/mq/1
+ */
 static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -247,10 +286,19 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	if (!hctx->nr_ctx)
 		return 0;
 
+	/*
+	 * 把0或者1添加到/sys/block/nvme0n1/mq
+	 */
 	ret = kobject_add(&hctx->kobj, q->mq_kobj, "%u", hctx->queue_num);
 	if (ret)
 		return ret;
 
+	/*
+	 * # ls /sys/block/nvme0n1/mq/0/
+	 * cpu0  cpu1  cpu_list  nr_reserved_tags  nr_tags
+	 *
+	 * 添加的cpu0和cpu1
+	 */
 	hctx_for_each_ctx(hctx, ctx, i) {
 		ret = kobject_add(&ctx->kobj, &hctx->kobj, "cpu%u", ctx->cpu);
 		if (ret)
@@ -277,6 +325,10 @@ void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 	q->mq_sysfs_init_done = false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3339| <<blk_mq_alloc_and_init_hctx>> blk_mq_hctx_kobj_init(hctx);
+ */
 void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx)
 {
 	kobject_init(&hctx->kobj, &blk_mq_hw_ktype);
@@ -309,6 +361,15 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|372| <<blk_mq_register_dev>> ret = __blk_mq_register_dev(dev, q);
+ *   - block/blk-sysfs.c|968| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ *
+ * 因为blk_mq_register_dev()没人调用
+ *
+ * 所以这个函数相当于只被blk_register_queue()调用!!!!!!!!!!!!!!!!
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -317,6 +378,12 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	WARN_ON_ONCE(!q->kobj.parent);
 	lockdep_assert_held(&q->sysfs_lock);
 
+	/*
+	 * 比如: /sys/block/nvme0n1/mq
+	 *
+	 * # ls /sys/block/nvme0n1/mq/
+	 * 0  1
+	 */
 	ret = kobject_add(q->mq_kobj, kobject_get(&dev->kobj), "%s", "mq");
 	if (ret < 0)
 		goto out;
@@ -324,6 +391,12 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	kobject_uevent(q->mq_kobj, KOBJ_ADD);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 为了/sys/block/nvme0n1/mq/ 下面添加0和1
+		 *
+		 * # ls /sys/block/nvme0n1/mq/
+		 * 0  1
+		 */
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
 			goto unreg;
@@ -344,6 +417,9 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/*
+ * 没人调用!!!
+ */
 int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	int ret;
@@ -355,6 +431,10 @@ int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3968| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_unregister(q);
+ */
 void blk_mq_sysfs_unregister(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -371,6 +451,10 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|3988| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index a4931fc..6e102023 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -13,6 +13,15 @@
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
 
+/*
+ * PATH-A入口: blk_mq_queue_tag_busy_iter()
+ *     针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+ *     数据结构是bt_iter_data
+ * PATH-B入口: blk_mq_tagset_busy_iter()
+ *     针对每一个tagset->tags[tagset->nr_hw_queues].sb
+ *     数据结构是bt_tags_iter_data
+ */
+
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
 	if (!tags)
@@ -27,8 +36,26 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|72| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ *
+ * 为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+ * 如果之前没设置, 增加hctx->tags->active_queues
+ * BLK_MQ_S_TAG_ACTIVE的设置取消和tags->active_queues的增加减少是一致的
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/* test_and_set_bit(): Set a bit and return its old value */
+	/*
+	 * BLK_MQ_S_TAG_ACTIVE在以下被使用:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|33| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|57| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|76| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * hctx->tags是struct blk_mq_tags *
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -39,6 +66,13 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|79| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|471| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true); --> 只被blk_set_queue_dying()调用
+ *
+ * 临时记住一点, 会wake_up()!
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -50,6 +84,12 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-tag.h|136| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ *
+ * BLK_MQ_S_TAG_ACTIVE的设置取消和tags->active_queues的增加减少是一致的
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -59,6 +99,9 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 
 	atomic_dec(&tags->active_queues);
 
+	/*
+	 * 临时记住一点, 会wake_up()!
+	 */
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
@@ -66,11 +109,23 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called only by:
+ *   - block/blk-mq-tag.c|131| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ *
+ * For shared tag users, we track the number of currently active users
+ * and attempt to provide a fair share of the tag depth for each of them.
+ * hctx_may_queue()估计是为了在不是data->hctx->sched_tags的情况下防止共享set
+ * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
 	unsigned int depth, users;
 
+	/*
+	 * 如果BLK_MQ_F_TAG_SHARED没设置就返回true
+	 */
 	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return true;
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
@@ -93,9 +148,33 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|181| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|204| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|210| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *
+ * 如果不是BLK_MQ_REQ_INTERNAL(data->hctx->sched_tags)并且tagset可能被共享
+ * 则用hctx_may_queue()为了在不是data->hctx->sched_tags的情况下防止共享set
+ * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+ *
+ * 否则用sbitmap分配bit!
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 *
+	 * hctx_may_queue():
+	 * For shared tag users, we track the number of currently active users
+	 * and attempt to provide a fair share of the tag depth for each of them.
+	 *
+	 * hctx_may_queue()估计是为了在不是data->hctx->sched_tags的情况下防止共享set
+	 * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+	 */
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -105,16 +184,36 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|601| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1425| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则返回使用data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
+	/* 声明一个struct sbq_wait */
 	DEFINE_SBQ_WAIT(wait);
 	unsigned int tag_offset;
 	bool drop_ctx;
 	int tag;
 
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|1418| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|994| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|935| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 */
 	if (data->flags & BLK_MQ_REQ_RESERVED) {
 		if (unlikely(!tags->nr_reserved_tags)) {
 			WARN_ON_ONCE(1);
@@ -127,15 +226,33 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		tag_offset = tags->nr_reserved_tags;
 	}
 
+	/*
+	 * 如果不是BLK_MQ_REQ_INTERNAL(data->hctx->sched_tags)并且tagset可能被共享
+	 * 则用hctx_may_queue()为了在不是data->hctx->sched_tags的情况下防止共享set
+	 * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+	 *
+	 * 否则用sbitmap分配bit!
+	 */
 	tag = __blk_mq_get_tag(data, bt);
 	if (tag != -1)
 		goto found_tag;
 
+	/*
+	 * BLK_MQ_REQ_NOWAIT: return when out of requests
+	 */
 	if (data->flags & BLK_MQ_REQ_NOWAIT)
 		return BLK_MQ_TAG_FAIL;
 
+	/*
+	 * 并不会hang
+	 *
+	 * bt根据BLK_MQ_REQ_RESERVED要么是tags->breserved_tags要么是tags->bitmap_tags
+	 */
 	ws = bt_wait_ptr(bt, data->hctx);
 	drop_ctx = data->ctx == NULL;
+	/*
+	 * 如果分配不到tag, 就一直在循环跑下去
+	 */
 	do {
 		struct sbitmap_queue *bt_prev;
 
@@ -160,15 +277,33 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		if (tag != -1)
 			break;
 
+		/*
+		 * 会调用put_cpu()
+		 */
 		if (data->ctx)
 			blk_mq_put_ctx(data->ctx);
 
 		bt_prev = bt;
+		/*
+		 * 调用schedule()
+		 * TASK_UNINTERRUPTIBLE!!!
+		 */
 		io_schedule();
 
 		sbitmap_finish_wait(bt, ws, &wait);
 
+		/*
+		 * 会调用get_cpu()
+		 */
 		data->ctx = blk_mq_get_ctx(data->q);
+		/*
+		 * 如果是REQ_HIPRI, 返回HCTX_TYPE_POLL
+		 * 如果是REQ_OP_READ, 返回HCTX_TYPE_READ
+		 * 其他的, 返回默认的HCTX_TYPE_DEFAULT
+		 * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+		 * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+		 * 返回的仍然可以是HCTX_TYPE_DEFAULT
+		 */
 		data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
 						data->ctx);
 		tags = blk_mq_tags_from_data(data);
@@ -197,6 +332,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|713| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|715| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|285| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
@@ -218,10 +359,26 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|302| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ *
+ *
+ * PATH-A调用最早起源于四个函数:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *         上面四个函数调用====> blk_mq_queue_tag_busy_iter()-->bt_for_each()-->bt_iter()
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
+	/* 定义就在上面, 包括fn */
 	struct bt_iter_data *iter_data = data;
 	struct blk_mq_hw_ctx *hctx = iter_data->hctx;
+	/*
+	 * 不是sched_tags哦
+	 */
 	struct blk_mq_tags *tags = hctx->tags;
 	bool reserved = iter_data->reserved;
 	struct request *rq;
@@ -253,6 +410,18 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called only by:
+ *   - block/blk-mq-tag.c|575| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|576| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ *
+ * PATH-A调用最早起源于四个函数:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *         上面四个函数调用====> blk_mq_queue_tag_busy_iter()-->bt_for_each()-->bt_iter()
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -273,6 +442,16 @@ struct bt_tags_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|461| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -307,6 +486,17 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|478| <<blk_mq_all_tag_busy_iter>> bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|479| <<blk_mq_all_tag_busy_iter>> bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, false);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, bool reserved)
 {
@@ -331,6 +521,16 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-tag.c|513| <<blk_mq_tagset_busy_iter>> blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ */
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -349,12 +549,34 @@ static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * 被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ *
+ * PATH-A入口: blk_mq_queue_tag_busy_iter()
+ *     针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+ *     数据结构是bt_iter_data
+ * PATH-B入口: blk_mq_tagset_busy_iter()
+ *     针对每一个tagset->tags[tagset->nr_hw_queues].sb
+ *     数据结构是bt_tags_iter_data
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
 	int i;
 
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
+		/*
+		 * iterate over all started requests in a tag map
+		 */
 		if (tagset->tags && tagset->tags[i])
 			blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
 	}
@@ -375,6 +597,31 @@ EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *
+ * PATH-A调用最早起源于四个函数:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *         上面四个函数调用====> blk_mq_queue_tag_busy_iter()-->bt_for_each()-->bt_iter()
+ *
+ * PATH-A入口: blk_mq_queue_tag_busy_iter()
+ *     针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+ * PATH-B入口: blk_mq_tagset_busy_iter()
+ *     针对每一个tagset->tags[tagset->nr_hw_queues].sb
+ *
+ * To iterating over requests triggers race conditions with request execution.
+ *
+ * That race isn't new.
+ * You should only iterate when your queues are quieced to ensure the
+ * request sent to a callback is stable.
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -388,10 +635,46 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 	 * synchronize_rcu() to ensure this function left the critical section
 	 * below.
 	 */
+	/*
+	 * 使用的例子: 
+	 *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|406| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-core.c|415| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|447| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|453| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|544| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-mq-tag.c|401| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|305| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *   - block/blk-mq.c|319| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|327| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-mq.c|366| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-mq.c|1245| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *   - block/blk-sysfs.c|924| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 *   - block/blk.h|66| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|617| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|627| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 */
 	if (!percpu_ref_tryget(&q->q_usage_counter))
 		return;
 
+	/*
+	 * 遍历每一个request_queue->queue_hw_ctx[i]
+	 *
+	 * q->queue_hw_ctx第一维在以下分配:
+	 *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+	 *
+	 * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+	 *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 在以下设置了hctx->tags:
+		 *   - block/blk-mq.c|2302| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+		 *   - block/blk-mq.c|2504| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+		 */
 		struct blk_mq_tags *tags = hctx->tags;
 
 		/*
@@ -405,9 +688,17 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 			bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
 		bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
 	}
+	/* 就是调用percpu_ref_put(&q->q_usage_counter); */
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|613| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|615| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, round_robin,
+ *
+ * blk_mq_alloc_rq_map()-->blk_mq_init_tags()-->blk_mq_init_bitmap_tags()-->bt_alloc()
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -415,6 +706,10 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq-tag.c|506| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -435,6 +730,12 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2068| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ *
+ * 分配一个blk_mq_tags,初始化其nr_tags, nr_reserved_tags和sbitmap
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -456,6 +757,12 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2514| <<blk_mq_free_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2538| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2547| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ */
 void blk_mq_free_tags(struct blk_mq_tags *tags)
 {
 	sbitmap_queue_free(&tags->bitmap_tags);
@@ -463,6 +770,11 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3683| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3686| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -476,6 +788,10 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 	 * If we are allowed to grow beyond the original size, allocate
 	 * a new set of tags before freeing the old one.
 	 */
+	/*
+	 * 如果新的depth比以前的大, 需要重新分配
+	 * 否则直接改变sbitmap大小就可以了
+	 */
 	if (tdepth > tags->nr_tags) {
 		struct blk_mq_tag_set *set = hctx->queue->tag_set;
 		struct blk_mq_tags *new;
@@ -491,6 +807,9 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 		if (tdepth > 16 * BLKDEV_MAX_RQ)
 			return -EINVAL;
 
+		/*
+		 * 会设置nr_tags为tdepth
+		 */
 		new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
 				tags->nr_reserved_tags);
 		if (!new)
@@ -509,6 +828,9 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 		 * Don't need (or can't) update reserved tags here, they
 		 * remain static and should never need resizing.
 		 */
+		/*
+		 * nr_tags不会变化
+		 */
 		sbitmap_queue_resize(&tags->bitmap_tags,
 				tdepth - tags->nr_reserved_tags);
 	}
@@ -528,6 +850,16 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 调用的几个例子:
+ *   - drivers/nvme/host/nvme.h|124| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/scsi_debug.c|3786| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5707| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/virtio_scsi.c|501| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ *
+ * 给一个request rq
+ * 根据rq->mq_hctx->queue_num和rq->tag拼凑出一个所有队列全局的tag
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..203e65a 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -8,15 +8,60 @@
  * Tag address space map.
  */
 struct blk_mq_tags {
+	/*
+	 * 只在一处设置:
+	 */
 	unsigned int nr_tags;
+	/*
+	 * 只在一处设置:
+	 *   - block/blk-mq-tag.c|454| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * active_queues使用的地方:
+	 *   - block/blk-mq-debugfs.c|477| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|60| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|85| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
+	/*
+	 * 分配指针数组的地方:
+	 *   - block/blk-mq.c|2073| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *
+	 * 设置指针内容的地方:
+	 *   - block/blk-mq-tag.h|92| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|307| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1056| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *
+	 * 总体使用的地方:
+	 *   - block/blk-mq-tag.c|241| <<bt_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.c|300| <<bt_tags_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.h|130| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|515| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1091| <<blk_mq_tag_to_rq>> prefetch(tags->rqs[tag]);
+	 *   - block/blk-mq.c|1092| <<blk_mq_tag_to_rq>> return tags->rqs[tag];
+	 *   - block/blk-mq.c|1430| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|2481| <<blk_mq_free_rqs>> if (tags->rqs && set->ops->exit_request) {
+	 *   - block/blk-mq.c|2508| <<blk_mq_free_rq_map>> kfree(tags->rqs);
+	 *   - block/blk-mq.c|2509| <<blk_mq_free_rq_map>> tags->rqs = NULL;
+	 *   - block/blk-mq.c|2533| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2536| <<blk_mq_alloc_rq_map>> if (!tags->rqs) {
+	 *   - block/blk-mq.c|2545| <<blk_mq_alloc_rq_map>> kfree(tags->rqs);
+	 */
 	struct request **rqs;
+	/*
+	 * 分配指针数组的地方:
+	 *   - block/blk-mq.c|2081| <<blk_mq_alloc_rq_map>> tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *
+	 * 设置指针内容的地方 (指向的都是下面page_list的内容):
+	 *   - block/blk-mq.c|2173| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -36,11 +81,22 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|147| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|198| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1484| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(&hctx->tags->bitmap_tags, hctx)->wait;
+ *
+ * 并不会hang
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/*
+	 * &hctx->wait_index会被更新
+	 */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -53,14 +109,33 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|394| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1078| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ *
+ * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+ * 则不会调用__blk_mq_tag_busy()
+ * __blk_mq_tag_busy()会为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+ * 如果之前没设置, 增加hctx->tags->active_queues
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return false;
 
+	/*
+	 * 为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+	 * 如果之前没设置, 增加hctx->tags->active_queues
+	 */
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1263| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2697| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,9 +150,29 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|220| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|303| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
+	/* 使用tags->rqs的地方:
+	 *   - block/blk-mq-tag.c|241| <<bt_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.c|300| <<bt_tags_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.h|130| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|515| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1091| <<blk_mq_tag_to_rq>> prefetch(tags->rqs[tag]);
+	 *   - block/blk-mq.c|1092| <<blk_mq_tag_to_rq>> return tags->rqs[tag];
+	 *   - block/blk-mq.c|1430| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|2481| <<blk_mq_free_rqs>> if (tags->rqs && set->ops->exit_request) {
+	 *   - block/blk-mq.c|2508| <<blk_mq_free_rq_map>> kfree(tags->rqs);
+	 *   - block/blk-mq.c|2509| <<blk_mq_free_rq_map>> tags->rqs = NULL;
+	 *   - block/blk-mq.c|2533| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2536| <<blk_mq_alloc_rq_map>> if (!tags->rqs) {
+	 *   - block/blk-mq.c|2545| <<blk_mq_alloc_rq_map>> kfree(tags->rqs);
+	 */
 	hctx->tags->rqs[tag] = rq;
 }
 
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 3708271..76190b8 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -29,12 +29,23 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|694| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[0], vblk->vdev, 0);
+ *   - drivers/scsi/virtio_scsi.c|674| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
+	/*
+	 * 可能设置为:
+	 *   - drivers/virtio/virtio_pci_legacy.c|211| <<global>> .get_vq_affinity = vp_get_vq_affinity,
+	 *   - drivers/virtio/virtio_pci_modern.c|462| <<global>> .get_vq_affinity = vp_get_vq_affinity,
+	 *   - drivers/virtio/virtio_pci_modern.c|478| <<global>> .get_vq_affinity = vp_get_vq_affinity,
+	 */
 	if (!vdev->config->get_vq_affinity)
 		goto fallback;
 
diff --git a/block/blk-mq.c b/block/blk-mq.c
index fa024bc..fb4bc0c 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -38,9 +38,137 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 一共两处对queue_rq()的调用:
+ *   - blk_mq_try_issue_directly()-->__blk_mq_issue_directly()
+ *   - blk_mq_dispatch_rq_list()
+ */
+
+/*
+ * 一共三处缓存request的地方:
+ * 1. hctx->dispatch
+ * 2. ctx->rq_lists
+ * 3. scheduler自己的数据
+ *
+ *
+ * 下发hctx->dispatch的地方:
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+ *
+ * 添加到hctx->dispatch的地方:
+ *   - block/blk-mq-sched.c|422| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+ *   - block/blk-mq.c|1535| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+ *   - block/blk-mq.c|1935| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+ *   - block/blk-mq.c|2537| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+ *
+ *
+ * 下发ctx->rq_lists的地方:
+ *   - block/blk-mq.c|1148| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+ *   - block/blk-mq.c|1204| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+ *   - block/blk-mq.c|2528| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+ *
+ * 添加到ctx->rq_lists的地方:
+ *   - block/blk-mq-sched.c|377| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/blk-mq.c|1906| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+ *   - block/blk-mq.c|1908| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+ *   - block/blk-mq.c|1966| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+ *
+ *
+ * 调用scheduler的insert_requests()和requeue_request()缓存request的地方:
+ *   - block/blk-mq-sched.c|464| <<blk_mq_sched_insert_request>> e->type->ops.insert_requests(hctx, &list, at_head);
+ *   - block/blk-mq-sched.c|484| <<blk_mq_sched_insert_requests>> e->type->ops.insert_requests(hctx, list, false);
+ *   - block/blk-mq-sched.h|76| <<blk_mq_sched_requeue_request>> e->type->ops.requeue_request(rq);
+ *
+ * 调用scheduler的dispatch_request()准备下发request的地方:
+ *   - block/blk-mq-sched.c|111| <<blk_mq_do_dispatch_sched>> rq = e->type->ops.dispatch_request(hctx);
+ *
+ *
+ * blk_mq_sched_dispatch_requests()
+ * blk_mq_sched_bypass_insert()
+ * blk_mq_request_bypass_insert()
+ * blk_mq_hctx_notify_dead()
+ * flush_busy_ctx()
+ * dispatch_rq_from_ctx()
+ * blk_mq_attempt_merge()
+ * __blk_mq_insert_req_list()
+ * blk_mq_insert_requests()
+ * blk_mq_sched_insert_requests()
+ * blk_mq_sched_requeue_request()
+ * blk_mq_do_dispatch_sched()
+ */
+
+/*
+ * sched中下发的主要函数:
+ *
+ * - blk_mq_do_dispatch_sched()
+ *   对于参数的hctx, 如果没有调度器就返回
+ *   否则用调度器的dispatch_request()取出下一个request
+ *   用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ *
+ * - blk_mq_do_dispatch_ctx()
+ *   dequeue request one by one from sw queue if queue is busy
+ *   不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ *   然后为这个request调用queue_rq()
+ *   最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ *
+ *
+ * 插入的主要函数是blk_mq_sched_insert_request()
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
+ 
+/*
+ * BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+ * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+ *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+ *
+ * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+ *
+ * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+ *
+ * RQF_xxx用在request->rq_flags
+ *
+ * MQ_RQ_xxx用在request->state
+ *
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+
+/*
+ * 关于下发request到硬件驱动时的timeout:
+ *
+ * 比如virtio_queue_rq()调用blk_mq_start_request()
+ * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+ *
+ * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+ * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+ *
+ * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+ * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+ *
+ * - nvme的例子是nvme_timeout()
+ * - scsi的例子是scsi_timeout()
+ */
+
+/*
+ * 关于blk-mq complete的例子:
+ * 
+ * virtblk_done()调用blk_mq_complete_request()
+ * 调用__blk_mq_complete_request()
+ * 根据情况调用__blk_complete_request()或者q->mq_ops->complete()
+ *
+ * q->mq_ops->complete()的几个例子:
+ * - nvme_pci_complete_rq()
+ * - blkif_complete_rq()
+ * - virtblk_request_done()
+ * - scsi_softirq_done()
+ */
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2888| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3442| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, bytes, bucket;
@@ -59,10 +187,43 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
 }
 
 /*
+ * sched中下发的主要函数:
+ *
+ * - blk_mq_do_dispatch_sched()
+ *   对于参数的hctx, 如果没有调度器就返回
+ *   否则用调度器的dispatch_request()取出下一个request
+ *   用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ *
+ * - blk_mq_do_dispatch_ctx()
+ *   dequeue request one by one from sw queue if queue is busy
+ *   不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ *   然后为这个request调用queue_rq()
+ *   最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ *
+ *
+ * 插入的主要函数是blk_mq_sched_insert_request()
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
+/*
  * Check if any of the ctx's have pending work in this hardware queue
  */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * hctx->dispatch在以下插入新元素:
+	 *   - block/blk-mq-sched.c|189| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1303| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1663| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * hctx->ctx_map在以下被修改:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * blk_mq_sched_has_work():
+	 * 如果支持elevator, 用elevator的has_work查看是否有work
+	 */
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
 			blk_mq_sched_has_work(hctx);
@@ -71,6 +232,13 @@ static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1693| <<__blk_mq_insert_request>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *   - block/blk-mq.c|1730| <<blk_mq_insert_requests>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *
+ * 把ctx在hctx->ctx_map对应的bit设置
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
@@ -80,6 +248,12 @@ static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 		sbitmap_set_bit(&hctx->ctx_map, bit);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2722| <<blk_mq_hctx_notify_dead>> blk_mq_hctx_clear_pending(hctx, ctx);
+ *
+ * 把ctx在hctx->ctx_map对应的bit清空
+ */
 static void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,
 				      struct blk_mq_ctx *ctx)
 {
@@ -108,6 +282,10 @@ static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|74| <<part_in_flight>> return blk_mq_in_flight(q, part);
+ */
 unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	unsigned inflight[2];
@@ -119,6 +297,10 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight[0];
 }
 
+/*
+ * used only by:
+ *   - block/blk-mq.c|196| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ */
 static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 				     struct request *rq, void *priv,
 				     bool reserved)
@@ -131,6 +313,10 @@ static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|94| <<part_in_flight_rw>> blk_mq_in_flight_rw(q, part, inflight);
+ */
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2])
 {
@@ -140,12 +326,33 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|283| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|245| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3903| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|317| <<pmem_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3852| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	int freeze_depth;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 *
+	 * 其他使用的地方:
+	 *   - block/blk-core.c|500| <<blk_queue_enter>> (atomic_read(&q->mq_freeze_depth) == 0 &&
+	 */
 	freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
 	if (freeze_depth == 1) {
+		/*
+		 * 核心思想是把ref->percpu_count_ptr设置上__PERCPU_REF_DEAD然后percpu_ref_put(ref)
+		 * q_usage_counter的release是blk_queue_usage_counter_release()
+		 */
 		percpu_ref_kill(&q->q_usage_counter);
 		if (queue_is_mq(q))
 			blk_mq_run_hw_queues(q, false);
@@ -153,8 +360,27 @@ void blk_freeze_queue_start(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|246| <<blk_freeze_queue>> blk_mq_freeze_queue_wait(q);
+ *   - drivers/nvme/host/core.c|3841| <<nvme_wait_freeze>> blk_mq_freeze_queue_wait(ns->queue);
+ *
+ * 核心思想是等待所有get的都put (针对q->q_usage_counter)
+ */
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *
+	 * 这里唤醒了说明所有get的都put了
+	 */
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
@@ -172,6 +398,11 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);
  * Guarantee no request is in use, so we can change any data structure of
  * the queue afterward.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|386| <<blk_cleanup_queue>> blk_freeze_queue(q);
+ *   - block/blk-mq.c|399| <<blk_mq_freeze_queue>> blk_freeze_queue(q);
+ */
 void blk_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -182,6 +413,9 @@ void blk_freeze_queue(struct request_queue *q)
 	 * exported to drivers as the only user for unfreeze is blk_mq.
 	 */
 	blk_freeze_queue_start(q);
+	/*
+	 * 核心思想是等待所有get的都put (针对q->q_usage_counter)
+	 */
 	blk_mq_freeze_queue_wait(q);
 }
 
@@ -195,14 +429,61 @@ void blk_mq_freeze_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue);
 
+/*
+ * called by:
+ *   - block/blk-cgroup.c|1420| <<blkcg_activate_policy>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-cgroup.c|1462| <<blkcg_deactivate_policy>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-iolatency.c|874| <<iolatency_set_limit>> blk_mq_unfreeze_queue(blkg->q);
+ *   - block/blk-mq.c|3336| <<blk_mq_update_tag_set_depth>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-mq.c|4076| <<blk_mq_update_nr_requests>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-mq.c|4224| <<__blk_mq_update_nr_hw_queues>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-pm.c|90| <<blk_pre_runtime_suspend>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-sysfs.c|486| <<queue_wb_lat_store>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-zoned.c|479| <<blk_revalidate_disk_zones>> blk_mq_unfreeze_queue(q);
+ *   - block/blk-zoned.c|491| <<blk_revalidate_disk_zones>> blk_mq_unfreeze_queue(q);
+ *   - block/elevator.c|650| <<elevator_switch>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/aoe/aoedev.c|232| <<aoedev_downdev>> blk_mq_unfreeze_queue(d->blkq);
+ *   - drivers/block/ataflop.c|793| <<do_format>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/loop.c|226| <<__loop_update_dio>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|721| <<loop_change_fd>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1094| <<__loop_clr_fd>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1284| <<loop_set_status>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1504| <<loop_set_block_size>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/loop.c|1777| <<lo_release>> blk_mq_unfreeze_queue(lo->lo_queue);
+ *   - drivers/block/sunvdc.c|1140| <<vdc_queue_drain>> blk_mq_unfreeze_queue(q);
+ *   - drivers/block/swim3.c|841| <<release_drive>> blk_mq_unfreeze_queue(q);
+ *   - drivers/mtd/mtd_blkdevs.c|517| <<del_mtd_blktrans_dev>> blk_mq_unfreeze_queue(old->rq);
+ *   - drivers/nvme/host/core.c|1712| <<nvme_update_disk_info>> blk_mq_unfreeze_queue(disk->queue);
+ *   - drivers/nvme/host/core.c|3922| <<nvme_unfreeze>> blk_mq_unfreeze_queue(ns->queue);
+ *   - drivers/scsi/scsi_lib.c|2570| <<scsi_device_quiesce>> blk_mq_unfreeze_queue(q);
+ */
 void blk_mq_unfreeze_queue(struct request_queue *q)
 {
 	int freeze_depth;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 *
+	 * 其他使用的地方:
+	 *   - block/blk-core.c|500| <<blk_queue_enter>> (atomic_read(&q->mq_freeze_depth) == 0 &&
+	 */
 	freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
 	WARN_ON_ONCE(freeze_depth < 0);
 	if (!freeze_depth) {
 		percpu_ref_resurrect(&q->q_usage_counter);
+		/*
+		 * 使用的例子:
+		 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+		 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+		 *   - block/blk-core.c|527| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+		 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+		 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+		 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+		 */
 		wake_up_all(&q->mq_freeze_wq);
 	}
 }
@@ -212,6 +493,19 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  * FIXME: replace the scsi_internal_device_*block_nowait() calls in the
  * mpt3sas driver such that this function can be removed.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|297| <<blk_mq_quiesce_queue>> blk_mq_quiesce_queue_nowait(q);
+ *   - drivers/scsi/scsi_lib.c|2667| <<scsi_internal_device_block_nowait>> blk_mq_quiesce_queue_nowait(q);
+ *
+ * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+ *
+ * 会判断QUEUE_FLAG_QUIESCED是否设置了的地方:
+ *   - block/blk-mq-sched.c|286| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|2104| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|2483| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+ *   - drivers/mmc/core/queue.c|464| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
@@ -227,20 +521,63 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
  * sure no dispatch can happen until the queue is unquiesced via
  * blk_mq_unquiesce_queue().
  */
+/*
+ * called by:
+ *   - block/blk-core.c|360| <<blk_cleanup_queue>> blk_mq_quiesce_queue(q);
+ *   - block/blk-mq.c|3352| <<blk_mq_update_nr_requests>> blk_mq_quiesce_queue(q);
+ *   - block/elevator.c|645| <<elevator_switch>> blk_mq_quiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|958| <<virtblk_freeze>> blk_mq_quiesce_queue(vblk->disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2699| <<scsi_internal_device_block>> blk_mq_quiesce_queue(q)
+ *
+ * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+ * 然后根据hctx->flags & BLK_MQ_F_BLOCKING的情况使用synchronize_srcu()或者synchronize_rcu()
+ *
+ * 会判断QUEUE_FLAG_QUIESCED是否设置了的地方:
+ *   - block/blk-mq-sched.c|286| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|2104| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+ *   - block/blk-mq.c|2483| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+ *   - drivers/mmc/core/queue.c|464| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+ */
 void blk_mq_quiesce_queue(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 	bool rcu = false;
 
+	/*
+	 * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+	 *
+	 * 会判断QUEUE_FLAG_QUIESCED是否设置了的地方:
+	 *   - block/blk-mq-sched.c|286| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+	 *   - block/blk-mq.c|2104| <<blk_mq_run_hw_queue>> need_run = !blk_queue_quiesced(hctx->queue) &&
+	 *   - block/blk-mq.c|2483| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+	 *   - drivers/mmc/core/queue.c|464| <<mmc_cleanup_queue>> if (blk_queue_quiesced(q))
+	 */
 	blk_mq_quiesce_queue_nowait(q);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu
+		 *
+		 * 会设置到BLK_MQ_F_BLOCKING的地方, 非常少: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x20)
+		 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+		 *   - drivers/block/null_blk_main.c|1559| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+		 *   - drivers/block/paride/pd.c|910| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+		 *   - drivers/cdrom/gdrom.c|795| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+		 *   - drivers/ide/ide-probe.c|785| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+		 *   - drivers/mmc/core/queue.c|413| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+		 *   - drivers/mtd/mtd_blkdevs.c|448| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+		 */
 		if (hctx->flags & BLK_MQ_F_BLOCKING)
 			synchronize_srcu(hctx->srcu);
 		else
 			rcu = true;
 	}
+	/* wait until a grace period has elapsed */
+	/*
+	 * 该函数由RCU写端调用,它将阻塞写者,直到所有读执行单元完成对
+	 * 临界区的访问后,写者才可以继续下一步操作
+	 */
 	if (rcu)
 		synchronize_rcu();
 }
@@ -253,15 +590,36 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  * This function recovers queue into the state before quiescing
  * which is done by blk_mq_quiesce_queue.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3453| <<blk_mq_update_nr_requests>> blk_mq_unquiesce_queue(q);
+ *   - block/blk-sysfs.c|485| <<queue_wb_lat_store>> blk_mq_unquiesce_queue(q);
+ *   - block/elevator.c|649| <<elevator_switch>> blk_mq_unquiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|975| <<virtblk_restore>> blk_mq_unquiesce_queue(vblk->disk->queue);
+ *   - drivers/nvme/host/pci.c|1627| <<nvme_dev_remove_admin>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1661| <<nvme_alloc_admin_tags>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2709| <<scsi_start_queue>> blk_mq_unquiesce_queue(q);
+ *
+ * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+ * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+ */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
 	blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
 
 	/* dispatch requests which are inserted during quiescing */
+	/*
+	 * The LLD should handle requests when queue is dying.
+	 * 所以现在就算设置成了dying仍然要下发request
+	 */
 	blk_mq_run_hw_queues(q, true);
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * 只被以下调用:
+ *   - block/blk-core.c|286| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -272,6 +630,9 @@ void blk_mq_wake_waiters(struct request_queue *q)
 			blk_mq_tag_wakeup_all(hctx->tags, true);
 }
 
+/*
+ * 没人调用
+ */
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 {
 	return blk_mq_has_free_tags(hctx->tags);
@@ -282,14 +643,31 @@ EXPORT_SYMBOL(blk_mq_can_queue);
  * Only need start/end time stamping if we have stats enabled, or using
  * an IO scheduler.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|423| <<blk_mq_rq_ctx_init>> if (blk_mq_need_time_stamp(rq))
+ *   - block/blk-mq.c|640| <<__blk_mq_end_request>> if (blk_mq_need_time_stamp(rq))
+ */
 static inline bool blk_mq_need_time_stamp(struct request *rq)
 {
 	return (rq->rq_flags & RQF_IO_STAT) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|397| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+ *
+ * 根据参数在data->hctx->sched_tags或者data->hctx->tags的tags->static_rqs[tag]
+ * 取出一个request, 初始化一下
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则返回使用data->hctx->sched_tags 
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
 	req_flags_t rq_flags = 0;
@@ -311,8 +689,24 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	rq->q = data->q;
 	rq->mq_ctx = data->ctx;
 	rq->mq_hctx = data->hctx;
+	/*
+	 * 如果BLK_MQ_REQ_INTERNAL
+	 *     rq_flags是0
+	 * 如果没有BLK_MQ_REQ_INTERNAL
+	 *     如果是BLK_MQ_F_TAG_SHARED, rq_flags是RQF_MQ_INFLIGHT
+	 */
 	rq->rq_flags = rq_flags;
 	rq->cmd_flags = op;
+	/*
+	 * 在以下使用:
+	 *   - block/blk-core.c|400| <<blk_queue_enter>> const bool pm = flags & BLK_MQ_REQ_PREEMPT;
+	 *   - block/blk-core.c|583| <<blk_get_request>> WARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));
+	 *   - block/blk-mq.c|568| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_PREEMPT)
+	 *
+	 * 在以下设置:
+	 *   - drivers/ide/ide-pm.c|80| <<generic_ide_resume>> rq = blk_get_request(drive->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_PREEMPT);
+	 *   - drivers/scsi/scsi_lib.c|262| <<__scsi_execute>> REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, BLK_MQ_REQ_PREEMPT);
+	 */
 	if (data->flags & BLK_MQ_REQ_PREEMPT)
 		rq->rq_flags |= RQF_PREEMPT;
 	if (blk_queue_io_stat(data->q))
@@ -347,6 +741,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|423| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|474| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|1932| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -356,6 +756,7 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	unsigned int tag;
 	bool put_ctx_on_error = false;
 
+	/* 相当于: percpu_ref_get(&q->q_usage_counter); */
 	blk_queue_enter_live(q);
 	data->q = q;
 	if (likely(!data->ctx)) {
@@ -365,9 +766,15 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	if (likely(!data->hctx))
 		data->hctx = blk_mq_map_queue(q, data->cmd_flags,
 						data->ctx);
+	/*
+	 * BLK_MQ_REQ_NOWAIT: return when out of requests
+	 */
 	if (data->cmd_flags & REQ_NOWAIT)
 		data->flags |= BLK_MQ_REQ_NOWAIT;
 
+	/*
+	 * 如果支持scheduler, 就设置BLK_MQ_REQ_INTERNAL
+	 */
 	if (e) {
 		data->flags |= BLK_MQ_REQ_INTERNAL;
 
@@ -381,6 +788,12 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		    !(data->flags & BLK_MQ_REQ_RESERVED))
 			e->type->ops.limit_depth(data->cmd_flags, data);
 	} else {
+		/*
+		 * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+		 * 则不会调用__blk_mq_tag_busy()
+		 * __blk_mq_tag_busy()会为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+		 * 如果之前没设置, 增加hctx->tags->active_queues
+		 */
 		blk_mq_tag_busy(data->hctx);
 	}
 
@@ -394,6 +807,10 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		return NULL;
 	}
 
+	/*
+	 * 根据参数在data->hctx->sched_tags或者data->hctx->tags的tags->static_rqs[tag]
+	 * 取出一个request, 初始化一下
+	 */
 	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
 	if (!op_is_flush(data->cmd_flags)) {
 		rq->elv.icq = NULL;
@@ -409,6 +826,16 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|739| <<blk_get_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|994| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/block/sx8.c|511| <<carm_array_info>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/block/sx8.c|564| <<carm_send_special>> rq = blk_mq_alloc_request(host->oob_q, REQ_OP_DRV_OUT, 0);
+ *   - drivers/ide/ide-atapi.c|201| <<ide_prep_sense>> sense_rq = blk_mq_alloc_request(drive->queue, REQ_OP_DRV_IN,
+ *   - drivers/nvme/host/core.c|491| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/scsi/fnic/fnic_scsi.c|2272| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -435,6 +862,10 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|437| <<nvme_alloc_request>> req = blk_mq_alloc_request_hctx(q, op, flags,
+ */
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
 {
@@ -481,6 +912,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|755| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|1187| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -494,10 +930,28 @@ static void __blk_mq_free_request(struct request *rq)
 		blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
 	if (sched_tag != -1)
 		blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+	/*
+	 * 如果之前设置了BLK_MQ_S_SCHED_RESTART到hctx->state
+	 * 取消这个bit, 触发blk_mq_run_hw_queue(hctx, true);
+	 */
 	blk_mq_sched_restart(hctx);
+	/* 就是调用percpu_ref_put(&q->q_usage_counter); */
 	blk_queue_exit(q);
 }
 
+/*
+ * 主要调用的几个地方:
+ *   - block/bfq-iosched.c|1893| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|594| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|768| <<__blk_mq_end_request>> blk_mq_free_request(rq->next_rq);
+ *   - block/blk-mq.c|769| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|483| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/nvme/host/core.c|801| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|902| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|912| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1220| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|2241| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -557,6 +1011,28 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|197| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|379| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1632| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|2294| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/loop.c|485| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/null_blk_main.c|620| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|225| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/nvme/host/core.c|281| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *
+ * Bart Van Assche:
+ * Calling blk_mq_end_request() from outside the .queue_rq() or .complete()
+ * callback functions is wrong.
+ *
+ * Keith Busch:
+ * This callback can only see requests in MQ_RQ_IDLE state, and
+ * bkl_mq_end_request() is the correct way to end those that never entered
+ * a driver's queue_rq().
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -596,6 +1072,13 @@ static void __blk_mq_complete_request(struct request *rq)
 	}
 
 	/*
+	 * complete的几个例子:
+	 *   - virtblk_request_done()
+	 *   - scsi_softirq_done()
+	 *   - nvme_pci_complete_rq()
+	 */
+
+	/*
 	 * For a polled request, always complete locallly, it's pointless
 	 * to redirect the completion.
 	 */
@@ -620,18 +1103,32 @@ static void __blk_mq_complete_request(struct request *rq)
 	put_cpu();
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2095| <<__blk_mq_run_hw_queue>> hctx_unlock(hctx, srcu_idx);
+ *   - block/blk-mq.c|2217| <<blk_mq_run_hw_queue>> hctx_unlock(hctx, srcu_idx);
+ *   - block/blk-mq.c|2621| <<blk_mq_try_issue_directly>> hctx_unlock(hctx, srcu_idx);
+ */
 static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 	__releases(hctx->srcu)
 {
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING))
 		rcu_read_unlock();
 	else
 		srcu_read_unlock(hctx->srcu, srcu_idx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2090| <<__blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|2210| <<blk_mq_run_hw_queue>> hctx_lock(hctx, &srcu_idx);
+ *   - block/blk-mq.c|2586| <<blk_mq_try_issue_directly>> hctx_lock(hctx, &srcu_idx);
+ */
 static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
 	__acquires(hctx->srcu)
 {
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		/* shut up gcc false positive */
 		*srcu_idx = 0;
@@ -657,12 +1154,48 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|296| <<bt_tags_iter>> if (rq && blk_mq_request_started(rq))
+ *   - block/blk-mq.c|718| <<__blk_mq_requeue_request>> if (blk_mq_request_started(rq)) {
+ *   - drivers/block/nbd.c|648| <<nbd_read_stat>> if (!req || !blk_mq_request_started(req)) {
+ */
 int blk_mq_request_started(struct request *rq)
 {
+	/*
+	 * MQ_RQ_IDLE
+	 * MQ_RQ_IN_FLIGHT
+	 * MQ_RQ_COMPLETE
+	 */
 	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
 
+/*
+ * 被以下的例子调用:
+ *   - drivers/block/null_blk_main.c|1332| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|318| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/nvme/host/pci.c|942| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1638| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1722| <<scsi_queue_rq>> blk_mq_start_request(req);
+ *
+ * 关于下发request到硬件驱动时的timeout:
+ *
+ * 比如virtio_queue_rq()调用blk_mq_start_request()
+ * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+ *
+ * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+ * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+ *
+ * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+ * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+ *
+ * - nvme的例子是nvme_timeout()
+ * - scsi的例子是scsi_timeout()
+ *
+ * 设置request->state为MQ_RQ_IN_FLIGHT
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -685,6 +1218,9 @@ void blk_mq_start_request(struct request *rq)
 	blk_add_timer(rq);
 	WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
 
+	/*
+	 * q->dma_drain_size似乎目前只被libata-scsi使用
+	 */
 	if (q->dma_drain_size && blk_rq_bytes(rq)) {
 		/*
 		 * Make sure space for the drain appears.  We know we can do
@@ -754,6 +1290,13 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|137| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|878| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ *
+ * 把request插入q->requeue_list, 根据参数调用blk_mq_kick_requeue_list(q)
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -782,6 +1325,10 @@ EXPORT_SYMBOL(blk_mq_add_to_requeue_list);
 
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
+	/*
+	 * requeue_work被初始化为:
+	 *   - block/blk-mq.c|3666| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+	 */
 	kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
 }
 EXPORT_SYMBOL(blk_mq_kick_requeue_list);
@@ -805,6 +1352,10 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 }
 EXPORT_SYMBOL(blk_mq_tag_to_rq);
 
+/*
+ * 只在以下被使用, 被drivers/md/dm.c的md_in_flight()调用:
+ *   - block/blk-mq.c|1217| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ */
 static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			       void *priv, bool reserved)
 {
@@ -822,18 +1373,34 @@ static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - drivers/md/dm.c|666| <<md_in_flight>> return blk_mq_queue_inflight(md->queue);
+ */
 bool blk_mq_queue_inflight(struct request_queue *q)
 {
 	bool busy = false;
 
+	/*
+	 * 针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+	 * 调用blk_mq_rq_inflight()
+	 */
 	blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
 	return busy;
 }
 EXPORT_SYMBOL_GPL(blk_mq_queue_inflight);
 
+/*
+ * called only by:
+ *  - block/blk-mq.c|1054| <<blk_mq_check_expired>> blk_mq_rq_timed_out(rq, reserved);
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	req->rq_flags |= RQF_TIMED_OUT;
+	/*
+	 * nvme的例子是nvme_timeout()
+	 * scsi的例子是scsi_timeout()
+	 */
 	if (req->q->mq_ops->timeout) {
 		enum blk_eh_timer_return ret;
 
@@ -866,6 +1433,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|1113| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -904,6 +1475,20 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * timeout_work可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+ *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+ *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ *
+ * timeout_work使用的地方:
+ *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+ *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+ *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+ *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+ *
+ * 在以下使用:
+ *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -928,6 +1513,13 @@ static void blk_mq_timeout_work(struct work_struct *work)
 	if (!percpu_ref_tryget(&q->q_usage_counter))
 		return;
 
+	/*
+	 * To iterating over requests triggers race conditions with request execution
+	 *
+	 * That race isn't new.
+	 * You should only iterate when your queues are quieced to ensure the
+	 * request sent to a callback is stable.
+	 */
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
 
 	if (next != 0) {
@@ -953,6 +1545,13 @@ struct flush_busy_ctx_data {
 	struct list_head *list;
 };
 
+/*
+ * 只在以下被调用:
+ *   - block/blk-mq.c|1063| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+ *
+ * 把ctx->rq_list[type]的request们拼接到flush_busy_ctx_data->list
+ * 把ctx在hctx->ctx_map清空
+ */
 static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
 {
 	struct flush_busy_ctx_data *flush_data = data;
@@ -971,6 +1570,12 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
  * Process software queues that have been marked busy, splicing them
  * to the for-dispatch
  */
+/*
+ * 对于hctx->ctx_map中每个设置的bit对应的ctx
+ * 把ctx->rq_list[type]的request们拼接到参数的list
+ * 把ctx在hctx->ctx_map清空
+ * 最后参数list中的就是这个hctx->ctx_map中每一个ctx的rq_list的总和拼接
+ */
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 {
 	struct flush_busy_ctx_data data = {
@@ -978,6 +1583,15 @@ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 		.list = list,
 	};
 
+	/*
+	 * hctx->ctx_map在以下被设置:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * flush_busy_ctx():
+	 * 把ctx->rq_list[type]的request们拼接到flush_busy_ctx_data->list
+	 * 把ctx在hctx->ctx_map清空
+	 */
 	sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
 }
 EXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);
@@ -987,6 +1601,14 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * used only by:
+ *   - block/blk-mq.c|1395| <<blk_mq_dequeue_from_ctx>> dispatch_rq_from_ctx, &data);
+ *
+ * 从bitnr对应的hctx->ctx_map的ctx中的ctx->rq_lists[type].next取出一个request
+ * 放入dispatch_data->rq
+ * 如果取完空了就把hctx->ctx_map对应的bit清掉
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1007,6 +1629,12 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * 只被如下调用:
+ *   - block/blk-mq-sched.c|221| <<blk_mq_do_dispatch_ctx>> rq = blk_mq_dequeue_from_ctx(hctx, ctx);
+ *
+ * 从start代表的第一个hctx->ctx_map开始, 取出一个request, 就取出一个啊!!!
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1016,6 +1644,15 @@ struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 		.rq   = NULL,
 	};
 
+	/*
+	 * 从bitnr对应的hctx->ctx_map的ctx中的ctx->rq_lists[type].next取出一个request (就一个!!)
+	 * 放入dispatch_data->rq
+	 * 如果取完空了就把hctx->ctx_map对应的bit清掉
+	 *
+	 * 因为dispatch_rq_from_ctx()成功返回0, 所以实际就取出一个的
+	 *
+	 * 如果fn返回true继续下一个bit, 如果返回false就直接返回了
+	 */
 	__sbitmap_for_each_set(&hctx->ctx_map, off,
 			       dispatch_rq_from_ctx, &data);
 
@@ -1030,8 +1667,24 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1103| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1128| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1207| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1239| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|1822| <<blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ *
+ * 如果request->tag不为-1, 直接返回
+ * 否则分配一个给driver用的(不是internal的)tag!
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 */
 	struct blk_mq_alloc_data data = {
 		.q = rq->q,
 		.hctx = rq->mq_hctx,
@@ -1046,6 +1699,10 @@ bool blk_mq_get_driver_tag(struct request *rq)
 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
 		data.flags |= BLK_MQ_REQ_RESERVED;
 
+	/*
+	 * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+	 * 则不会调用__blk_mq_tag_busy()
+	 */
 	shared = blk_mq_tag_busy(data.hctx);
 	rq->tag = blk_mq_get_tag(&data);
 	if (rq->tag >= 0) {
@@ -1060,6 +1717,10 @@ bool blk_mq_get_driver_tag(struct request *rq)
 	return rq->tag != -1;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2990| <<blk_mq_init_hctx>> init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+ */
 static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
 				int flags, void *key)
 {
@@ -1081,6 +1742,11 @@ static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
  * restart. For both cases, take care to check the condition again after
  * marking us as waiting.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1761| <<blk_mq_dispatch_rq_list>> if (!blk_mq_mark_tag_wait(hctx, rq)) {
+ *     blk_mq_dispatch_rq_list()为参数list中的每一个request调用queue_rq()
+ */
 static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 				 struct request *rq)
 {
@@ -1088,7 +1754,22 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 	wait_queue_entry_t *wait;
 	bool ret;
 
+	/*
+	 * For non-shared tags, we can simply mark us needing a restart.
+	 */
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED)) {
+		/*
+		 * 在以下使用:
+		 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+		 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+		 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+		 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *
+		 * 用来触发blk_mq_run_hw_queue()
+		 */
 		if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 			set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 
@@ -1100,13 +1781,38 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 		 * Don't clear RESTART here, someone else could have set it.
 		 * At most this will cost an extra queue run.
 		 */
+		/*
+		 * 如果request->tag不为-1, 直接返回
+		 * 否则分配一个给driver用的(不是internal的)tag!
+		 *
+		 * 这样就不会hang? 返回-1分配tag失败??
+		 */
 		return blk_mq_get_driver_tag(rq);
 	}
 
+	/* func是blk_mq_dispatch_wake() */
 	wait = &hctx->dispatch_wait;
+	/*
+	 * hctx->dispatch_wait已经wait上了??
+	 *
+	 * 如果wait不在任何waitqueue上, 
+	 * list_empty_careful(&wait->entry)返回true=1,
+	 * !list_empty_careful(&wait->entry)返回false=0
+	 * return false就不会执行
+	 *
+	 * 如果wait已经在某个waitqueue上了
+	 * list_empty_careful(&wait->entry)返回false=0
+	 * !list_empty_careful(&wait->entry)返回true=1
+	 * return false执行!!!!
+	 */
 	if (!list_empty_careful(&wait->entry))
 		return false;
 
+	/*
+	 * 在以下设置了tags:
+	 *   - block/blk-mq.c|2302| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+	 *   - block/blk-mq.c|2504| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+	 */
 	wq = &bt_wait_ptr(&hctx->tags->bitmap_tags, hctx)->wait;
 
 	spin_lock_irq(&wq->lock);
@@ -1118,6 +1824,12 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 	}
 
 	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
+	/*
+	 * 在blk_mq_put_tag()会唤醒!!!!!!!!!!!!!!
+	 * 唤醒的时候调用blk_mq_dispatch_wake()
+	 *
+	 * 唤醒就是调用blk_mq_run_hw_queue(hctx, true);
+	 */
 	__add_wait_queue(wq, wait);
 
 	/*
@@ -1136,6 +1848,9 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 	 * We got a tag, remove ourselves from the wait queue to ensure
 	 * someone else gets the wakeup.
 	 */
+	/*
+	 * 如果分到了就在waitqueue上删除, 不要被唤醒了
+	 */
 	list_del_init(&wait->entry);
 	spin_unlock(&hctx->dispatch_wait_lock);
 	spin_unlock_irq(&wq->lock);
@@ -1177,6 +1892,15 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|114| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|164| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|208| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 为参数list中的每一个request调用queue_rq()
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1189,6 +1913,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	if (list_empty(list))
 		return false;
 
+	/* list_is_singular(): tests whether a list has just one entry */
 	WARN_ON(!list_is_singular(list) && got_budget);
 
 	/*
@@ -1200,10 +1925,20 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 
 		rq = list_first_entry(list, struct request, queuelist);
 
+		/*
+		 * 在以下设置:
+		 *   - block/blk-flush.c|298| <<blk_kick_flush>> flush_rq->mq_hctx = first_rq->mq_hctx;
+		 *   - block/blk-mq.c|317| <<blk_mq_rq_ctx_init>> rq->mq_hctx = data->hctx;
+		 *   - block/blk-mq.c|502| <<__blk_mq_free_request>> rq->mq_hctx = NULL;
+		 */
 		hctx = rq->mq_hctx;
 		if (!got_budget && !blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * 如果request->tag不为-1, 直接返回
+		 * 否则分配一个给driver用的(不是internal的)tag!
+		 */
 		if (!blk_mq_get_driver_tag(rq)) {
 			/*
 			 * The initial allocation attempt failed, so we need to
@@ -1331,6 +2066,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1530| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1699| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1366,9 +2106,13 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * __blk_mq_run_hw_queue()是唯一调用blk_mq_sched_dispatch_requests()的地方
+	 */
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
 }
@@ -1434,6 +2178,7 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 	if (unlikely(blk_mq_hctx_stopped(hctx)))
 		return;
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		int cpu = get_cpu();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
@@ -1455,6 +2200,22 @@ void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|79| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|452| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|476| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|157| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1198| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1455| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1639| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1704| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1724| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1798| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2106| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|709| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1469,6 +2230,10 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 	 * quiesced.
 	 */
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * 如果q->queue_flags没设置QUEUE_FLAG_QUIESCED
+	 * 并且有待处理的request
+	 */
 	need_run = !blk_queue_quiesced(hctx->queue) &&
 		blk_mq_hctx_has_pending(hctx);
 	hctx_unlock(hctx, srcu_idx);
@@ -1590,6 +2355,10 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2327| <<blk_mq_init_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1605,6 +2374,12 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 	__blk_mq_run_hw_queue(hctx);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|1820| <<__blk_mq_insert_request>> __blk_mq_insert_req_list(hctx, rq, at_head);
+ *
+ * 把request添加到对应的request->mq_ctx的ctx->rq_lists[type]
+ */
 static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    bool at_head)
@@ -1622,6 +2397,10 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * 把request放入request->mq_ctx的rq_lists
+ * 然后把ctx在hctx->ctx_map对应的bit设置
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1637,6 +2416,10 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * 直接把request插入hctx->dispatch,
+ * 如果参数run_queue是true就调用blk_mq_run_hw_queue()
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_hw_ctx *hctx = rq->mq_hctx;
@@ -1649,6 +2432,13 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|429| <<blk_mq_sched_insert_requests>> blk_mq_insert_requests(hctx, ctx, list);
+ *
+ * 把list中的request们放入ctx->rq_lists[type]
+ * 把ctx在hctx->ctx_map对应的bit设置
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1667,6 +2457,7 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 
 	spin_lock(&ctx->lock);
 	list_splice_tail_init(list, &ctx->rq_lists[type]);
+	/* 把ctx在hctx->ctx_map对应的bit设置 */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 	spin_unlock(&ctx->lock);
 }
@@ -1749,6 +2540,12 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2046| <<blk_mq_try_issue_directly>> ret = __blk_mq_issue_directly(hctx, rq, cookie, last);
+ *
+ * 为参数的request调用queue_rq()!
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1788,6 +2585,15 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1267| <<blk_insert_cloned_request>> return blk_mq_try_issue_directly(rq->mq_hctx, rq, &unused, true, true);
+ *   - block/blk-mq.c|2089| <<blk_mq_try_issue_list_directly>> ret = blk_mq_try_issue_directly(hctx, rq, &unused,
+ *   - block/blk-mq.c|2213| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2220| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie, false, true);
+ *
+ * 核心思想是为参数的request调用queue_rq()!
+ */
 blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1829,6 +2635,9 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	 *.queue_rq() to the hardware dispatch list.
 	 */
 	force = true;
+	/*
+	 * 为参数的request调用queue_rq()!
+	 */
 	ret = __blk_mq_issue_directly(hctx, rq, cookie, last);
 out_unlock:
 	hctx_unlock(hctx, srcu_idx);
@@ -1860,6 +2669,10 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|484| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1904,7 +2717,16 @@ static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 
 static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 {
+	/*
+	 * Reads are always treated as synchronous, as are requests with the FUA or
+	 * PREFLUSH flag.  Other operations may be marked as synchronous using the
+	 * REQ_SYNC flag.
+	 */
 	const int is_sync = op_is_sync(bio->bi_opf);
+	/*
+	 * Check if the bio or request is one that needs special treatment in the
+	 * flush state machine.
+	 */
 	const int is_flush_fua = op_is_flush(bio->bi_opf);
 	struct blk_mq_alloc_data data = { .flags = 0};
 	struct request *rq;
@@ -1914,6 +2736,10 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	blk_queue_bounce(q, &bio);
 
+	/*
+	 * 根据块设备请求队列的limits.max_sectors和limits.max_segmetns
+	 * 来拆分bio,适应设备缓存.会在函数blk_set_default_limits中设置
+	 */
 	blk_queue_split(q, &bio);
 
 	if (!bio_integrity_prep(bio))
@@ -2053,6 +2879,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|527| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|633| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2846| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2110,6 +2942,12 @@ static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|671| <<blk_mq_sched_alloc_tags>> ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
+ *   - block/blk-mq-tag.c|811| <<blk_mq_tag_update_depth>> ret = blk_mq_alloc_rqs(set, new, hctx->queue_num, tdepth);
+ *   - block/blk-mq.c|3002| <<__blk_mq_alloc_rq_map>> ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
+ */
 int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx, unsigned int depth)
 {
@@ -2192,6 +3030,10 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  * software queue to the hw queue dispatch list, and ensure that it
  * gets run.
  */
+/*
+ * used by:
+ *   - block/blk-mq.c|4186| <<blk_mq_init>> blk_mq_hctx_notify_dead);
+ */
 static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2228,6 +3070,12 @@ static void blk_mq_remove_cpuhp(struct blk_mq_hw_ctx *hctx)
 }
 
 /* hctx->ctxs will be freed in queue's release handler */
+/*
+ * called by:
+ *   - block/blk-mq.c|2940| <<blk_mq_exit_hw_queues>> blk_mq_exit_hctx(q, set, hctx, i);
+ *   - block/blk-mq.c|3494| <<blk_mq_realloc_hw_ctxs>> blk_mq_exit_hctx(q, set, hctxs[i], i);
+ *   - block/blk-mq.c|3526| <<blk_mq_realloc_hw_ctxs>> blk_mq_exit_hctx(q, set, hctx, j);
+ */
 static void blk_mq_exit_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
@@ -2249,6 +3097,10 @@ static void blk_mq_exit_hctx(struct request_queue *q,
 	sbitmap_free(&hctx->ctx_map);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|3852| <<blk_mq_free_queue>> blk_mq_exit_hw_queues(q, set, set->nr_hw_queues);
+ */
 static void blk_mq_exit_hw_queues(struct request_queue *q,
 		struct blk_mq_tag_set *set, int nr_queue)
 {
@@ -2263,6 +3115,10 @@ static void blk_mq_exit_hw_queues(struct request_queue *q,
 	}
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|3260| <<blk_mq_alloc_and_init_hctx>> if (blk_mq_init_hctx(q, set, hctx, hctx_idx)) {
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2314,6 +3170,7 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx, node))
 		goto free_fq;
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (hctx->flags & BLK_MQ_F_BLOCKING)
 		init_srcu_struct(hctx->srcu);
 
@@ -2333,6 +3190,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|2941| <<blk_mq_init_allocated_queue>> blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+ */
 static void blk_mq_init_cpu_queues(struct request_queue *q,
 				   unsigned int nr_hw_queues)
 {
@@ -2392,6 +3253,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2915| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3295| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2404,6 +3270,7 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/* 遍历每一个request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2430,6 +3297,14 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		}
 
 		ctx = per_cpu_ptr(q->queue_ctx, i);
+		/*
+		 * 设置nr_maps的地方:
+		 *   - block/blk-mq.c|2713| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3058| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3069| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - drivers/nvme/host/pci.c|2322| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2324| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 */
 		for (j = 0; j < set->nr_maps; j++) {
 			if (!set->map[j].nr_queues) {
 				ctx->hctxs[j] = blk_mq_map_queue_type(q,
@@ -2459,6 +3334,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			BUG_ON(!hctx->nr_ctx);
 		}
 
+		/*
+		 * 这里很重要, 把剩下的的j网上的type都map到HCTX_TYPE_DEFAULT的hctx
+		 */
 		for (; j < HCTX_MAX_TYPES; j++)
 			ctx->hctxs[j] = blk_mq_map_queue_type(q,
 					HCTX_TYPE_DEFAULT, i);
@@ -2532,6 +3410,9 @@ static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * 核心思想是把request从tagset->tag_list删除
+ */
 static void blk_mq_del_queue_tag_set(struct request_queue *q)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -2548,6 +3429,13 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2914| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ *
+ * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+ * 多个request_queue可能共享1个blk_mq_tag_set
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2570,6 +3458,12 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|3299| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ *
+ * 分配软件的request_queue->queue_ctx!
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
 	struct blk_mq_ctxs *ctxs;
@@ -2603,6 +3497,10 @@ static int blk_mq_alloc_ctxs(struct request_queue *q)
  * and headache because q->mq_kobj shouldn't have been introduced,
  * but we can't group ctx/kctx kobj without it.
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|862| <<__blk_release_queue>> blk_mq_release(q);
+ */
 void blk_mq_release(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2688,6 +3586,10 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 	return hw_ctx_size;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|3245| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2707,6 +3609,16 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		return NULL;
 	}
 
+	/*
+	 * 是伴随着RQF_MQ_INFLIGHT的:
+	 *   - block/blk-mq-debugfs.c|641| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 *   - block/blk-mq-tag.c|98| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq.c|511| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|751| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|1428| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|3205| <<blk_mq_alloc_and_init_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq.h|290| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 */
 	atomic_set(&hctx->nr_active, 0);
 	hctx->numa_node = node;
 	hctx->queue_num = hctx_idx;
@@ -2721,10 +3633,22 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return hctx;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2881| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3287| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
 	int i, j, end;
+	/*
+	 * q->queue_hw_ctx第一维在以下分配:
+	 *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+	 *
+	 * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+	 *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+	 */
 	struct blk_mq_hw_ctx **hctxs = q->queue_hw_ctx;
 
 	/* protect against switching io scheduler  */
@@ -2739,6 +3663,9 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 		 * we need to realloc the hctx. If allocation fails, fallback
 		 * to use the previous one.
 		 */
+		/*
+		 * 如果已经分配了就不用分配了
+		 */
 		if (hctxs[i] && (hctxs[i]->numa_node == node))
 			continue;
 
@@ -2791,20 +3718,47 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
  * have more than the CPUs (software queues). For multiple sets, the tag_set
  * user may have set ->nr_hw_queues larger.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->nr_queues = nr_hw_queues(set);
+ *   - block/blk-mq.c|3620| <<blk_mq_alloc_tag_set>> set->tags = kcalloc_node(nr_hw_queues(set), sizeof(struct blk_mq_tags *),
+ *   - block/blk-mq.c|3670| <<blk_mq_free_tag_set>> for (i = 0; i < nr_hw_queues(set); i++)
+ *
+ * 实现如下:
+ *   if (set->nr_maps == 1)
+ *       return nr_cpu_ids;
+ *   return max(set->nr_hw_queues, nr_cpu_ids);
+ */
 static unsigned int nr_hw_queues(struct blk_mq_tag_set *set)
 {
 	if (set->nr_maps == 1)
 		return nr_cpu_ids;
 
+	/*
+	 * set->nr_hw_queues在非驱动更新的地方:
+	 *   - block/blk-mq.c|3189| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3630| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1; --> kdump情况下
+	 *   - block/blk-mq.c|3639| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids; --> set->nr_maps==1的情况下
+	 *   - block/blk-mq.c|3874| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3882| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 */
 	return max(set->nr_hw_queues, nr_cpu_ids);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3167| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q);
+ *   - drivers/md/dm-rq.c|546| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q)
 {
 	/* mark the queue as mq asap */
 	q->mq_ops = set->ops;
 
+	/*
+	 * struct blk_stat_callback
+	 */
 	q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
 					     blk_mq_poll_stats_bkt,
 					     BLK_MQ_POLL_STATS_BKTS, q);
@@ -2817,7 +3771,21 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	/* init q->mq_kobj and sw queues' kobjects */
 	blk_mq_sysfs_init(q);
 
+	/*
+	 * Maximum number of hardware queues we support. For single sets, we'll never
+	 * have more than the CPUs (software queues). For multiple sets, the tag_set
+	 * user may have set ->nr_hw_queues larger.
+	 */
+	/*
+	 * nr_hw_queues(set)的实现:
+	 *   if (set->nr_maps == 1)
+	 *       return nr_cpu_ids;
+	 *   return max(set->nr_hw_queues, nr_cpu_ids);
+	 */
 	q->nr_queues = nr_hw_queues(set);
+	/*
+	 * 注意, 数量是上面的q->nr_queues = nr_hw_queues(set)!!!!!!!!!!
+	 */
 	q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
 						GFP_KERNEL, set->numa_node);
 	if (!q->queue_hw_ctx)
@@ -2856,6 +3824,10 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	q->poll_nsec = -1;
 
 	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+	/*
+	 * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+	 * 多个request_queue可能共享1个blk_mq_tag_set
+	 */
 	blk_mq_add_queue_tag_set(set, q);
 	blk_mq_map_swqueue(q);
 
@@ -2879,10 +3851,15 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_allocated_queue);
 
+/*
+ * called by only:
+ *   - block/blk-core.c|464| <<blk_cleanup_queue>> blk_mq_free_queue(q);
+ */
 void blk_mq_free_queue(struct request_queue *q)
 {
 	struct blk_mq_tag_set	*set = q->tag_set;
 
+	/* 核心思想是把request从tagset->tag_list删除 */
 	blk_mq_del_queue_tag_set(q);
 	blk_mq_exit_hw_queues(q, set, set->nr_hw_queues);
 }
@@ -2939,6 +3916,13 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by"
+ *   - block/blk-mq.c|3831| <<blk_mq_alloc_tag_set>> ret = blk_mq_update_queue_map(set);
+ *   - block/blk-mq.c|4055| <<__blk_mq_update_nr_hw_queues>> blk_mq_update_queue_map(set);
+ *
+ * 初始化软件队列到硬件队列的mapping???
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2964,6 +3948,7 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		return set->ops->map_queues(set);
 	} else {
 		BUG_ON(set->nr_maps > 1);
+		/* 初始化blk_mq_queue_map->map */
 		return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
 	}
 }
@@ -2974,6 +3959,23 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * vblk->tag_set.ops = &virtio_mq_ops;
+ * vblk->tag_set.queue_depth = virtblk_queue_depth;
+ * vblk->tag_set.numa_node = NUMA_NO_NODE;
+ * vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+ * vblk->tag_set.cmd_size =
+ *                    sizeof(struct virtblk_req) +
+ *                    sizeof(struct scatterlist) * sg_elems;
+ * vblk->tag_set.driver_data = vblk;
+ * vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+ *
+ * err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ * if (err)
+ *	goto out_put_disk;
+ *
+ * q = blk_mq_init_queue(&vblk->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -2990,6 +3992,7 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (!set->ops->queue_rq)
 		return -EINVAL;
 
+	/* 要么都设置, 要么都不设置 */
 	if (!set->ops->get_budget ^ !set->ops->put_budget)
 		return -EINVAL;
 
@@ -3021,6 +4024,11 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)
 		set->nr_hw_queues = nr_cpu_ids;
 
+	/*
+	 * 每个queue一个 struct blk_mq_tags
+	 *
+	 * 类型是struct blk_mq_tags **tags
+	 */
 	set->tags = kcalloc_node(nr_hw_queues(set), sizeof(struct blk_mq_tags *),
 				 GFP_KERNEL, set->numa_node);
 	if (!set->tags)
@@ -3028,6 +4036,10 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * nr_hw_queues可能比cpu数量多,
+		 * 猜测是每一个sw queue对应的hw queue??? 所以用cpu的数量
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
@@ -3036,10 +4048,16 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 		set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
 	}
 
+	/*
+	 * 初始化软件队列到硬件队列的mapping???
+	 */
 	ret = blk_mq_update_queue_map(set);
 	if (ret)
 		goto out_free_mq_map;
 
+	/*
+	 * 分配每一个set->tags
+	 */
 	ret = blk_mq_alloc_rq_maps(set);
 	if (ret)
 		goto out_free_mq_map;
@@ -3077,6 +4095,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3086,13 +4108,26 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 	if (!set)
 		return -EINVAL;
 
+	/*
+	 * nr_requests在以下设置:
+	 *   - block/blk-mq-sched.c|563| <<blk_mq_init_sched>> q->nr_requests = q->tag_set->queue_depth;
+	 *   - block/blk-mq-sched.c|572| <<blk_mq_init_sched>> q->nr_requests = 2 * min_t(unsigned int , q->tag_set->queue_depth,
+	 *   - block/blk-mq.c|3416| <<blk_mq_init_allocated_queue>> q->nr_requests = set->queue_depth;
+	 *   - block/blk-mq.c|3723| <<blk_mq_update_nr_requests>> q->nr_requests = nr;
+	 *   - block/blk-settings.c|123| <<blk_queue_make_request>> q->nr_requests = BLKDEV_MAX_RQ;
+	 */
 	if (q->nr_requests == nr)
 		return 0;
 
 	blk_mq_freeze_queue(q);
+	/*
+	 * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+	 * 然后根据hctx->flags & BLK_MQ_F_BLOCKING的情况使用synchronize_srcu()或者synchronize_rcu()
+	 */
 	blk_mq_quiesce_queue(q);
 
 	ret = 0;
+	/* 遍历每一个request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (!hctx->tags)
 			continue;
@@ -3114,6 +4149,10 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 	if (!ret)
 		q->nr_requests = nr;
 
+	/*
+	 * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+	 * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+	 */
 	blk_mq_unquiesce_queue(q);
 	blk_mq_unfreeze_queue(q);
 
@@ -3190,6 +4229,19 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|3905| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ *
+ * 最终被以下间接调用:
+ *   - drivers/block/nbd.c|1154| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2492| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2347| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|891| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1595| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|484| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3253,6 +4305,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1154| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2492| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2347| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|891| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1595| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|484| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
diff --git a/block/blk-mq.h b/block/blk-mq.h
index c11353a..0904e22 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -7,6 +7,9 @@
 
 struct blk_mq_tag_set;
 
+/*
+ * 这个结构保存的真正的request_queue中每个ctx的实体!
+ */
 struct blk_mq_ctxs {
 	struct kobject kobj;
 	struct blk_mq_ctx __percpu	*queue_ctx;
@@ -18,10 +21,20 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 添加的地方:
+		 *   - block/blk-mq.c|1808| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1810| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1861| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|2922| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -99,10 +112,29 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *
  * @flags: request command flags
  * @cpu: cpu ctx
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|389| <<__blk_mq_sched_bio_merge>> struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, bio->bi_opf, ctx);
+ *   - block/blk-mq-tag.c|182| <<blk_mq_get_tag>> data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
+ *   - block/blk-mq.c|580| <<blk_mq_get_request>> data->hctx = blk_mq_map_queue(q, data->cmd_flags,
+ *   - block/blk.h|41| <<blk_get_flush_queue>> return blk_mq_map_queue(q, REQ_OP_FLUSH, ctx)->fq;
+ *
+ * 如果是REQ_HIPRI, 返回HCTX_TYPE_POLL
+ * 如果是REQ_OP_READ, 返回HCTX_TYPE_READ
+ * 其他的, 返回默认的HCTX_TYPE_DEFAULT
+ * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+ * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+ * 返回的仍然可以是HCTX_TYPE_DEFAULT
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 						     unsigned int flags,
 						     struct blk_mq_ctx *ctx)
 {
+	/*
+	 * HCTX_TYPE_DEFAULT = 0
+	 * HCTX_TYPE_READ    = 1
+	 * HCTX_TYPE_POLL    = 2
+	 */
 	enum hctx_type type = HCTX_TYPE_DEFAULT;
 
 	/*
@@ -112,7 +144,12 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		type = HCTX_TYPE_POLL;
 	else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
 		type = HCTX_TYPE_READ;
-	
+
+	/*
+	 * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+	 * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+	 * 返回的仍然可以是HCTX_TYPE_DEFAULT
+	 */
 	return ctx->hctxs[type];
 }
 
@@ -134,6 +171,12 @@ void blk_mq_release(struct request_queue *q);
  */
 static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
 {
+	/*
+	 * 返回的mq_rq_state类型:
+	 * - MQ_RQ_IDLE
+	 * - MQ_RQ_IN_FLIGHT
+	 * - MQ_RQ_COMPLETE
+	 */
 	return READ_ONCE(rq->state);
 }
 
@@ -149,6 +192,9 @@ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
  * care about preemption, since we know the ctx's are persistent. This does
  * mean that we can't rely on ctx always matching the currently running CPU.
  */
+/*
+ * 会调用get_cpu()
+ */
 static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
 {
 	return __blk_mq_get_ctx(q, get_cpu());
@@ -171,14 +217,37 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|115| <<blk_mq_get_tag>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq-tag.c|179| <<blk_mq_get_tag>> tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq.c|297| <<blk_mq_rq_ctx_init>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *
+ * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+ * 则返回使用data->hctx->sched_tags
+ * 否则返回data->hctx->tags
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * BLK_MQ_REQ_INTERNAL在以下被设置:
+	 *   - block/blk-mq.c|372| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
 		return data->hctx->sched_tags;
 
 	return data->hctx->tags;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|207| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1833| <<__blk_mq_delay_run_hw_queue>> if (unlikely(blk_mq_hctx_stopped(hctx)))
+ *   - block/blk-mq.c|1907| <<blk_mq_run_hw_queues>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1928| <<blk_mq_queue_stopped>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1991| <<blk_mq_start_stopped_hw_queue>> if (!blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|2263| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+ */
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -193,6 +262,9 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part);
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2]);
 
+/*
+ * 调用q->mq_ops->put_budget(hctx)
+ */
 static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -201,6 +273,9 @@ static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 		q->mq_ops->put_budget(hctx);
 }
 
+/*
+ * 调用q->mq_ops->get_budget(hctx)
+ */
 static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -210,18 +285,34 @@ static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 	return true;
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ */
 static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 					   struct request *rq)
 {
 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
 	rq->tag = -1;
 
+	/*
+	 * 是伴随着hctx->nr_active的, 每次设置的时候, hctx->nr_active就增长
+	 * 取消的时候, hctx->nr_active就减少
+	 *   - block/blk-mq.c|510| <<blk_mq_rq_ctx_init>> rq_flags = RQF_MQ_INFLIGHT;
+	 *   - block/blk-mq.c|750| <<blk_mq_free_request>> if (rq->rq_flags & RQF_MQ_INFLIGHT)
+	 *   - block/blk-mq.c|1427| <<blk_mq_get_driver_tag>> rq->rq_flags |= RQF_MQ_INFLIGHT;
+	 *   - block/blk-mq.h|288| <<__blk_mq_put_driver_tag>> if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+	 *   - block/blk-mq.h|289| <<__blk_mq_put_driver_tag>> rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+	 */
 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
 		atomic_dec(&hctx->nr_active);
 	}
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ * 如果rq->tag和rq->internal_tag有一个已经是-1了就不用了
+ */
 static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
 {
@@ -231,6 +322,10 @@ static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ * 如果rq->tag和rq->internal_tag有一个已经是-1了就不用了
+ */
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	if (rq->tag == -1 || rq->internal_tag == -1)
@@ -239,6 +334,13 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|60| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3480| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ *
+ * 把每个blk_mq_queue_map->mq_map[cpu]都设置成0
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 6375afa..a58b773 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -20,6 +20,10 @@ EXPORT_SYMBOL(blk_max_low_pfn);
 
 unsigned long blk_max_pfn;
 
+/*
+ * 被很多地方调用, 其中一个例子
+ *   - block/blk-mq.c|2849| <<blk_mq_init_allocated_queue>> blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);
+ */
 void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
 {
 	q->rq_timeout = timeout;
@@ -33,6 +37,11 @@ EXPORT_SYMBOL_GPL(blk_queue_rq_timeout);
  * Description:
  *   Returns a queue_limit struct to its default state.
  */
+/*
+ * called by:
+ *   - block/blk-settings.c|73| <<blk_set_stacking_limits>> blk_set_default_limits(lim);
+ *   - block/blk-settings.c|119| <<blk_queue_make_request>> blk_set_default_limits(&q->limits);
+ */
 void blk_set_default_limits(struct queue_limits *lim)
 {
 	lim->max_segments = BLK_MAX_SEGMENTS;
@@ -178,6 +187,13 @@ EXPORT_SYMBOL(blk_queue_bounce_limit);
  *    per-device basis in /sys/block/<device>/queue/max_sectors_kb.
  *    The soft limit can not exceed max_hw_sectors.
  **/
+/*
+ * 被很多调用, 这里是几个调用的例子:
+ *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+ *   - drivers/block/virtio_blk.c|827| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+ *   - drivers/nvme/host/core.c|1972| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+ *   - drivers/block/loop.c|1968| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+ */
 void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
 {
 	struct queue_limits *limits = &q->limits;
@@ -264,6 +280,14 @@ EXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);
  *    Enables a low level driver to set an upper limit on the number of
  *    hw data segments in a request.
  **/
+/*
+ * 选择的一些地方:
+ *   - drivers/block/virtio_blk.c|824| <<virtblk_probe>> blk_queue_max_segments(q, vblk->sg_elems-2);
+ *   - drivers/block/xen-blkfront.c|954| <<blkif_set_queue_limits>> blk_queue_max_segments(rq, segments / GRANTS_PER_PSEG);
+ *   - drivers/block/xen-blkfront.c|2027| <<blkif_recover>> blk_queue_max_segments(info->rq, segs / GRANTS_PER_PSEG);
+ *   - drivers/nvme/host/core.c|1973| <<nvme_set_queue_limits>> blk_queue_max_segments(q, min_t(u32, max_segments, USHRT_MAX));
+ *   - drivers/scsi/scsi_lib.c|1828| <<__scsi_init_queue>> blk_queue_max_segments(q, min_t(unsigned short , shost->sg_tablesize,
+ */
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
@@ -462,6 +486,11 @@ EXPORT_SYMBOL(blk_queue_io_opt);
  * @t:	the stacking driver (top)
  * @b:  the underlying device (bottom)
  **/
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_nl.c|1377| <<drbd_setup_queue_param>> blk_queue_stack_limits(q, b);
+ *   - drivers/nvme/host/core.c|1649| <<__nvme_revalidate_disk>> blk_queue_stack_limits(ns->head->disk->queue, ns->queue)
+ */
 void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b)
 {
 	blk_stack_limits(&t->limits, &b->limits, 0);
@@ -489,6 +518,12 @@ EXPORT_SYMBOL(blk_queue_stack_limits);
  *    queue_limits will have the misaligned flag set to indicate that
  *    the alignment_offset is undefined.
  */
+/*
+ * called by:
+ *   - block/blk-settings.c|483| <<blk_queue_stack_limits>> blk_stack_limits(&t->limits, &b->limits, 0);
+ *   - block/blk-settings.c|650| <<bdev_stack_limits>> return blk_stack_limits(t, &bq->limits, start);
+ *   - drivers/md/dm-table.c|1541| <<dm_calculate_queue_limits>> if (blk_stack_limits(limits, &ti_limits, 0) < 0)
+ */
 int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 		     sector_t start)
 {
@@ -716,6 +751,10 @@ EXPORT_SYMBOL(blk_queue_update_dma_pad);
  * this routine, you must set the limit to one fewer than your device
  * can support otherwise there won't be room for the drain buffer.
  */
+/*
+ * called only by:
+ *   - drivers/ata/libata-scsi.c|1290| <<ata_scsi_dev_config>> blk_queue_dma_drain(q, atapi_drain_needed, buf, ATAPI_MAX_DRAIN);
+ */
 int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 457d9ba..78878a1 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -95,6 +95,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|604| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -136,6 +140,12 @@ void __blk_complete_request(struct request *req)
 		 * entries there, someone already raised the irq but it
 		 * hasn't run yet.
 		 */
+		/*
+		 * block/blk-softirq.c|154| <<blk_softirq_init>> open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
+		 *
+		 * 对于blk_cpu_done链表上的每一个request调用rq->q->mq_ops->complete(rq)
+		 * 并且删除这个request
+		 */
 		if (list->next == &req->ipi_list)
 			raise_softirq_irqoff(BLOCK_SOFTIRQ);
 	} else if (raise_blk_irq(ccpu, req))
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 696a041..8ca76e2 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -17,6 +17,13 @@ struct blk_queue_stats {
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|197| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|85| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|93| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|145| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -25,6 +32,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|208| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|92| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -39,6 +51,11 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|220| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|72| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -47,6 +64,10 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|627| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -75,6 +96,10 @@ void blk_stat_add(struct request *rq, u64 now)
 	rcu_read_unlock();
 }
 
+/*
+ * used by:
+ *   - block/blk-stat.c|146| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -97,6 +122,11 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3075| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|828| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -131,6 +161,11 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3579| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|851| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -179,6 +214,11 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2450| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|442| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -188,6 +228,10 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|500| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -203,6 +247,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|546| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|857| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index 5968591..f1832f64 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -510,6 +510,15 @@ static ssize_t queue_wc_store(struct request_queue *q, const char *page,
 	if (set == -1)
 		return -EINVAL;
 
+	/*
+	 * When read, this file will display whether the device has write back
+	 * caching enabled or not. It will return "write back" for the former
+	 * case, and "write through" for the latter. Writing to this file can
+	 * change the kernels view of the device, but it doesn't alter the
+	 * device state. This means that it might not be safe to toggle the
+	 * setting from "write back" to "write through", since that will also
+	 * eliminate cache flushes issued by the kernel.
+	 */
 	if (set)
 		blk_queue_flag_set(QUEUE_FLAG_WC, q);
 	else
@@ -831,6 +840,10 @@ static void blk_free_queue_rcu(struct rcu_head *rcu_head)
  *     of the request queue reaches zero, blk_release_queue is called to release
  *     all allocated resources of the request queue.
  */
+/*
+ * used only by:
+ *   - block/blk-sysfs.c|916| <<blk_release_queue>> INIT_WORK(&q->release_work, __blk_release_queue);
+ */
 static void __blk_release_queue(struct work_struct *work)
 {
 	struct request_queue *q = container_of(work, typeof(*q), release_work);
@@ -839,6 +852,7 @@ static void __blk_release_queue(struct work_struct *work)
 		blk_stat_remove_callback(q, q->poll_cb);
 	blk_stat_free_callback(q->poll_cb);
 
+	/* test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags) */
 	if (!blk_queue_dead(q)) {
 		/*
 		 * Last reference was dropped without having called
@@ -872,6 +886,33 @@ static void __blk_release_queue(struct work_struct *work)
 	call_rcu(&q->rcu_head, blk_free_queue_rcu);
 }
 
+/*
+ * 一个例子: echo 1 > /sys/devices/pci0000\:00/0000\:00\:04.0/remove
+ * [0] CPU: 3 PID: 2634 Comm: bash Not tainted 5.0.0+ #3
+ * [0] blk_release_queue
+ * [0] kobject_put
+ * [0] disk_release
+ * [0] device_release
+ * [0] kobject_put
+ * [0] virtblk_remove
+ * [0] virtio_dev_remove
+ * [0] device_release_driver_internal
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] unregister_virtio_device
+ * [0] virtio_pci_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] pci_stop_bus_device
+ * [0] pci_stop_and_remove_bus_device_locked
+ * [0] remove_store
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void blk_release_queue(struct kobject *kobj)
 {
 	struct request_queue *q =
@@ -896,6 +937,11 @@ struct kobj_type blk_queue_ktype = {
  * blk_register_queue - register a block layer queue with sysfs
  * @disk: Disk of which the request queue should be registered with sysfs.
  */
+/*
+ * called by:
+ *   - block/genhd.c|725| <<__device_add_disk>> blk_register_queue(disk);
+ *   - drivers/md/dm.c|2270| <<dm_setup_md_queue>> blk_register_queue(md->disk);
+ */
 int blk_register_queue(struct gendisk *disk)
 {
 	int ret;
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 124c261..bf1cef2 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -108,6 +108,11 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|839| <<blk_mq_start_request>> blk_add_timer(rq);
+ *   - block/blk-mq.c|1009| <<blk_mq_rq_timed_out>> blk_add_timer(req);
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -143,6 +148,15 @@ void blk_add_timer(struct request *req)
 		 * modifying the timer because expires for value X
 		 * will be X + something.
 		 */
+		/*
+		 * 注意! 这里的q是request_queue, q->timeout应该是
+		 *
+		 * q->timeout设置的地方:
+		 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+		 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+		 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+		 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+		 */
 		if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
 			mod_timer(&q->timeout, expiry);
 	}
diff --git a/block/blk.h b/block/blk.h
index 5d636ee..cfdf112 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -16,10 +16,37 @@ extern struct dentry *blk_debugfs_root;
 
 struct blk_flush_queue {
 	unsigned int		flush_queue_delayed:1;
+	/*
+	 * 在以下修改:
+	 *   - block/blk-flush.c|327| <<blk_kick_flush>> fq->flush_pending_idx ^= 1;
+	 *
+	 *   Issue flush and toggle pending_idx.  This makes pending_idx
+	 *   different from running_idx, which means flush is in flight.
+	 */
 	unsigned int		flush_pending_idx:1;
+	/*
+	 * 在以下修改:
+	 *   - block/blk-flush.c|294| <<flush_end_io>> fq->flush_running_idx ^= 1;
+	 *
+	 * 在以下被使用:
+	 *   - block/blk-flush.c|290| <<flush_end_io>> running = &fq->flush_queue[fq->flush_running_idx];
+	 *   - block/blk-flush.c|291| <<flush_end_io>> BUG_ON(fq->flush_pending_idx == fq->flush_running_idx);
+	 *   - block/blk-flush.c|338| <<blk_kick_flush>> if (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))
+	 */
 	unsigned int		flush_running_idx:1;
+	/*
+	 * 只在以下修改:
+	 *   - block/blk-flush.c|234| <<blk_flush_complete_seq>> fq->flush_pending_since = jiffies;
+	 *
+	 * 在以下使用:
+	 *   - block/blk-flush.c|467| <<blk_kick_flush>> fq->flush_pending_since + FLUSH_PENDING_TIMEOUT))
+	 */
 	unsigned long		flush_pending_since;
 	struct list_head	flush_queue[2];
+	/*
+	 * 在以下添加元素, 删除的还不知道:
+	 *   - block/blk-flush.c|239| <<blk_flush_complete_seq>> list_move_tail(&rq->flush.list, &fq->flush_data_in_flight);
+	 */
 	struct list_head	flush_data_in_flight;
 	struct request		*flush_rq;
 
@@ -27,7 +54,21 @@ struct blk_flush_queue {
 	 * flush_rq shares tag with this rq, both can't be active
 	 * at the same time
 	 */
+	/*
+	 * used by:
+	 *   - block/blk-flush.c|283| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+	 *   - block/blk-flush.c|372| <<blk_kick_flush>> fq->orig_rq = first_rq;
+	 */
 	struct request		*orig_rq;
+	/*
+	 * used by:
+	 *   - block/blk-flush.c|409| <<flush_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|445| <<flush_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|575| <<mq_flush_data_end_io>> spin_lock_irqsave(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|577| <<mq_flush_data_end_io>> spin_unlock_irqrestore(&fq->mq_flush_lock, flags);
+	 *   - block/blk-flush.c|695| <<blk_insert_flush>> spin_lock_irq(&fq->mq_flush_lock);
+	 *   - block/blk-flush.c|703| <<blk_insert_flush>> spin_unlock_irq(&fq->mq_flush_lock);
+	 */
 	spinlock_t		mq_flush_lock;
 };
 
@@ -55,6 +96,13 @@ void blk_rq_bio_prep(struct request_queue *q, struct request *rq,
 			struct bio *bio);
 void blk_freeze_queue(struct request_queue *q);
 
+/*
+ * called by:
+ *   - block/blk-core.c|1180| <<generic_make_request>> blk_queue_enter_live(q);
+ *   - block/blk-mq.c|760| <<blk_mq_get_request>> blk_queue_enter_live(q);
+ *
+ * 相当于: percpu_ref_get(&q->q_usage_counter);
+ */
 static inline void blk_queue_enter_live(struct request_queue *q)
 {
 	/*
diff --git a/block/genhd.c b/block/genhd.c
index 7032678..3e01ba0 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -913,6 +913,11 @@ EXPORT_SYMBOL(bdget_disk);
  * filesystem can't be mounted and thus to give the victim some idea of what
  * went wrong
  */
+/*
+ * called by:
+ *   - init/do_mounts.c|437| <<mount_block_root>> printk_all_partitions();
+ *   - init/do_mounts.c|450| <<mount_block_root>> printk_all_partitions();
+ */
 void __init printk_all_partitions(void)
 {
 	struct class_dev_iter iter;
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index b16a887..ef944a1 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -916,6 +916,9 @@ static int virtblk_probe(struct virtio_device *vdev)
 	return err;
 }
 
+/*
+ * struct virtio_driver virtio_blk.remove = virtblk_remove()
+ */
 static void virtblk_remove(struct virtio_device *vdev)
 {
 	struct virtio_blk *vblk = vdev->priv;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 07bf2bf..31a42fe 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -71,6 +71,10 @@ MODULE_PARM_DESC(streams, "turn on support for Streams write directives");
  * serialization purposes. nvme_delete_wq host controller deletion
  * works which flush reset works for serialization.
  */
+/*
+ * 在以下初始化:
+ *   - drivers/nvme/host/core.c|3940| <<nvme_core_init>> nvme_wq = alloc_workqueue("nvme-wq",
+ */
 struct workqueue_struct *nvme_wq;
 EXPORT_SYMBOL_GPL(nvme_wq);
 
@@ -94,6 +98,11 @@ static void nvme_put_subsystem(struct nvme_subsystem *subsys);
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					   unsigned nsid);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1295| <<nvme_update_formats>> nvme_set_queue_dying(ns);
+ *   - drivers/nvme/host/core.c|3815| <<nvme_kill_queues>> nvme_set_queue_dying(ns);
+ */
 static void nvme_set_queue_dying(struct nvme_ns *ns)
 {
 	/*
@@ -105,6 +114,13 @@ static void nvme_set_queue_dying(struct nvme_ns *ns)
 	revalidate_disk(ns->disk);
 	blk_set_queue_dying(ns->queue);
 	/* Forcibly unquiesce queues to avoid blocking dispatch */
+	/*
+	 * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+	 * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+	 *
+	 * flush them through the low level driver's queue_rq().
+	 * 把queue中剩下的request刷一下
+	 */
 	blk_mq_unquiesce_queue(ns->queue);
 }
 
@@ -117,6 +133,21 @@ static void nvme_queue_scan(struct nvme_ctrl *ctrl)
 		queue_work(nvme_wq, &ctrl->scan_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|134| <<nvme_reset_ctrl_sync>> ret = nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|976| <<nvme_keep_alive_work>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|3598| <<nvme_fw_act_work>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|770| <<nvme_fc_ctrl_connectivity_loss>> if (nvme_reset_ctrl(&ctrl->ctrl)) {
+ *   - drivers/nvme/host/fc.c|2074| <<nvme_fc_error_recovery>> nvme_reset_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/multipath.c|84| <<nvme_failover_req>> nvme_reset_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/multipath.c|501| <<nvme_anatt_timeout>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/pci.c|1299| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|1342| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2993| <<nvme_resume>> nvme_reset_ctrl(&ndev->ctrl);
+ *   - drivers/nvme/host/pci.c|3032| <<nvme_slot_reset>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/target/loop.c|138| <<nvme_loop_timeout>> nvme_reset_ctrl(&iod->queue->ctrl->ctrl);
+ */
 int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
@@ -282,6 +313,17 @@ void nvme_complete_rq(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_complete_rq);
 
+/*
+ * 全部都是用blk_mq_tagset_busy_iter()在调用这个函数:
+ *   - drivers/nvme/host/pci.c|2554| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2555| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|917| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set, nvme_cancel_request,
+ *   - drivers/nvme/host/rdma.c|929| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(&ctrl->tag_set, nvme_cancel_request,
+ *   - drivers/nvme/host/tcp.c|1689| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset, nvme_cancel_request, ctrl);
+ *   - drivers/nvme/host/tcp.c|1701| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset, nvme_cancel_request, ctrl);
+ *   - drivers/nvme/target/loop.c|425| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|434| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ */
 bool nvme_cancel_request(struct request *req, void *data, bool reserved)
 {
 	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
@@ -416,9 +458,23 @@ static void nvme_put_ns(struct nvme_ns *ns)
 	kref_put(&ns->kref, nvme_free_ns);
 }
 
+/*
+ * 如果req->rq_flags没设置RQF_DONTPREP, 要设置RQF_DONTPREP
+ */
 static inline void nvme_clear_nvme_request(struct request *req)
 {
+	/*
+	 * don't call prep for this one
+	 * 设置的地方:
+	 *   - drivers/block/skd_main.c|501| <<skd_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+	 *   - drivers/ide/ide-io.c|464| <<ide_issue_rq>> rq->rq_flags |= RQF_DONTPREP;
+	 *   - drivers/mmc/core/queue.c|301| <<mmc_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+	 *   - drivers/nvme/host/core.c|466| <<nvme_clear_nvme_request>> req->rq_flags |= RQF_DONTPREP;
+	 *   - drivers/scsi/scsi_lib.c|147| <<scsi_mq_requeue_cmd>> cmd->request->rq_flags &= ~RQF_DONTPREP;
+	 *   - drivers/scsi/scsi_lib.c|1720| <<scsi_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+	 */
 	if (!(req->rq_flags & RQF_DONTPREP)) {
+		/* 获取request后面driver specific的部分 */
 		nvme_req(req)->retries = 0;
 		nvme_req(req)->flags = 0;
 		req->rq_flags |= RQF_DONTPREP;
@@ -547,6 +603,10 @@ static void nvme_assign_write_stream(struct nvme_ctrl *ctrl,
 		req->q->write_hints[streamid] += blk_rq_bytes(req) >> 9;
 }
 
+/*
+ * 处理REQ_OP_FLUSH:
+ *   - drivers/nvme/host/core.c|785| <<nvme_setup_cmd>> nvme_setup_flush(ns, cmd);
+ */
 static inline void nvme_setup_flush(struct nvme_ns *ns,
 		struct nvme_command *cmnd)
 {
@@ -554,9 +614,18 @@ static inline void nvme_setup_flush(struct nvme_ns *ns,
 	cmnd->common.nsid = cpu_to_le32(ns->head->ns_id);
 }
 
+/*
+ * 处理REQ_OP_DISCARD或者作为REQ_OP_WRITE_ZEROES的quirk:
+ *   - drivers/nvme/host/core.c|671| <<nvme_setup_write_zeroes>> return nvme_setup_discard(ns, req, cmnd);
+ *   - drivers/nvme/host/core.c|791| <<nvme_setup_cmd>> ret = nvme_setup_discard(ns, req, cmd);
+ */
 static blk_status_t nvme_setup_discard(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmnd)
 {
+	/*
+	 * Number of discard segments (or ranges) the driver needs to fill in.
+	 * Each discard bio merged into a request is counted as one segment.
+	 */
 	unsigned short segments = blk_rq_nr_discard_segments(req), n = 0;
 	struct nvme_dsm_range *range;
 	struct bio *bio;
@@ -576,6 +645,7 @@ static blk_status_t nvme_setup_discard(struct nvme_ns *ns, struct request *req,
 	}
 
 	__rq_for_each_bio(bio, req) {
+		/* 把一个namespace的vector转换成在namespace的block number */
 		u64 slba = nvme_block_nr(ns, bio->bi_iter.bi_sector);
 		u32 nlb = bio->bi_iter.bi_size >> ns->lba_shift;
 
@@ -608,6 +678,10 @@ static blk_status_t nvme_setup_discard(struct nvme_ns *ns, struct request *req,
 	return BLK_STS_OK;
 }
 
+/*
+ * 处理REQ_OP_WRITE_ZEROES:
+ *   - drivers/nvme/host/core.c|788| <<nvme_setup_cmd>> ret = nvme_setup_write_zeroes(ns, req, cmd);
+ */
 static inline blk_status_t nvme_setup_write_zeroes(struct nvme_ns *ns,
 		struct request *req, struct nvme_command *cmnd)
 {
@@ -616,6 +690,7 @@ static inline blk_status_t nvme_setup_write_zeroes(struct nvme_ns *ns,
 
 	cmnd->write_zeroes.opcode = nvme_cmd_write_zeroes;
 	cmnd->write_zeroes.nsid = cpu_to_le32(ns->head->ns_id);
+	/* 把一个namespace的vector转换成在namespace的block numbe */
 	cmnd->write_zeroes.slba =
 		cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
 	cmnd->write_zeroes.length =
@@ -641,6 +716,7 @@ static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
 
 	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
 	cmnd->rw.nsid = cpu_to_le32(ns->head->ns_id);
+	/* 把一个namespace的vector转换成在namespace的block number */
 	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
 	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
 
@@ -701,11 +777,22 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2303| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|956| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|1737| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|1997| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|159| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ *
+ * 把req->tag存入cmd->common.command_id
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
 	blk_status_t ret = BLK_STS_OK;
 
+	/* 如果req->rq_flags没设置RQF_DONTPREP, 要设置RQF_DONTPREP */
 	nvme_clear_nvme_request(req);
 
 	memset(cmd, 0, sizeof(*cmd));
@@ -845,6 +932,11 @@ static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
 	return ERR_PTR(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1313| <<nvme_submit_io>> return nvme_submit_user_cmd(ns->queue, &c,
+ *   - drivers/nvme/host/core.c|1429| <<nvme_user_cmd>> status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
+ */
 static int nvme_submit_user_cmd(struct request_queue *q,
 		struct nvme_command *cmd, void __user *ubuffer,
 		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
@@ -3313,6 +3405,13 @@ static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3442| <<nvme_validate_ns>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|3462| <<nvme_remove_invalid_namespaces>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|3492| <<nvme_scan_ns_list>> nvme_ns_remove(ns);
+ *   - drivers/nvme/host/core.c|3604| <<nvme_remove_namespaces>> nvme_ns_remove(ns);
+ */
 static void nvme_ns_remove(struct nvme_ns *ns)
 {
 	if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
@@ -3710,6 +3809,14 @@ static void nvme_free_ctrl(struct device *dev)
  * earliest initialization so that we have the initialized structured around
  * during probing.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3077| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+ *   - drivers/nvme/host/pci.c|3070| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+ *   - drivers/nvme/host/tcp.c|2195| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+ *   - drivers/nvme/target/loop.c|595| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+ */
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks)
 {
@@ -3790,6 +3897,12 @@ EXPORT_SYMBOL_GPL(nvme_init_ctrl);
  * Call this function when the driver determines it is unable to get the
  * controller in a state capable of servicing IO.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3543| <<nvme_remove_namespaces>> nvme_kill_queues(ctrl);
+ *   - drivers/nvme/host/pci.c|2630| <<nvme_remove_dead_ctrl>> nvme_kill_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2722| <<nvme_reset_work>> nvme_kill_queues(&dev->ctrl);
+ */
 void nvme_kill_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -3843,6 +3956,11 @@ void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1271| <<nvme_passthru_start>> nvme_start_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|2463| <<nvme_dev_disable>> nvme_start_freeze(&dev->ctrl);
+ */
 void nvme_start_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index b91f183..a6e1306 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -112,6 +112,9 @@ enum {
 	NVME_REQ_USERCMD		= (1 << 1),
 };
 
+/*
+ * 获取request后面driver specific的部分
+ */
 static inline struct nvme_request *nvme_req(struct request *req)
 {
 	return blk_mq_rq_to_pdu(req);
@@ -173,6 +176,11 @@ struct nvme_ctrl {
 
 	u32 ctrl_config;
 	u16 mtfa;
+	/*
+	 * 在以下修改:
+	 *   - drivers/nvme/host/pci.c|1399| <<nvme_free_queues>> dev->ctrl.queue_count--;
+	 *   - drivers/nvme/host/pci.c|1517| <<nvme_alloc_queue>> dev->ctrl.queue_count++;
+	 */
 	u32 queue_count;
 
 	u64 cap;
@@ -383,6 +391,9 @@ static inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)
 	return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
 }
 
+/*
+ * 把一个namespace的vector转换成在namespace的block number
+ */
 static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
 {
 	return (sector >> (ns->lba_shift - 9));
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index f54718b..bbfc151 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -39,6 +39,12 @@
 #define NVME_MAX_KB_SZ	4096
 #define NVME_MAX_SEGS	127
 
+/*
+ * 没有修改的地方, 只能通过参数
+ * 在queue_request_irq()设置中断函数的时候:
+ *   use_threaded_interrupts = 1 --> nvme_irq_check()
+ *   use_threaded_interrupts = 0 --> nvme_irq();
+ */
 static int use_threaded_interrupts;
 module_param(use_threaded_interrupts, int, 0);
 
@@ -46,6 +52,11 @@ static bool use_cmb_sqes = true;
 module_param(use_cmb_sqes, bool, 0444);
 MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
 
+/*
+ * used by only:
+ *   - drivers/nvme/host/pci.c|2083| <<nvme_setup_host_mem>> u64 max = (u64)max_host_mem_size_mb * SZ_1M;
+ *   - drivers/nvme/host/pci.c|2093| <<nvme_setup_host_mem>> min >> ilog2(SZ_1M), max_host_mem_size_mb);
+ */
 static unsigned int max_host_mem_size_mb = 128;
 module_param(max_host_mem_size_mb, uint, 0444);
 MODULE_PARM_DESC(max_host_mem_size_mb,
@@ -63,6 +74,10 @@ static const struct kernel_param_ops io_queue_depth_ops = {
 	.get = param_get_int,
 };
 
+/*
+ * used by only:
+ *   - drivers/nvme/host/pci.c|2504| <<nvme_pci_enable>> io_queue_depth);
+ */
 static int io_queue_depth = 1024;
 module_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);
 MODULE_PARM_DESC(io_queue_depth, "set io queue depth, should >= 2");
@@ -73,12 +88,22 @@ static const struct kernel_param_ops queue_count_ops = {
 	.get = param_get_int,
 };
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|286| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2138| <<nvme_calc_io_queues>> unsigned int this_w_queues = write_queues;
+ */
 static int write_queues;
 module_param_cb(write_queues, &queue_count_ops, &write_queues, 0644);
 MODULE_PARM_DESC(write_queues,
 	"Number of queues to use for writes. If not set, reads and writes "
 	"will share a queue set.");
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|286| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2192| <<nvme_setup_irqs>> this_p_queues = poll_queues;
+ */
 static int poll_queues = 0;
 module_param_cb(poll_queues, &queue_count_ops, &poll_queues, 0644);
 MODULE_PARM_DESC(poll_queues, "Number of queues to use for polled IO.");
@@ -100,10 +125,32 @@ struct nvme_dev {
 	struct device *dev;
 	struct dma_pool *prp_page_pool;
 	struct dma_pool *prp_small_pool;
+	/*
+	 * 在以下被修改:
+	 *   - drivers/nvme/host/pci.c|1410| <<nvme_suspend_queue>> nvmeq->dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|1547| <<nvme_init_queue>> dev->online_queues++;
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_create_queue>> dev->online_queues--;
+	 */
 	unsigned online_queues;
+	/*
+	 * 对于pci就在一处修改:
+	 *   - drivers/nvme/host/pci.c|2252| <<nvme_setup_io_queues>> dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
+	 */
 	unsigned max_qid;
 	unsigned io_queues[HCTX_MAX_TYPES];
+	/*
+	 * 只在以下设置:
+	 *   - drivers/nvme/host/pci.c|2394| <<nvme_setup_io_queues>> dev->num_vecs = result;
+	 *     result来自nvme_setup_irqs()
+	 */
 	unsigned int num_vecs;
+	/*
+	 * 设置的地方:
+	 *   - drivers/nvme/host/pci.c|2327| <<nvme_setup_io_queues>> dev->q_depth = result;
+	 *   - drivers/nvme/host/pci.c|2550| <<nvme_pci_enable>> dev->q_depth = min_t(int , NVME_CAP_MQES(dev->ctrl.cap) + 1,
+	 *   - drivers/nvme/host/pci.c|2560| <<nvme_pci_enable>> dev->q_depth = 2;
+	 *   - drivers/nvme/host/pci.c|2567| <<nvme_pci_enable>> dev->q_depth = 64;
+	 */
 	int q_depth;
 	u32 db_stride;
 	void __iomem *bar;
@@ -112,11 +159,21 @@ struct nvme_dev {
 	struct mutex shutdown_lock;
 	bool subsystem;
 	u64 cmb_size;
+	/*
+	 * 在以下设置:
+	 *   - drivers/nvme/host/pci.c|2017| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2361| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+	 *   - drivers/nvme/host/pci.c|2367| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	bool cmb_use_sqes;
 	u32 cmbsz;
 	u32 cmbloc;
 	struct nvme_ctrl ctrl;
 
+	/*
+	 * 在以下分配:
+	 *   - drivers/nvme/host/pci.c|3061| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 */
 	mempool_t *iod_mempool;
 
 	/* shadow doorbell buffer support: */
@@ -176,27 +233,95 @@ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
  * An NVM Express queue.  Each device has at least two (one for admin
  * commands and one for I/O commands).
  */
+/*
+ * 下发到sq写入tail
+ * 从cq接收写入head
+ */
 struct nvme_queue {
 	struct device *q_dmadev;
 	struct nvme_dev *dev;
 	spinlock_t sq_lock;
+	/*
+	 * 根据是否使用cmb在两处分配在两处分配, sq的ring buffer:
+	 *   - drivers/nvme/host/pci.c|1544| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(depth));
+	 *   - drivers/nvme/host/pci.c|1553| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
+	 */
 	struct nvme_command *sq_cmds;
 	 /* only used for poll queues: */
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
+	/*
+	 * 在以下分配:
+	 *   - drivers/nvme/host/pci.c|1593| <<nvme_alloc_queue>> nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(depth),
+	 */
 	volatile struct nvme_completion *cqes;
+	/*
+	 * pci在以下设置:
+	 *   - drivers/nvme/host/pci.c|492| <<nvme_admin_init_hctx>> nvmeq->tags = &dev->admin_tagset.tags[0];
+	 *   - drivers/nvme/host/pci.c|500| <<nvme_admin_exit_hctx>> nvmeq->tags = NULL;
+	 *   - drivers/nvme/host/pci.c|510| <<nvme_init_hctx>> nvmeq->tags = &dev->tagset.tags[hctx_idx];
+	 */
 	struct blk_mq_tags **tags;
+	/*
+	 * 根据是否使用cmb在两处分配:
+	 *   - drivers/nvme/host/pci.c|1559| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+	 *   - drivers/nvme/host/pci.c|1568| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+	 */
 	dma_addr_t sq_dma_addr;
+	/*
+	 * 在以下分配:
+	 *   - drivers/nvme/host/pci.c|1594| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 */
 	dma_addr_t cq_dma_addr;
+	/*
+	 * 在以下被设置:
+	 *   - drivers/nvme/host/pci.c|1621| <<nvme_alloc_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|1663| <<nvme_init_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|2340| <<nvme_setup_io_queues>> adminq->q_db = dev->dbs;
+	 */
 	u32 __iomem *q_db;
+	/*
+	 * 在以下被设置:
+	 *   - drivers/nvme/host/pci.c|1622| <<nvme_alloc_queue>> nvmeq->q_depth = depth;
+	 */
 	u16 q_depth;
+	/*
+	 * 在一下设置, admin的vector永远是0吧:
+	 *   - drivers/nvme/host/pci.c|1541| <<nvme_suspend_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|1646| <<nvme_alloc_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|1723| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1736| <<nvme_create_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|1874| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 *   - drivers/nvme/host/pci.c|1878| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|2390| <<nvme_setup_io_queues>> adminq->cq_vector = -1;
+	 */
 	s16 cq_vector;
+
+	/*
+	 * 下发到sq写入tail
+	 * 从cq接收写入head
+	 */
 	u16 sq_tail;
 	u16 last_sq_tail;
 	u16 cq_head;
 	u16 last_cq_head;
 	u16 qid;
+	/*
+	 * 设置的地方:
+	 *   - drivers/nvme/host/pci.c|1154| <<nvme_update_cq_head>> nvmeq->cq_phase = !nvmeq->cq_phase;
+	 *   - drivers/nvme/host/pci.c|1652| <<nvme_alloc_queue>> nvmeq->cq_phase = 1;
+	 *   - drivers/nvme/host/pci.c|1694| <<nvme_init_queue>> nvmeq->cq_phase = 1;
+	 */
 	u8 cq_phase;
 	unsigned long flags;
+/*
+ * 使用的地方:
+ *   - drivers/nvme/host/pci.c|935| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+ *   - drivers/nvme/host/pci.c|1414| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1613| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|1763| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|2224| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+ *   - drivers/nvme/host/pci.c|2274| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+ */
 #define NVMEQ_ENABLED		0
 #define NVMEQ_SQ_CMB		1
 #define NVMEQ_DELETE_ERROR	2
@@ -216,6 +341,10 @@ struct nvme_queue {
 struct nvme_iod {
 	struct nvme_request req;
 	struct nvme_queue *nvmeq;
+	/*
+	 * 只在一处设置:
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_init_iod>> iod->use_sgl = nvme_pci_use_sgls(dev, rq);
+	 */
 	bool use_sgl;
 	int aborted;
 	int npages;		/* In the PRP list. 0 means small pool in use */
@@ -398,6 +527,11 @@ static int nvme_pci_npages_sgl(unsigned int num_seg)
 	return DIV_ROUND_UP(num_seg * sizeof(struct nvme_sgl_desc), PAGE_SIZE);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|537| <<nvme_pci_cmd_size>> unsigned int alloc_size = nvme_pci_iod_alloc_size(dev,
+ *   - drivers/nvme/host/pci.c|3038| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 		unsigned int size, unsigned int nseg, bool use_sgl)
 {
@@ -514,6 +648,11 @@ static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
 /*
  * Write sq tail if we are asked to, or if the next command would wrap.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|682| <<nvme_submit_cmd>> nvme_write_sq_db(nvmeq, write_sq);
+ *   - drivers/nvme/host/pci.c|692| <<nvme_commit_rqs>> nvme_write_sq_db(nvmeq, true);
+ */
 static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
 {
 	if (!write_sq) {
@@ -537,10 +676,23 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
  * @cmd: The command to send
  * @write_sq: whether to write to the SQ doorbell
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1013| <<nvme_queue_rq>> nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+ *   - drivers/nvme/host/pci.c|1201| <<nvme_pci_submit_async_event>> nvme_submit_cmd(nvmeq, &c, true);
+ */
 static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
 	spin_lock(&nvmeq->sq_lock);
+	/*
+	 * sq_cmds根据是否使用cmb在两处分配在两处分配, sq的ring buffer:
+	 *   - drivers/nvme/host/pci.c|1544| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(depth));
+	 *   - drivers/nvme/host/pci.c|1553| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
+	 *
+	 * 下发到sq写入tail
+	 * 从cq接收写入head
+	 */
 	memcpy(&nvmeq->sq_cmds[nvmeq->sq_tail], cmd, sizeof(*cmd));
 	if (++nvmeq->sq_tail == nvmeq->q_depth)
 		nvmeq->sq_tail = 0;
@@ -656,6 +808,10 @@ static void nvme_print_sgl(struct scatterlist *sgl, int nents)
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1013| <<nvme_map_data>> ret = nvme_pci_setup_prps(dev, req, &cmnd->rw);
+ */
 static blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmnd)
 {
@@ -770,6 +926,10 @@ static void nvme_pci_sgl_set_seg(struct nvme_sgl_desc *sge,
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1011| <<nvme_map_data>> ret = nvme_pci_setup_sgls(dev, req, &cmnd->rw, nr_mapped);
+ */
 static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 		struct request *req, struct nvme_rw_command *cmd, int entries)
 {
@@ -829,6 +989,10 @@ static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
 	return BLK_STS_OK;
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1081| <<nvme_queue_rq>> ret = nvme_map_data(dev, req, &cmnd);
+ */
 static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 		struct nvme_command *cmnd)
 {
@@ -840,6 +1004,10 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	int nr_mapped;
 
 	sg_init_table(iod->sg, blk_rq_nr_phys_segments(req));
+	/*
+	 * map a request to scatterlist, return number of sg entries setup. Caller
+	 * must make sure sg can hold rq->nr_phys_segments entries
+	 */
 	iod->nents = blk_rq_map_sg(q, req, iod->sg);
 	if (!iod->nents)
 		goto out;
@@ -855,6 +1023,10 @@ static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
 	if (!nr_mapped)
 		goto out;
 
+	/*
+	 * 在以下设置iod->use_sgl:
+	 *   - drivers/nvme/host/pci.c|741| <<nvme_init_iod>> iod->use_sgl = nvme_pci_use_sgls(dev, rq);
+	 */
 	if (iod->use_sgl)
 		ret = nvme_pci_setup_sgls(dev, req, &cmnd->rw, nr_mapped);
 	else
@@ -922,6 +1094,15 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	 * We should not need to do this, but we're still using this to
 	 * ensure we can drain requests on a dying queue.
 	 */
+	/*
+	 * 这里没什么用处吧, xen-blkfront也会有这样的问题但是没有类似的代码
+	 * The nvme driver checked the queue state on every IO so this path could
+	 * be used to flush out entered requests to a failed completion.
+	 *
+	 * 其实可以Use the blk-mq's tag iterator to end all entered requests when the queue
+	 * isn't going to be restarted so the IO path doesn't have to deal with
+	 * these conditions.
+	 */
 	if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
 		return BLK_STS_IOERR;
 
@@ -933,12 +1114,38 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (ret)
 		goto out_free_cmd;
 
+	/*
+	 * Number of physical segments as sent to the device.
+	 *
+	 * Normally this is the number of discontiguous data segments sent by the
+	 * submitter.  But for data-less command like discard we might have no
+	 * actual data segments submitted, but the driver might have to add it's
+	 * own special payload.  In that case we still return 1 here so that this
+	 * special payload will be mapped.
+	 */
 	if (blk_rq_nr_phys_segments(req)) {
 		ret = nvme_map_data(dev, req, &cmnd);
 		if (ret)
 			goto out_cleanup_iod;
 	}
 
+	/*
+	 * 关于下发request到硬件驱动时的timeout:
+	 *
+	 * 比如virtio_queue_rq()调用blk_mq_start_request()
+	 * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+	 *
+	 * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+	 * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+	 *
+	 * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+	 * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+	 *              
+	 * - nvme的例子是nvme_timeout()
+	 * - scsi的例子是scsi_timeout()
+	 *      
+	 * 设置request->state为MQ_RQ_IN_FLIGHT
+	 */
 	blk_mq_start_request(req);
 	nvme_submit_cmd(nvmeq, &cmnd, bd->last);
 	return BLK_STS_OK;
@@ -1461,6 +1668,10 @@ static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 	return q_depth;
 }
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|1550| <<nvme_alloc_queue>> if (nvme_alloc_sq_cmds(dev, nvmeq, qid, depth))
+ */
 static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 				int qid, int depth)
 {
@@ -1483,6 +1694,13 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1728| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|1766| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+ *
+ * 初始化nvme_dev->queues[qid]并且nvme_dev->ctrl.queue_count++
+ */
 static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 {
 	struct nvme_queue *nvmeq = &dev->queues[qid];
@@ -1533,6 +1751,11 @@ static int queue_request_irq(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1579| <<nvme_create_queue>> nvme_init_queue(nvmeq, qid);
+ *   - drivers/nvme/host/pci.c|1730| <<nvme_pci_configure_admin_queue>> nvme_init_queue(nvmeq, 0);
+ */
 static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1548,6 +1771,10 @@ static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 	wmb(); /* ensure the first interrupt sees the initialization */
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1764| <<nvme_create_io_queues>> ret = nvme_create_queue(&dev->queues[i], i, polled);
+ */
 static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1668,6 +1895,12 @@ static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
 	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1870| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2383| <<nvme_setup_io_queues>> result = nvme_remap_bar(dev, size);
+ *   - drivers/nvme/host/pci.c|2948| <<nvme_dev_map>> if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+ */
 static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -1738,12 +1971,29 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	return result;
 }
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|2228| <<nvme_setup_io_queues>> result = nvme_create_io_queues(dev);
+ */
 static int nvme_create_io_queues(struct nvme_dev *dev)
 {
 	unsigned i, max, rw_queues;
 	int ret = 0;
 
+	/*
+	 * 修改queue_count的地方:
+	 *   - drivers/nvme/host/pci.c|1399| <<nvme_free_queues>> dev->ctrl.queue_count--;
+	 *   - drivers/nvme/host/pci.c|1517| <<nvme_alloc_queue>> dev->ctrl.queue_count++;
+	 *
+	 * 有两处nvme_pci_configure_admin_queue()和nvme_create_io_queues()
+	 * 会调用nvme_alloc_queue()来增加dev->ctrl.queue_count
+	 *
+	 * 如果此时admin queue已经有了, dev->ctrl.queue_count是1, 否则是0吧
+	 */
 	for (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {
+		/*
+		 * 初始化nvme_dev->queues[qid]并且nvme_dev->ctrl.queue_count++
+		 */
 		if (nvme_alloc_queue(dev, i, dev->q_depth)) {
 			ret = -ENOMEM;
 			break;
@@ -2072,6 +2322,10 @@ static void nvme_calc_io_queues(struct nvme_dev *dev, unsigned int irq_queues)
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2246| <<nvme_setup_io_queues>> result = nvme_setup_irqs(dev, nr_io_queues);
+ */
 static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2150,13 +2404,22 @@ static void nvme_disable_io_queues(struct nvme_dev *dev)
 		__nvme_disable_io_queues(dev, nvme_admin_delete_cq);
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2627| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+ */
 static int nvme_setup_io_queues(struct nvme_dev *dev)
 {
+	/*
+	 * queues最早在以下分配:
+	 *   - drivers/nvme/host/pci.c|2785| <<nvme_probe>> dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
+	 */
 	struct nvme_queue *adminq = &dev->queues[0];
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	int result, nr_io_queues;
 	unsigned long size;
 
+	/* num_possible_cpus() + write_queues + poll_queues */
 	nr_io_queues = max_io_queues();
 	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
 	if (result < 0)
@@ -2312,6 +2575,10 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 /*
  * return error value only when tagset allocation failed
  */
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|2700| <<nvme_reset_work>> if (nvme_dev_add(dev))
+ */
 static int nvme_dev_add(struct nvme_dev *dev)
 {
 	int ret;
@@ -2319,6 +2586,13 @@ static int nvme_dev_add(struct nvme_dev *dev)
 	if (!dev->ctrl.tagset) {
 		dev->tagset.ops = &nvme_mq_ops;
 		dev->tagset.nr_hw_queues = dev->online_queues - 1;
+		/*
+		 * nvme设置nr_maps的地方:
+		 *   - drivers/nvme/host/pci.c|2378| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2380| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 *   - drivers/nvme/host/rdma.c|742| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+		 *   - drivers/nvme/host/tcp.c|1441| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+		 */
 		dev->tagset.nr_maps = 2; /* default + read */
 		if (dev->io_queues[HCTX_TYPE_POLL])
 			dev->tagset.nr_maps++;
@@ -2435,6 +2709,20 @@ static void nvme_pci_disable(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1288| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1315| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1331| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2530| <<nvme_remove_dead_ctrl>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2552| <<nvme_reset_work>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2839| <<nvme_reset_prepare>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_shutdown>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2868| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2875| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2892| <<nvme_suspend>> nvme_dev_disable(ndev, true);
+ *   - drivers/nvme/host/pci.c|2924| <<nvme_error_detected>> nvme_dev_disable(dev, false);
+ */
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 {
 	bool dead = true;
@@ -2470,6 +2758,9 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 	nvme_suspend_queue(&dev->queues[0]);
 	nvme_pci_disable(dev);
 
+	/*
+	 * 针对每一个tagset->tags[tagset->nr_hw_queues].sb
+	 */
 	blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
 	blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
 
@@ -2533,6 +2824,17 @@ static void nvme_remove_dead_ctrl(struct nvme_dev *dev, int status)
 		nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|2902| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|151| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|163| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|178| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/pci.c|3002| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3078| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ */
 static void nvme_reset_work(struct work_struct *work)
 {
 	struct nvme_dev *dev =
@@ -2697,6 +2999,10 @@ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.get_address		= nvme_pci_get_address,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3020| <<nvme_probe>> result = nvme_dev_map(dev);
+ */
 static int nvme_dev_map(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2744,6 +3050,10 @@ static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 	return 0;
 }
 
+/*
+ * used by only:
+ *   - drivers/nvme/host/pci.c|3078| <<nvme_probe>> async_schedule(nvme_async_probe, dev);
+ */
 static void nvme_async_probe(void *data, async_cookie_t cookie)
 {
 	struct nvme_dev *dev = data;
@@ -2768,6 +3078,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (!dev)
 		return -ENOMEM;
 
+	/*
+	 * 在hotplug的情况下max_queue_count()的数量就有些多了
+	 */
 	dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
 					GFP_KERNEL, node);
 	if (!dev->queues)
@@ -2776,6 +3089,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	dev->dev = get_device(&pdev->dev);
 	pci_set_drvdata(pdev, dev);
 
+	/* 只在这里被调用 */
 	result = nvme_dev_map(dev);
 	if (result)
 		goto put_pci;
@@ -2784,6 +3098,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
 	mutex_init(&dev->shutdown_lock);
 
+	/* 只在这里被调用 */
 	result = nvme_setup_prp_pools(dev);
 	if (result)
 		goto unmap;
@@ -2798,6 +3113,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 						NVME_MAX_SEGS, true);
 	WARN_ON_ONCE(alloc_size > PAGE_SIZE);
 
+	/*
+	 * 这里分配的size是上面最大的NVME_MAX_KB_SZ和NVME_MAX_SEGS
+	 */
 	dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
 						mempool_kfree,
 						(void *) alloc_size,
@@ -2807,6 +3125,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 		goto release_pools;
 	}
 
+	/* 在pci上只被这里调用 */
 	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
 			quirks);
 	if (result)
@@ -2815,6 +3134,11 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	dev_info(dev->ctrl.device, "pci function %s\n", dev_name(&pdev->dev));
 
 	nvme_get_ctrl(&dev->ctrl);
+	/*
+	 * schedule a function for asynchronous execution
+	 *
+	 * nvme_async_probe()只在这里被调用
+	 */
 	async_schedule(nvme_async_probe, dev);
 
 	return 0;
@@ -2833,12 +3157,18 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	return result;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_prepare = nvme_reset_prepare()
+ */
 static void nvme_reset_prepare(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
 	nvme_dev_disable(dev, false);
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_done = nvme_reset_done()
+ */
 static void nvme_reset_done(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2856,6 +3186,17 @@ static void nvme_shutdown(struct pci_dev *pdev)
  * state. This function must not have any dependencies on the device state in
  * order to proceed.
  */
+/*
+ * 调用的一个例子:
+ * [0] nvme_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] nvme_remove_dead_ctrl_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_remove(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2905,6 +3246,9 @@ static int nvme_resume(struct device *dev)
 
 static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
 
+/*
+ * struct pci_error_handlers nvme_err_handler.error_detected = nvme_error_detected()
+ */
 static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 						pci_channel_state_t state)
 {
@@ -2931,6 +3275,9 @@ static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 	return PCI_ERS_RESULT_NEED_RESET;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.slot_reset = nvme_slot_reset()
+ */
 static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2941,6 +3288,9 @@ static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 	return PCI_ERS_RESULT_RECOVERED;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.error_resume = nvme_error_resume()
+ */
 static void nvme_error_resume(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2956,6 +3306,10 @@ static const struct pci_error_handlers nvme_err_handler = {
 	.reset_done	= nvme_reset_done,
 };
 
+/*
+ * 最后一个field是quirk
+ * 在代码中可以用ns->ctrl->quirks & xxx 判断是否有某个quirk
+ */
 static const struct pci_device_id nvme_id_table[] = {
 	{ PCI_VDEVICE(INTEL, 0x0953),
 		.driver_data = NVME_QUIRK_STRIPE_SIZE |
@@ -3010,6 +3364,11 @@ static struct pci_driver nvme_driver = {
 	.driver		= {
 		.pm	= &nvme_dev_pm_ops,
 	},
+	/*
+	 * called by:
+	 *   - drivers/pci/pci-sysfs.c|608| <<sriov_numvfs_store>> ret = pdev->driver->sriov_configure(pdev, 0);
+	 *   - drivers/pci/pci-sysfs.c|620| <<sriov_numvfs_store>> ret = pdev->driver->sriov_configure(pdev, num_vfs);
+	 */
 	.sriov_configure = pci_sriov_configure_simple,
 	.err_handler	= &nvme_err_handler,
 };
diff --git a/drivers/scsi/scsi_lib.c b/drivers/scsi/scsi_lib.c
index 6cadbe9..07b6786 100644
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -514,6 +514,11 @@ static void scsi_starved_list_run(struct Scsi_Host *shost)
  * Notes:      The previous command was completely finished, start
  *             a new one if possible.
  */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|536| <<scsi_requeue_run_queue>> scsi_run_queue(q);
+ *   - drivers/scsi/scsi_lib.c|544| <<scsi_run_host_queues>> scsi_run_queue(sdev->request_queue);
+ */
 static void scsi_run_queue(struct request_queue *q)
 {
 	struct scsi_device *sdev = q->queuedata;
@@ -526,6 +531,14 @@ static void scsi_run_queue(struct request_queue *q)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * used by:
+ *   - drivers/scsi/scsi_scan.c|245| <<scsi_alloc_sdev>> INIT_WORK(&sdev->requeue_work, scsi_requeue_run_queue);
+ *
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|631| <<scsi_end_request>> kblockd_schedule_work(&sdev->requeue_work);
+ *   - drivers/scsi/scsi_sysfs.c|1401| <<__scsi_remove_device>> cancel_work_sync(&sdev->requeue_work);
+ */
 void scsi_requeue_run_queue(struct work_struct *work)
 {
 	struct scsi_device *sdev;
@@ -577,6 +590,14 @@ static void scsi_mq_uninit_cmd(struct scsi_cmnd *cmd)
 }
 
 /* Returns false when no more bytes to process, true if there are more */
+/*
+ * called by:
+ *   - drivers/scsi/scsi_lib.c|819| <<scsi_io_completion_action>> if (!scsi_end_request(req, blk_stat, blk_rq_err_bytes(req), 0))
+ *   - drivers/scsi/scsi_lib.c|961| <<scsi_io_completion>> if (scsi_end_request(req, BLK_STS_OK, blk_rq_bytes(req),
+ *   - drivers/scsi/scsi_lib.c|973| <<scsi_io_completion>> if (scsi_end_request(req, BLK_STS_IOERR, blk_rq_bytes(req),
+ *   - drivers/scsi/scsi_lib.c|993| <<scsi_io_completion>> if (likely(!scsi_end_request(req, blk_stat, good_bytes, 0)))
+ *   - drivers/scsi/scsi_lib.c|999| <<scsi_io_completion>> if (scsi_end_request(req, blk_stat, blk_rq_bytes(req), 0))
+ */
 static bool scsi_end_request(struct request *req, blk_status_t error,
 		unsigned int bytes, unsigned int bidi_bytes)
 {
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b0c814b..e46047a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,39 +15,133 @@ struct blk_flush_queue;
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 在以下插入新元素:
+		 *   - block/blk-mq-sched.c|189| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1303| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1663| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
 
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|254| <<blk_sync_queue>> cancel_delayed_work_sync(&hctx->run_work);
+	 *   - block/blk-mq.c|2171| <<__blk_mq_delay_run_hw_queue>> kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
+	 *   - block/blk-mq.c|2273| <<blk_mq_stop_hw_queue>> cancel_delayed_work(&hctx->run_work);
+	 *   - block/blk-mq.c|2344| <<blk_mq_run_work_fn>> hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
+	 *   - block/blk-mq.c|3106| <<blk_mq_init_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+	 */
 	struct delayed_work	run_work;
+	/*
+	 * 只在以下修改数据cpumask:
+	 *   - block/blk-mq.c|2990| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *   - block/blk-mq.c|3040| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 */
 	cpumask_var_t		cpumask;
 	int			next_cpu;
 	int			next_cpu_batch;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
+	/*
+	 * 设置sched_data的地方:
+	 *   - block/kyber-iosched.c|515| <<kyber_init_hctx>> hctx->sched_data = khd;
+	 */
 	void			*sched_data;
 	struct request_queue	*queue;
 	struct blk_flush_queue	*fq;
 
 	void			*driver_data;
 
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-debugfs.c|467| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|142| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|67| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|79| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|991| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1029| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2267| <<blk_mq_exit_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2313| <<blk_mq_init_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2346| <<blk_mq_init_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2512| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 *
+	 * 在以下被设置:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * 搜索的时候要用sbitmap_clear_bit()也试一下
+	 */
 	struct sbitmap		ctx_map;
 
+	/*
+	 * 用到dispatch_from的地方:
+	 *   - block/blk-mq-sched.c|137| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+	 *   - block/blk-mq-sched.c|166| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+	 *   - block/blk-mq.c|2428| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+	 */
 	struct blk_mq_ctx	*dispatch_from;
+	/*
+	 * 修改dispatch_busy的地方:
+	 *   - block/blk-mq.c|1190| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 */
 	unsigned int		dispatch_busy;
 
 	unsigned short		type;
+	/*
+	 * 在以下被修改:
+	 *   - block/blk-mq.c|2471| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	unsigned short		nr_ctx;
+	/*
+	 * 分配ctxs数组指针的地方:
+	 *   - block/blk-mq.c|2308| <<blk_mq_init_hctx>> hctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),
+	 *
+	 * 设置每个指针元素的地方:
+	 *   - block/blk-mq.c|2471| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;
+	/*
+	 * used by:
+	 *   - block/blk-mq.c|1582| <<blk_mq_dispatch_wake>> hctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);
+	 *   - block/blk-mq.c|1643| <<blk_mq_mark_tag_wait>> wait = &hctx->dispatch_wait;
+	 *   - block/blk-mq.c|1877| <<blk_mq_dispatch_rq_list>> (no_tag && list_empty_careful(&hctx->dispatch_wait.entry)))
+	 *   - block/blk-mq.c|2972| <<blk_mq_init_hctx>> init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+	 *   - block/blk-mq.c|2973| <<blk_mq_init_hctx>> INIT_LIST_HEAD(&hctx->dispatch_wait.entry);
+	 */
 	wait_queue_entry_t	dispatch_wait;
+	/*
+	 * used by only:
+	 *   - block/blk-mq-tag.h|80| <<bt_wait_ptr>> return sbq_wait_ptr(bt, &hctx->wait_index);
+	 */
 	atomic_t		wait_index;
 
+	/*
+	 * 在以下设置了tags:
+	 *   - block/blk-mq.c|2302| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+	 *   - block/blk-mq.c|2504| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+	 */
 	struct blk_mq_tags	*tags;
+	/*
+	 * 在以下分配:
+	 *   - block/blk-mq-sched.c|453| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+	 */
 	struct blk_mq_tags	*sched_tags;
 
+	/*
+	 * 在以下进行使用:
+	 *   - block/blk-mq-debugfs.c|607| <<hctx_queued_show>> seq_printf(m, "%lu\n", hctx->queued);
+	 *   - block/blk-mq-debugfs.c|616| <<hctx_queued_write>> hctx->queued = 0;
+	 *   - block/blk-mq.c|418| <<blk_mq_get_request>> data->hctx->queued++;
+	 */
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
@@ -56,14 +150,48 @@ struct blk_mq_hw_ctx {
 	unsigned int		numa_node;
 	unsigned int		queue_num;
 
+	/*
+	 * 是伴随着RQF_MQ_INFLIGHT的:
+	 *   - block/blk-mq-debugfs.c|641| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 *   - block/blk-mq-tag.c|98| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq.c|511| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|751| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|1428| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|3205| <<blk_mq_alloc_and_init_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq.h|290| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 */
 	atomic_t		nr_active;
 	unsigned int		nr_expired;
 
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|2662| <<blk_mq_hctx_notify_dead>> hctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);
+	 *   - block/blk-mq.c|2687| <<blk_mq_remove_cpuhp>> &hctx->cpuhp_dead);
+	 *   - block/blk-mq.c|2742| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
+	 */
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;
 
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|560| <<hctx_io_poll_show>> seq_printf(m, "considered=%lu\n", hctx->poll_considered);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3992| <<blk_poll>> hctx->poll_considered++;
+	 */
 	unsigned long		poll_considered;
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|561| <<hctx_io_poll_show>> seq_printf(m, "invoked=%lu\n", hctx->poll_invoked);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3998| <<blk_poll>> hctx->poll_invoked++;
+	 */
 	unsigned long		poll_invoked;
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|562| <<hctx_io_poll_show>> seq_printf(m, "success=%lu\n", hctx->poll_success);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|4002| <<blk_poll>> hctx->poll_success++;
+	 */
 	unsigned long		poll_success;
 
 #ifdef CONFIG_BLK_DEBUG_FS
@@ -76,6 +204,9 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_queue_map {
+	/*
+	 * 猜测是每一个sw queue对应的hw queue???
+	 */
 	unsigned int *mq_map;
 	unsigned int nr_queues;
 	unsigned int queue_offset;
@@ -97,8 +228,24 @@ struct blk_mq_tag_set {
 	 * share maps between types.
 	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|2713| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3058| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3069| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2322| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2324| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 */
 	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
+	/*
+	 * set->nr_hw_queues在非驱动更新的地方:
+	 *   - block/blk-mq.c|3189| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3630| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1; --> kdump情况下
+	 *   - block/blk-mq.c|3639| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids; --> set->nr_maps==1的情况下
+	 *   - block/blk-mq.c|3874| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3882| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 */
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
 	unsigned int		reserved_tags;
@@ -111,6 +258,10 @@ struct blk_mq_tag_set {
 	struct blk_mq_tags	**tags;
 
 	struct mutex		tag_list_lock;
+	/*
+	 * 在以下添加新元素:
+	 *   - block/blk-mq.c|3297| <<blk_mq_add_queue_tag_set>> list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
+	 */
 	struct list_head	tag_list;
 };
 
@@ -168,6 +319,10 @@ struct blk_mq_ops {
 	/*
 	 * Called on request timeout
 	 */
+	/*
+	 * 几个例子:
+	 *   - nvme_timeout()
+	 */
 	timeout_fn		*timeout;
 
 	/*
@@ -175,6 +330,12 @@ struct blk_mq_ops {
 	 */
 	poll_fn			*poll;
 
+	/*
+	 * 几个例子:
+	 *   - virtblk_request_done()
+	 *   - scsi_softirq_done()
+	 *   - nvme_pci_complete_rq()
+	 */
 	complete_fn		*complete;
 
 	/*
@@ -216,24 +377,149 @@ struct blk_mq_ops {
 };
 
 enum {
+	/*
+	 * 下面的BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+	 * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+	 *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 */
+	/*
+	 * 主要使用的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - block/blk-mq-sched.c|334| <<__blk_mq_sched_bio_merge>> if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
+	 *
+	 * 某些设置的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - drivers/block/loop.c|1954| <<loop_add>> lo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/null_blk_main.c|1553| <<null_init_tag_set>> set->flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/virtio_blk.c|787| <<virtblk_probe>> vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/xen-blkfront.c|980| <<xlvbd_init_blk_queue>> info->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/nvme/host/pci.c|2334| <<nvme_dev_add>> dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/scsi/scsi_lib.c|1902| <<scsi_mq_setup_tags>> shost->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 */
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x2):
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|88| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|96| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq.c|305| <<blk_mq_rq_ctx_init>> if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
+	 *   - block/blk-mq.c|1109| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|1239| <<blk_mq_dispatch_rq_list>> if (hctx->flags & BLK_MQ_F_TAG_SHARED)
+	 *   - block/blk-mq.c|2298| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2533| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2535| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2561| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2578| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|2579| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2583| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_SHARED)
+	 *
+	 * 设置和删除的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - block/blk-mq.c|2544| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2590| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2309| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2546| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2572| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu
+	 *
+	 * 会设置到BLK_MQ_F_BLOCKING的地方, 非常少: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x20)
+	 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1559| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|910| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|795| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|785| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|413| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|448| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 使用BLK_MQ_F_NO_SCHED的地方, 主要是第一个: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40)
+	 *   - block/blk-mq.c|2891| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
+	 *   - block/elevator.c|691| <<elv_support_iosched>> if (q->tag_set && (q->tag_set->flags & BLK_MQ_F_NO_SCHED))
+	 *
+	 * 设置BLK_MQ_F_NO_SCHED的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40)
+	 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1555| <<null_init_tag_set>> set->flags |= BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/fc.c|3057| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/pci.c|1643| <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/rdma.c|728| <<nvme_rdma_alloc_tagset>> set->flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/target/loop.c|363| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|394| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
+	 *   - include/linux/blk-mq.h|398| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> << BLK_MQ_F_ALLOC_POLICY_START_BIT)
+	 */
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|395| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+	 *   - include/linux/blk-mq.h|397| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> ((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
+	 */
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
+	/*
+	 * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+	 */
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq.c|1550| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1575| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1596| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1620| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 */
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|33| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|57| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|76| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
+	/*
+	 * 使用的一个例子:
+	 *   - block/blk-mq.c|3051| <<blk_mq_alloc_tag_set>> if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
+	 */
 	BLK_MQ_MAX_DEPTH	= 10240,
 
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|2555| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|260| <<hctx_flags_show>> const int alloc_policy = BLK_MQ_FLAG_TO_ALLOC_POLICY(hctx->flags);
+ *   - block/blk-mq.c|2124| <<blk_mq_alloc_rq_map>> BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
+ */
 #define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
 	((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
 		((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|270| <<hctx_flags_show>> hctx->flags ^ BLK_ALLOC_POLICY_TO_MQ_FLAG(alloc_policy),
+ *   - drivers/block/skd_main.c|2846| <<skd_cons_disk>> BLK_ALLOC_POLICY_TO_MQ_FLAG(BLK_TAG_ALLOC_FIFO);
+ *   - drivers/scsi/scsi_lib.c|1904| <<scsi_mq_setup_tags>> BLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);
+ */
 #define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \
 	((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
 		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
@@ -259,13 +545,60 @@ bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 bool blk_mq_queue_inflight(struct request_queue *q);
 
 enum {
+	/*
+	 * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+	 */
 	/* return when out of requests */
+	/*
+	 * 设置的地方:return when out of requests
+	 *   - block/blk-core.c|1024| <<generic_make_request>> flags = BLK_MQ_REQ_NOWAIT;
+	 *   - block/blk-core.c|1079| <<generic_make_request>> flags = BLK_MQ_REQ_NOWAIT;
+	 *   - block/blk-core.c|1145| <<direct_make_request>> if (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {
+	 *   - block/blk-mq.c|583| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_NOWAIT;
+	 *   - block/blk-mq.c|1409| <<blk_mq_get_driver_tag>> .flags = BLK_MQ_REQ_NOWAIT,
+	 *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/ide/ide-park.c|50| <<issue_park_cmd>> rq = blk_get_request(q, REQ_OP_DRV_IN, BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/md/dm-mpath.c|517| <<multipath_clone_and_map>> BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 *   - drivers/nvme/host/pci.c|1354| <<nvme_timeout>> BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+	 *   - drivers/nvme/host/pci.c|2265| <<nvme_delete_queue>> req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+	 *   - drivers/scsi/fnic/fnic_scsi.c|2272| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+	 */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|1418| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|994| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|935| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 在以下被设置:
+	 *   - block/blk-mq.c|372| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *
+	 * 在以下被使用:
+	 *   - block/blk-mq-tag.c|104| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|297| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
+	/*
+	 * 在以下使用:
+	 *   - block/blk-core.c|400| <<blk_queue_enter>> const bool pm = flags & BLK_MQ_REQ_PREEMPT;
+	 *   - block/blk-core.c|583| <<blk_get_request>> WARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));
+	 *   - block/blk-mq.c|568| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_PREEMPT)
+	 *
+	 * 在以下设置:
+	 *   - drivers/ide/ide-pm.c|80| <<generic_ide_resume>> rq = blk_get_request(drive->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_PREEMPT);
+	 *   - drivers/scsi/scsi_lib.c|262| <<__scsi_execute>> REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, BLK_MQ_REQ_PREEMPT);
+	 */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
 };
 
@@ -348,6 +681,15 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/*
+ * 遍历每一个request_queue->queue_hw_ctx[i]
+ *
+ * q->queue_hw_ctx第一维在以下分配:
+ *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+ *
+ * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+ *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index d66bf5f..3abd285 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -340,6 +340,18 @@ enum req_flag_bits {
 #define REQ_PRIO		(1ULL << __REQ_PRIO)
 #define REQ_NOMERGE		(1ULL << __REQ_NOMERGE)
 #define REQ_IDLE		(1ULL << __REQ_IDLE)
+/*
+ * 设置的地方:
+ *   - block/bio-integrity.c|89| <<bio_integrity_alloc>> bio->bi_opf |= REQ_INTEGRITY;
+ *   - block/bio-integrity.c|123| <<bio_integrity_free>> bio->bi_opf &= ~REQ_INTEGRITY;
+ *   - drivers/md/dm-integrity.c|1262| <<integrity_end_io>> bio->bi_opf |= REQ_INTEGRITY;
+ *   - drivers/md/dm-integrity.c|1791| <<dm_integrity_map_continue>> bio->bi_opf &= ~REQ_INTEGRITY;
+ *   - drivers/nvme/host/core.c|969| <<nvme_submit_user_cmd>> req->cmd_flags |= REQ_INTEGRITY;
+ *
+ * 使用的地方:
+ *   - include/linux/bio.h|325| <<bio_integrity>> if (bio->bi_opf & REQ_INTEGRITY)
+ *   - include/linux/blkdev.h|1760| <<blk_integrity_rq>> return rq->cmd_flags & REQ_INTEGRITY;
+ */
 #define REQ_INTEGRITY		(1ULL << __REQ_INTEGRITY)
 #define REQ_FUA			(1ULL << __REQ_FUA)
 #define REQ_PREFLUSH		(1ULL << __REQ_PREFLUSH)
@@ -347,6 +359,13 @@ enum req_flag_bits {
 #define REQ_BACKGROUND		(1ULL << __REQ_BACKGROUND)
 #define REQ_NOWAIT		(1ULL << __REQ_NOWAIT)
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 设置和取消REQ_HIPRI的地方:
+ *   - drivers/nvme/host/core.c|756| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1272| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|838| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|405| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index faed9d9..d663e24 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -62,6 +62,9 @@ typedef void (rq_end_io_fn)(struct request *, blk_status_t);
  * request flags */
 typedef __u32 __bitwise req_flags_t;
 
+/*
+ * RQF_xxx用在request->rq_flags
+ */
 /* elevator knows about this request */
 #define RQF_SORTED		((__force req_flags_t)(1 << 0))
 /* drive already may have started this one */
@@ -69,12 +72,41 @@ typedef __u32 __bitwise req_flags_t;
 /* may not be passed by ioscheduler */
 #define RQF_SOFTBARRIER		((__force req_flags_t)(1 << 3))
 /* request for flush sequence */
+/*
+ * used by:
+ *   - block/blk-core.c|207| <<req_bio_endio>> if (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))
+ *   - block/blk-core.c|1372| <<blk_account_io_done>> if (blk_do_io_stat(req) && !(req->rq_flags & RQF_FLUSH_SEQ)) {
+ *   - block/blk-flush.c|253| <<blk_flush_restore_request>> rq->rq_flags &= ~RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|543| <<blk_kick_flush>> flush_rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-flush.c|694| <<blk_insert_flush>> rq->rq_flags |= RQF_FLUSH_SEQ;
+ *   - block/blk-mq-sched.c|530| <<blk_mq_sched_bypass_insert>> if (rq->rq_flags & RQF_FLUSH_SEQ) {
+ *   - block/blk-mq-sched.c|569| <<blk_mq_sched_insert_request>> if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+ *   - include/linux/blkdev.h|124| <<RQF_NOMERGE_FLAGS>> (RQF_STARTED | RQF_SOFTBARRIER | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)
+ */
 #define RQF_FLUSH_SEQ		((__force req_flags_t)(1 << 4))
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
 /* track inflight for MQ */
+/*
+ * 是伴随着hctx->nr_active的, 每次设置的时候, hctx->nr_active就增长
+ * 取消的时候, hctx->nr_active就减少
+ *   - block/blk-mq.c|510| <<blk_mq_rq_ctx_init>> rq_flags = RQF_MQ_INFLIGHT;
+ *   - block/blk-mq.c|750| <<blk_mq_free_request>> if (rq->rq_flags & RQF_MQ_INFLIGHT)
+ *   - block/blk-mq.c|1427| <<blk_mq_get_driver_tag>> rq->rq_flags |= RQF_MQ_INFLIGHT;
+ *   - block/blk-mq.h|288| <<__blk_mq_put_driver_tag>> if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ *   - block/blk-mq.h|289| <<__blk_mq_put_driver_tag>> rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ */
 #define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
 /* don't call prep for this one */
+/*
+ * 设置的地方:
+ *   - drivers/block/skd_main.c|501| <<skd_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/ide/ide-io.c|464| <<ide_issue_rq>> rq->rq_flags |= RQF_DONTPREP;
+ *   - drivers/mmc/core/queue.c|301| <<mmc_mq_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/nvme/host/core.c|466| <<nvme_clear_nvme_request>> req->rq_flags |= RQF_DONTPREP;
+ *   - drivers/scsi/scsi_lib.c|147| <<scsi_mq_requeue_cmd>> cmd->request->rq_flags &= ~RQF_DONTPREP;
+ *   - drivers/scsi/scsi_lib.c|1720| <<scsi_queue_rq>> req->rq_flags |= RQF_DONTPREP;
+ */
 #define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
 /* set for "ide_preempt" requests and also for requests for which the SCSI
    "quiesce" state must be ignored. */
@@ -114,9 +146,37 @@ typedef __u32 __bitwise req_flags_t;
 /*
  * Request state for blk-mq.
  */
+/*
+ * MQ_RQ_xxx用在request->state
+ */
 enum mq_rq_state {
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|536| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|719| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2127| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|672| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *   - block/blk-mq.c|693| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|696| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|825| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|863| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|593| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|3389| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -128,12 +188,31 @@ enum mq_rq_state {
  */
 struct request {
 	struct request_queue *q;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-flush.c|297| <<blk_kick_flush>> flush_rq->mq_ctx = first_rq->mq_ctx;
+	 *   - block/blk-mq.c|316| <<blk_mq_rq_ctx_init>> rq->mq_ctx = data->ctx;
+	 */
 	struct blk_mq_ctx *mq_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-flush.c|298| <<blk_kick_flush>> flush_rq->mq_hctx = first_rq->mq_hctx;
+	 *   - block/blk-mq.c|317| <<blk_mq_rq_ctx_init>> rq->mq_hctx = data->hctx;
+	 *   - block/blk-mq.c|502| <<__blk_mq_free_request>> rq->mq_hctx = NULL;
+	 */
 	struct blk_mq_hw_ctx *mq_hctx;
 
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 
+	/*
+	 * 在以下修改:
+	 *   - block/blk-core.c|116| <<blk_rq_init>> rq->internal_tag = -1;
+	 *   - block/blk-flush.c|224| <<flush_end_io>> flush_rq->internal_tag = -1;
+	 *   - block/blk-flush.c|305| <<blk_kick_flush>> flush_rq->internal_tag = first_rq->internal_tag;
+	 *   - block/blk-mq.c|303| <<blk_mq_rq_ctx_init>> rq->internal_tag = tag;
+	 *   - block/blk-mq.c|310| <<blk_mq_rq_ctx_init>> rq->internal_tag = -1;
+	 */
 	int internal_tag;
 
 	/* the following two fields are internal, NEVER access directly */
@@ -142,6 +221,7 @@ struct request {
 	sector_t __sector;		/* sector cursor */
 
 	struct bio *bio;
+	/* 指向请求的尾部 */
 	struct bio *biotail;
 
 	struct list_head queuelist;
@@ -317,9 +397,34 @@ struct queue_limits {
 	unsigned long		seg_boundary_mask;
 	unsigned long		virt_boundary_mask;
 
+	/*
+	 * max_hw_sectors和max_sectors设置的地方主要是blk_queue_max_hw_sectors()
+	 *
+	 * 调用blk_queue_max_hw_sectors()的地方很多, 例子是:
+	 *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+	 *   - drivers/block/virtio_blk.c|827| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+	 *   - drivers/nvme/host/core.c|1972| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+	 *   - drivers/block/loop.c|1968| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+	 */
 	unsigned int		max_hw_sectors;
+	/*
+	 * max_dev_sectors在以下被修改:
+	 *   - block/blk-settings.c|54| <<blk_set_default_limits>> lim->max_dev_sectors = 0;
+	 *   - block/blk-settings.c|90| <<blk_set_stacking_limits>> lim->max_dev_sectors = UINT_MAX;
+	 *   - block/blk-settings.c|526| <<blk_stack_limits>> t->max_dev_sectors = min_not_zero(t->max_dev_sectors, b->max_dev_sectors);
+	 *   - drivers/s390/block/dasd.c|3155| <<dasd_setup_queue>> q->limits.max_dev_sectors = max;
+	 *   - drivers/scsi/sd.c|3120| <<sd_revalidate_disk>> q->limits.max_dev_sectors = logical_to_sectors(sdp, dev_max);
+	 */
 	unsigned int		max_dev_sectors;
 	unsigned int		chunk_sectors;
+	/*
+	 * 设置max_sectors的地方:
+	 *   - block/blk-settings.c|53| <<blk_set_default_limits>> lim->max_sectors = lim->max_hw_sectors = BLK_SAFE_MAX_SECTORS;
+	 *   - block/blk-settings.c|89| <<blk_set_stacking_limits>> lim->max_sectors = UINT_MAX;
+	 *   - block/blk-settings.c|211| <<blk_queue_max_hw_sectors>> limits->max_sectors = max_sectors;
+	 *   - block/blk-settings.c|524| <<blk_stack_limits>> t->max_sectors = min_not_zero(t->max_sectors, b->max_sectors);
+	 *   - block/blk-sysfs.c|239| <<queue_max_sectors_store>> q->limits.max_sectors = max_sectors_kb << 1;
+	 */
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
 	unsigned int		physical_block_size;
@@ -404,13 +509,38 @@ struct request_queue {
 	const struct blk_mq_ops	*mq_ops;
 
 	/* sw queues */
+	/*
+	 * 实体保存在struct blk_mq_ctxs结构中
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq.c|3382| <<blk_mq_init_allocated_queue>> q->nr_queues = nr_hw_queues(set);
+	 *
+	 * nr_hw_queues()的实现:
+	 *   if (set->nr_maps == 1)
+	 *       return nr_cpu_ids;
+	 *   return max(set->nr_hw_queues, nr_cpu_ids);
+	 */
 	unsigned int		nr_queues;
 
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
+	/*
+	 * q->queue_hw_ctx第一维在以下分配:
+	 *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+	 *
+	 * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+	 *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+	 */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq.c|3325| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *
+	 * 在blk_mq_alloc_tag_set()中在nr_maps==1的情况下会设置为nr_cpu_ids
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -468,8 +598,23 @@ struct request_queue {
 	/*
 	 * queue settings
 	 */
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq-sched.c|563| <<blk_mq_init_sched>> q->nr_requests = q->tag_set->queue_depth;
+	 *   - block/blk-mq-sched.c|572| <<blk_mq_init_sched>> q->nr_requests = 2 * min_t(unsigned int , q->tag_set->queue_depth,
+	 *   - block/blk-mq.c|3416| <<blk_mq_init_allocated_queue>> q->nr_requests = set->queue_depth;
+	 *   - block/blk-mq.c|3723| <<blk_mq_update_nr_requests>> q->nr_requests = nr;
+	 *   - block/blk-settings.c|123| <<blk_queue_make_request>> q->nr_requests = BLKDEV_MAX_RQ;
+	 */
 	unsigned long		nr_requests;	/* Max # of requests */
 
+	/*
+	 * 设置dma_drain_size的地方:
+	 *   - block/blk-settings.c|764| <<blk_queue_dma_drain>> q->dma_drain_size = size;
+	 *   - drivers/ata/libata-scsi.c|1390| <<ata_scsi_slave_destroy>> q->dma_drain_size = 0;
+	 *
+	 * 似乎目前只被libata-scsi使用
+	 */
 	unsigned int		dma_drain_size;
 	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
@@ -481,7 +626,25 @@ struct request_queue {
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
+	/*
+	 * 使用的主要地方:
+	 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+	 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+	 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+	 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+	 */
 	struct timer_list	timeout;
+	/*
+	 * 可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+	 *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+	 *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+	 *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 */
 	struct work_struct	timeout_work;
 
 	struct list_head	icq_list;
@@ -533,10 +696,26 @@ struct request_queue {
 
 	struct list_head	requeue_list;
 	spinlock_t		requeue_lock;
+	/*
+	 * 使用的地方:
+	 *   - block/blk-core.c|252| <<blk_sync_queue>> cancel_delayed_work_sync(&q->requeue_work);
+	 *   - block/blk-mq.c|1245| <<blk_mq_requeue_work>> container_of(work, struct request_queue, requeue_work.work);
+	 *   - block/blk-mq.c|1310| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+	 *   - block/blk-mq.c|1317| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+	 *   - block/blk-mq.c|3781| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+	 */
 	struct delayed_work	requeue_work;
 
 	struct mutex		sysfs_lock;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 *
+	 * 其他使用的地方:
+	 *   - block/blk-core.c|500| <<blk_queue_enter>> (atomic_read(&q->mq_freeze_depth) == 0 &&
+	 */
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -548,7 +727,43 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|527| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_queue_head_t	mq_freeze_wq;
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|544| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|453| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *
+	 *   - block/blk-core.c|406| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-mq-tag.c|401| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|1245| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk.h|66| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|617| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *
+	 *   - block/blk-core.c|415| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|447| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|627| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-mq.c|305| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *
+	 *   - block/blk-mq.c|319| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *   - block/blk-mq.c|327| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-mq.c|366| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-sysfs.c|924| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 */
 	struct percpu_ref	q_usage_counter;
 	struct list_head	all_q_node;
 
@@ -572,7 +787,19 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
+/*
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+/*
+ * used by:
+ *   - include/linux/blkdev.h|678| <<blk_queue_stopped>> #define blk_queue_stopped(q) test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
+/*
+ * 在以下设置:
+ *   - block/blk-core.c|276| <<blk_set_queue_dying>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+ *   - block/blk-core.c|337| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DYING, q);
+ */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_BIDI		2	/* queue supports bidi requests */
 #define QUEUE_FLAG_NOMERGES     3	/* disable merge attempts */
@@ -586,16 +813,55 @@ struct request_queue {
 #define QUEUE_FLAG_ADD_RANDOM	10	/* Contributes to random pool */
 #define QUEUE_FLAG_SECERASE	11	/* supports secure erase */
 #define QUEUE_FLAG_SAME_FORCE	12	/* force complete on same CPU */
+/*
+ * used by:
+ *   - block/blk-core.c|390| <<blk_cleanup_queue>> blk_queue_flag_set(QUEUE_FLAG_DEAD, q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|161| <<mtip_check_surprise_removal>> blk_queue_flag_set(QUEUE_FLAG_DEAD, dd->queue);
+ *   - include/linux/blkdev.h|858| <<blk_queue_dead>> #define blk_queue_dead(q) test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_DEAD		13	/* queue tear-down finished */
+/*
+ * used by:
+ *   - block/blk-sysfs.c|964| <<blk_register_queue>> blk_queue_flag_set(QUEUE_FLAG_INIT_DONE, q);
+ *   - include/linux/blkdev.h|878| <<blk_queue_init_done>> #define blk_queue_init_done(q) test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_INIT_DONE	14	/* queue is initialized */
 #define QUEUE_FLAG_POLL		16	/* IO polling enabled if set */
+/*
+ * 一些设置的地方:
+ *   - block/blk-settings.c|865| <<blk_queue_write_cache>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-settings.c|867| <<blk_queue_write_cache>> blk_queue_flag_clear(QUEUE_FLAG_WC, q);
+ *   - block/blk-sysfs.c|514| <<queue_wc_store>> blk_queue_flag_set(QUEUE_FLAG_WC, q);
+ *   - block/blk-sysfs.c|516| <<queue_wc_store>> blk_queue_flag_clear(QUEUE_FLAG_WC, q);
+ *
+ *   - drivers/block/loop.c|964| <<loop_set_fd>> blk_queue_write_cache(lo->lo_queue, true, false);
+ *   - drivers/block/null_blk_main.c|1680| <<null_add_dev>> blk_queue_write_cache(nullb->q, true, true);
+ *   - drivers/block/virtio_blk.c|606| <<virtblk_update_cache_mode>> blk_queue_write_cache(vblk->disk->queue, writeback, false);
+ *   - drivers/block/xen-blkfront.c|1014| <<xlvbd_flush>> blk_queue_write_cache(info->rq, info->feature_flush ? true : false,
+ *   - drivers/nvme/host/core.c|2031| <<nvme_set_queue_limits>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/nvme/host/multipath.c|315| <<nvme_mpath_alloc_disk>> blk_queue_write_cache(q, vwc, vwc);
+ *   - drivers/ide/ide-disk.c|554| <<update_flush>> blk_queue_write_cache(drive->queue, wc, false);
+ *   - drivers/scsi/sd.c|153| <<sd_set_flush_flag>> blk_queue_write_cache(sdkp->disk->queue, wc, fua);
+ */
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 使用的例子:
+ *   - block/blk-mq.c|791| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|185| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|195| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|226| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * used by:
+ *   - block/blk-mq.c|278| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|320| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
@@ -606,9 +872,45 @@ void blk_queue_flag_set(unsigned int flag, struct request_queue *q);
 void blk_queue_flag_clear(unsigned int flag, struct request_queue *q);
 bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);
 
+/* 没人调用了 */
 #define blk_queue_stopped(q)	test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+/*
+ * called by:
+ *   - block/blk-cgroup.c|206| <<blkg_create>> if (blk_queue_dying(q)) {
+ *   - block/blk-core.c|556| <<blk_queue_enter>> blk_queue_dying(q));
+ *   - block/blk-core.c|557| <<blk_queue_enter>> if (blk_queue_dying(q))
+ *   - block/blk-core.c|716| <<blk_get_queue>> if (likely(!blk_queue_dying(q))) {
+ *   - block/blk-core.c|1182| <<generic_make_request>> if (!blk_queue_dying(q) && (bio->bi_opf & REQ_NOWAIT))
+ *   - block/blk-core.c|1263| <<generic_make_request>> if (unlikely(!blk_queue_dying(q) &&
+ *   - block/blk-core.c|1300| <<direct_make_request>> if (nowait && !blk_queue_dying(q))
+ *   - block/blk-mq-sysfs.c|70| <<blk_mq_sysfs_show>> if (!blk_queue_dying(q))
+ *   - block/blk-mq-sysfs.c|93| <<blk_mq_sysfs_store>> if (!blk_queue_dying(q))
+ *   - block/blk-mq-sysfs.c|116| <<blk_mq_hw_sysfs_show>> if (!blk_queue_dying(q))
+ *   - block/blk-mq-sysfs.c|140| <<blk_mq_hw_sysfs_store>> if (!blk_queue_dying(q))
+ *   - block/blk-sysfs.c|793| <<queue_attr_show>> if (blk_queue_dying(q)) {
+ *   - block/blk-sysfs.c|815| <<queue_attr_store>> if (blk_queue_dying(q)) {
+ *   - drivers/ide/ide-pm.c|47| <<ide_pm_execute_rq>> if (unlikely(blk_queue_dying(q))) {
+ *   - drivers/md/bcache/super.c|881| <<cached_dev_status_update>> if (blk_queue_dying(q))
+ *   - drivers/md/dm-mpath.c|520| <<multipath_clone_and_map>> if (blk_queue_dying(q)) {
+ *   - drivers/md/dm-mpath.c|1525| <<activate_or_offline_path>> if (pgpath->is_active && !blk_queue_dying(q))
+ *   - drivers/nvme/host/core.c|307| <<nvme_complete_rq>> if (!blk_queue_dying(req->q)) {
+ *   - drivers/nvme/host/core.c|3906| <<nvme_kill_queues>> if (ctrl->admin_q && !blk_queue_dying(ctrl->admin_q))
+ *   - drivers/nvme/host/pci.c|1848| <<nvme_dev_remove_admin>> if (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q)) {
+ *   - drivers/scsi/scsi_lib.c|1457| <<scsi_mq_lld_busy>> if (blk_queue_dying(q))
+ */
 #define blk_queue_dying(q)	test_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|164| <<queue_state_write>> if (blk_queue_dead(q))
+ *   - block/blk-sysfs.c|851| <<__blk_release_queue>> if (!blk_queue_dead(q)) {
+ */
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
+/*
+ * called by:
+ *   - block/blk-core.c|444| <<blk_cleanup_queue>> if (queue_is_mq(q) && blk_queue_init_done(q))
+ *   - block/blk-sysfs.c|856| <<__blk_release_queue>> WARN_ONCE(blk_queue_init_done(q),
+ *   - block/blk-sysfs.c|963| <<blk_register_queue>> if (!blk_queue_init_done(q)) {
+ */
 #define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index b297cd1..c5b6796 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -61,8 +61,33 @@ typedef void (percpu_ref_func_t)(struct percpu_ref *);
 
 /* flags set in the lower bits of percpu_ref->percpu_count_ptr */
 enum {
+	/*
+	 * used by:
+	 *   - lib/percpu-refcount.c|74| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|168| <<__percpu_ref_switch_to_atomic>> if (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {
+	 *   - lib/percpu-refcount.c|175| <<__percpu_ref_switch_to_atomic>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|194| <<__percpu_ref_switch_to_percpu>> if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
+	 *   - lib/percpu-refcount.c|209| <<__percpu_ref_switch_to_percpu>> ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
+	 */
 	__PERCPU_REF_ATOMIC	= 1LU << 0,	/* operating in atomic mode */
+	/*
+	 * used by:
+	 *   - include/linux/percpu-refcount.h|257| <<percpu_ref_tryget_live>> } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
+	 *   - include/linux/percpu-refcount.h|315| <<percpu_ref_is_dying>> return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|79| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|225| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+	 *   - lib/percpu-refcount.c|335| <<percpu_ref_kill_and_confirm>> WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
+	 *   - lib/percpu-refcount.c|338| <<percpu_ref_kill_and_confirm>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|386| <<percpu_ref_resurrect>> WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	 *   - lib/percpu-refcount.c|389| <<percpu_ref_resurrect>> ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
+	 */
 	__PERCPU_REF_DEAD	= 1LU << 1,	/* (being) killed */
+	/*
+	 * used by:
+	 *   - include/linux/percpu-refcount.h|161| <<__ref_is_percpu>> if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
+	 *   - lib/percpu-refcount.c|42| <<percpu_count_ptr>> (ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);
+	 *   - lib/percpu-refcount.c|109| <<percpu_ref_exit>> ref->percpu_count_ptr = __PERCPU_REF_ATOMIC_DEAD;
+	 */
 	__PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
 
 	__PERCPU_REF_FLAG_BITS	= 2,
@@ -92,8 +117,25 @@ struct percpu_ref {
 	 * mode; if set, then get/put will manipulate the atomic_t.
 	 */
 	unsigned long		percpu_count_ptr;
+	/*
+	 * release调用的地方:
+	 *   - include/linux/percpu-refcount.h|310| <<percpu_ref_put_many>> ref->release(ref);
+	 */
 	percpu_ref_func_t	*release;
+	/*
+	 * 在以下设置:
+	 *   - lib/percpu-refcount.c|107| <<percpu_ref_init>> ref->confirm_switch = NULL;
+	 *   - lib/percpu-refcount.c|145| <<percpu_ref_call_confirm_rcu>> ref->confirm_switch = NULL;
+	 *   - lib/percpu-refcount.c|214| <<__percpu_ref_switch_to_atomic>> ref->confirm_switch = confirm_switch ?: percpu_ref_noop_confirm_switch;
+	 */
 	percpu_ref_func_t	*confirm_switch;
+	/*
+	 * 使用的地方:
+	 *   - lib/percpu-refcount.c|88| <<percpu_ref_init>> ref->force_atomic = flags & PERCPU_REF_INIT_ATOMIC;
+	 *   - lib/percpu-refcount.c|258| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+	 *   - lib/percpu-refcount.c|291| <<percpu_ref_switch_to_atomic>> ref->force_atomic = true;
+	 *   - lib/percpu-refcount.c|337| <<percpu_ref_switch_to_percpu>> ref->force_atomic = false;
+	 */
 	bool			force_atomic:1;
 	struct rcu_head		rcu;
 };
@@ -123,8 +165,29 @@ void percpu_ref_reinit(struct percpu_ref *ref);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - block/blk-cgroup.c|415| <<blkg_destroy>> percpu_ref_kill(&blkg->refcnt);
+ *   - block/blk-mq.c|349| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+ *   - drivers/dax/pmem.c|56| <<dax_pmem_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/infiniband/sw/rdmavt/mr.c|275| <<rvt_free_lkey>> percpu_ref_kill(&mr->refcount);
+ *   - drivers/nvme/target/core.c|585| <<nvmet_ns_disable>> percpu_ref_kill(&ns->ref);
+ *   - drivers/pci/p2pdma.c|96| <<pci_p2pdma_percpu_kill>> percpu_ref_kill(ref);
+ *   - drivers/target/target_core_transport.c|2942| <<target_sess_cmd_list_set_waiting>> percpu_ref_kill(&se_sess->cmd_count);
+ *   - drivers/target/target_core_transport.c|2974| <<transport_clear_lun_ref>> percpu_ref_kill(&lun->lun_ref);
+ *   - fs/aio.c|621| <<free_ioctx_users>> percpu_ref_kill(&ctx->reqs);
+ *   - fs/aio.c|840| <<kill_ioctx>> percpu_ref_kill(&ctx->users);
+ *   - include/linux/genhd.h|696| <<hd_struct_kill>> percpu_ref_kill(&part->ref);
+ *   - kernel/cgroup/cgroup.c|2131| <<cgroup_kill_sb>> percpu_ref_kill(&root->cgrp.self.refcnt);
+ *   - kernel/cgroup/cgroup.c|5246| <<__acquires>> percpu_ref_kill(&cgrp->self.refcnt);
+ *   - mm/backing-dev.c|526| <<cgwb_kill>> percpu_ref_kill(&wb->refcnt);
+ *   - mm/hmm.c|990| <<hmm_devmem_ref_kill>> percpu_ref_kill(ref);
+ *
+ * 核心思想是把ref->percpu_count_ptr设置上__PERCPU_REF_DEAD然后percpu_ref_put(ref)
+ */
 static inline void percpu_ref_kill(struct percpu_ref *ref)
 {
+	/* 核心思想是把ref->percpu_count_ptr设置上__PERCPU_REF_DEAD然后percpu_ref_put(ref) */
 	percpu_ref_kill_and_confirm(ref, NULL);
 }
 
@@ -134,6 +197,20 @@ static inline void percpu_ref_kill(struct percpu_ref *ref)
  * because doing so forces the compiler to generate two conditional
  * branches as it can't assume that @ref->percpu_count is not NULL.
  */
+/*
+ * called by:
+ *   - include/linux/percpu-refcount.h|212| <<percpu_ref_get_many>> if (__ref_is_percpu(ref, &percpu_count))
+ *   - include/linux/percpu-refcount.h|249| <<percpu_ref_tryget>> if (__ref_is_percpu(ref, &percpu_count)) {
+ *   - include/linux/percpu-refcount.h|283| <<percpu_ref_tryget_live>> if (__ref_is_percpu(ref, &percpu_count)) {
+ *   - include/linux/percpu-refcount.h|316| <<percpu_ref_put_many>> if (__ref_is_percpu(ref, &percpu_count))
+ *   - include/linux/percpu-refcount.h|368| <<percpu_ref_is_zero>> if (__ref_is_percpu(ref, &percpu_count))
+ *   - lib/percpu-refcount.c|400| <<percpu_ref_resurrect>> WARN_ON_ONCE(__ref_is_percpu(ref, &percpu_count));
+ *
+ * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+ * 则返回false
+ * 否则更新参数为ref->percpu_count_ptr
+ * 并且返回true
+ */
 static inline bool __ref_is_percpu(struct percpu_ref *ref,
 					  unsigned long __percpu **percpu_countp)
 {
@@ -150,6 +227,9 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
 	 * The smp_read_barrier_depends() implied by READ_ONCE() pairs
 	 * with smp_store_release() in __percpu_ref_switch_to_percpu().
 	 */
+	/*
+	 * unsigned long percpu_count_ptr;
+	 */
 	percpu_ptr = READ_ONCE(ref->percpu_count_ptr);
 
 	/*
@@ -158,6 +238,10 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
 	 * visible without ATOMIC if we race with percpu_ref_kill().  DEAD
 	 * implies ATOMIC anyway.  Test them together.
 	 */
+	/*
+	 * 如果最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 返回false
+	 */
 	if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
 		return false;
 
@@ -174,12 +258,24 @@ static inline bool __ref_is_percpu(struct percpu_ref *ref,
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - include/linux/cgroup.h|322| <<css_get_many>> percpu_ref_get_many(&css->refcnt, n);
+ *   - include/linux/percpu-refcount.h|285| <<percpu_ref_get>> percpu_ref_get_many(ref, 1);
+ *   - kernel/memremap.c|248| <<devm_memremap_pages>> percpu_ref_get_many(pgmap->ref, pfn_end(pgmap) - pfn_first(pgmap));
+ */
 static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_add(*percpu_count, nr);
 	else
@@ -196,6 +292,26 @@ static inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk.h|110| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+ *   - drivers/md/md.c|8124| <<md_write_start>> percpu_ref_get(&mddev->writes_pending);
+ *   - drivers/md/md.c|8169| <<md_write_inc>> percpu_ref_get(&mddev->writes_pending);
+ *   - drivers/nvme/target/core.c|383| <<nvmet_find_namespace>> percpu_ref_get(&ns->ref);
+ *   - drivers/scsi/scsi_lib.c|617| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+ *   - drivers/target/target_core_device.c|127| <<transport_lookup_cmd_lun>> percpu_ref_get(&se_lun->lun_ref);
+ *   - drivers/target/target_core_transport.c|2770| <<target_get_sess_cmd>> percpu_ref_get(&se_sess->cmd_count);
+ *   - fs/aio.c|772| <<ioctx_alloc>> percpu_ref_get(&ctx->users);
+ *   - fs/aio.c|773| <<ioctx_alloc>> percpu_ref_get(&ctx->reqs);
+ *   - fs/aio.c|1023| <<aio_get_req>> percpu_ref_get(&ctx->reqs);
+ *   - include/linux/backing-dev-defs.h|253| <<wb_get>> percpu_ref_get(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|490| <<blkg_get>> percpu_ref_get(&blkg->refcnt);
+ *   - include/linux/cgroup.h|309| <<css_get>> percpu_ref_get(&css->refcnt);
+ *   - include/linux/genhd.h|681| <<hd_struct_get>> percpu_ref_get(&part->ref);
+ *   - include/rdma/rdmavt_mr.h|132| <<rvt_get_mr>> percpu_ref_get(&mr->refcount);
+ *   - lib/percpu-refcount.c|257| <<__percpu_ref_switch_to_atomic>> percpu_ref_get(ref);
+ *   - lib/percpu-refcount.c|513| <<percpu_ref_resurrect>> percpu_ref_get(ref);
+ */
 static inline void percpu_ref_get(struct percpu_ref *ref)
 {
 	percpu_ref_get_many(ref, 1);
@@ -210,6 +326,18 @@ static inline void percpu_ref_get(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|660| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - block/blk-mq.c|1380| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+ *   - include/linux/backing-dev-defs.h|242| <<wb_tryget>> return percpu_ref_tryget(&wb->refcnt);
+ *   - include/linux/blk-cgroup.h|502| <<blkg_tryget>> return blkg && percpu_ref_tryget(&blkg->refcnt);
+ *   - include/linux/cgroup.h|339| <<css_tryget>> return percpu_ref_tryget(&css->refcnt);
+ *
+ * percpu_ref_tryget和percpu_ref_tryget_live的不同
+ * percpu_ref_tryget_live只有在ref->percpu_count_ptr没有设置__PERCPU_REF_DEAD的时候才会增加ref->count
+ * percpu_ref_tryget在__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD两种情况都可以增加ref->count
+ */
 static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -217,10 +345,17 @@ static inline bool percpu_ref_tryget(struct percpu_ref *ref)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
 		ret = true;
 	} else {
+		/* 在不是0时加1 */
 		ret = atomic_long_inc_not_zero(&ref->count);
 	}
 
@@ -244,6 +379,36 @@ static inline bool percpu_ref_tryget(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|445| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+ *   - drivers/nvme/target/core.c|880| <<nvmet_req_init>> if (unlikely(!percpu_ref_tryget_live(&sq->ref))) {
+ *   - drivers/pci/p2pdma.c|561| <<pci_alloc_p2pmem>> if (unlikely(!percpu_ref_tryget_live(&pdev->p2pdma->devmap_ref)))
+ *   - drivers/target/target_core_alua.c|973| <<core_alua_queue_state_change_ua>> if (!percpu_ref_tryget_live(&lun->lun_ref))
+ *   - drivers/target/target_core_device.c|83| <<transport_lookup_cmd_lun>> if (!percpu_ref_tryget_live(&se_lun->lun_ref)) {
+ *   - drivers/target/target_core_device.c|174| <<transport_lookup_tmr_lun>> if (!percpu_ref_tryget_live(&se_lun->lun_ref)) {
+ *   - drivers/target/target_core_pr.c|735| <<__core_scsi3_alloc_registration>> if (!percpu_ref_tryget_live(&lun_tmp->lun_ref))
+ *   - fs/aio.c|1051| <<lookup_ioctx>> if (percpu_ref_tryget_live(&ctx->users))
+ *   - include/linux/cgroup.h|356| <<css_tryget_online>> return percpu_ref_tryget_live(&css->refcnt);
+ *   - include/linux/genhd.h|686| <<hd_struct_try_get>> return percpu_ref_tryget_live(&part->ref);
+ *   - kernel/cgroup/cgroup-v1.c|1146| <<cgroup1_mount>> if (!percpu_ref_tryget_live(&ss->root->cgrp.self.refcnt)) {
+ *   - kernel/cgroup/cgroup-v1.c|1201| <<cgroup1_mount>> !percpu_ref_tryget_live(&root->cgrp.self.refcnt)) {
+ *   - kernel/memremap.c|305| <<get_dev_pagemap>> if (pgmap && !percpu_ref_tryget_live(pgmap->ref))
+ *
+ * 如果ref->percpu_count_ptr的最后两位有__PERCPU_REF_DEAD被设置
+ * 函数返回false
+ *
+ * 如果ref->percpu_count_ptr的最后两位有__PERCPU_REF_ATOMIC被设置
+ * 则用atomic_long_inc_not_zero(&ref->count)增加ref->count
+ *
+ * 否则用this_cpu_inc(*percpu_count)增加ref->percpu_count_ptr
+ * 并且返回true
+ *
+ *
+ * percpu_ref_tryget和percpu_ref_tryget_live的不同
+ * percpu_ref_tryget_live只有在ref->percpu_count_ptr没有设置__PERCPU_REF_DEAD的时候才会增加ref->count
+ * percpu_ref_tryget在__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD两种情况都可以增加ref->count
+ */
 static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
@@ -251,6 +416,12 @@ static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count)) {
 		this_cpu_inc(*percpu_count);
 		ret = true;
@@ -273,12 +444,29 @@ static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - include/linux/cgroup.h|402| <<css_put_many>> percpu_ref_put_many(&css->refcnt, n);
+ *   - include/linux/percpu-refcount.h|326| <<percpu_ref_put>> percpu_ref_put_many(ref, 1);
+ *
+ * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+ * 调用this_cpu_sub(*percpu_count, nr);用percpu的减少nr
+ * 
+ * 否则用atomic_long_sub_and_test(nr, &ref->count))从ref->count减少nr
+ * 如果结果是0了就调用ref->release(ref)
+ */
 static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
 
 	rcu_read_lock_sched();
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false                            
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		this_cpu_sub(*percpu_count, nr);
 	else if (unlikely(atomic_long_sub_and_test(nr, &ref->count)))
@@ -296,6 +484,17 @@ static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 调用的一个例子:
+ *   - block/blk-core.c|448| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter); 
+ *
+ * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+ * 调用this_cpu_sub(*percpu_count, 1);用percpu的减少1
+ * 
+ * 否则用atomic_long_sub_and_test(nr, &ref->count))从ref->count减少1
+ * 如果结果是0了就调用ref->release(ref)
+ * q_usage_counter的release是blk_queue_usage_counter_release()
+ */
 static inline void percpu_ref_put(struct percpu_ref *ref)
 {
 	percpu_ref_put_many(ref, 1);
@@ -310,6 +509,15 @@ static inline void percpu_ref_put(struct percpu_ref *ref)
  * This function is safe to call as long as @ref is between init and exit
  * and the caller is responsible for synchronizing against state changes.
  */
+/*
+ * called by:
+ *   - drivers/pci/p2pdma.c|93| <<pci_p2pdma_percpu_kill>> if (percpu_ref_is_dying(ref))
+ *   - include/linux/backing-dev-defs.h|282| <<wb_dying>> return percpu_ref_is_dying(&wb->refcnt);
+ *   - include/linux/cgroup.h|377| <<css_is_dying>> return !(css->flags & CSS_NO_REF) && percpu_ref_is_dying(&css->refcnt);
+ *   - kernel/cgroup/cgroup.c|2856| <<__acquires>> if (!css || !percpu_ref_is_dying(&css->refcnt))
+ *   - kernel/cgroup/cgroup.c|2970| <<cgroup_apply_control_enable>> WARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));
+ *   - kernel/cgroup/cgroup.c|3016| <<cgroup_apply_control_disable>> WARN_ON_ONCE(css && percpu_ref_is_dying(&css->refcnt));
+ */
 static inline bool percpu_ref_is_dying(struct percpu_ref *ref)
 {
 	return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
@@ -323,10 +531,25 @@ static inline bool percpu_ref_is_dying(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|363| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+ *   - block/blk-mq.c|371| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+ *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+ *   - drivers/md/md.c|2350| <<set_in_sync>> percpu_ref_is_zero(&mddev->writes_pending)) {
+ *   - drivers/target/target_core_transport.c|2959| <<target_wait_for_sess_cmds>> percpu_ref_is_zero(&se_sess->cmd_count),
+ *   - lib/percpu-refcount.c|477| <<percpu_ref_reinit>> WARN_ON_ONCE(!percpu_ref_is_zero(ref));
+ */
 static inline bool percpu_ref_is_zero(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
 
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 则返回false
+	 * 否则更新参数为ref->percpu_count_ptr
+	 * 并且返回true
+	 */
 	if (__ref_is_percpu(ref, &percpu_count))
 		return false;
 	return !atomic_long_read(&ref->count);
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index 14d5581..a6e8be0 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -138,6 +138,17 @@ struct sbitmap_queue {
 	/*
 	 * @ws_active: count of currently active ws waitqueues
 	 */
+	/*
+	 * 增加减少的地方:
+	 *   - lib/sbitmap.c|428| <<sbitmap_queue_init_node>> atomic_set(&sbq->ws_active, 0);
+	 *   - lib/sbitmap.c|738| <<sbitmap_add_wait_queue>> atomic_inc(&sbq->ws_active);
+	 *   - lib/sbitmap.c|759| <<sbitmap_prepare_to_wait>> atomic_inc(&sbq->ws_active);
+	 *   - lib/sbitmap.c|748| <<sbitmap_del_wait_queue>> atomic_dec(&sbq_wait->sbq->ws_active);
+	 *   - lib/sbitmap.c|777| <<sbitmap_finish_wait>> atomic_dec(&sbq->ws_active);
+	 *
+	 * 使用的地方:
+	 *   - lib/sbitmap.c|560| <<sbq_wake_ptr>> if (!atomic_read(&sbq->ws_active))
+	 */
 	atomic_t ws_active;
 
 	/**
@@ -236,7 +247,26 @@ bool sbitmap_any_bit_set(const struct sbitmap *sb);
  */
 bool sbitmap_any_bit_clear(const struct sbitmap *sb);
 
+/*
+ * called by:
+ *   - include/linux/sbitmap.h|264| <<__sbitmap_for_each_set>> index = SB_NR_TO_INDEX(sb, start);
+ *   - include/linux/sbitmap.h|315| <<__sbitmap_word>> return &sb->map[SB_NR_TO_INDEX(sb, bitnr)].word;
+ *   - include/linux/sbitmap.h|338| <<sbitmap_deferred_clear_bit>> unsigned long *addr = &sb->map[SB_NR_TO_INDEX(sb, bitnr)].cleared;
+ *   - lib/sbitmap.c|175| <<sbitmap_get>> index = SB_NR_TO_INDEX(sb, alloc_hint);
+ *   - lib/sbitmap.c|211| <<sbitmap_get_shallow>> index = SB_NR_TO_INDEX(sb, alloc_hint);
+ */
 #define SB_NR_TO_INDEX(sb, bitnr) ((bitnr) >> (sb)->shift)
+/*
+ * called by:
+ *   - include/linux/sbitmap.h|279| <<__sbitmap_for_each_set>> nr = SB_NR_TO_BIT(sb, start);
+ *   - include/linux/sbitmap.h|336| <<sbitmap_set_bit>> set_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - include/linux/sbitmap.h|341| <<sbitmap_clear_bit>> clear_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - include/linux/sbitmap.h|354| <<sbitmap_deferred_clear_bit>> set_bit(SB_NR_TO_BIT(sb, bitnr), addr);
+ *   - include/linux/sbitmap.h|360| <<sbitmap_clear_bit_unlock>> clear_bit_unlock(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - include/linux/sbitmap.h|365| <<sbitmap_test_bit>> return test_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - lib/sbitmap.c|183| <<sbitmap_get>> alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
+ *   - lib/sbitmap.c|217| <<sbitmap_get_shallow>> SB_NR_TO_BIT(sb, alloc_hint), true);
+ */
 #define SB_NR_TO_BIT(sb, bitnr) ((bitnr) & ((1U << (sb)->shift) - 1U))
 
 typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);
@@ -251,6 +281,13 @@ typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);
  * This is inline even though it's non-trivial so that the function calls to the
  * callback will hopefully get optimized away.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1488| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+ *   - include/linux/sbitmap.h|311| <<sbitmap_for_each_set>> __sbitmap_for_each_set(sb, 0, fn, data);
+ *
+ * 如果fn返回true继续下一个bit, 如果返回false就直接返回了
+ */
 static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 					  unsigned int start,
 					  sb_for_each_fn fn, void *data)
@@ -259,6 +296,7 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 	unsigned int nr;
 	unsigned int scanned = 0;
 
+	/* 因为start不一定是0, 所以会绕回开始 */
 	if (start >= sb->depth)
 		start = 0;
 	index = SB_NR_TO_INDEX(sb, start);
@@ -266,6 +304,9 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 
 	while (scanned < sb->depth) {
 		unsigned long word;
+		/*
+		 * sb->map是struct sbitmap_word *map;
+		 */
 		unsigned int depth = min_t(unsigned int,
 					   sb->map[index].depth - nr,
 					   sb->depth - scanned);
@@ -285,6 +326,7 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 			nr = find_next_bit(&word, depth, nr);
 			if (nr >= depth)
 				break;
+			/* Should return true to continue or false to break early. */
 			if (!fn(sb, (index << sb->shift) + nr, data))
 				return;
 
@@ -303,6 +345,9 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
  * @fn: Callback. Should return true to continue or false to break early.
  * @data: Pointer to pass to callback.
  */
+/*
+ * 如果fn返回true继续下一个bit, 如果返回false就直接返回了
+ */
 static inline void sbitmap_for_each_set(struct sbitmap *sb, sb_for_each_fn fn,
 					void *data)
 {
diff --git a/lib/kobject.c b/lib/kobject.c
index b72e00f..39fa0b3 100644
--- a/lib/kobject.c
+++ b/lib/kobject.c
@@ -630,6 +630,11 @@ EXPORT_SYMBOL(kobject_get_unless_zero);
  * kobject_cleanup - free kobject resources.
  * @kobj: object to cleanup
  */
+/*
+ * called by:
+ *   - lib/kobject.c|675| <<kobject_delayed_cleanup>> kobject_cleanup(container_of(to_delayed_work(work),
+ *   - lib/kobject.c|691| <<kobject_release>> kobject_cleanup(kobj);
+ */
 static void kobject_cleanup(struct kobject *kobj)
 {
 	struct kobj_type *t = get_ktype(kobj);
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 9877682..83f6d3b 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -33,11 +33,41 @@
 
 #define PERCPU_COUNT_BIAS	(1LU << (BITS_PER_LONG - 1))
 
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|258| <<__percpu_ref_switch_mode>> lockdep_assert_held(&percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|266| <<__percpu_ref_switch_mode>> percpu_ref_switch_lock);
+ *   - lib/percpu-refcount.c|303| <<percpu_ref_switch_to_atomic>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|308| <<percpu_ref_switch_to_atomic>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|349| <<percpu_ref_switch_to_percpu>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|354| <<percpu_ref_switch_to_percpu>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|380| <<percpu_ref_kill_and_confirm>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|389| <<percpu_ref_kill_and_confirm>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|431| <<percpu_ref_resurrect>> spin_lock_irqsave(&percpu_ref_switch_lock, flags);
+ *   - lib/percpu-refcount.c|440| <<percpu_ref_resurrect>> spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
+ */
 static DEFINE_SPINLOCK(percpu_ref_switch_lock);
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|146| <<percpu_ref_call_confirm_rcu>> wake_up_all(&percpu_ref_switch_waitq);
+ *   - lib/percpu-refcount.c|255| <<__percpu_ref_switch_mode>> wait_event_lock_irq(percpu_ref_switch_waitq, !ref->confirm_switch,
+ *   - lib/percpu-refcount.c|309| <<percpu_ref_switch_to_atomic_sync>> wait_event(percpu_ref_switch_waitq, !ref->confirm_switch);
+ */
 static DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);
 
+/*
+ * 去掉percpu_ref->percpu_count_ptr的最后两位并返回
+ * 关于percpu_ref->percpu_count_ptr:
+ * The low bit of the pointer indicates whether the ref is in percpu
+ * mode; if set, then get/put will manipulate the atomic_t.
+ */
 static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
 {
+	/*
+	 * 关于percpu_ref->percpu_count_ptr:
+	 * The low bit of the pointer indicates whether the ref is in percpu
+	 * mode; if set, then get/put will manipulate the atomic_t.
+	 */
 	return (unsigned long __percpu *)
 		(ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);
 }
@@ -56,6 +86,12 @@ static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
  * Note that @release must not sleep - it may potentially be called from RCU
  * callback context by percpu_ref_kill().
  */
+/*
+ * 一个调用的例子block/blk-core.c|534| <<blk_alloc_queue_node>> :
+ *  534         if (percpu_ref_init(&q->q_usage_counter,
+ *  535                                 blk_queue_usage_counter_release,
+ *  536                                 PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+ */
 int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 		    unsigned int flags, gfp_t gfp)
 {
@@ -82,6 +118,10 @@ int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 
 	atomic_long_set(&ref->count, start_count);
 
+	/*
+	 * release调用的地方:
+	 *   - include/linux/percpu-refcount.h|310| <<percpu_ref_put_many>> ref->release(ref);
+	 */
 	ref->release = release;
 	ref->confirm_switch = NULL;
 	return 0;
@@ -98,8 +138,19 @@ EXPORT_SYMBOL_GPL(percpu_ref_init);
  * where percpu_ref_init() succeeded but other parts of the initialization
  * of the embedding object failed.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|419| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+ *   - block/blk-core.c|606| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+ */
 void percpu_ref_exit(struct percpu_ref *ref)
 {
+	/*
+	 * 去掉percpu_ref->percpu_count_ptr的最后两位并返回
+	 * 关于percpu_ref->percpu_count_ptr:
+	 * The low bit of the pointer indicates whether the ref is in percpu
+	 * mode; if set, then get/put will manipulate the atomic_t.
+	 */
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
 
 	if (percpu_count) {
@@ -111,6 +162,10 @@ void percpu_ref_exit(struct percpu_ref *ref)
 }
 EXPORT_SYMBOL_GPL(percpu_ref_exit);
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|210| <<percpu_ref_switch_to_atomic_rcu>> percpu_ref_call_confirm_rcu(rcu);
+ */
 static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -120,9 +175,20 @@ static void percpu_ref_call_confirm_rcu(struct rcu_head *rcu)
 	wake_up_all(&percpu_ref_switch_waitq);
 
 	/* drop ref from percpu_ref_switch_to_atomic() */
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 调用this_cpu_sub(*percpu_count, nr);用percpu的减少nr
+	 * 
+	 * 否则用atomic_long_sub_and_test(nr, &ref->count))从ref->count减少nr
+	 * 如果结果是0了就调用ref->release(ref)
+	 */
 	percpu_ref_put(ref);
 }
 
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|240| <<__percpu_ref_switch_to_atomic>> call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
+ */
 static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 {
 	struct percpu_ref *ref = container_of(rcu, struct percpu_ref, rcu);
@@ -158,10 +224,18 @@ static void percpu_ref_switch_to_atomic_rcu(struct rcu_head *rcu)
 	percpu_ref_call_confirm_rcu(rcu);
 }
 
+/*
+ * used by:
+ *   - lib/percpu-refcount.c|237| <<__percpu_ref_switch_to_atomic>> ref->confirm_switch = confirm_switch ?: percpu_ref_noop_confirm_switch;
+ */
 static void percpu_ref_noop_confirm_switch(struct percpu_ref *ref)
 {
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|265| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_atomic(ref, confirm_switch);
+ */
 static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 					  percpu_ref_func_t *confirm_switch)
 {
@@ -184,6 +258,10 @@ static void __percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 	call_rcu(&ref->rcu, percpu_ref_switch_to_atomic_rcu);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|284| <<__percpu_ref_switch_mode>> __percpu_ref_switch_to_percpu(ref);
+ */
 static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count = percpu_count_ptr(ref);
@@ -209,6 +287,13 @@ static void __percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 			  ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
 }
 
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|319| <<percpu_ref_switch_to_atomic>> __percpu_ref_switch_mode(ref, confirm_switch);
+ *   - lib/percpu-refcount.c|365| <<percpu_ref_switch_to_percpu>> __percpu_ref_switch_mode(ref, NULL);
+ *   - lib/percpu-refcount.c|407| <<percpu_ref_kill_and_confirm>> __percpu_ref_switch_mode(ref, confirm_kill);
+ *   - lib/percpu-refcount.c|475| <<percpu_ref_resurrect>> __percpu_ref_switch_mode(ref, NULL);
+ */
 static void __percpu_ref_switch_mode(struct percpu_ref *ref,
 				     percpu_ref_func_t *confirm_switch)
 {
@@ -248,6 +333,10 @@ static void __percpu_ref_switch_mode(struct percpu_ref *ref,
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by:
+ *   - lib/percpu-refcount.c|308| <<percpu_ref_switch_to_atomic_sync>> percpu_ref_switch_to_atomic(ref, NULL);
+ */
 void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_switch)
 {
@@ -270,6 +359,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic);
  * switch to complete.  Caller must ensure that no other thread
  * will switch back to percpu mode.
  */
+/*
+ * called by:
+ *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+ *   - drivers/md/md.c|2347| <<set_in_sync>> percpu_ref_switch_to_atomic_sync(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref)
 {
 	percpu_ref_switch_to_atomic(ref, NULL);
@@ -295,6 +389,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_atomic_sync);
  * mode.  If the caller ensures that @ref is not in the process of
  * switching to atomic mode, this function can be called from any context.
  */
+/*
+ * called by:
+ *   - block/blk-sysfs.c|965| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+ *   - drivers/md/md.c|2361| <<set_in_sync>> percpu_ref_switch_to_percpu(&mddev->writes_pending);
+ */
 void percpu_ref_switch_to_percpu(struct percpu_ref *ref)
 {
 	unsigned long flags;
@@ -325,6 +424,14 @@ EXPORT_SYMBOL_GPL(percpu_ref_switch_to_percpu);
  *
  * There are no implied RCU grace periods between kill and release.
  */
+/*
+ * called by:
+ *   - drivers/nvme/target/core.c|734| <<nvmet_sq_destroy>> percpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);
+ *   - include/linux/percpu-refcount.h|170| <<percpu_ref_kill>> percpu_ref_kill_and_confirm(ref, NULL);
+ *   - kernel/cgroup/cgroup.c|5162| <<kill_css>> percpu_ref_kill_and_confirm(&css->refcnt, css_killed_ref_fn);
+ *
+ *  核心思想是把ref->percpu_count_ptr设置上__PERCPU_REF_DEAD然后percpu_ref_put(ref)
+ */
 void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 				 percpu_ref_func_t *confirm_kill)
 {
@@ -337,6 +444,13 @@ void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
 
 	ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
 	__percpu_ref_switch_mode(ref, confirm_kill);
+	/*
+	 * 如果ref->percpu_count_ptr的最后两位__PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD有设置的
+	 * 调用this_cpu_sub(*percpu_count, 1);用percpu的减少1
+	 * 
+	 * 否则用atomic_long_sub_and_test(1, &ref->count))从ref->count减少1
+	 * 如果结果是0了就调用ref->release(ref)
+	 */
 	percpu_ref_put(ref);
 
 	spin_unlock_irqrestore(&percpu_ref_switch_lock, flags);
@@ -354,6 +468,10 @@ EXPORT_SYMBOL_GPL(percpu_ref_kill_and_confirm);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called only by:
+ *   - kernel/cgroup/cgroup-v1.c|1263| <<cgroup1_mount>> percpu_ref_reinit(&root->cgrp.self.refcnt);
+ */
 void percpu_ref_reinit(struct percpu_ref *ref)
 {
 	WARN_ON_ONCE(!percpu_ref_is_zero(ref));
@@ -376,6 +494,11 @@ EXPORT_SYMBOL_GPL(percpu_ref_reinit);
  * Note that percpu_ref_tryget[_live]() are safe to perform on @ref while
  * this function is in progress.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|410| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+ *   - lib/percpu-refcount.c|408| <<percpu_ref_reinit>> percpu_ref_resurrect(ref);
+ */
 void percpu_ref_resurrect(struct percpu_ref *ref)
 {
 	unsigned long __percpu *percpu_count;
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 5b382c1..565bc97 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -23,6 +23,12 @@
 /*
  * See if we have deferred clears that we can batch move
  */
+/*
+ * called by:
+ *   - lib/sbitmap.c|108| <<sbitmap_resize>> sbitmap_deferred_clear(sb, i);
+ *   - lib/sbitmap.c|163| <<sbitmap_find_bit_in_index>> if (!sbitmap_deferred_clear(sb, index))
+ *   - lib/sbitmap.c|223| <<sbitmap_get_shallow>> if (sbitmap_deferred_clear(sb, index))
+ */
 static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 {
 	unsigned long mask, val;
@@ -54,6 +60,12 @@ static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2977| <<blk_mq_init_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+ *   - block/kyber-iosched.c|492| <<kyber_init_hctx>> if (sbitmap_init_node(&khd->kcq_map[i], hctx->nr_ctx,
+ *   - lib/sbitmap.c|404| <<sbitmap_queue_init_node>> ret = sbitmap_init_node(&sbq->sb, depth, shift, flags, node);
+ */
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 		      gfp_t flags, int node)
 {
@@ -167,6 +179,10 @@ static int sbitmap_find_bit_in_index(struct sbitmap *sb, int index,
 	return nr;
 }
 
+/*
+ * called by only:
+ *   - lib/sbitmap.c|475| <<__sbitmap_queue_get>> nr = sbitmap_get(&sbq->sb, hint, sbq->round_robin);
+ */
 int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 {
 	unsigned int i, index;
@@ -379,6 +395,12 @@ static unsigned int sbq_calc_wake_batch(struct sbitmap_queue *sbq,
 	return wake_batch;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|705| <<bt_alloc>> return sbitmap_queue_init_node(bt, depth, -1, round_robin, GFP_KERNEL,
+ *   - block/kyber-iosched.c|399| <<kyber_queue_data_alloc>> ret = sbitmap_queue_init_node(&kqd->domain_tokens[i],
+ *   - drivers/target/target_core_transport.c|297| <<transport_alloc_session_tags>> rc = sbitmap_queue_init_node(&se_sess->sess_tag_pool, tag_num, -1,
+ */
 int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 			    int shift, bool round_robin, gfp_t flags, int node)
 {
@@ -422,6 +444,11 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
 
+/*
+ * called by:
+ *   - lib/sbitmap.c|452| <<sbitmap_queue_resize>> sbitmap_queue_update_wake_batch(sbq, depth);
+ *   - lib/sbitmap.c|527| <<sbitmap_queue_min_shallow_depth>> sbitmap_queue_update_wake_batch(sbq, sbq->sb.depth);
+ */
 static void sbitmap_queue_update_wake_batch(struct sbitmap_queue *sbq,
 					    unsigned int depth)
 {
@@ -448,6 +475,13 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|184| <<__blk_mq_get_tag>> return __sbitmap_queue_get(bt);
+ *   - block/kyber-iosched.c|723| <<kyber_get_domain_token>> nr = __sbitmap_queue_get(domain_tokens);
+ *   - block/kyber-iosched.c|740| <<kyber_get_domain_token>> nr = __sbitmap_queue_get(domain_tokens);
+ *   - include/linux/sbitmap.h|477| <<sbitmap_queue_get>> nr = __sbitmap_queue_get(sbq);
+ */
 int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 {
 	unsigned int hint, depth;
@@ -515,10 +549,19 @@ void sbitmap_queue_min_shallow_depth(struct sbitmap_queue *sbq,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_min_shallow_depth);
 
+/*
+ * called only by:
+ *   - lib/sbitmap.c|583| <<__sbq_wake_up>> ws = sbq_wake_ptr(sbq);
+ */
 static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
 
+	/*
+	 * 如果ws_active是0就不处理了, 算是一种优化吧
+	 * 所以一定要保证挂上wait的就要增加
+	 * 否则要等好长时间才会处理
+	 */
 	if (!atomic_read(&sbq->ws_active))
 		return NULL;
 
@@ -540,6 +583,10 @@ static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - lib/sbitmap.c|620| <<sbitmap_queue_wake_up>> while (__sbq_wake_up(sbq))
+ */
 static bool __sbq_wake_up(struct sbitmap_queue *sbq)
 {
 	struct sbq_wait_state *ws;
@@ -581,6 +628,11 @@ static bool __sbq_wake_up(struct sbitmap_queue *sbq)
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|321| <<blk_mq_get_tag>> sbitmap_queue_wake_up(bt_prev);
+ *   - lib/sbitmap.c|637| <<sbitmap_queue_clear>> sbitmap_queue_wake_up(sbq);
+ */
 void sbitmap_queue_wake_up(struct sbitmap_queue *sbq)
 {
 	while (__sbq_wake_up(sbq))
@@ -588,9 +640,29 @@ void sbitmap_queue_wake_up(struct sbitmap_queue *sbq)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|348| <<blk_mq_put_tag>> sbitmap_queue_clear(&tags->bitmap_tags, real_tag, ctx->cpu);
+ *   - block/blk-mq-tag.c|351| <<blk_mq_put_tag>> sbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);
+ *   - block/kyber-iosched.c|558| <<rq_clear_domain_token>> sbitmap_queue_clear(&kqd->domain_tokens[sched_domain], nr,
+ *   - include/target/target_core_base.h|946| <<target_free_tag>> sbitmap_queue_clear(&sess->sess_tag_pool, cmd->map_tag, cmd->map_cpu);
+ */
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
+	/*
+	 * 这里缺少一个barrier:
+	 * smp_mb__before_atomic();
+	 *
+	 * Once the clear bit is set, the bit may be allocated out.
+	 *
+	 * Orders READ/WRITE on the asssociated instance(such as request
+	 * of blk_mq) by this bit for avoiding race with re-allocation,
+	 * and its pair is the memory barrier implied in __sbitmap_get_word.
+	 *
+	 * One invariant is that the clear bit has to be zero when the bit
+	 * is in use.
+	 */
 	sbitmap_deferred_clear_bit(&sbq->sb, nr);
 
 	/*
@@ -607,6 +679,13 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|78| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->bitmap_tags);
+ *   - block/blk-mq-tag.c|80| <<blk_mq_tag_wakeup_all>> sbitmap_queue_wake_all(&tags->breserved_tags);
+ *
+ * 临时记住一点, 会wake_up()!
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -664,6 +743,10 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
 
+/*
+ * called by only:
+ *   - block/kyber-iosched.c|734| <<kyber_get_domain_token>> sbitmap_add_wait_queue(domain_tokens, ws, wait);
+ */
 void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
 			    struct sbq_wait_state *ws,
 			    struct sbq_wait *sbq_wait)
@@ -698,6 +781,12 @@ void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
 }
 EXPORT_SYMBOL_GPL(sbitmap_prepare_to_wait);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|293| <<blk_mq_get_tag>> sbitmap_finish_wait(bt, ws, &wait);
+ *   - block/blk-mq-tag.c|329| <<blk_mq_get_tag>> sbitmap_finish_wait(bt, ws, &wait);
+ *   - drivers/target/iscsi/iscsi_target_util.c|172| <<iscsit_wait_for_tag>> sbitmap_finish_wait(sbq, ws, &wait);
+ */
 void sbitmap_finish_wait(struct sbitmap_queue *sbq, struct sbq_wait_state *ws,
 			 struct sbq_wait *sbq_wait)
 {
-- 
2.7.4

