From aa956852acc1efa4bb26ffeecdfe4ae2f0b58466 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 26 Mar 2019 05:47:09 +0800
Subject: [PATCH 1/1] block comment for block and drivers for
 linux-block:for-5.1/block-20190302

This is for linux-block: for-5.1/block-20190302

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/blk-core.c                |   2 +
 block/blk-merge.c               |   4 +
 block/blk-mq-cpumap.c           |  15 +
 block/blk-mq-debugfs.c          |  12 +
 block/blk-mq-pci.c              |   7 +
 block/blk-mq-sched.c            | 228 +++++++++++
 block/blk-mq-sched.h            |  22 ++
 block/blk-mq-sysfs.c            |  84 ++++
 block/blk-mq-tag.c              | 332 ++++++++++++++++
 block/blk-mq-tag.h              |  92 +++++
 block/blk-mq-virtio.c           |  11 +
 block/blk-mq.c                  | 840 ++++++++++++++++++++++++++++++++++++++++
 block/blk-mq.h                  | 104 ++++-
 block/blk-settings.c            |  39 ++
 block/blk-softirq.c             |  10 +
 block/blk-stat.c                |  49 +++
 block/blk-sysfs.c               |  32 ++
 block/blk-timeout.c             |  14 +
 block/blk.h                     |   3 +
 block/genhd.c                   |   5 +
 drivers/nvme/host/core.c        |  69 ++++
 drivers/nvme/host/nvme.h        |   5 +
 drivers/nvme/host/pci.c         | 313 +++++++++++++++
 include/linux/blk-mq.h          | 322 +++++++++++++++
 include/linux/blk_types.h       |   7 +
 include/linux/blkdev.h          | 199 ++++++++++
 include/linux/percpu-refcount.h |  38 ++
 include/linux/sbitmap.h         |  34 ++
 lib/percpu-refcount.c           |  13 +
 lib/sbitmap.c                   |  16 +
 30 files changed, 2920 insertions(+), 1 deletion(-)

diff --git a/block/blk-core.c b/block/blk-core.c
index 6b78ec5..b67a9d5 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -442,6 +442,7 @@ int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
 	}
 }
 
+/* 就是调用percpu_ref_put(&q->q_usage_counter); */
 void blk_queue_exit(struct request_queue *q)
 {
 	percpu_ref_put(&q->q_usage_counter);
@@ -496,6 +497,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	if (!q->backing_dev_info)
 		goto fail_split;
 
+	/* struct blk_queue_stats */
 	q->stats = blk_alloc_queue_stats();
 	if (!q->stats)
 		goto fail_stats;
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 22467f4..9fcb5d5 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -318,6 +318,10 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	return do_split ? new : NULL;
 }
 
+/*
+ * 根据块设备请求队列的limits.max_sectors和limits.max_segmetns
+ * 来拆分bio,适应设备缓存.会在函数blk_set_default_limits中设置
+ */
 void blk_queue_split(struct request_queue *q, struct bio **bio)
 {
 	struct bio *split, *res;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 03a5348..4b4c4da 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -31,6 +31,21 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-rdma.c|50| <<blk_mq_rdma_map_queues>> return blk_mq_map_queues(map);
+ *   - block/blk-mq-virtio.c|52| <<blk_mq_virtio_map_queues>> return blk_mq_map_queues(qmap);
+ *   - block/blk-mq.c|3486| <<blk_mq_update_queue_map>> return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - block/blk-mq.c|3782| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/pci.c|506| <<nvme_pci_map_queues>> blk_mq_map_queues(map);
+ *   - drivers/nvme/host/rdma.c|1814| <<nvme_rdma_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+ *   - drivers/nvme/host/tcp.c|2080| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+ *   - drivers/nvme/host/tcp.c|2081| <<nvme_tcp_map_queues>> blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6940| <<qla2xxx_map_queues>> rc = blk_mq_map_queues(qmap);
+ *   - drivers/scsi/scsi_lib.c|1818| <<scsi_map_queues>> return blk_mq_map_queues(&set->map[0]);
+ *
+ * 初始化blk_mq_queue_map->map
+ */
 int blk_mq_map_queues(struct blk_mq_queue_map *qmap)
 {
 	unsigned int *map = qmap->mq_map;
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index bac34b7..d4e1bfe 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -1058,6 +1058,14 @@ void blk_mq_debugfs_unregister_queue_rqos(struct request_queue *q)
 	q->rqos_debugfs_dir = NULL;
 }
 
+/*
+ * echo kyber > /sys/block/nvme0n1/queue/scheduler后获得了
+ *
+ * !/sys/kernel/debug/block/nvme0n1/sched
+ *
+ * # ls /sys/kernel/debug/block/nvme0n1/sched --> kyber的例子
+ * async_depth  discard_tokens  other_tokens  read_tokens  write_tokens
+ */
 int blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx)
 {
@@ -1081,6 +1089,10 @@ int blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|760| <<blk_mq_exit_sched>> blk_mq_debugfs_unregister_sched_hctx(hctx);
+ */
 void blk_mq_debugfs_unregister_sched_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	debugfs_remove_recursive(hctx->sched_debugfs_dir);
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index 1dce185..de49b40 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -31,6 +31,12 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|504| <<nvme_pci_map_queues>> blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+ *   - drivers/scsi/qla2xxx/qla_os.c|6942| <<qla2xxx_map_queues>> rc = blk_mq_pci_map_queues(qmap, vha->hw->pdev, vha->irq_offset);
+ *   - drivers/scsi/smartpqi/smartpqi_init.c|5792| <<pqi_map_queues>> return blk_mq_pci_map_queues(&shost->tag_set.map[0],
+ */
 int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 			    int offset)
 {
@@ -42,6 +48,7 @@ int blk_mq_pci_map_queues(struct blk_mq_queue_map *qmap, struct pci_dev *pdev,
 		if (!mask)
 			goto fallback;
 
+		/* 猜测是每一个sw queue对应的hw queue??? */
 		for_each_cpu(cpu, mask)
 			qmap->mq_map[cpu] = qmap->queue_offset + queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 4090553..e91b09d 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -16,6 +16,28 @@
 #include "blk-mq-tag.h"
 #include "blk-wbt.h"
 
+/*
+ * sched中下发的主要函数:
+ *
+ * - blk_mq_do_dispatch_sched()
+ *   对于参数的hctx, 如果没有调度器就返回
+ *   否则用调度器的dispatch_request()取出下一个request
+ *   用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ *
+ * - blk_mq_do_dispatch_ctx()
+ *   dequeue request one by one from sw queue if queue is busy
+ *   不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ *   然后为这个request调用queue_rq()
+ *   最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ *
+ *
+ * 插入的主要函数是blk_mq_sched_insert_request()
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
+
+/*
+ * 没有任何调用!!!!!!!!!!
+ */
 void blk_mq_sched_free_hctx_data(struct request_queue *q,
 				 void (*exit)(struct blk_mq_hw_ctx *))
 {
@@ -31,6 +53,10 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_free_hctx_data);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|633| <<blk_mq_get_request>> blk_mq_sched_assign_ioc(rq);
+ */
 void blk_mq_sched_assign_ioc(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -45,14 +71,17 @@ void blk_mq_sched_assign_ioc(struct request *rq)
 		return;
 
 	spin_lock_irq(&q->queue_lock);
+	/* lookup io_cq from ioc */
 	icq = ioc_lookup_icq(ioc, q);
 	spin_unlock_irq(&q->queue_lock);
 
 	if (!icq) {
+		/* create and link io_cq */
 		icq = ioc_create_icq(ioc, q, GFP_ATOMIC);
 		if (!icq)
 			return;
 	}
+	/* increment reference count to io_context */
 	get_io_context(icq->ioc);
 	rq->elv.icq = icq;
 }
@@ -61,8 +90,25 @@ void blk_mq_sched_assign_ioc(struct request *rq)
  * Mark a hardware queue as needing a restart. For shared queues, maintain
  * a count of how many hardware queues are marked for restart.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|237| <<blk_mq_sched_dispatch_requests>> blk_mq_sched_mark_restart_hctx(hctx);
+ *   - block/mq-deadline.c|396| <<dd_dispatch_request>> blk_mq_sched_mark_restart_hctx(hctx);
+ */
 void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 
@@ -70,8 +116,28 @@ void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);
 
+/*
+ * called by;
+ *   - block/blk-flush.c|338| <<mq_flush_data_end_io>> blk_mq_sched_restart(hctx);
+ *   - block/blk-mq.c|733| <<__blk_mq_free_request>> blk_mq_sched_restart(hctx);
+ *
+ * 如果之前设置了BLK_MQ_S_SCHED_RESTART到hctx->state
+ * 取消这个bit, 触发blk_mq_run_hw_queue(hctx, true);
+ */
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 	clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
@@ -84,6 +150,15 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|211| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *   - block/blk-mq-sched.c|216| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_sched(hctx);
+ *
+ * 对于参数的hctx, 如果没有调度器就返回
+ * 否则用调度器的dispatch_request()取出下一个request
+ * 用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -111,9 +186,14 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 		 * in blk_mq_dispatch_rq_list().
 		 */
 		list_add(&rq->queuelist, &rq_list);
+	/* 为参数list中的每一个request调用queue_rq() */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|231| <<blk_mq_do_dispatch_ctx>> ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
+ */
 static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
 					  struct blk_mq_ctx *ctx)
 {
@@ -130,6 +210,16 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|213| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> blk_mq_do_dispatch_ctx(hctx);
+ *
+ * dequeue request one by one from sw queue if queue is busy
+ * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ * 然后为这个request调用queue_rq()
+ * 最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -139,12 +229,26 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 	do {
 		struct request *rq;
 
+		/*
+		 * hctx->ctx_map在以下被设置:
+		 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+		 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+		 *
+		 * 这个函数的调用路径是blk_mq_do_dispatch_ctx()-->blk_mq_dequeue_from_ctx()-->对于每一个ctx调用dispatch_rq_from_ctx()
+		 * dispatch_rq_from_ctx()会把ctx_map对应的bit给clear掉
+		 */
 		if (!sbitmap_any_bit_set(&hctx->ctx_map))
 			break;
 
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * blk_mq_dequeue_from_ctx()只在这里被调用
+		 * blk_mq_do_dispatch_ctx()-->blk_mq_dequeue_from_ctx()-->对于每一个ctx调用dispatch_rq_from_ctx()
+		 *
+		 * 从start代表的第一个hctx->ctx_map开始, 取出一个request 就取出一个啊!!!
+		 */
 		rq = blk_mq_dequeue_from_ctx(hctx, ctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -161,11 +265,16 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		/* round robin for fair dispatch */
 		ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
 
+	/* 为参数list中的每一个request调用queue_rq() */
 	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
 
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1495| <<__blk_mq_run_hw_queue>> blk_mq_sched_dispatch_requests(hctx);
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -205,23 +314,55 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	 */
 	if (!list_empty(&rq_list)) {
 		blk_mq_sched_mark_restart_hctx(hctx);
+		/* 为参数list中的每一个request调用queue_rq() */
 		if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+			/*
+			 * blk_mq_do_dispatch_sched():
+			 * 对于参数的hctx, 如果没有调度器就返回
+			 * 否则用调度器的dispatch_request()取出下一个request
+			 * 用blk_mq_dispatch_rq_list()下发
+			 *
+			 * blk_mq_do_dispatch_ctx()
+			 * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+			 * 然后为这个request调用queue_rq()
+			 */
 			if (has_sched_dispatch)
 				blk_mq_do_dispatch_sched(hctx);
 			else
 				blk_mq_do_dispatch_ctx(hctx);
 		}
 	} else if (has_sched_dispatch) {
+		/*
+		 * 对于参数的hctx, 如果没有调度器就返回
+		 * 否则用调度器的dispatch_request()取出下一个request
+		 * 用blk_mq_dispatch_rq_list()下发
+		 */
 		blk_mq_do_dispatch_sched(hctx);
 	} else if (hctx->dispatch_busy) {
 		/* dequeue request one by one from sw queue if queue is busy */
+		/*
+		 * 不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+		 * 然后为这个request调用queue_rq()
+		 */
 		blk_mq_do_dispatch_ctx(hctx);
 	} else {
+		/*
+		 * 对于hctx->ctx_map中每个设置的bit对应的ctx
+		 * 把ctx->rq_list[type]的request们拼接到参数的list
+		 * 把ctx在hctx->ctx_map清空
+		 * 最后参数list中的就是这个hctx->ctx_map中每一个ctx的rq_list的总和拼
+		 */
 		blk_mq_flush_busy_ctxs(hctx, &rq_list);
+		/* 为参数list中的每一个request调用queue_rq() */
 		blk_mq_dispatch_rq_list(q, &rq_list, false);
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|1890| <<bfq_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ *   - block/mq-deadline.c|479| <<dd_bio_merge>> ret = blk_mq_sched_try_merge(q, bio, &free);
+ */
 bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 			    struct request **merged_request)
 {
@@ -258,6 +399,11 @@ EXPORT_SYMBOL_GPL(blk_mq_sched_try_merge);
  * Iterate list of requests and see if we can merge this bio with any
  * of them.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|438| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/kyber-iosched.c|586| <<kyber_bio_merge>> merged = blk_mq_bio_list_merge(hctx->queue, rq_list, bio);
+ */
 bool blk_mq_bio_list_merge(struct request_queue *q, struct list_head *list,
 			   struct bio *bio)
 {
@@ -301,6 +447,10 @@ EXPORT_SYMBOL_GPL(blk_mq_bio_list_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-sched.c|395| <<__blk_mq_sched_bio_merge>> ret = blk_mq_attempt_merge(q, hctx, ctx, bio);
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_hw_ctx *hctx,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
@@ -309,6 +459,10 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/*
+	 * Iterate list of requests and see if we can merge this bio with any
+	 * of them.
+	 */
 	if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
 		ctx->rq_merged++;
 		return true;
@@ -317,6 +471,10 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.h|38| <<blk_mq_sched_bio_merge>> return __blk_mq_sched_bio_merge(q, bio);
+ */
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
@@ -343,6 +501,11 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|4734| <<bfq_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq)) {
+ *   - block/mq-deadline.c|504| <<dd_insert_request>> if (blk_mq_sched_try_insert_merge(q, rq))
+ */
 bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
 	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
@@ -355,6 +518,10 @@ void blk_mq_sched_request_inserted(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|533| <<blk_mq_sched_insert_request>> if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -373,6 +540,23 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 	return false;
 }
 
+/*
+ * called by:
+ *   - block/blk-exec.c|61| <<blk_execute_rq_nowait>> blk_mq_sched_insert_request(rq, at_head, true, false);
+ *   - block/blk-mq.c|1049| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, true, false, false);
+ *   - block/blk-mq.c|1055| <<blk_mq_requeue_work>> blk_mq_sched_insert_request(rq, false, false, false);
+ *   - block/blk-mq.c|2328| <<blk_mq_try_issue_directly>> blk_mq_sched_insert_request(rq, false,
+ *   - block/blk-mq.c|2361| <<blk_mq_try_issue_list_directly>> blk_mq_sched_insert_request(rq, false, true, false);
+ *   - block/blk-mq.c|2505| <<blk_mq_make_request>> blk_mq_sched_insert_request(rq, false, true, true);
+ *
+ * 1. 如果有RQF_FLUSH_SEQ则blk_insert_flush()
+ * 2. 试试能否blk_mq_sched_bypass_insert(): 比如是否有RQF_FLUSH_SEQ
+ * 3. 如果支持IO调度则用scheduler的.insert_requests()
+ * 4. 否则把request放入request->mq_ctx的rq_lists, 然后把ctx在hctx->ctx_map对应的bit设置
+ * 根据参数是否blk_mq_run_hw_queue()!
+ *
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -383,10 +567,16 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 
 	/* flush rq in flush machinery need to be dispatched directly */
 	if (!(rq->rq_flags & RQF_FLUSH_SEQ) && op_is_flush(rq->cmd_flags)) {
+		/*
+		 * insert a new PREFLUSH/FUA request
+		 */
 		blk_insert_flush(rq);
 		goto run;
 	}
 
+	/*
+	 * 如果又存在IO调度器, 又有driver tag是不正常的
+	 */
 	WARN_ON(e && (rq->tag != -1));
 
 	if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
@@ -399,6 +589,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		e->type->ops.insert_requests(hctx, &list, at_head);
 	} else {
 		spin_lock(&ctx->lock);
+		/*
+		 * 把request放入request->mq_ctx的rq_lists
+		 * 然后把ctx在hctx->ctx_map对应的bit设置
+		 */
 		__blk_mq_insert_request(hctx, rq, at_head);
 		spin_unlock(&ctx->lock);
 	}
@@ -408,6 +602,11 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2179| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx,
+ *   - block/blk-mq.c|2200| <<blk_mq_flush_plug_list>> blk_mq_sched_insert_requests(this_hctx, this_ctx, &rq_list,
+ */
 void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -423,6 +622,11 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 		 * busy in case of 'none' scheduler, and this way may save
 		 * us one extra enqueue & dequeue to sw queue.
 		 */
+		/*
+		 * blk_mq_insert_requests():
+		 * 把list中的request们放入ctx->rq_lists[type]
+		 * 把ctx在hctx->ctx_map对应的bit设置
+		 */
 		if (!hctx->dispatch_busy && !e && !run_queue_async)
 			blk_mq_try_issue_list_directly(hctx, list);
 		else
@@ -432,6 +636,11 @@ void blk_mq_sched_insert_requests(struct blk_mq_hw_ctx *hctx,
 	blk_mq_run_hw_queue(hctx, run_queue_async);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|615| <<blk_mq_sched_alloc_tags>> blk_mq_sched_free_tags(set, hctx, hctx_idx);
+ *   - block/blk-mq-sched.c|627| <<blk_mq_sched_tags_teardown>> blk_mq_sched_free_tags(set, hctx, i);
+ */
 static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -443,6 +652,10 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|497| <<blk_mq_init_sched>> ret = blk_mq_sched_alloc_tags(q, hctx, i);
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -462,6 +675,11 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|684| <<blk_mq_init_sched>> blk_mq_sched_tags_teardown(q);
+ *   - block/blk-mq-sched.c|709| <<blk_mq_exit_sched>> blk_mq_sched_tags_teardown(q);
+ */
 static void blk_mq_sched_tags_teardown(struct request_queue *q)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -472,6 +690,11 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 		blk_mq_sched_free_tags(set, hctx, i);
 }
 
+/*
+ * called by:
+ *   - block/elevator.c|577| <<elevator_switch_mq>> ret = blk_mq_init_sched(q, new_e);
+ *   - block/elevator.c|623| <<elevator_init_mq>> err = blk_mq_init_sched(q, e);
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -526,6 +749,11 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|592| <<blk_mq_init_sched>> blk_mq_exit_sched(q, eq);
+ *   - block/elevator.c|184| <<elevator_exit>> blk_mq_exit_sched(q, e);
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index c7bdb52..e66aae3 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -76,6 +76,12 @@ static inline void blk_mq_sched_requeue_request(struct request *rq)
 		e->type->ops.requeue_request(rq);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|192| <<blk_mq_hctx_has_pending>> blk_mq_sched_has_work(hctx);
+ *
+ * 如果支持elevator, 用elevator的has_work查看是否有work
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
@@ -86,8 +92,24 @@ static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 	return false;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|1724| <<blk_mq_dispatch_rq_list>> needs_restart = blk_mq_sched_needs_restart(hctx);
+ */
 static inline bool blk_mq_sched_needs_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 }
 
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 3f9c3f4..62463c9 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -143,23 +143,38 @@ static ssize_t blk_mq_hw_sysfs_store(struct kobject *kobj,
 	return res;
 }
 
+/*
+ * 打印/sys/block/nvme0n1/mq/0/nr_tags
+ */
 static ssize_t blk_mq_hw_sysfs_nr_tags_show(struct blk_mq_hw_ctx *hctx,
 					    char *page)
 {
 	return sprintf(page, "%u\n", hctx->tags->nr_tags);
 }
 
+/*
+ * 打印/sys/block/nvme0n1/mq/0/nr_reserved_tags
+ */
 static ssize_t blk_mq_hw_sysfs_nr_reserved_tags_show(struct blk_mq_hw_ctx *hctx,
 						     char *page)
 {
 	return sprintf(page, "%u\n", hctx->tags->nr_reserved_tags);
 }
 
+/*
+ * 打印/sys/block/nvme0n1/mq/1/cpu_list
+ * 也就是当前hctx(hw queue) map了哪些cpu
+ */
 static ssize_t blk_mq_hw_sysfs_cpus_show(struct blk_mq_hw_ctx *hctx, char *page)
 {
 	unsigned int i, first = 1;
 	ssize_t ret = 0;
 
+	/*
+	 * 只在以下修改数据cpumask:
+	 *   - block/blk-mq.c|2990| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *   - block/blk-mq.c|3040| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 */
 	for_each_cpu(i, hctx->cpumask) {
 		if (first)
 			ret += sprintf(ret + page, "%u", i);
@@ -177,6 +192,13 @@ static struct attribute *default_ctx_attrs[] = {
 	NULL,
 };
 
+/*
+ * ls /sys/block/nvme0n1/mq/0/
+ * cpu0  cpu1  cpu_list  nr_reserved_tags  nr_tags
+ *
+ * 下面添加的nr_tags, nr_reserved_tags和cpu_list
+ */
+
 static struct blk_mq_hw_ctx_sysfs_entry blk_mq_hw_sysfs_nr_tags = {
 	.attr = {.name = "nr_tags", .mode = 0444 },
 	.show = blk_mq_hw_sysfs_nr_tags_show,
@@ -190,6 +212,9 @@ static struct blk_mq_hw_ctx_sysfs_entry blk_mq_hw_sysfs_cpus = {
 	.show = blk_mq_hw_sysfs_cpus_show,
 };
 
+/*
+ * 用做struct kobj_type blk_mq_hw_ktype.default_attrs
+ */
 static struct attribute *default_hw_ctx_attrs[] = {
 	&blk_mq_hw_sysfs_nr_tags.attr,
 	&blk_mq_hw_sysfs_nr_reserved_tags.attr,
@@ -224,6 +249,12 @@ static struct kobj_type blk_mq_hw_ktype = {
 	.release	= blk_mq_hw_sysfs_release,
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|276| <<blk_mq_unregister_dev>> blk_mq_unregister_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|344| <<__blk_mq_register_dev>> blk_mq_unregister_hctx(q->queue_hw_ctx[i]);
+ *   - block/blk-mq-sysfs.c|373| <<blk_mq_sysfs_unregister>> blk_mq_unregister_hctx(hctx);
+ */
 static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_ctx *ctx;
@@ -238,6 +269,14 @@ static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 	kobject_del(&hctx->kobj);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|332| <<__blk_mq_register_dev>> ret = blk_mq_register_hctx(hctx);
+ *   - block/blk-mq-sysfs.c|393| <<blk_mq_sysfs_register>> ret = blk_mq_register_hctx(hctx);
+ *
+ * 比如/sys/block/nvme0n1/mq/0
+ * 比如/sys/block/nvme0n1/mq/1
+ */
 static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -247,10 +286,19 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	if (!hctx->nr_ctx)
 		return 0;
 
+	/*
+	 * 把0或者1添加到/sys/block/nvme0n1/mq
+	 */
 	ret = kobject_add(&hctx->kobj, q->mq_kobj, "%u", hctx->queue_num);
 	if (ret)
 		return ret;
 
+	/*
+	 * # ls /sys/block/nvme0n1/mq/0/
+	 * cpu0  cpu1  cpu_list  nr_reserved_tags  nr_tags
+	 *
+	 * 添加的cpu0和cpu1
+	 */
 	hctx_for_each_ctx(hctx, ctx, i) {
 		ret = kobject_add(&ctx->kobj, &hctx->kobj, "cpu%u", ctx->cpu);
 		if (ret)
@@ -277,6 +325,10 @@ void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 	q->mq_sysfs_init_done = false;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3339| <<blk_mq_alloc_and_init_hctx>> blk_mq_hctx_kobj_init(hctx);
+ */
 void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx)
 {
 	kobject_init(&hctx->kobj, &blk_mq_hw_ktype);
@@ -309,6 +361,15 @@ void blk_mq_sysfs_init(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sysfs.c|372| <<blk_mq_register_dev>> ret = __blk_mq_register_dev(dev, q);
+ *   - block/blk-sysfs.c|968| <<blk_register_queue>> __blk_mq_register_dev(dev, q);
+ *
+ * 因为blk_mq_register_dev()没人调用
+ *
+ * 所以这个函数相当于只被blk_register_queue()调用!!!!!!!!!!!!!!!!
+ */
 int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -317,6 +378,12 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	WARN_ON_ONCE(!q->kobj.parent);
 	lockdep_assert_held(&q->sysfs_lock);
 
+	/*
+	 * 比如: /sys/block/nvme0n1/mq
+	 *
+	 * # ls /sys/block/nvme0n1/mq/
+	 * 0  1
+	 */
 	ret = kobject_add(q->mq_kobj, kobject_get(&dev->kobj), "%s", "mq");
 	if (ret < 0)
 		goto out;
@@ -324,6 +391,12 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	kobject_uevent(q->mq_kobj, KOBJ_ADD);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 为了/sys/block/nvme0n1/mq/ 下面添加0和1
+		 *
+		 * # ls /sys/block/nvme0n1/mq/
+		 * 0  1
+		 */
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
 			goto unreg;
@@ -344,6 +417,9 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/*
+ * 没人调用!!!
+ */
 int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 {
 	int ret;
@@ -355,6 +431,10 @@ int blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3968| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_unregister(q);
+ */
 void blk_mq_sysfs_unregister(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -371,6 +451,10 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|3988| <<__blk_mq_update_nr_hw_queues>> blk_mq_sysfs_register(q);
+ */
 int blk_mq_sysfs_register(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index a4931fc..d0460b8 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -13,6 +13,15 @@
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
 
+/*
+ * PATH-A入口: blk_mq_queue_tag_busy_iter()
+ *     针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+ *     数据结构是bt_iter_data
+ * PATH-B入口: blk_mq_tagset_busy_iter()
+ *     针对每一个tagset->tags[tagset->nr_hw_queues].sb
+ *     数据结构是bt_tags_iter_data
+ */
+
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
 	if (!tags)
@@ -27,8 +36,26 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  * to get tag when first time, the other shared-tag users could reserve
  * budget for it.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.h|72| <<blk_mq_tag_busy>> return __blk_mq_tag_busy(hctx);
+ *
+ * 为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+ * 如果之前没设置, 增加hctx->tags->active_queues
+ * BLK_MQ_S_TAG_ACTIVE的设置取消和tags->active_queues的增加减少是一致的
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+	/* test_and_set_bit(): Set a bit and return its old value */
+	/*
+	 * BLK_MQ_S_TAG_ACTIVE在以下被使用:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|33| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|57| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|76| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *
+	 * hctx->tags是struct blk_mq_tags *
+	 */
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -39,6 +66,13 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|79| <<__blk_mq_tag_idle>> blk_mq_tag_wakeup_all(tags, false);
+ *   - block/blk-mq.c|471| <<blk_mq_wake_waiters>> blk_mq_tag_wakeup_all(hctx->tags, true);
+ *
+ * 临时记住一点, 会wake_up()!
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
@@ -50,6 +84,12 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-tag.h|136| <<blk_mq_tag_idle>> __blk_mq_tag_idle(hctx);
+ *
+ * BLK_MQ_S_TAG_ACTIVE的设置取消和tags->active_queues的增加减少是一致的
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
@@ -59,6 +99,9 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 
 	atomic_dec(&tags->active_queues);
 
+	/*
+	 * 临时记住一点, 会wake_up()!
+	 */
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
@@ -66,11 +109,23 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called only by:
+ *   - block/blk-mq-tag.c|131| <<__blk_mq_get_tag>> !hctx_may_queue(data->hctx, bt))
+ *
+ * For shared tag users, we track the number of currently active users
+ * and attempt to provide a fair share of the tag depth for each of them.
+ * hctx_may_queue()估计是为了在不是data->hctx->sched_tags的情况下防止共享set
+ * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
 	unsigned int depth, users;
 
+	/*
+	 * 如果BLK_MQ_F_TAG_SHARED没设置就返回true
+	 */
 	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return true;
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
@@ -93,9 +148,33 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|181| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|204| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *   - block/blk-mq-tag.c|210| <<blk_mq_get_tag>> tag = __blk_mq_get_tag(data, bt);
+ *
+ * 如果不是BLK_MQ_REQ_INTERNAL(data->hctx->sched_tags)并且tagset可能被共享
+ * 则用hctx_may_queue()为了在不是data->hctx->sched_tags的情况下防止共享set
+ * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+ *
+ * 否则用sbitmap分配bit!
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 *
+	 * hctx_may_queue():
+	 * For shared tag users, we track the number of currently active users
+	 * and attempt to provide a fair share of the tag depth for each of them.
+	 *
+	 * hctx_may_queue()估计是为了在不是data->hctx->sched_tags的情况下防止共享set
+	 * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+	 */
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -105,16 +184,36 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|601| <<blk_mq_get_request>> tag = blk_mq_get_tag(data);
+ *   - block/blk-mq.c|1425| <<blk_mq_get_driver_tag>> rq->tag = blk_mq_get_tag(&data);
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则返回使用data->hctx->sched_tags
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
+	/* 声明一个struct sbq_wait */
 	DEFINE_SBQ_WAIT(wait);
 	unsigned int tag_offset;
 	bool drop_ctx;
 	int tag;
 
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|1418| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|994| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|935| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 */
 	if (data->flags & BLK_MQ_REQ_RESERVED) {
 		if (unlikely(!tags->nr_reserved_tags)) {
 			WARN_ON_ONCE(1);
@@ -127,15 +226,33 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		tag_offset = tags->nr_reserved_tags;
 	}
 
+	/*
+	 * 如果不是BLK_MQ_REQ_INTERNAL(data->hctx->sched_tags)并且tagset可能被共享
+	 * 则用hctx_may_queue()为了在不是data->hctx->sched_tags的情况下防止共享set
+	 * 的某一个(nvme namespace或者scsi lun)使用过多的tag??
+	 *
+	 * 否则用sbitmap分配bit!
+	 */
 	tag = __blk_mq_get_tag(data, bt);
 	if (tag != -1)
 		goto found_tag;
 
+	/*
+	 * BLK_MQ_REQ_NOWAIT: return when out of requests
+	 */
 	if (data->flags & BLK_MQ_REQ_NOWAIT)
 		return BLK_MQ_TAG_FAIL;
 
+	/*
+	 * 并不会hang
+	 *
+	 * bt根据BLK_MQ_REQ_RESERVED要么是tags->breserved_tags要么是tags->bitmap_tags
+	 */
 	ws = bt_wait_ptr(bt, data->hctx);
 	drop_ctx = data->ctx == NULL;
+	/*
+	 * 如果分配不到tag, 就一直在循环跑下去
+	 */
 	do {
 		struct sbitmap_queue *bt_prev;
 
@@ -160,15 +277,33 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		if (tag != -1)
 			break;
 
+		/*
+		 * 会调用put_cpu()
+		 */
 		if (data->ctx)
 			blk_mq_put_ctx(data->ctx);
 
 		bt_prev = bt;
+		/*
+		 * 调用schedule()
+		 * TASK_UNINTERRUPTIBLE!!!
+		 */
 		io_schedule();
 
 		sbitmap_finish_wait(bt, ws, &wait);
 
+		/*
+		 * 会调用get_cpu()
+		 */
 		data->ctx = blk_mq_get_ctx(data->q);
+		/*
+		 * 如果是REQ_HIPRI, 返回HCTX_TYPE_POLL
+		 * 如果是REQ_OP_READ, 返回HCTX_TYPE_READ
+		 * 其他的, 返回默认的HCTX_TYPE_DEFAULT
+		 * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+		 * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+		 * 返回的仍然可以是HCTX_TYPE_DEFAULT
+		 */
 		data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
 						data->ctx);
 		tags = blk_mq_tags_from_data(data);
@@ -197,6 +332,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|713| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
+ *   - block/blk-mq.c|715| <<__blk_mq_free_request>> blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+ *   - block/blk-mq.h|285| <<__blk_mq_put_driver_tag>> blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
+ */
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
@@ -218,10 +359,26 @@ struct bt_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|302| <<bt_for_each>> sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
+ *
+ *
+ * PATH-A调用最早起源于四个函数:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *         上面四个函数调用====> blk_mq_queue_tag_busy_iter()-->bt_for_each()-->bt_iter()
+ */
 static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
+	/* 定义就在上面, 包括fn */
 	struct bt_iter_data *iter_data = data;
 	struct blk_mq_hw_ctx *hctx = iter_data->hctx;
+	/*
+	 * 不是sched_tags哦
+	 */
 	struct blk_mq_tags *tags = hctx->tags;
 	bool reserved = iter_data->reserved;
 	struct request *rq;
@@ -253,6 +410,18 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called only by:
+ *   - block/blk-mq-tag.c|575| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|576| <<blk_mq_queue_tag_busy_iter>> bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
+ *
+ * PATH-A调用最早起源于四个函数:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *         上面四个函数调用====> blk_mq_queue_tag_busy_iter()-->bt_for_each()-->bt_iter()
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -273,6 +442,16 @@ struct bt_tags_iter_data {
 	bool reserved;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|461| <<bt_tags_for_each>> sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -307,6 +486,17 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
  * @reserved:	Indicates whether @bt is the breserved_tags member or the
  *		bitmap_tags member of struct blk_mq_tags.
  */
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|478| <<blk_mq_all_tag_busy_iter>> bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, true);
+ *   - block/blk-mq-tag.c|479| <<blk_mq_all_tag_busy_iter>> bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, false);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, bool reserved)
 {
@@ -331,6 +521,16 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * called by only:
+ *   - block/blk-mq-tag.c|513| <<blk_mq_tagset_busy_iter>> blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ */
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
@@ -349,12 +549,34 @@ static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
  *		true to continue iterating tags, false to stop.
  * @priv:	Will be passed as second argument to @fn.
  */
+/*
+ * 被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *
+ * PATH-B被特别多的调用, 比如:
+ *   - block/blk-mq-debugfs.c|442| <<hctx_busy_show>> blk_mq_tagset_busy_iter(hctx->queue->tag_set, hctx_show_busy_rq, 
+ *   - drivers/nvme/host/pci.c|2487| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2488| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *           上面特别多的函数调用blk_mq_tagset_busy_iter()-->blk_mq_all_tag_busy_iter()-->bt_tags_for_each()-->bt_tags_iter()
+ *
+ * PATH-A入口: blk_mq_queue_tag_busy_iter()
+ *     针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+ *     数据结构是bt_iter_data
+ * PATH-B入口: blk_mq_tagset_busy_iter()
+ *     针对每一个tagset->tags[tagset->nr_hw_queues].sb
+ *     数据结构是bt_tags_iter_data
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
 	int i;
 
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
+		/*
+		 * iterate over all started requests in a tag map
+		 */
 		if (tagset->tags && tagset->tags[i])
 			blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
 	}
@@ -375,6 +597,31 @@ EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
  * called for all requests on all queues that share that tag set and not only
  * for requests associated with @q.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *
+ * PATH-A调用最早起源于四个函数:
+ *   - block/blk-mq.c|255| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ *   - block/blk-mq.c|282| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ *   - block/blk-mq.c|1121| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ *   - block/blk-mq.c|1249| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ *         上面四个函数调用====> blk_mq_queue_tag_busy_iter()-->bt_for_each()-->bt_iter()
+ *
+ * PATH-A入口: blk_mq_queue_tag_busy_iter()
+ *     针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+ * PATH-B入口: blk_mq_tagset_busy_iter()
+ *     针对每一个tagset->tags[tagset->nr_hw_queues].sb
+ *
+ * To iterating over requests triggers race conditions with request execution.
+ *
+ * That race isn't new.
+ * You should only iterate when your queues are quieced to ensure the
+ * request sent to a callback is stable.
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -388,10 +635,46 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 	 * synchronize_rcu() to ensure this function left the critical section
 	 * below.
 	 */
+	/*
+	 * 使用的例子: 
+	 *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|406| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-core.c|415| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|447| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|453| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|544| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-mq-tag.c|401| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|305| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *   - block/blk-mq.c|319| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|327| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-mq.c|366| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-mq.c|1245| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *   - block/blk-sysfs.c|924| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 *   - block/blk.h|66| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|617| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|627| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 */
 	if (!percpu_ref_tryget(&q->q_usage_counter))
 		return;
 
+	/*
+	 * 遍历每一个request_queue->queue_hw_ctx[i]
+	 *
+	 * q->queue_hw_ctx第一维在以下分配:
+	 *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+	 *
+	 * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+	 *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/*
+		 * 在以下设置了hctx->tags:
+		 *   - block/blk-mq.c|2302| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+		 *   - block/blk-mq.c|2504| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+		 */
 		struct blk_mq_tags *tags = hctx->tags;
 
 		/*
@@ -405,9 +688,17 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 			bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
 		bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
 	}
+	/* 就是调用percpu_ref_put(&q->q_usage_counter); */
 	blk_queue_exit(q);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|613| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->bitmap_tags, depth, round_robin, node))
+ *   - block/blk-mq-tag.c|615| <<blk_mq_init_bitmap_tags>> if (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, round_robin,
+ *
+ * blk_mq_alloc_rq_map()-->blk_mq_init_tags()-->blk_mq_init_bitmap_tags()-->bt_alloc()
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -415,6 +706,10 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq-tag.c|506| <<blk_mq_init_tags>> return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
@@ -435,6 +730,12 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2068| <<blk_mq_alloc_rq_map>> tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
+ *
+ * 分配一个blk_mq_tags,初始化其nr_tags, nr_reserved_tags和sbitmap
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -456,6 +757,12 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	return blk_mq_init_bitmap_tags(tags, node, alloc_policy);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2514| <<blk_mq_free_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2538| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ *   - block/blk-mq.c|2547| <<blk_mq_alloc_rq_map>> blk_mq_free_tags(tags);
+ */
 void blk_mq_free_tags(struct blk_mq_tags *tags)
 {
 	sbitmap_queue_free(&tags->bitmap_tags);
@@ -463,6 +770,11 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3683| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,
+ *   - block/blk-mq.c|3686| <<blk_mq_update_nr_requests>> ret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -476,6 +788,10 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 	 * If we are allowed to grow beyond the original size, allocate
 	 * a new set of tags before freeing the old one.
 	 */
+	/*
+	 * 如果新的depth比以前的大, 需要重新分配
+	 * 否则直接改变sbitmap大小就可以了
+	 */
 	if (tdepth > tags->nr_tags) {
 		struct blk_mq_tag_set *set = hctx->queue->tag_set;
 		struct blk_mq_tags *new;
@@ -491,6 +807,9 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 		if (tdepth > 16 * BLKDEV_MAX_RQ)
 			return -EINVAL;
 
+		/*
+		 * 会设置nr_tags为tdepth
+		 */
 		new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
 				tags->nr_reserved_tags);
 		if (!new)
@@ -509,6 +828,9 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 		 * Don't need (or can't) update reserved tags here, they
 		 * remain static and should never need resizing.
 		 */
+		/*
+		 * nr_tags不会变化
+		 */
 		sbitmap_queue_resize(&tags->bitmap_tags,
 				tdepth - tags->nr_reserved_tags);
 	}
@@ -528,6 +850,16 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 调用的几个例子:
+ *   - drivers/nvme/host/nvme.h|124| <<nvme_req_qid>> return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+ *   - drivers/scsi/scsi_debug.c|3786| <<get_queue>> u32 tag = blk_mq_unique_tag(cmnd->request);
+ *   - drivers/scsi/scsi_debug.c|5707| <<scsi_debug_queuecommand>> blk_mq_unique_tag(scp->request), b);
+ *   - drivers/scsi/virtio_scsi.c|501| <<virtscsi_pick_vq_mq>> u32 tag = blk_mq_unique_tag(sc->request);
+ *
+ * 给一个request rq
+ * 根据rq->mq_hctx->queue_num和rq->tag拼凑出一个所有队列全局的tag
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	return (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..4ab827b 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -8,15 +8,60 @@
  * Tag address space map.
  */
 struct blk_mq_tags {
+	/*
+	 * 只在一处设置:
+	 */
 	unsigned int nr_tags;
+	/*
+	 * 只在一处设置:
+	 *   - block/blk-mq-tag.c|454| <<blk_mq_init_tags>> tags->nr_reserved_tags = reserved_tags;
+	 */
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * active_queues使用的地方:
+	 *   - block/blk-mq-debugfs.c|477| <<blk_mq_debugfs_tags_show>> atomic_read(&tags->active_queues));
+	 *   - block/blk-mq-tag.c|34| <<__blk_mq_tag_busy>> atomic_inc(&hctx->tags->active_queues);
+	 *   - block/blk-mq-tag.c|60| <<__blk_mq_tag_idle>> atomic_dec(&tags->active_queues);
+	 *   - block/blk-mq-tag.c|85| <<hctx_may_queue>> users = atomic_read(&hctx->tags->active_queues);
+	 */
 	atomic_t active_queues;
 
 	struct sbitmap_queue bitmap_tags;
 	struct sbitmap_queue breserved_tags;
 
+	/*
+	 * 分配指针数组的地方:
+	 *   - block/blk-mq.c|2073| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *
+	 * 设置指针内容的地方:
+	 *   - block/blk-mq-tag.h|92| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|307| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1056| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *
+	 * 总体使用的地方:
+	 *   - block/blk-mq-tag.c|241| <<bt_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.c|300| <<bt_tags_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.h|130| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|515| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1091| <<blk_mq_tag_to_rq>> prefetch(tags->rqs[tag]);
+	 *   - block/blk-mq.c|1092| <<blk_mq_tag_to_rq>> return tags->rqs[tag];
+	 *   - block/blk-mq.c|1430| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|2481| <<blk_mq_free_rqs>> if (tags->rqs && set->ops->exit_request) {
+	 *   - block/blk-mq.c|2508| <<blk_mq_free_rq_map>> kfree(tags->rqs);
+	 *   - block/blk-mq.c|2509| <<blk_mq_free_rq_map>> tags->rqs = NULL;
+	 *   - block/blk-mq.c|2533| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2536| <<blk_mq_alloc_rq_map>> if (!tags->rqs) {
+	 *   - block/blk-mq.c|2545| <<blk_mq_alloc_rq_map>> kfree(tags->rqs);
+	 */
 	struct request **rqs;
+	/*
+	 * 分配指针数组的地方:
+	 *   - block/blk-mq.c|2081| <<blk_mq_alloc_rq_map>> tags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *
+	 * 设置指针内容的地方 (指向的都是下面page_list的内容):
+	 *   - block/blk-mq.c|2173| <<blk_mq_alloc_rqs>> tags->static_rqs[i] = rq;
+	 */
 	struct request **static_rqs;
 	struct list_head page_list;
 };
@@ -36,6 +81,14 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|147| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq-tag.c|198| <<blk_mq_get_tag>> ws = bt_wait_ptr(bt, data->hctx);
+ *   - block/blk-mq.c|1484| <<blk_mq_mark_tag_wait>> wq = &bt_wait_ptr(&hctx->tags->bitmap_tags, hctx)->wait;
+ *
+ * 并不会hang
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
@@ -53,14 +106,33 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|394| <<blk_mq_get_request>> blk_mq_tag_busy(data->hctx);
+ *   - block/blk-mq.c|1078| <<blk_mq_get_driver_tag>> shared = blk_mq_tag_busy(data.hctx);
+ *
+ * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+ * 则不会调用__blk_mq_tag_busy()
+ * __blk_mq_tag_busy()会为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+ * 如果之前没设置, 增加hctx->tags->active_queues
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return false;
 
+	/*
+	 * 为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+	 * 如果之前没设置, 增加hctx->tags->active_queues
+	 */
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1263| <<blk_mq_timeout_work>> blk_mq_tag_idle(hctx);
+ *   - block/blk-mq.c|2697| <<blk_mq_exit_hctx>> blk_mq_tag_idle(hctx);
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,9 +147,29 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - block/blk-flush.c|220| <<flush_end_io>> blk_mq_tag_set_rq(hctx, flush_rq->tag, fq->orig_rq);
+ *   - block/blk-flush.c|303| <<blk_kick_flush>> blk_mq_tag_set_rq(flush_rq->mq_hctx, first_rq->tag, flush_rq);
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
+	/* 使用tags->rqs的地方:
+	 *   - block/blk-mq-tag.c|241| <<bt_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.c|300| <<bt_tags_iter>> rq = tags->rqs[bitnr];
+	 *   - block/blk-mq-tag.h|130| <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   - block/blk-mq.c|515| <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|1091| <<blk_mq_tag_to_rq>> prefetch(tags->rqs[tag]);
+	 *   - block/blk-mq.c|1092| <<blk_mq_tag_to_rq>> return tags->rqs[tag];
+	 *   - block/blk-mq.c|1430| <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *   - block/blk-mq.c|2481| <<blk_mq_free_rqs>> if (tags->rqs && set->ops->exit_request) {
+	 *   - block/blk-mq.c|2508| <<blk_mq_free_rq_map>> kfree(tags->rqs);
+	 *   - block/blk-mq.c|2509| <<blk_mq_free_rq_map>> tags->rqs = NULL;
+	 *   - block/blk-mq.c|2533| <<blk_mq_alloc_rq_map>> tags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),
+	 *   - block/blk-mq.c|2536| <<blk_mq_alloc_rq_map>> if (!tags->rqs) {
+	 *   - block/blk-mq.c|2545| <<blk_mq_alloc_rq_map>> kfree(tags->rqs);
+	 */
 	hctx->tags->rqs[tag] = rq;
 }
 
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index 3708271..76190b8 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -29,12 +29,23 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - drivers/block/virtio_blk.c|694| <<virtblk_map_queues>> return blk_mq_virtio_map_queues(&set->map[0], vblk->vdev, 0);
+ *   - drivers/scsi/virtio_scsi.c|674| <<virtscsi_map_queues>> return blk_mq_virtio_map_queues(qmap, vscsi->vdev, 2);
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_queue_map *qmap,
 		struct virtio_device *vdev, int first_vec)
 {
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
+	/*
+	 * 可能设置为:
+	 *   - drivers/virtio/virtio_pci_legacy.c|211| <<global>> .get_vq_affinity = vp_get_vq_affinity,
+	 *   - drivers/virtio/virtio_pci_modern.c|462| <<global>> .get_vq_affinity = vp_get_vq_affinity,
+	 *   - drivers/virtio/virtio_pci_modern.c|478| <<global>> .get_vq_affinity = vp_get_vq_affinity,
+	 */
 	if (!vdev->config->get_vq_affinity)
 		goto fallback;
 
diff --git a/block/blk-mq.c b/block/blk-mq.c
index fa024bc..c72c79f 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -38,9 +38,137 @@
 #include "blk-mq-sched.h"
 #include "blk-rq-qos.h"
 
+/*
+ * 一共两处对queue_rq()的调用:
+ *   - blk_mq_try_issue_directly()-->__blk_mq_issue_directly()
+ *   - blk_mq_dispatch_rq_list()
+ */
+
+/*
+ * 一共三处缓存request的地方:
+ * 1. hctx->dispatch
+ * 2. ctx->rq_lists
+ * 3. scheduler自己的数据
+ *
+ *
+ * 下发hctx->dispatch的地方:
+ *   - block/blk-mq-sched.c|219| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+ *
+ * 添加到hctx->dispatch的地方:
+ *   - block/blk-mq-sched.c|422| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+ *   - block/blk-mq.c|1535| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+ *   - block/blk-mq.c|1935| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+ *   - block/blk-mq.c|2537| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+ *
+ *
+ * 下发ctx->rq_lists的地方:
+ *   - block/blk-mq.c|1148| <<flush_busy_ctx>> list_splice_tail_init(&ctx->rq_lists[type], flush_data->list);
+ *   - block/blk-mq.c|1204| <<dispatch_rq_from_ctx>> dispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);
+ *   - block/blk-mq.c|2528| <<blk_mq_hctx_notify_dead>> list_splice_init(&ctx->rq_lists[type], &tmp);
+ *
+ * 添加到ctx->rq_lists的地方:
+ *   - block/blk-mq-sched.c|377| <<blk_mq_attempt_merge>> if (blk_mq_bio_list_merge(q, &ctx->rq_lists[type], bio)) {
+ *   - block/blk-mq.c|1906| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+ *   - block/blk-mq.c|1908| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+ *   - block/blk-mq.c|1966| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+ *
+ *
+ * 调用scheduler的insert_requests()和requeue_request()缓存request的地方:
+ *   - block/blk-mq-sched.c|464| <<blk_mq_sched_insert_request>> e->type->ops.insert_requests(hctx, &list, at_head);
+ *   - block/blk-mq-sched.c|484| <<blk_mq_sched_insert_requests>> e->type->ops.insert_requests(hctx, list, false);
+ *   - block/blk-mq-sched.h|76| <<blk_mq_sched_requeue_request>> e->type->ops.requeue_request(rq);
+ *
+ * 调用scheduler的dispatch_request()准备下发request的地方:
+ *   - block/blk-mq-sched.c|111| <<blk_mq_do_dispatch_sched>> rq = e->type->ops.dispatch_request(hctx);
+ *
+ *
+ * blk_mq_sched_dispatch_requests()
+ * blk_mq_sched_bypass_insert()
+ * blk_mq_request_bypass_insert()
+ * blk_mq_hctx_notify_dead()
+ * flush_busy_ctx()
+ * dispatch_rq_from_ctx()
+ * blk_mq_attempt_merge()
+ * __blk_mq_insert_req_list()
+ * blk_mq_insert_requests()
+ * blk_mq_sched_insert_requests()
+ * blk_mq_sched_requeue_request()
+ * blk_mq_do_dispatch_sched()
+ */
+
+/*
+ * sched中下发的主要函数:
+ *
+ * - blk_mq_do_dispatch_sched()
+ *   对于参数的hctx, 如果没有调度器就返回
+ *   否则用调度器的dispatch_request()取出下一个request
+ *   用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ *
+ * - blk_mq_do_dispatch_ctx()
+ *   dequeue request one by one from sw queue if queue is busy
+ *   不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ *   然后为这个request调用queue_rq()
+ *   最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ *
+ *
+ * 插入的主要函数是blk_mq_sched_insert_request()
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
+ 
+/*
+ * BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+ * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+ *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+ *
+ * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+ *
+ * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+ *
+ * RQF_xxx用在request->rq_flags
+ *
+ * MQ_RQ_xxx用在request->state
+ *
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+
+/*
+ * 关于下发request到硬件驱动时的timeout:
+ *
+ * 比如virtio_queue_rq()调用blk_mq_start_request()
+ * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+ *
+ * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+ * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+ *
+ * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+ * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+ *
+ * - nvme的例子是nvme_timeout()
+ * - scsi的例子是scsi_timeout()
+ */
+
+/*
+ * 关于blk-mq complete的例子:
+ * 
+ * virtblk_done()调用blk_mq_complete_request()
+ * 调用__blk_mq_complete_request()
+ * 根据情况调用__blk_complete_request()或者q->mq_ops->complete()
+ *
+ * q->mq_ops->complete()的几个例子:
+ * - nvme_pci_complete_rq()
+ * - blkif_complete_rq()
+ * - virtblk_request_done()
+ * - scsi_softirq_done()
+ */
+
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2888| <<blk_mq_init_allocated_queue>> blk_mq_poll_stats_bkt,
+ *   - block/blk-mq.c|3442| <<blk_mq_poll_nsecs>> bucket = blk_mq_poll_stats_bkt(rq);
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, bytes, bucket;
@@ -59,10 +187,43 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
 }
 
 /*
+ * sched中下发的主要函数:
+ *
+ * - blk_mq_do_dispatch_sched()
+ *   对于参数的hctx, 如果没有调度器就返回
+ *   否则用调度器的dispatch_request()取出下一个request
+ *   用blk_mq_dispatch_rq_list()下发: 为参数list中的每一个request调用queue_rq()
+ *
+ * - blk_mq_do_dispatch_ctx()
+ *   dequeue request one by one from sw queue if queue is busy
+ *   不停在hctx->ctx_map中的每一个ctx->rq_lists中取出request
+ *   然后为这个request调用queue_rq()
+ *   最后直到sbitmap_any_bit_set(&hctx->ctx_map)都清空了
+ *
+ *
+ * 插入的主要函数是blk_mq_sched_insert_request()
+ * 核心思想: 如果不支持调度就把request放入request->mq_ctx的rq_lists (然后把ctx在hctx->ctx_map对应的bit设置)
+ */
+/*
  * Check if any of the ctx's have pending work in this hardware queue
  */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * hctx->dispatch在以下插入新元素:
+	 *   - block/blk-mq-sched.c|189| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+	 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|1303| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+	 *   - block/blk-mq.c|1663| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+	 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+	 *
+	 * hctx->ctx_map在以下被修改:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * blk_mq_sched_has_work():
+	 * 如果支持elevator, 用elevator的has_work查看是否有work
+	 */
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
 			blk_mq_sched_has_work(hctx);
@@ -71,6 +232,13 @@ static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1693| <<__blk_mq_insert_request>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *   - block/blk-mq.c|1730| <<blk_mq_insert_requests>> blk_mq_hctx_mark_pending(hctx, ctx);
+ *
+ * 把ctx在hctx->ctx_map对应的bit设置
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
@@ -80,6 +248,12 @@ static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 		sbitmap_set_bit(&hctx->ctx_map, bit);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2722| <<blk_mq_hctx_notify_dead>> blk_mq_hctx_clear_pending(hctx, ctx);
+ *
+ * 把ctx在hctx->ctx_map对应的bit清空
+ */
 static void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,
 				      struct blk_mq_ctx *ctx)
 {
@@ -108,6 +282,10 @@ static bool blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|74| <<part_in_flight>> return blk_mq_in_flight(q, part);
+ */
 unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 {
 	unsigned inflight[2];
@@ -119,6 +297,10 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part)
 	return inflight[0];
 }
 
+/*
+ * used only by:
+ *   - block/blk-mq.c|196| <<blk_mq_in_flight_rw>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
+ */
 static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 				     struct request *rq, void *priv,
 				     bool reserved)
@@ -131,6 +313,10 @@ static bool blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * called by:
+ *   - block/genhd.c|94| <<part_in_flight_rw>> blk_mq_in_flight_rw(q, part, inflight);
+ */
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2])
 {
@@ -140,10 +326,24 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|283| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|245| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - block/blk-pm.c|79| <<blk_pre_runtime_suspend>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|3903| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|317| <<pmem_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3852| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	int freeze_depth;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 */
 	freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
 	if (freeze_depth == 1) {
 		percpu_ref_kill(&q->q_usage_counter);
@@ -153,6 +353,11 @@ void blk_freeze_queue_start(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|246| <<blk_freeze_queue>> blk_mq_freeze_queue_wait(q);
+ *   - drivers/nvme/host/core.c|3841| <<nvme_wait_freeze>> blk_mq_freeze_queue_wait(ns->queue);
+ */
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
@@ -212,6 +417,13 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  * FIXME: replace the scsi_internal_device_*block_nowait() calls in the
  * mpt3sas driver such that this function can be removed.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|297| <<blk_mq_quiesce_queue>> blk_mq_quiesce_queue_nowait(q);
+ *   - drivers/scsi/scsi_lib.c|2667| <<scsi_internal_device_block_nowait>> blk_mq_quiesce_queue_nowait(q);
+ *
+ * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+ */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
@@ -227,20 +439,34 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);
  * sure no dispatch can happen until the queue is unquiesced via
  * blk_mq_unquiesce_queue().
  */
+/*
+ * called by:
+ *   - block/blk-core.c|360| <<blk_cleanup_queue>> blk_mq_quiesce_queue(q);
+ *   - block/blk-mq.c|3352| <<blk_mq_update_nr_requests>> blk_mq_quiesce_queue(q);
+ *   - block/elevator.c|645| <<elevator_switch>> blk_mq_quiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|958| <<virtblk_freeze>> blk_mq_quiesce_queue(vblk->disk->queue);
+ *   - drivers/scsi/scsi_lib.c|2699| <<scsi_internal_device_block>> blk_mq_quiesce_queue(q)
+ *
+ * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+ * 然后根据hctx->flags & BLK_MQ_F_BLOCKING的情况使用synchronize_srcu()或者synchronize_rcu()
+ */
 void blk_mq_quiesce_queue(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 	bool rcu = false;
 
+	/* 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED */
 	blk_mq_quiesce_queue_nowait(q);
 
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 		if (hctx->flags & BLK_MQ_F_BLOCKING)
 			synchronize_srcu(hctx->srcu);
 		else
 			rcu = true;
 	}
+	/* wait until a grace period has elapsed */
 	if (rcu)
 		synchronize_rcu();
 }
@@ -253,6 +479,19 @@ EXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);
  * This function recovers queue into the state before quiescing
  * which is done by blk_mq_quiesce_queue.
  */
+/*
+ * 部分调用的例子:
+ *   - block/blk-mq.c|3453| <<blk_mq_update_nr_requests>> blk_mq_unquiesce_queue(q);
+ *   - block/blk-sysfs.c|485| <<queue_wb_lat_store>> blk_mq_unquiesce_queue(q);
+ *   - block/elevator.c|649| <<elevator_switch>> blk_mq_unquiesce_queue(q);
+ *   - drivers/block/virtio_blk.c|975| <<virtblk_restore>> blk_mq_unquiesce_queue(vblk->disk->queue);
+ *   - drivers/nvme/host/pci.c|1627| <<nvme_dev_remove_admin>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/nvme/host/pci.c|1661| <<nvme_alloc_admin_tags>> blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+ *   - drivers/scsi/scsi_lib.c|2709| <<scsi_start_queue>> blk_mq_unquiesce_queue(q);
+ *
+ * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+ * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+ */
 void blk_mq_unquiesce_queue(struct request_queue *q)
 {
 	blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
@@ -262,6 +501,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * 只被以下调用:
+ *   - block/blk-core.c|286| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -272,6 +515,9 @@ void blk_mq_wake_waiters(struct request_queue *q)
 			blk_mq_tag_wakeup_all(hctx->tags, true);
 }
 
+/*
+ * 没人调用
+ */
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 {
 	return blk_mq_has_free_tags(hctx->tags);
@@ -282,14 +528,31 @@ EXPORT_SYMBOL(blk_mq_can_queue);
  * Only need start/end time stamping if we have stats enabled, or using
  * an IO scheduler.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|423| <<blk_mq_rq_ctx_init>> if (blk_mq_need_time_stamp(rq))
+ *   - block/blk-mq.c|640| <<__blk_mq_end_request>> if (blk_mq_need_time_stamp(rq))
+ */
 static inline bool blk_mq_need_time_stamp(struct request *rq)
 {
 	return (rq->rq_flags & RQF_IO_STAT) || rq->q->elevator;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|397| <<blk_mq_get_request>> rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
+ *
+ * 根据参数在data->hctx->sched_tags或者data->hctx->tags的tags->static_rqs[tag]
+ * 取出一个request, 初始化一下
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则返回使用data->hctx->sched_tags 
+	 * 否则返回data->hctx->tags
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
 	req_flags_t rq_flags = 0;
@@ -311,8 +574,24 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	rq->q = data->q;
 	rq->mq_ctx = data->ctx;
 	rq->mq_hctx = data->hctx;
+	/*
+	 * 如果BLK_MQ_REQ_INTERNAL
+	 *     rq_flags是0
+	 * 如果没有BLK_MQ_REQ_INTERNAL
+	 *     如果是BLK_MQ_F_TAG_SHARED, rq_flags是RQF_MQ_INFLIGHT
+	 */
 	rq->rq_flags = rq_flags;
 	rq->cmd_flags = op;
+	/*
+	 * 在以下使用:
+	 *   - block/blk-core.c|400| <<blk_queue_enter>> const bool pm = flags & BLK_MQ_REQ_PREEMPT;
+	 *   - block/blk-core.c|583| <<blk_get_request>> WARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));
+	 *   - block/blk-mq.c|568| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_PREEMPT)
+	 *
+	 * 在以下设置:
+	 *   - drivers/ide/ide-pm.c|80| <<generic_ide_resume>> rq = blk_get_request(drive->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_PREEMPT);
+	 *   - drivers/scsi/scsi_lib.c|262| <<__scsi_execute>> REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, BLK_MQ_REQ_PREEMPT);
+	 */
 	if (data->flags & BLK_MQ_REQ_PREEMPT)
 		rq->rq_flags |= RQF_PREEMPT;
 	if (blk_queue_io_stat(data->q))
@@ -347,6 +626,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|423| <<blk_mq_alloc_request>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|474| <<blk_mq_alloc_request_hctx>> rq = blk_mq_get_request(q, NULL, &alloc_data);
+ *   - block/blk-mq.c|1932| <<blk_mq_make_request>> rq = blk_mq_get_request(q, bio, &data);
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 					  struct bio *bio,
 					  struct blk_mq_alloc_data *data)
@@ -356,6 +641,7 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	unsigned int tag;
 	bool put_ctx_on_error = false;
 
+	/* 相当于: percpu_ref_get(&q->q_usage_counter); */
 	blk_queue_enter_live(q);
 	data->q = q;
 	if (likely(!data->ctx)) {
@@ -365,9 +651,15 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	if (likely(!data->hctx))
 		data->hctx = blk_mq_map_queue(q, data->cmd_flags,
 						data->ctx);
+	/*
+	 * BLK_MQ_REQ_NOWAIT: return when out of requests
+	 */
 	if (data->cmd_flags & REQ_NOWAIT)
 		data->flags |= BLK_MQ_REQ_NOWAIT;
 
+	/*
+	 * 如果支持scheduler, 就设置BLK_MQ_REQ_INTERNAL
+	 */
 	if (e) {
 		data->flags |= BLK_MQ_REQ_INTERNAL;
 
@@ -381,6 +673,12 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		    !(data->flags & BLK_MQ_REQ_RESERVED))
 			e->type->ops.limit_depth(data->cmd_flags, data);
 	} else {
+		/*
+		 * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+		 * 则不会调用__blk_mq_tag_busy()
+		 * __blk_mq_tag_busy()会为hctx->state设置BLK_MQ_S_TAG_ACTIVE
+		 * 如果之前没设置, 增加hctx->tags->active_queues
+		 */
 		blk_mq_tag_busy(data->hctx);
 	}
 
@@ -394,6 +692,10 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		return NULL;
 	}
 
+	/*
+	 * 根据参数在data->hctx->sched_tags或者data->hctx->tags的tags->static_rqs[tag]
+	 * 取出一个request, 初始化一下
+	 */
 	rq = blk_mq_rq_ctx_init(data, tag, data->cmd_flags);
 	if (!op_is_flush(data->cmd_flags)) {
 		rq->elv.icq = NULL;
@@ -435,6 +737,10 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 }
 EXPORT_SYMBOL(blk_mq_alloc_request);
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/core.c|437| <<nvme_alloc_request>> req = blk_mq_alloc_request_hctx(q, op, flags,
+ */
 struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 	unsigned int op, blk_mq_req_flags_t flags, unsigned int hctx_idx)
 {
@@ -481,6 +787,11 @@ struct request *blk_mq_alloc_request_hctx(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|755| <<blk_mq_free_request>> __blk_mq_free_request(rq);
+ *   - block/blk-mq.c|1187| <<blk_mq_check_expired>> __blk_mq_free_request(rq);
+ */
 static void __blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -494,10 +805,28 @@ static void __blk_mq_free_request(struct request *rq)
 		blk_mq_put_tag(hctx, hctx->tags, ctx, rq->tag);
 	if (sched_tag != -1)
 		blk_mq_put_tag(hctx, hctx->sched_tags, ctx, sched_tag);
+	/*
+	 * 如果之前设置了BLK_MQ_S_SCHED_RESTART到hctx->state
+	 * 取消这个bit, 触发blk_mq_run_hw_queue(hctx, true);
+	 */
 	blk_mq_sched_restart(hctx);
+	/* 就是调用percpu_ref_put(&q->q_usage_counter); */
 	blk_queue_exit(q);
 }
 
+/*
+ * 主要调用的几个地方:
+ *   - block/bfq-iosched.c|1893| <<bfq_bio_merge>> blk_mq_free_request(free);
+ *   - block/blk-core.c|594| <<blk_put_request>> blk_mq_free_request(req);
+ *   - block/blk-mq.c|768| <<__blk_mq_end_request>> blk_mq_free_request(rq->next_rq);
+ *   - block/blk-mq.c|769| <<__blk_mq_end_request>> blk_mq_free_request(rq);
+ *   - block/mq-deadline.c|483| <<dd_bio_merge>> blk_mq_free_request(free);
+ *   - drivers/nvme/host/core.c|801| <<__nvme_submit_sync_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|902| <<nvme_submit_user_cmd>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/core.c|912| <<nvme_keep_alive_end_io>> blk_mq_free_request(rq);
+ *   - drivers/nvme/host/pci.c|1220| <<abort_endio>> blk_mq_free_request(req);
+ *   - drivers/nvme/host/pci.c|2241| <<nvme_del_queue_end>> blk_mq_free_request(req);
+ */
 void blk_mq_free_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -557,6 +886,28 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * called by:
+ *   - block/blk-flush.c|197| <<blk_flush_complete_seq>> blk_mq_end_request(rq, error);
+ *   - block/blk-flush.c|379| <<blk_insert_flush>> blk_mq_end_request(rq, 0);
+ *   - block/blk-mq.c|1632| <<blk_mq_dispatch_rq_list>> blk_mq_end_request(rq, BLK_STS_IOERR);
+ *   - block/blk-mq.c|2294| <<blk_mq_try_issue_directly>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/loop.c|485| <<lo_complete_rq>> blk_mq_end_request(rq, ret);
+ *   - drivers/block/null_blk_main.c|620| <<end_cmd>> blk_mq_end_request(cmd->rq, cmd->error);
+ *   - drivers/block/virtio_blk.c|225| <<virtblk_request_done>> blk_mq_end_request(req, virtblk_result(vbr));
+ *   - drivers/block/xen-blkfront.c|918| <<blkif_complete_rq>> blk_mq_end_request(rq, blkif_req(rq)->error);
+ *   - drivers/block/xen-blkfront.c|2113| <<blkfront_resume>> blk_mq_end_request(shadow[j].request, BLK_STS_OK);
+ *   - drivers/nvme/host/core.c|281| <<nvme_complete_rq>> blk_mq_end_request(req, status);
+ *
+ * Bart Van Assche:
+ * Calling blk_mq_end_request() from outside the .queue_rq() or .complete()
+ * callback functions is wrong.
+ *
+ * Keith Busch:
+ * This callback can only see requests in MQ_RQ_IDLE state, and
+ * bkl_mq_end_request() is the correct way to end those that never entered
+ * a driver's queue_rq().
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -596,6 +947,13 @@ static void __blk_mq_complete_request(struct request *rq)
 	}
 
 	/*
+	 * complete的几个例子:
+	 *   - virtblk_request_done()
+	 *   - scsi_softirq_done()
+	 *   - nvme_pci_complete_rq()
+	 */
+
+	/*
 	 * For a polled request, always complete locallly, it's pointless
 	 * to redirect the completion.
 	 */
@@ -623,6 +981,7 @@ static void __blk_mq_complete_request(struct request *rq)
 static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 	__releases(hctx->srcu)
 {
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING))
 		rcu_read_unlock();
 	else
@@ -632,6 +991,7 @@ static void hctx_unlock(struct blk_mq_hw_ctx *hctx, int srcu_idx)
 static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
 	__acquires(hctx->srcu)
 {
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		/* shut up gcc false positive */
 		*srcu_idx = 0;
@@ -657,12 +1017,48 @@ bool blk_mq_complete_request(struct request *rq)
 }
 EXPORT_SYMBOL(blk_mq_complete_request);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|296| <<bt_tags_iter>> if (rq && blk_mq_request_started(rq))
+ *   - block/blk-mq.c|718| <<__blk_mq_requeue_request>> if (blk_mq_request_started(rq)) {
+ *   - drivers/block/nbd.c|648| <<nbd_read_stat>> if (!req || !blk_mq_request_started(req)) {
+ */
 int blk_mq_request_started(struct request *rq)
 {
+	/*
+	 * MQ_RQ_IDLE
+	 * MQ_RQ_IN_FLIGHT
+	 * MQ_RQ_COMPLETE
+	 */
 	return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
 
+/*
+ * 被以下的例子调用:
+ *   - drivers/block/null_blk_main.c|1332| <<null_queue_rq>> blk_mq_start_request(bd->rq);
+ *   - drivers/block/virtio_blk.c|318| <<virtio_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/block/xen-blkfront.c|891| <<blkif_queue_rq>> blk_mq_start_request(qd->rq);
+ *   - drivers/nvme/host/pci.c|942| <<nvme_queue_rq>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1638| <<scsi_mq_prep_fn>> blk_mq_start_request(req);
+ *   - drivers/scsi/scsi_lib.c|1722| <<scsi_queue_rq>> blk_mq_start_request(req);
+ *
+ * 关于下发request到硬件驱动时的timeout:
+ *
+ * 比如virtio_queue_rq()调用blk_mq_start_request()
+ * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+ *
+ * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+ * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+ *
+ * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+ * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+ *
+ * - nvme的例子是nvme_timeout()
+ * - scsi的例子是scsi_timeout()
+ *
+ * 设置request->state为MQ_RQ_IN_FLIGHT
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -685,6 +1081,9 @@ void blk_mq_start_request(struct request *rq)
 	blk_add_timer(rq);
 	WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
 
+	/*
+	 * q->dma_drain_size似乎目前只被libata-scsi使用
+	 */
 	if (q->dma_drain_size && blk_rq_bytes(rq)) {
 		/*
 		 * Make sure space for the drain appears.  We know we can do
@@ -754,6 +1153,11 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|137| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|878| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -805,6 +1209,10 @@ struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 }
 EXPORT_SYMBOL(blk_mq_tag_to_rq);
 
+/*
+ * 只在以下被使用, 被drivers/md/dm.c的md_in_flight()调用:
+ *   - block/blk-mq.c|1217| <<blk_mq_queue_inflight>> blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
+ */
 static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			       void *priv, bool reserved)
 {
@@ -822,18 +1230,34 @@ static bool blk_mq_rq_inflight(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	return true;
 }
 
+/*
+ * called by only:
+ *   - drivers/md/dm.c|666| <<md_in_flight>> return blk_mq_queue_inflight(md->queue);
+ */
 bool blk_mq_queue_inflight(struct request_queue *q)
 {
 	bool busy = false;
 
+	/*
+	 * 针对每一个request_queue->queue_hw_ctx[i]的tags->bitmap_tags.sb
+	 * 调用blk_mq_rq_inflight()
+	 */
 	blk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);
 	return busy;
 }
 EXPORT_SYMBOL_GPL(blk_mq_queue_inflight);
 
+/*
+ * called only by:
+ *  - block/blk-mq.c|1054| <<blk_mq_check_expired>> blk_mq_rq_timed_out(rq, reserved);
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	req->rq_flags |= RQF_TIMED_OUT;
+	/*
+	 * nvme的例子是nvme_timeout()
+	 * scsi的例子是scsi_timeout()
+	 */
 	if (req->q->mq_ops->timeout) {
 		enum blk_eh_timer_return ret;
 
@@ -866,6 +1290,10 @@ static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
 	return false;
 }
 
+/*
+ * used by:
+ *   - block/blk-mq.c|1113| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
+ */
 static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -904,6 +1332,20 @@ static bool blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	return true;
 }
 
+/*
+ * timeout_work可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+ *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+ *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ *
+ * timeout_work使用的地方:
+ *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+ *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+ *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+ *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+ *
+ * 在以下使用:
+ *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -928,6 +1370,13 @@ static void blk_mq_timeout_work(struct work_struct *work)
 	if (!percpu_ref_tryget(&q->q_usage_counter))
 		return;
 
+	/*
+	 * To iterating over requests triggers race conditions with request execution
+	 *
+	 * That race isn't new.
+	 * You should only iterate when your queues are quieced to ensure the
+	 * request sent to a callback is stable.
+	 */
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &next);
 
 	if (next != 0) {
@@ -953,6 +1402,13 @@ struct flush_busy_ctx_data {
 	struct list_head *list;
 };
 
+/*
+ * 只在以下被调用:
+ *   - block/blk-mq.c|1063| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+ *
+ * 把ctx->rq_list[type]的request们拼接到flush_busy_ctx_data->list
+ * 把ctx在hctx->ctx_map清空
+ */
 static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
 {
 	struct flush_busy_ctx_data *flush_data = data;
@@ -971,6 +1427,12 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
  * Process software queues that have been marked busy, splicing them
  * to the for-dispatch
  */
+/*
+ * 对于hctx->ctx_map中每个设置的bit对应的ctx
+ * 把ctx->rq_list[type]的request们拼接到参数的list
+ * 把ctx在hctx->ctx_map清空
+ * 最后参数list中的就是这个hctx->ctx_map中每一个ctx的rq_list的总和拼接
+ */
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 {
 	struct flush_busy_ctx_data data = {
@@ -978,6 +1440,15 @@ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 		.list = list,
 	};
 
+	/*
+	 * hctx->ctx_map在以下被设置:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * flush_busy_ctx():
+	 * 把ctx->rq_list[type]的request们拼接到flush_busy_ctx_data->list
+	 * 把ctx在hctx->ctx_map清空
+	 */
 	sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
 }
 EXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);
@@ -987,6 +1458,14 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * used only by:
+ *   - block/blk-mq.c|1395| <<blk_mq_dequeue_from_ctx>> dispatch_rq_from_ctx, &data);
+ *
+ * 从bitnr对应的hctx->ctx_map的ctx中的ctx->rq_lists[type].next取出一个request
+ * 放入dispatch_data->rq
+ * 如果取完空了就把hctx->ctx_map对应的bit清掉
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1007,6 +1486,12 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * 只被如下调用:
+ *   - block/blk-mq-sched.c|221| <<blk_mq_do_dispatch_ctx>> rq = blk_mq_dequeue_from_ctx(hctx, ctx);
+ *
+ * 从start代表的第一个hctx->ctx_map开始, 取出一个request, 就取出一个啊!!!
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1016,6 +1501,15 @@ struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 		.rq   = NULL,
 	};
 
+	/*
+	 * 从bitnr对应的hctx->ctx_map的ctx中的ctx->rq_lists[type].next取出一个request (就一个!!)
+	 * 放入dispatch_data->rq
+	 * 如果取完空了就把hctx->ctx_map对应的bit清掉
+	 *
+	 * 因为dispatch_rq_from_ctx()成功返回0, 所以实际就取出一个的
+	 *
+	 * 如果fn返回true继续下一个bit, 如果返回false就直接返回了
+	 */
 	__sbitmap_for_each_set(&hctx->ctx_map, off,
 			       dispatch_rq_from_ctx, &data);
 
@@ -1030,8 +1524,24 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1103| <<blk_mq_mark_tag_wait>> return blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1128| <<blk_mq_mark_tag_wait>> ret = blk_mq_get_driver_tag(rq);
+ *   - block/blk-mq.c|1207| <<blk_mq_dispatch_rq_list>> if (!blk_mq_get_driver_tag(rq)) {
+ *   - block/blk-mq.c|1239| <<blk_mq_dispatch_rq_list>> bd.last = !blk_mq_get_driver_tag(nxt);
+ *   - block/blk-mq.c|1822| <<blk_mq_try_issue_directly>> if (!blk_mq_get_driver_tag(rq)) {
+ *
+ * 如果request->tag不为-1, 直接返回
+ * 否则分配一个给driver用的(不是internal的)tag!
+ */
 bool blk_mq_get_driver_tag(struct request *rq)
 {
+	/*
+	 * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+	 * 则使用data->hctx->sched_tags
+	 * 否则使用data->hctx->tags
+	 */
 	struct blk_mq_alloc_data data = {
 		.q = rq->q,
 		.hctx = rq->mq_hctx,
@@ -1046,6 +1556,10 @@ bool blk_mq_get_driver_tag(struct request *rq)
 	if (blk_mq_tag_is_reserved(data.hctx->sched_tags, rq->internal_tag))
 		data.flags |= BLK_MQ_REQ_RESERVED;
 
+	/*
+	 * 如果blk_mq_hw_ctx->flags没有设置BLK_MQ_F_TAG_SHARED
+	 * 则不会调用__blk_mq_tag_busy()
+	 */
 	shared = blk_mq_tag_busy(data.hctx);
 	rq->tag = blk_mq_get_tag(&data);
 	if (rq->tag >= 0) {
@@ -1089,6 +1603,18 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,
 	bool ret;
 
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED)) {
+		/*
+		 * 在以下使用:
+		 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+		 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+		 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+		 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+		 *
+		 * 用来触发blk_mq_run_hw_queue()
+		 */
 		if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 			set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 
@@ -1177,6 +1703,15 @@ static void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)
 /*
  * Returns true if we did some work AND can potentially do more.
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|114| <<blk_mq_do_dispatch_sched>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|164| <<blk_mq_do_dispatch_ctx>> } while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+ *   - block/blk-mq-sched.c|208| <<blk_mq_sched_dispatch_requests>> if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+ *   - block/blk-mq-sched.c|221| <<blk_mq_sched_dispatch_requests>> blk_mq_dispatch_rq_list(q, &rq_list, false);
+ *
+ * 为参数list中的每一个request调用queue_rq()
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1189,6 +1724,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	if (list_empty(list))
 		return false;
 
+	/* list_is_singular(): tests whether a list has just one entry */
 	WARN_ON(!list_is_singular(list) && got_budget);
 
 	/*
@@ -1200,10 +1736,20 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 
 		rq = list_first_entry(list, struct request, queuelist);
 
+		/*
+		 * 在以下设置:
+		 *   - block/blk-flush.c|298| <<blk_kick_flush>> flush_rq->mq_hctx = first_rq->mq_hctx;
+		 *   - block/blk-mq.c|317| <<blk_mq_rq_ctx_init>> rq->mq_hctx = data->hctx;
+		 *   - block/blk-mq.c|502| <<__blk_mq_free_request>> rq->mq_hctx = NULL;
+		 */
 		hctx = rq->mq_hctx;
 		if (!got_budget && !blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * 如果request->tag不为-1, 直接返回
+		 * 否则分配一个给driver用的(不是internal的)tag!
+		 */
 		if (!blk_mq_get_driver_tag(rq)) {
 			/*
 			 * The initial allocation attempt failed, so we need to
@@ -1331,6 +1877,11 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|1530| <<__blk_mq_delay_run_hw_queue>> __blk_mq_run_hw_queue(hctx);
+ *   - block/blk-mq.c|1699| <<blk_mq_run_work_fn>> __blk_mq_run_hw_queue(hctx);
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1366,9 +1917,13 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	might_sleep_if(hctx->flags & BLK_MQ_F_BLOCKING);
 
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * __blk_mq_run_hw_queue()是唯一调用blk_mq_sched_dispatch_requests()的地方
+	 */
 	blk_mq_sched_dispatch_requests(hctx);
 	hctx_unlock(hctx, srcu_idx);
 }
@@ -1434,6 +1989,7 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 	if (unlikely(blk_mq_hctx_stopped(hctx)))
 		return;
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		int cpu = get_cpu();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
@@ -1455,6 +2011,22 @@ void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|79| <<blk_mq_sched_restart>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq-sched.c|452| <<blk_mq_sched_insert_request>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq-sched.c|476| <<blk_mq_sched_insert_requests>> blk_mq_run_hw_queue(hctx, run_queue_async);
+ *   - block/blk-mq-tag.c|157| <<blk_mq_get_tag>> blk_mq_run_hw_queue(data->hctx, false);
+ *   - block/blk-mq.c|1198| <<blk_mq_dispatch_wake>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1455| <<blk_mq_dispatch_rq_list>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/blk-mq.c|1639| <<blk_mq_run_hw_queues>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1704| <<blk_mq_start_hw_queue>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|1724| <<blk_mq_start_stopped_hw_queue>> blk_mq_run_hw_queue(hctx, async);
+ *   - block/blk-mq.c|1798| <<blk_mq_request_bypass_insert>> blk_mq_run_hw_queue(hctx, false);
+ *   - block/blk-mq.c|2106| <<blk_mq_make_request>> blk_mq_run_hw_queue(data.hctx, true);
+ *   - block/blk-mq.c|2373| <<blk_mq_hctx_notify_dead>> blk_mq_run_hw_queue(hctx, true);
+ *   - block/kyber-iosched.c|709| <<kyber_domain_wake>> blk_mq_run_hw_queue(hctx, true);
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1469,6 +2041,10 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 	 * quiesced.
 	 */
 	hctx_lock(hctx, &srcu_idx);
+	/*
+	 * 如果q->queue_flags没设置QUEUE_FLAG_QUIESCED
+	 * 并且有待处理的request
+	 */
 	need_run = !blk_queue_quiesced(hctx->queue) &&
 		blk_mq_hctx_has_pending(hctx);
 	hctx_unlock(hctx, srcu_idx);
@@ -1590,6 +2166,10 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * used by:
+ *   - block/blk-mq.c|2327| <<blk_mq_init_hctx>> INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1605,6 +2185,12 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 	__blk_mq_run_hw_queue(hctx);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|1820| <<__blk_mq_insert_request>> __blk_mq_insert_req_list(hctx, rq, at_head);
+ *
+ * 把request添加到对应的request->mq_ctx的ctx->rq_lists[type]
+ */
 static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    bool at_head)
@@ -1622,6 +2208,10 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 		list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
 }
 
+/*
+ * 把request放入request->mq_ctx的rq_lists
+ * 然后把ctx在hctx->ctx_map对应的bit设置
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1649,6 +2239,13 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|429| <<blk_mq_sched_insert_requests>> blk_mq_insert_requests(hctx, ctx, list);
+ *
+ * 把list中的request们放入ctx->rq_lists[type]
+ * 把ctx在hctx->ctx_map对应的bit设置
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1667,6 +2264,7 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 
 	spin_lock(&ctx->lock);
 	list_splice_tail_init(list, &ctx->rq_lists[type]);
+	/* 把ctx在hctx->ctx_map对应的bit设置 */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 	spin_unlock(&ctx->lock);
 }
@@ -1749,6 +2347,12 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2046| <<blk_mq_try_issue_directly>> ret = __blk_mq_issue_directly(hctx, rq, cookie, last);
+ *
+ * 为参数的request调用queue_rq()!
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie, bool last)
@@ -1788,6 +2392,15 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1267| <<blk_insert_cloned_request>> return blk_mq_try_issue_directly(rq->mq_hctx, rq, &unused, true, true);
+ *   - block/blk-mq.c|2089| <<blk_mq_try_issue_list_directly>> ret = blk_mq_try_issue_directly(hctx, rq, &unused,
+ *   - block/blk-mq.c|2213| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, same_queue_rq,
+ *   - block/blk-mq.c|2220| <<blk_mq_make_request>> blk_mq_try_issue_directly(data.hctx, rq, &cookie, false, true);
+ *
+ * 核心思想是为参数的request调用queue_rq()!
+ */
 blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1829,6 +2442,9 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	 *.queue_rq() to the hardware dispatch list.
 	 */
 	force = true;
+	/*
+	 * 为参数的request调用queue_rq()!
+	 */
 	ret = __blk_mq_issue_directly(hctx, rq, cookie, last);
 out_unlock:
 	hctx_unlock(hctx, srcu_idx);
@@ -1860,6 +2476,10 @@ blk_status_t blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq-sched.c|484| <<blk_mq_sched_insert_requests>> blk_mq_try_issue_list_directly(hctx, list);
+ */
 void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,
 		struct list_head *list)
 {
@@ -1904,7 +2524,16 @@ static void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)
 
 static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 {
+	/*
+	 * Reads are always treated as synchronous, as are requests with the FUA or
+	 * PREFLUSH flag.  Other operations may be marked as synchronous using the
+	 * REQ_SYNC flag.
+	 */
 	const int is_sync = op_is_sync(bio->bi_opf);
+	/*
+	 * Check if the bio or request is one that needs special treatment in the
+	 * flush state machine.
+	 */
 	const int is_flush_fua = op_is_flush(bio->bi_opf);
 	struct blk_mq_alloc_data data = { .flags = 0};
 	struct request *rq;
@@ -1914,6 +2543,10 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	blk_queue_bounce(q, &bio);
 
+	/*
+	 * 根据块设备请求队列的limits.max_sectors和limits.max_segmetns
+	 * 来拆分bio,适应设备缓存.会在函数blk_set_default_limits中设置
+	 */
 	blk_queue_split(q, &bio);
 
 	if (!bio_integrity_prep(bio))
@@ -2053,6 +2686,12 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|527| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+ *   - block/blk-mq-tag.c|633| <<blk_mq_tag_update_depth>> new = blk_mq_alloc_rq_map(set, hctx->queue_num, tdepth,
+ *   - block/blk-mq.c|2846| <<__blk_mq_alloc_rq_map>> set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2110,6 +2749,12 @@ static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|671| <<blk_mq_sched_alloc_tags>> ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
+ *   - block/blk-mq-tag.c|811| <<blk_mq_tag_update_depth>> ret = blk_mq_alloc_rqs(set, new, hctx->queue_num, tdepth);
+ *   - block/blk-mq.c|3002| <<__blk_mq_alloc_rq_map>> ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
+ */
 int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx, unsigned int depth)
 {
@@ -2192,6 +2837,10 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
  * software queue to the hw queue dispatch list, and ensure that it
  * gets run.
  */
+/*
+ * used by:
+ *   - block/blk-mq.c|4186| <<blk_mq_init>> blk_mq_hctx_notify_dead);
+ */
 static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2263,6 +2912,10 @@ static void blk_mq_exit_hw_queues(struct request_queue *q,
 	}
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|3260| <<blk_mq_alloc_and_init_hctx>> if (blk_mq_init_hctx(q, set, hctx, hctx_idx)) {
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2314,6 +2967,7 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	if (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx, node))
 		goto free_fq;
 
+	/* 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu */
 	if (hctx->flags & BLK_MQ_F_BLOCKING)
 		init_srcu_struct(hctx->srcu);
 
@@ -2333,6 +2987,10 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|2941| <<blk_mq_init_allocated_queue>> blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+ */
 static void blk_mq_init_cpu_queues(struct request_queue *q,
 				   unsigned int nr_hw_queues)
 {
@@ -2392,6 +3050,11 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2915| <<blk_mq_init_allocated_queue>> blk_mq_map_swqueue(q);
+ *   - block/blk-mq.c|3295| <<__blk_mq_update_nr_hw_queues>> blk_mq_map_swqueue(q);
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, j, hctx_idx;
@@ -2404,6 +3067,7 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/* 遍历每一个request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2430,6 +3094,14 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		}
 
 		ctx = per_cpu_ptr(q->queue_ctx, i);
+		/*
+		 * 设置nr_maps的地方:
+		 *   - block/blk-mq.c|2713| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3058| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - block/blk-mq.c|3069| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+		 *   - drivers/nvme/host/pci.c|2322| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2324| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 */
 		for (j = 0; j < set->nr_maps; j++) {
 			if (!set->map[j].nr_queues) {
 				ctx->hctxs[j] = blk_mq_map_queue_type(q,
@@ -2459,6 +3131,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			BUG_ON(!hctx->nr_ctx);
 		}
 
+		/*
+		 * 这里很重要, 把剩下的的j网上的type都map到HCTX_TYPE_DEFAULT的hctx
+		 */
 		for (; j < HCTX_MAX_TYPES; j++)
 			ctx->hctxs[j] = blk_mq_map_queue_type(q,
 					HCTX_TYPE_DEFAULT, i);
@@ -2548,6 +3223,13 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2914| <<blk_mq_init_allocated_queue>> blk_mq_add_queue_tag_set(set, q);
+ *
+ * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+ * 多个request_queue可能共享1个blk_mq_tag_set
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2570,6 +3252,12 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 }
 
 /* All allocations will be freed in release handler of q->mq_kobj */
+/*
+ * called by:
+ *   - block/blk-mq.c|3299| <<blk_mq_init_allocated_queue>> if (blk_mq_alloc_ctxs(q))
+ *
+ * 分配软件的request_queue->queue_ctx!
+ */
 static int blk_mq_alloc_ctxs(struct request_queue *q)
 {
 	struct blk_mq_ctxs *ctxs;
@@ -2688,6 +3376,10 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 	return hw_ctx_size;
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|3245| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		struct blk_mq_tag_set *set, struct request_queue *q,
 		int hctx_idx, int node)
@@ -2707,6 +3399,16 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 		return NULL;
 	}
 
+	/*
+	 * 是伴随着RQF_MQ_INFLIGHT的:
+	 *   - block/blk-mq-debugfs.c|641| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 *   - block/blk-mq-tag.c|98| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq.c|511| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|751| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|1428| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|3205| <<blk_mq_alloc_and_init_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq.h|290| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 */
 	atomic_set(&hctx->nr_active, 0);
 	hctx->numa_node = node;
 	hctx->queue_num = hctx_idx;
@@ -2721,10 +3423,22 @@ static struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(
 	return hctx;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2881| <<blk_mq_init_allocated_queue>> blk_mq_realloc_hw_ctxs(set, q);
+ *   - block/blk-mq.c|3287| <<__blk_mq_update_nr_hw_queues>> blk_mq_realloc_hw_ctxs(set, q);
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
 	int i, j, end;
+	/*
+	 * q->queue_hw_ctx第一维在以下分配:
+	 *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+	 *
+	 * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+	 *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+	 */
 	struct blk_mq_hw_ctx **hctxs = q->queue_hw_ctx;
 
 	/* protect against switching io scheduler  */
@@ -2739,6 +3453,9 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 		 * we need to realloc the hctx. If allocation fails, fallback
 		 * to use the previous one.
 		 */
+		/*
+		 * 如果已经分配了就不用分配了
+		 */
 		if (hctxs[i] && (hctxs[i]->numa_node == node))
 			continue;
 
@@ -2791,20 +3508,47 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
  * have more than the CPUs (software queues). For multiple sets, the tag_set
  * user may have set ->nr_hw_queues larger.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->nr_queues = nr_hw_queues(set);
+ *   - block/blk-mq.c|3620| <<blk_mq_alloc_tag_set>> set->tags = kcalloc_node(nr_hw_queues(set), sizeof(struct blk_mq_tags *),
+ *   - block/blk-mq.c|3670| <<blk_mq_free_tag_set>> for (i = 0; i < nr_hw_queues(set); i++)
+ *
+ * 实现如下:
+ *   if (set->nr_maps == 1)
+ *       return nr_cpu_ids;
+ *   return max(set->nr_hw_queues, nr_cpu_ids);
+ */
 static unsigned int nr_hw_queues(struct blk_mq_tag_set *set)
 {
 	if (set->nr_maps == 1)
 		return nr_cpu_ids;
 
+	/*
+	 * set->nr_hw_queues在非驱动更新的地方:
+	 *   - block/blk-mq.c|3189| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3630| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1; --> kdump情况下
+	 *   - block/blk-mq.c|3639| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids; --> set->nr_maps==1的情况下
+	 *   - block/blk-mq.c|3874| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3882| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 */
 	return max(set->nr_hw_queues, nr_cpu_ids);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3167| <<blk_mq_init_queue>> q = blk_mq_init_allocated_queue(set, uninit_q);
+ *   - drivers/md/dm-rq.c|546| <<dm_mq_init_request_queue>> q = blk_mq_init_allocated_queue(md->tag_set, md->queue);
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q)
 {
 	/* mark the queue as mq asap */
 	q->mq_ops = set->ops;
 
+	/*
+	 * struct blk_stat_callback
+	 */
 	q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
 					     blk_mq_poll_stats_bkt,
 					     BLK_MQ_POLL_STATS_BKTS, q);
@@ -2817,7 +3561,21 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	/* init q->mq_kobj and sw queues' kobjects */
 	blk_mq_sysfs_init(q);
 
+	/*
+	 * Maximum number of hardware queues we support. For single sets, we'll never
+	 * have more than the CPUs (software queues). For multiple sets, the tag_set
+	 * user may have set ->nr_hw_queues larger.
+	 */
+	/*
+	 * nr_hw_queues(set)的实现:
+	 *   if (set->nr_maps == 1)
+	 *       return nr_cpu_ids;
+	 *   return max(set->nr_hw_queues, nr_cpu_ids);
+	 */
 	q->nr_queues = nr_hw_queues(set);
+	/*
+	 * 注意, 数量是上面的q->nr_queues = nr_hw_queues(set)!!!!!!!!!!
+	 */
 	q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
 						GFP_KERNEL, set->numa_node);
 	if (!q->queue_hw_ctx)
@@ -2856,6 +3614,10 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	q->poll_nsec = -1;
 
 	blk_mq_init_cpu_queues(q, set->nr_hw_queues);
+	/*
+	 * 核心思想是把request_queue->tag_set_list链接到blk_mq_tag_set->tag_list
+	 * 多个request_queue可能共享1个blk_mq_tag_set
+	 */
 	blk_mq_add_queue_tag_set(set, q);
 	blk_mq_map_swqueue(q);
 
@@ -2939,6 +3701,13 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by"
+ *   - block/blk-mq.c|3831| <<blk_mq_alloc_tag_set>> ret = blk_mq_update_queue_map(set);
+ *   - block/blk-mq.c|4055| <<__blk_mq_update_nr_hw_queues>> blk_mq_update_queue_map(set);
+ *
+ * 初始化软件队列到硬件队列的mapping???
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues && !is_kdump_kernel()) {
@@ -2964,6 +3733,7 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		return set->ops->map_queues(set);
 	} else {
 		BUG_ON(set->nr_maps > 1);
+		/* 初始化blk_mq_queue_map->map */
 		return blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
 	}
 }
@@ -2974,6 +3744,23 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if it's too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * vblk->tag_set.ops = &virtio_mq_ops;
+ * vblk->tag_set.queue_depth = virtblk_queue_depth;
+ * vblk->tag_set.numa_node = NUMA_NO_NODE;
+ * vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+ * vblk->tag_set.cmd_size =
+ *                    sizeof(struct virtblk_req) +
+ *                    sizeof(struct scatterlist) * sg_elems;
+ * vblk->tag_set.driver_data = vblk;
+ * vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+ *
+ * err = blk_mq_alloc_tag_set(&vblk->tag_set);
+ * if (err)
+ *	goto out_put_disk;
+ *
+ * q = blk_mq_init_queue(&vblk->tag_set);
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int i, ret;
@@ -2990,6 +3777,7 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (!set->ops->queue_rq)
 		return -EINVAL;
 
+	/* 要么都设置, 要么都不设置 */
 	if (!set->ops->get_budget ^ !set->ops->put_budget)
 		return -EINVAL;
 
@@ -3021,6 +3809,7 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)
 		set->nr_hw_queues = nr_cpu_ids;
 
+	/* 每个queue一个 struct blk_mq_tags */
 	set->tags = kcalloc_node(nr_hw_queues(set), sizeof(struct blk_mq_tags *),
 				 GFP_KERNEL, set->numa_node);
 	if (!set->tags)
@@ -3028,6 +3817,10 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 
 	ret = -ENOMEM;
 	for (i = 0; i < set->nr_maps; i++) {
+		/*
+		 * nr_hw_queues可能比cpu数量多,
+		 * 猜测是每一个sw queue对应的hw queue??? 所以用cpu的数量
+		 */
 		set->map[i].mq_map = kcalloc_node(nr_cpu_ids,
 						  sizeof(set->map[i].mq_map[0]),
 						  GFP_KERNEL, set->numa_node);
@@ -3036,6 +3829,9 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 		set->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;
 	}
 
+	/*
+	 * 初始化软件队列到硬件队列的mapping???
+	 */
 	ret = blk_mq_update_queue_map(set);
 	if (ret)
 		goto out_free_mq_map;
@@ -3077,6 +3873,10 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|81| <<queue_requests_store>> err = blk_mq_update_nr_requests(q, nr);
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -3086,13 +3886,26 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 	if (!set)
 		return -EINVAL;
 
+	/*
+	 * nr_requests在以下设置:
+	 *   - block/blk-mq-sched.c|563| <<blk_mq_init_sched>> q->nr_requests = q->tag_set->queue_depth;
+	 *   - block/blk-mq-sched.c|572| <<blk_mq_init_sched>> q->nr_requests = 2 * min_t(unsigned int , q->tag_set->queue_depth,
+	 *   - block/blk-mq.c|3416| <<blk_mq_init_allocated_queue>> q->nr_requests = set->queue_depth;
+	 *   - block/blk-mq.c|3723| <<blk_mq_update_nr_requests>> q->nr_requests = nr;
+	 *   - block/blk-settings.c|123| <<blk_queue_make_request>> q->nr_requests = BLKDEV_MAX_RQ;
+	 */
 	if (q->nr_requests == nr)
 		return 0;
 
 	blk_mq_freeze_queue(q);
+	/*
+	 * 把request_queue->queue_flags设置上QUEUE_FLAG_QUIESCED
+	 * 然后根据hctx->flags & BLK_MQ_F_BLOCKING的情况使用synchronize_srcu()或者synchronize_rcu()
+	 */
 	blk_mq_quiesce_queue(q);
 
 	ret = 0;
+	/* 遍历每一个request_queue->queue_hw_ctx[i] */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (!hctx->tags)
 			continue;
@@ -3114,6 +3927,10 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 	if (!ret)
 		q->nr_requests = nr;
 
+	/*
+	 * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+	 * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+	 */
 	blk_mq_unquiesce_queue(q);
 	blk_mq_unfreeze_queue(q);
 
@@ -3190,6 +4007,19 @@ static void blk_mq_elv_switch_back(struct list_head *head,
 	mutex_unlock(&q->sysfs_lock);
 }
 
+/*
+ * called by only:
+ *   - block/blk-mq.c|3905| <<blk_mq_update_nr_hw_queues>> __blk_mq_update_nr_hw_queues(set, nr_hw_queues);
+ *
+ * 最终被以下间接调用:
+ *   - drivers/block/nbd.c|1154| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2492| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2347| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|891| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1595| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|484| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
@@ -3253,6 +4083,16 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1154| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2121| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2492| <<nvme_fc_recreate_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2347| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|891| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/host/tcp.c|1595| <<nvme_tcp_configure_io_queues>> blk_mq_update_nr_hw_queues(ctrl->tagset,
+ *   - drivers/nvme/target/loop.c|484| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
diff --git a/block/blk-mq.h b/block/blk-mq.h
index c11353a..0904e22 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -7,6 +7,9 @@
 
 struct blk_mq_tag_set;
 
+/*
+ * 这个结构保存的真正的request_queue中每个ctx的实体!
+ */
 struct blk_mq_ctxs {
 	struct kobject kobj;
 	struct blk_mq_ctx __percpu	*queue_ctx;
@@ -18,10 +21,20 @@ struct blk_mq_ctxs {
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 添加的地方:
+		 *   - block/blk-mq.c|1808| <<__blk_mq_insert_req_list>> list_add(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1810| <<__blk_mq_insert_req_list>> list_add_tail(&rq->queuelist, &ctx->rq_lists[type]);
+		 *   - block/blk-mq.c|1861| <<blk_mq_insert_requests>> list_splice_tail_init(list, &ctx->rq_lists[type]);
+		 */
 		struct list_head	rq_lists[HCTX_MAX_TYPES];
 	} ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|2922| <<blk_mq_map_swqueue>> ctx->index_hw[hctx->type] = hctx->nr_ctx;
+	 */
 	unsigned short		index_hw[HCTX_MAX_TYPES];
 	struct blk_mq_hw_ctx 	*hctxs[HCTX_MAX_TYPES];
 
@@ -99,10 +112,29 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *
  * @flags: request command flags
  * @cpu: cpu ctx
  */
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|389| <<__blk_mq_sched_bio_merge>> struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, bio->bi_opf, ctx);
+ *   - block/blk-mq-tag.c|182| <<blk_mq_get_tag>> data->hctx = blk_mq_map_queue(data->q, data->cmd_flags,
+ *   - block/blk-mq.c|580| <<blk_mq_get_request>> data->hctx = blk_mq_map_queue(q, data->cmd_flags,
+ *   - block/blk.h|41| <<blk_get_flush_queue>> return blk_mq_map_queue(q, REQ_OP_FLUSH, ctx)->fq;
+ *
+ * 如果是REQ_HIPRI, 返回HCTX_TYPE_POLL
+ * 如果是REQ_OP_READ, 返回HCTX_TYPE_READ
+ * 其他的, 返回默认的HCTX_TYPE_DEFAULT
+ * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+ * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+ * 返回的仍然可以是HCTX_TYPE_DEFAULT
+ */
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 						     unsigned int flags,
 						     struct blk_mq_ctx *ctx)
 {
+	/*
+	 * HCTX_TYPE_DEFAULT = 0
+	 * HCTX_TYPE_READ    = 1
+	 * HCTX_TYPE_POLL    = 2
+	 */
 	enum hctx_type type = HCTX_TYPE_DEFAULT;
 
 	/*
@@ -112,7 +144,12 @@ static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		type = HCTX_TYPE_POLL;
 	else if ((flags & REQ_OP_MASK) == REQ_OP_READ)
 		type = HCTX_TYPE_READ;
-	
+
+	/*
+	 * ctx->hctxs[]中不用的type都被blk_mq_map_swqueue()给map到了默认的HCTX_TYPE_DEFAULT
+	 * 所以当不支持这个多type的时候, 即使输入是HCTX_TYPE_POLL或者HCTX_TYPE_READ
+	 * 返回的仍然可以是HCTX_TYPE_DEFAULT
+	 */
 	return ctx->hctxs[type];
 }
 
@@ -134,6 +171,12 @@ void blk_mq_release(struct request_queue *q);
  */
 static inline enum mq_rq_state blk_mq_rq_state(struct request *rq)
 {
+	/*
+	 * 返回的mq_rq_state类型:
+	 * - MQ_RQ_IDLE
+	 * - MQ_RQ_IN_FLIGHT
+	 * - MQ_RQ_COMPLETE
+	 */
 	return READ_ONCE(rq->state);
 }
 
@@ -149,6 +192,9 @@ static inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,
  * care about preemption, since we know the ctx's are persistent. This does
  * mean that we can't rely on ctx always matching the currently running CPU.
  */
+/*
+ * 会调用get_cpu()
+ */
 static inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)
 {
 	return __blk_mq_get_ctx(q, get_cpu());
@@ -171,14 +217,37 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|115| <<blk_mq_get_tag>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq-tag.c|179| <<blk_mq_get_tag>> tags = blk_mq_tags_from_data(data);
+ *   - block/blk-mq.c|297| <<blk_mq_rq_ctx_init>> struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+ *
+ * 如果参数的blk_mq_alloc_data->flags设置了BLK_MQ_REQ_INTERNAL,
+ * 则返回使用data->hctx->sched_tags
+ * 否则返回data->hctx->tags
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * BLK_MQ_REQ_INTERNAL在以下被设置:
+	 *   - block/blk-mq.c|372| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
 		return data->hctx->sched_tags;
 
 	return data->hctx->tags;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-sched.c|207| <<blk_mq_sched_dispatch_requests>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
+ *   - block/blk-mq.c|1833| <<__blk_mq_delay_run_hw_queue>> if (unlikely(blk_mq_hctx_stopped(hctx)))
+ *   - block/blk-mq.c|1907| <<blk_mq_run_hw_queues>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1928| <<blk_mq_queue_stopped>> if (blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|1991| <<blk_mq_start_stopped_hw_queue>> if (!blk_mq_hctx_stopped(hctx))
+ *   - block/blk-mq.c|2263| <<blk_mq_try_issue_directly>> if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q))) {
+ */
 static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 {
 	return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -193,6 +262,9 @@ unsigned int blk_mq_in_flight(struct request_queue *q, struct hd_struct *part);
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2]);
 
+/*
+ * 调用q->mq_ops->put_budget(hctx)
+ */
 static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -201,6 +273,9 @@ static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 		q->mq_ops->put_budget(hctx);
 }
 
+/*
+ * 调用q->mq_ops->get_budget(hctx)
+ */
 static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -210,18 +285,34 @@ static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 	return true;
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ */
 static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 					   struct request *rq)
 {
 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
 	rq->tag = -1;
 
+	/*
+	 * 是伴随着hctx->nr_active的, 每次设置的时候, hctx->nr_active就增长
+	 * 取消的时候, hctx->nr_active就减少
+	 *   - block/blk-mq.c|510| <<blk_mq_rq_ctx_init>> rq_flags = RQF_MQ_INFLIGHT;
+	 *   - block/blk-mq.c|750| <<blk_mq_free_request>> if (rq->rq_flags & RQF_MQ_INFLIGHT)
+	 *   - block/blk-mq.c|1427| <<blk_mq_get_driver_tag>> rq->rq_flags |= RQF_MQ_INFLIGHT;
+	 *   - block/blk-mq.h|288| <<__blk_mq_put_driver_tag>> if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+	 *   - block/blk-mq.h|289| <<__blk_mq_put_driver_tag>> rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+	 */
 	if (rq->rq_flags & RQF_MQ_INFLIGHT) {
 		rq->rq_flags &= ~RQF_MQ_INFLIGHT;
 		atomic_dec(&hctx->nr_active);
 	}
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ * 如果rq->tag和rq->internal_tag有一个已经是-1了就不用了
+ */
 static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
 {
@@ -231,6 +322,10 @@ static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 
+/*
+ * 把request->tag还回去, request->tag设成-1
+ * 如果rq->tag和rq->internal_tag有一个已经是-1了就不用了
+ */
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	if (rq->tag == -1 || rq->internal_tag == -1)
@@ -239,6 +334,13 @@ static inline void blk_mq_put_driver_tag(struct request *rq)
 	__blk_mq_put_driver_tag(rq->mq_hctx, rq);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq-pci.c|60| <<blk_mq_pci_map_queues>> blk_mq_clear_mq_map(qmap);
+ *   - block/blk-mq.c|3480| <<blk_mq_update_queue_map>> blk_mq_clear_mq_map(&set->map[i]);
+ *
+ * 把每个blk_mq_queue_map->mq_map[cpu]都设置成0
+ */
 static inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)
 {
 	int cpu;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 6375afa..a58b773 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -20,6 +20,10 @@ EXPORT_SYMBOL(blk_max_low_pfn);
 
 unsigned long blk_max_pfn;
 
+/*
+ * 被很多地方调用, 其中一个例子
+ *   - block/blk-mq.c|2849| <<blk_mq_init_allocated_queue>> blk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);
+ */
 void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)
 {
 	q->rq_timeout = timeout;
@@ -33,6 +37,11 @@ EXPORT_SYMBOL_GPL(blk_queue_rq_timeout);
  * Description:
  *   Returns a queue_limit struct to its default state.
  */
+/*
+ * called by:
+ *   - block/blk-settings.c|73| <<blk_set_stacking_limits>> blk_set_default_limits(lim);
+ *   - block/blk-settings.c|119| <<blk_queue_make_request>> blk_set_default_limits(&q->limits);
+ */
 void blk_set_default_limits(struct queue_limits *lim)
 {
 	lim->max_segments = BLK_MAX_SEGMENTS;
@@ -178,6 +187,13 @@ EXPORT_SYMBOL(blk_queue_bounce_limit);
  *    per-device basis in /sys/block/<device>/queue/max_sectors_kb.
  *    The soft limit can not exceed max_hw_sectors.
  **/
+/*
+ * 被很多调用, 这里是几个调用的例子:
+ *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+ *   - drivers/block/virtio_blk.c|827| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+ *   - drivers/nvme/host/core.c|1972| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+ *   - drivers/block/loop.c|1968| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+ */
 void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
 {
 	struct queue_limits *limits = &q->limits;
@@ -264,6 +280,14 @@ EXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);
  *    Enables a low level driver to set an upper limit on the number of
  *    hw data segments in a request.
  **/
+/*
+ * 选择的一些地方:
+ *   - drivers/block/virtio_blk.c|824| <<virtblk_probe>> blk_queue_max_segments(q, vblk->sg_elems-2);
+ *   - drivers/block/xen-blkfront.c|954| <<blkif_set_queue_limits>> blk_queue_max_segments(rq, segments / GRANTS_PER_PSEG);
+ *   - drivers/block/xen-blkfront.c|2027| <<blkif_recover>> blk_queue_max_segments(info->rq, segs / GRANTS_PER_PSEG);
+ *   - drivers/nvme/host/core.c|1973| <<nvme_set_queue_limits>> blk_queue_max_segments(q, min_t(u32, max_segments, USHRT_MAX));
+ *   - drivers/scsi/scsi_lib.c|1828| <<__scsi_init_queue>> blk_queue_max_segments(q, min_t(unsigned short , shost->sg_tablesize,
+ */
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
@@ -462,6 +486,11 @@ EXPORT_SYMBOL(blk_queue_io_opt);
  * @t:	the stacking driver (top)
  * @b:  the underlying device (bottom)
  **/
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_nl.c|1377| <<drbd_setup_queue_param>> blk_queue_stack_limits(q, b);
+ *   - drivers/nvme/host/core.c|1649| <<__nvme_revalidate_disk>> blk_queue_stack_limits(ns->head->disk->queue, ns->queue)
+ */
 void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b)
 {
 	blk_stack_limits(&t->limits, &b->limits, 0);
@@ -489,6 +518,12 @@ EXPORT_SYMBOL(blk_queue_stack_limits);
  *    queue_limits will have the misaligned flag set to indicate that
  *    the alignment_offset is undefined.
  */
+/*
+ * called by:
+ *   - block/blk-settings.c|483| <<blk_queue_stack_limits>> blk_stack_limits(&t->limits, &b->limits, 0);
+ *   - block/blk-settings.c|650| <<bdev_stack_limits>> return blk_stack_limits(t, &bq->limits, start);
+ *   - drivers/md/dm-table.c|1541| <<dm_calculate_queue_limits>> if (blk_stack_limits(limits, &ti_limits, 0) < 0)
+ */
 int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 		     sector_t start)
 {
@@ -716,6 +751,10 @@ EXPORT_SYMBOL(blk_queue_update_dma_pad);
  * this routine, you must set the limit to one fewer than your device
  * can support otherwise there won't be room for the drain buffer.
  */
+/*
+ * called only by:
+ *   - drivers/ata/libata-scsi.c|1290| <<ata_scsi_dev_config>> blk_queue_dma_drain(q, atapi_drain_needed, buf, ATAPI_MAX_DRAIN);
+ */
 int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 457d9ba..78878a1 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -95,6 +95,10 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|604| <<__blk_mq_complete_request>> __blk_complete_request(rq);
+ */
 void __blk_complete_request(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -136,6 +140,12 @@ void __blk_complete_request(struct request *req)
 		 * entries there, someone already raised the irq but it
 		 * hasn't run yet.
 		 */
+		/*
+		 * block/blk-softirq.c|154| <<blk_softirq_init>> open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
+		 *
+		 * 对于blk_cpu_done链表上的每一个request调用rq->q->mq_ops->complete(rq)
+		 * 并且删除这个request
+		 */
 		if (list->next == &req->ipi_list)
 			raise_softirq_irqoff(BLOCK_SOFTIRQ);
 	} else if (raise_blk_irq(ccpu, req))
diff --git a/block/blk-stat.c b/block/blk-stat.c
index 696a041..8ca76e2 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -17,6 +17,13 @@ struct blk_queue_stats {
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|197| <<latency_stat_init>> blk_rq_stat_init(&stat->rqs);
+ *   - block/blk-stat.c|85| <<blk_stat_timer_fn>> blk_rq_stat_init(&cb->stat[bucket]);
+ *   - block/blk-stat.c|93| <<blk_stat_timer_fn>> blk_rq_stat_init(&cpu_stat[bucket]);
+ *   - block/blk-stat.c|145| <<blk_stat_add_callback>> blk_rq_stat_init(&cpu_stat[bucket]);
+ */
 void blk_rq_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -25,6 +32,11 @@ void blk_rq_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by:
+ *   - block/blk-iolatency.c|208| <<latency_stat_sum>> blk_rq_stat_sum(&sum->rqs, &stat->rqs);
+ *   - block/blk-stat.c|92| <<blk_stat_timer_fn>> blk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);
+ */
 void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -39,6 +51,11 @@ void blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called by:
+ *   - block/blk-iolatency.c|220| <<latency_stat_record_time>> blk_rq_stat_add(&stat->rqs, req_time);
+ *   - block/blk-stat.c|72| <<blk_stat_add>> blk_rq_stat_add(stat, value);
+ */
 void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -47,6 +64,10 @@ void blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|627| <<__blk_mq_end_request>> blk_stat_add(rq, now);
+ */
 void blk_stat_add(struct request *rq, u64 now)
 {
 	struct request_queue *q = rq->q;
@@ -75,6 +96,10 @@ void blk_stat_add(struct request *rq, u64 now)
 	rcu_read_unlock();
 }
 
+/*
+ * used by:
+ *   - block/blk-stat.c|146| <<blk_stat_alloc_callback>> timer_setup(&cb->timer, blk_stat_timer_fn, 0);
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
@@ -97,6 +122,11 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3075| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ *   - block/blk-wbt.c|828| <<wbt_init>> rwb->cb = blk_stat_alloc_callback(wb_timer_fn, wbt_data_dir, 2, rwb);
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -131,6 +161,11 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 	return cb;
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|3579| <<blk_poll_stats_enable>> blk_stat_add_callback(q, q->poll_cb);
+ *   - block/blk-wbt.c|851| <<wbt_init>> blk_stat_add_callback(q, rwb->cb);
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -179,6 +214,11 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
 
+/*
+ * called by:
+ *   - block/blk-throttle.c|2450| <<blk_throtl_register_queue>> blk_stat_enable_accounting(q);
+ *   - block/kyber-iosched.c|442| <<kyber_init_sched>> blk_stat_enable_accounting(q);
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -188,6 +228,10 @@ void blk_stat_enable_accounting(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_stat_enable_accounting);
 
+/*
+ * called by:
+ *   - block/blk-core.c|500| <<blk_alloc_queue_node>> q->stats = blk_alloc_queue_stats();
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -203,6 +247,11 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|546| <<blk_alloc_queue_node>> blk_free_queue_stats(q->stats);
+ *   - block/blk-sysfs.c|857| <<__blk_release_queue>> blk_free_queue_stats(q->stats);
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index 5968591..8d65a92 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -872,6 +872,33 @@ static void __blk_release_queue(struct work_struct *work)
 	call_rcu(&q->rcu_head, blk_free_queue_rcu);
 }
 
+/*
+ * 一个例子: echo 1 > /sys/devices/pci0000\:00/0000\:00\:04.0/remove
+ * [0] CPU: 3 PID: 2634 Comm: bash Not tainted 5.0.0+ #3
+ * [0] blk_release_queue
+ * [0] kobject_put
+ * [0] disk_release
+ * [0] device_release
+ * [0] kobject_put
+ * [0] virtblk_remove
+ * [0] virtio_dev_remove
+ * [0] device_release_driver_internal
+ * [0] bus_remove_device
+ * [0] device_del
+ * [0] device_unregister
+ * [0] unregister_virtio_device
+ * [0] virtio_pci_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] pci_stop_bus_device
+ * [0] pci_stop_and_remove_bus_device_locked
+ * [0] remove_store
+ * [0] kernfs_fop_write
+ * [0] vfs_write
+ * [0] ksys_write
+ * [0] do_syscall_64
+ * [0] entry_SYSCALL_64_after_hwframe
+ */
 static void blk_release_queue(struct kobject *kobj)
 {
 	struct request_queue *q =
@@ -896,6 +923,11 @@ struct kobj_type blk_queue_ktype = {
  * blk_register_queue - register a block layer queue with sysfs
  * @disk: Disk of which the request queue should be registered with sysfs.
  */
+/*
+ * called by:
+ *   - block/genhd.c|725| <<__device_add_disk>> blk_register_queue(disk);
+ *   - drivers/md/dm.c|2270| <<dm_setup_md_queue>> blk_register_queue(md->disk);
+ */
 int blk_register_queue(struct gendisk *disk)
 {
 	int ret;
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 124c261..bf1cef2 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -108,6 +108,11 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|839| <<blk_mq_start_request>> blk_add_timer(rq);
+ *   - block/blk-mq.c|1009| <<blk_mq_rq_timed_out>> blk_add_timer(req);
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -143,6 +148,15 @@ void blk_add_timer(struct request *req)
 		 * modifying the timer because expires for value X
 		 * will be X + something.
 		 */
+		/*
+		 * 注意! 这里的q是request_queue, q->timeout应该是
+		 *
+		 * q->timeout设置的地方:
+		 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+		 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+		 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+		 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+		 */
 		if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
 			mod_timer(&q->timeout, expiry);
 	}
diff --git a/block/blk.h b/block/blk.h
index 5d636ee..73016c3 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -55,6 +55,9 @@ void blk_rq_bio_prep(struct request_queue *q, struct request *rq,
 			struct bio *bio);
 void blk_freeze_queue(struct request_queue *q);
 
+/*
+ * 相当于: percpu_ref_get(&q->q_usage_counter);
+ */
 static inline void blk_queue_enter_live(struct request_queue *q)
 {
 	/*
diff --git a/block/genhd.c b/block/genhd.c
index 7032678..3e01ba0 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -913,6 +913,11 @@ EXPORT_SYMBOL(bdget_disk);
  * filesystem can't be mounted and thus to give the victim some idea of what
  * went wrong
  */
+/*
+ * called by:
+ *   - init/do_mounts.c|437| <<mount_block_root>> printk_all_partitions();
+ *   - init/do_mounts.c|450| <<mount_block_root>> printk_all_partitions();
+ */
 void __init printk_all_partitions(void)
 {
 	struct class_dev_iter iter;
diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c
index 07bf2bf..4fd31ab 100644
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@ -71,6 +71,10 @@ MODULE_PARM_DESC(streams, "turn on support for Streams write directives");
  * serialization purposes. nvme_delete_wq host controller deletion
  * works which flush reset works for serialization.
  */
+/*
+ * 在以下初始化:
+ *   - drivers/nvme/host/core.c|3940| <<nvme_core_init>> nvme_wq = alloc_workqueue("nvme-wq",
+ */
 struct workqueue_struct *nvme_wq;
 EXPORT_SYMBOL_GPL(nvme_wq);
 
@@ -94,6 +98,11 @@ static void nvme_put_subsystem(struct nvme_subsystem *subsys);
 static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
 					   unsigned nsid);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1295| <<nvme_update_formats>> nvme_set_queue_dying(ns);
+ *   - drivers/nvme/host/core.c|3815| <<nvme_kill_queues>> nvme_set_queue_dying(ns);
+ */
 static void nvme_set_queue_dying(struct nvme_ns *ns)
 {
 	/*
@@ -105,6 +114,13 @@ static void nvme_set_queue_dying(struct nvme_ns *ns)
 	revalidate_disk(ns->disk);
 	blk_set_queue_dying(ns->queue);
 	/* Forcibly unquiesce queues to avoid blocking dispatch */
+	/*
+	 * 把request_queue->queue_flags清除QUEUE_FLAG_QUIESCED
+	 * 然后用blk_mq_run_hw_queues()来dispatch requests which are inserted during quiescing
+	 *
+	 * flush them through the low level driver's queue_rq().
+	 * 把queue中剩下的request刷一下
+	 */
 	blk_mq_unquiesce_queue(ns->queue);
 }
 
@@ -117,6 +133,21 @@ static void nvme_queue_scan(struct nvme_ctrl *ctrl)
 		queue_work(nvme_wq, &ctrl->scan_work);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|134| <<nvme_reset_ctrl_sync>> ret = nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|976| <<nvme_keep_alive_work>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/core.c|3598| <<nvme_fw_act_work>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/fc.c|770| <<nvme_fc_ctrl_connectivity_loss>> if (nvme_reset_ctrl(&ctrl->ctrl)) {
+ *   - drivers/nvme/host/fc.c|2074| <<nvme_fc_error_recovery>> nvme_reset_ctrl(&ctrl->ctrl);
+ *   - drivers/nvme/host/multipath.c|84| <<nvme_failover_req>> nvme_reset_ctrl(ns->ctrl);
+ *   - drivers/nvme/host/multipath.c|501| <<nvme_anatt_timeout>> nvme_reset_ctrl(ctrl);
+ *   - drivers/nvme/host/pci.c|1299| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|1342| <<nvme_timeout>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2993| <<nvme_resume>> nvme_reset_ctrl(&ndev->ctrl);
+ *   - drivers/nvme/host/pci.c|3032| <<nvme_slot_reset>> nvme_reset_ctrl(&dev->ctrl);
+ *   - drivers/nvme/target/loop.c|138| <<nvme_loop_timeout>> nvme_reset_ctrl(&iod->queue->ctrl->ctrl);
+ */
 int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
 {
 	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
@@ -282,6 +313,17 @@ void nvme_complete_rq(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_complete_rq);
 
+/*
+ * 全部都是用blk_mq_tagset_busy_iter()在调用这个函数:
+ *   - drivers/nvme/host/pci.c|2554| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2555| <<nvme_dev_disable>> blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+ *   - drivers/nvme/host/rdma.c|917| <<nvme_rdma_teardown_admin_queue>> blk_mq_tagset_busy_iter(&ctrl->admin_tag_set, nvme_cancel_request,
+ *   - drivers/nvme/host/rdma.c|929| <<nvme_rdma_teardown_io_queues>> blk_mq_tagset_busy_iter(&ctrl->tag_set, nvme_cancel_request,
+ *   - drivers/nvme/host/tcp.c|1689| <<nvme_tcp_teardown_admin_queue>> blk_mq_tagset_busy_iter(ctrl->admin_tagset, nvme_cancel_request, ctrl);
+ *   - drivers/nvme/host/tcp.c|1701| <<nvme_tcp_teardown_io_queues>> blk_mq_tagset_busy_iter(ctrl->tagset, nvme_cancel_request, ctrl);
+ *   - drivers/nvme/target/loop.c|425| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ *   - drivers/nvme/target/loop.c|434| <<nvme_loop_shutdown_ctrl>> nvme_cancel_request, &ctrl->ctrl);
+ */
 bool nvme_cancel_request(struct request *req, void *data, bool reserved)
 {
 	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
@@ -701,6 +743,14 @@ void nvme_cleanup_cmd(struct request *req)
 }
 EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|2303| <<nvme_fc_queue_rq>> ret = nvme_setup_cmd(ns, rq, sqe);
+ *   - drivers/nvme/host/pci.c|956| <<nvme_queue_rq>> ret = nvme_setup_cmd(ns, req, &cmnd);
+ *   - drivers/nvme/host/rdma.c|1737| <<nvme_rdma_queue_rq>> ret = nvme_setup_cmd(ns, rq, c);
+ *   - drivers/nvme/host/tcp.c|1997| <<nvme_tcp_setup_cmd_pdu>> ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+ *   - drivers/nvme/target/loop.c|159| <<nvme_loop_queue_rq>> ret = nvme_setup_cmd(ns, req, &iod->cmd);
+ */
 blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
 		struct nvme_command *cmd)
 {
@@ -3710,6 +3760,14 @@ static void nvme_free_ctrl(struct device *dev)
  * earliest initialization so that we have the initialized structured around
  * during probing.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/fc.c|3077| <<nvme_fc_init_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+ *   - drivers/nvme/host/pci.c|3070| <<nvme_probe>> result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+ *   - drivers/nvme/host/tcp.c|2195| <<nvme_tcp_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+ *   - drivers/nvme/target/loop.c|595| <<nvme_loop_create_ctrl>> ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,
+ */
 int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
 		const struct nvme_ctrl_ops *ops, unsigned long quirks)
 {
@@ -3790,6 +3848,12 @@ EXPORT_SYMBOL_GPL(nvme_init_ctrl);
  * Call this function when the driver determines it is unable to get the
  * controller in a state capable of servicing IO.
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|3543| <<nvme_remove_namespaces>> nvme_kill_queues(ctrl);
+ *   - drivers/nvme/host/pci.c|2630| <<nvme_remove_dead_ctrl>> nvme_kill_queues(&dev->ctrl);
+ *   - drivers/nvme/host/pci.c|2722| <<nvme_reset_work>> nvme_kill_queues(&dev->ctrl);
+ */
 void nvme_kill_queues(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
@@ -3843,6 +3907,11 @@ void nvme_wait_freeze(struct nvme_ctrl *ctrl)
 }
 EXPORT_SYMBOL_GPL(nvme_wait_freeze);
 
+/*
+ * called by:
+ *   - drivers/nvme/host/core.c|1271| <<nvme_passthru_start>> nvme_start_freeze(ctrl);
+ *   - drivers/nvme/host/pci.c|2463| <<nvme_dev_disable>> nvme_start_freeze(&dev->ctrl);
+ */
 void nvme_start_freeze(struct nvme_ctrl *ctrl)
 {
 	struct nvme_ns *ns;
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index b91f183..7dd1342 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -173,6 +173,11 @@ struct nvme_ctrl {
 
 	u32 ctrl_config;
 	u16 mtfa;
+	/*
+	 * 在以下修改:
+	 *   - drivers/nvme/host/pci.c|1399| <<nvme_free_queues>> dev->ctrl.queue_count--;
+	 *   - drivers/nvme/host/pci.c|1517| <<nvme_alloc_queue>> dev->ctrl.queue_count++;
+	 */
 	u32 queue_count;
 
 	u64 cap;
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index f54718b..bb8a156 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -39,6 +39,12 @@
 #define NVME_MAX_KB_SZ	4096
 #define NVME_MAX_SEGS	127
 
+/*
+ * 没有修改的地方, 只能通过参数
+ * 在queue_request_irq()设置中断函数的时候:
+ *   use_threaded_interrupts = 1 --> nvme_irq_check()
+ *   use_threaded_interrupts = 0 --> nvme_irq();
+ */
 static int use_threaded_interrupts;
 module_param(use_threaded_interrupts, int, 0);
 
@@ -46,6 +52,11 @@ static bool use_cmb_sqes = true;
 module_param(use_cmb_sqes, bool, 0444);
 MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
 
+/*
+ * used by only:
+ *   - drivers/nvme/host/pci.c|2083| <<nvme_setup_host_mem>> u64 max = (u64)max_host_mem_size_mb * SZ_1M;
+ *   - drivers/nvme/host/pci.c|2093| <<nvme_setup_host_mem>> min >> ilog2(SZ_1M), max_host_mem_size_mb);
+ */
 static unsigned int max_host_mem_size_mb = 128;
 module_param(max_host_mem_size_mb, uint, 0444);
 MODULE_PARM_DESC(max_host_mem_size_mb,
@@ -63,6 +74,10 @@ static const struct kernel_param_ops io_queue_depth_ops = {
 	.get = param_get_int,
 };
 
+/*
+ * used by only:
+ *   - drivers/nvme/host/pci.c|2504| <<nvme_pci_enable>> io_queue_depth);
+ */
 static int io_queue_depth = 1024;
 module_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);
 MODULE_PARM_DESC(io_queue_depth, "set io queue depth, should >= 2");
@@ -73,12 +88,22 @@ static const struct kernel_param_ops queue_count_ops = {
 	.get = param_get_int,
 };
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|286| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2138| <<nvme_calc_io_queues>> unsigned int this_w_queues = write_queues;
+ */
 static int write_queues;
 module_param_cb(write_queues, &queue_count_ops, &write_queues, 0644);
 MODULE_PARM_DESC(write_queues,
 	"Number of queues to use for writes. If not set, reads and writes "
 	"will share a queue set.");
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|286| <<max_io_queues>> return num_possible_cpus() + write_queues + poll_queues;
+ *   - drivers/nvme/host/pci.c|2192| <<nvme_setup_irqs>> this_p_queues = poll_queues;
+ */
 static int poll_queues = 0;
 module_param_cb(poll_queues, &queue_count_ops, &poll_queues, 0644);
 MODULE_PARM_DESC(poll_queues, "Number of queues to use for polled IO.");
@@ -100,10 +125,32 @@ struct nvme_dev {
 	struct device *dev;
 	struct dma_pool *prp_page_pool;
 	struct dma_pool *prp_small_pool;
+	/*
+	 * 在以下被修改:
+	 *   - drivers/nvme/host/pci.c|1410| <<nvme_suspend_queue>> nvmeq->dev->online_queues--;
+	 *   - drivers/nvme/host/pci.c|1547| <<nvme_init_queue>> dev->online_queues++;
+	 *   - drivers/nvme/host/pci.c|1592| <<nvme_create_queue>> dev->online_queues--;
+	 */
 	unsigned online_queues;
+	/*
+	 * 对于pci就在一处修改:
+	 *   - drivers/nvme/host/pci.c|2252| <<nvme_setup_io_queues>> dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
+	 */
 	unsigned max_qid;
 	unsigned io_queues[HCTX_MAX_TYPES];
+	/*
+	 * 只在以下设置:
+	 *   - drivers/nvme/host/pci.c|2394| <<nvme_setup_io_queues>> dev->num_vecs = result;
+	 *     result来自nvme_setup_irqs()
+	 */
 	unsigned int num_vecs;
+	/*
+	 * 设置的地方:
+	 *   - drivers/nvme/host/pci.c|2327| <<nvme_setup_io_queues>> dev->q_depth = result;
+	 *   - drivers/nvme/host/pci.c|2550| <<nvme_pci_enable>> dev->q_depth = min_t(int , NVME_CAP_MQES(dev->ctrl.cap) + 1,
+	 *   - drivers/nvme/host/pci.c|2560| <<nvme_pci_enable>> dev->q_depth = 2;
+	 *   - drivers/nvme/host/pci.c|2567| <<nvme_pci_enable>> dev->q_depth = 64;
+	 */
 	int q_depth;
 	u32 db_stride;
 	void __iomem *bar;
@@ -112,11 +159,21 @@ struct nvme_dev {
 	struct mutex shutdown_lock;
 	bool subsystem;
 	u64 cmb_size;
+	/*
+	 * 在以下设置:
+	 *   - drivers/nvme/host/pci.c|2017| <<nvme_map_cmb>> dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+	 *   - drivers/nvme/host/pci.c|2361| <<nvme_setup_io_queues>> if (dev->cmb_use_sqes) {
+	 *   - drivers/nvme/host/pci.c|2367| <<nvme_setup_io_queues>> dev->cmb_use_sqes = false;
+	 */
 	bool cmb_use_sqes;
 	u32 cmbsz;
 	u32 cmbloc;
 	struct nvme_ctrl ctrl;
 
+	/*
+	 * 在以下分配:
+	 *   - drivers/nvme/host/pci.c|3061| <<nvme_probe>> dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+	 */
 	mempool_t *iod_mempool;
 
 	/* shadow doorbell buffer support: */
@@ -176,27 +233,95 @@ static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
  * An NVM Express queue.  Each device has at least two (one for admin
  * commands and one for I/O commands).
  */
+/*
+ * 下发到sq写入tail
+ * 从cq接收写入head
+ */
 struct nvme_queue {
 	struct device *q_dmadev;
 	struct nvme_dev *dev;
 	spinlock_t sq_lock;
+	/*
+	 * 根据是否使用cmb在两处分配在两处分配, sq的ring buffer:
+	 *   - drivers/nvme/host/pci.c|1544| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(depth));
+	 *   - drivers/nvme/host/pci.c|1553| <<nvme_alloc_sq_cmds>> nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
+	 */
 	struct nvme_command *sq_cmds;
 	 /* only used for poll queues: */
 	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
+	/*
+	 * 在以下分配:
+	 *   - drivers/nvme/host/pci.c|1593| <<nvme_alloc_queue>> nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(depth),
+	 */
 	volatile struct nvme_completion *cqes;
+	/*
+	 * pci在以下设置:
+	 *   - drivers/nvme/host/pci.c|492| <<nvme_admin_init_hctx>> nvmeq->tags = &dev->admin_tagset.tags[0];
+	 *   - drivers/nvme/host/pci.c|500| <<nvme_admin_exit_hctx>> nvmeq->tags = NULL;
+	 *   - drivers/nvme/host/pci.c|510| <<nvme_init_hctx>> nvmeq->tags = &dev->tagset.tags[hctx_idx];
+	 */
 	struct blk_mq_tags **tags;
+	/*
+	 * 根据是否使用cmb在两处分配:
+	 *   - drivers/nvme/host/pci.c|1559| <<nvme_alloc_sq_cmds>> nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+	 *   - drivers/nvme/host/pci.c|1568| <<nvme_alloc_sq_cmds>> &nvmeq->sq_dma_addr, GFP_KERNEL);
+	 */
 	dma_addr_t sq_dma_addr;
+	/*
+	 * 在以下分配:
+	 *   - drivers/nvme/host/pci.c|1594| <<nvme_alloc_queue>> &nvmeq->cq_dma_addr, GFP_KERNEL);
+	 */
 	dma_addr_t cq_dma_addr;
+	/*
+	 * 在以下被设置:
+	 *   - drivers/nvme/host/pci.c|1621| <<nvme_alloc_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|1663| <<nvme_init_queue>> nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	 *   - drivers/nvme/host/pci.c|2340| <<nvme_setup_io_queues>> adminq->q_db = dev->dbs;
+	 */
 	u32 __iomem *q_db;
+	/*
+	 * 在以下被设置:
+	 *   - drivers/nvme/host/pci.c|1622| <<nvme_alloc_queue>> nvmeq->q_depth = depth;
+	 */
 	u16 q_depth;
+	/*
+	 * 在一下设置, admin的vector永远是0吧:
+	 *   - drivers/nvme/host/pci.c|1541| <<nvme_suspend_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|1646| <<nvme_alloc_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|1723| <<nvme_create_queue>> nvmeq->cq_vector = vector;
+	 *   - drivers/nvme/host/pci.c|1736| <<nvme_create_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|1874| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = 0;
+	 *   - drivers/nvme/host/pci.c|1878| <<nvme_pci_configure_admin_queue>> nvmeq->cq_vector = -1;
+	 *   - drivers/nvme/host/pci.c|2390| <<nvme_setup_io_queues>> adminq->cq_vector = -1;
+	 */
 	s16 cq_vector;
+
+	/*
+	 * 下发到sq写入tail
+	 * 从cq接收写入head
+	 */
 	u16 sq_tail;
 	u16 last_sq_tail;
 	u16 cq_head;
 	u16 last_cq_head;
 	u16 qid;
+	/*
+	 * 设置的地方:
+	 *   - drivers/nvme/host/pci.c|1154| <<nvme_update_cq_head>> nvmeq->cq_phase = !nvmeq->cq_phase;
+	 *   - drivers/nvme/host/pci.c|1652| <<nvme_alloc_queue>> nvmeq->cq_phase = 1;
+	 *   - drivers/nvme/host/pci.c|1694| <<nvme_init_queue>> nvmeq->cq_phase = 1;
+	 */
 	u8 cq_phase;
 	unsigned long flags;
+/*
+ * 使用的地方:
+ *   - drivers/nvme/host/pci.c|935| <<nvme_queue_rq>> if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+ *   - drivers/nvme/host/pci.c|1414| <<nvme_suspend_queue>> if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+ *   - drivers/nvme/host/pci.c|1613| <<nvme_create_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|1763| <<nvme_pci_configure_admin_queue>> set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+ *   - drivers/nvme/host/pci.c|2224| <<nvme_setup_io_queues>> clear_bit(NVMEQ_ENABLED, &adminq->flags);
+ *   - drivers/nvme/host/pci.c|2274| <<nvme_setup_io_queues>> set_bit(NVMEQ_ENABLED, &adminq->flags);
+ */
 #define NVMEQ_ENABLED		0
 #define NVMEQ_SQ_CMB		1
 #define NVMEQ_DELETE_ERROR	2
@@ -398,6 +523,11 @@ static int nvme_pci_npages_sgl(unsigned int num_seg)
 	return DIV_ROUND_UP(num_seg * sizeof(struct nvme_sgl_desc), PAGE_SIZE);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|537| <<nvme_pci_cmd_size>> unsigned int alloc_size = nvme_pci_iod_alloc_size(dev,
+ *   - drivers/nvme/host/pci.c|3038| <<nvme_probe>> alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+ */
 static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
 		unsigned int size, unsigned int nseg, bool use_sgl)
 {
@@ -537,6 +667,11 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
  * @cmd: The command to send
  * @write_sq: whether to write to the SQ doorbell
  */
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1013| <<nvme_queue_rq>> nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+ *   - drivers/nvme/host/pci.c|1201| <<nvme_pci_submit_async_event>> nvme_submit_cmd(nvmeq, &c, true);
+ */
 static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
@@ -922,6 +1057,15 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	 * We should not need to do this, but we're still using this to
 	 * ensure we can drain requests on a dying queue.
 	 */
+	/*
+	 * 这里没什么用处吧, xen-blkfront也会有这样的问题但是没有类似的代码
+	 * The nvme driver checked the queue state on every IO so this path could
+	 * be used to flush out entered requests to a failed completion.
+	 *
+	 * 其实可以Use the blk-mq's tag iterator to end all entered requests when the queue
+	 * isn't going to be restarted so the IO path doesn't have to deal with
+	 * these conditions.
+	 */
 	if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
 		return BLK_STS_IOERR;
 
@@ -939,6 +1083,23 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 			goto out_cleanup_iod;
 	}
 
+	/*
+	 * 关于下发request到硬件驱动时的timeout:
+	 *
+	 * 比如virtio_queue_rq()调用blk_mq_start_request()
+	 * 然后用blk_add_timer()mod上request_queue->timeout=blk_rq_timed_out_timer()
+	 *
+	 * 如果timer触发了, blk_rq_timed_out_timer()会通过kblockd_schedule_work(&q->timeout_work)
+	 * 调用可能初始化为blk_mq_timeout_work()或者blk_timeout_work()的request_queue->timeout_work
+	 *
+	 * blk_mq_timeout_work()用blk_mq_check_expire()查看每一个正在下发的request,
+	 * blk_mq_check_expired()-->blk_mq_rq_timed_out()-->req->q->mq_ops->timeout(req, reserved)
+	 *              
+	 * - nvme的例子是nvme_timeout()
+	 * - scsi的例子是scsi_timeout()
+	 *      
+	 * 设置request->state为MQ_RQ_IN_FLIGHT
+	 */
 	blk_mq_start_request(req);
 	nvme_submit_cmd(nvmeq, &cmnd, bd->last);
 	return BLK_STS_OK;
@@ -1461,6 +1622,10 @@ static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
 	return q_depth;
 }
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|1550| <<nvme_alloc_queue>> if (nvme_alloc_sq_cmds(dev, nvmeq, qid, depth))
+ */
 static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 				int qid, int depth)
 {
@@ -1483,6 +1648,13 @@ static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1728| <<nvme_pci_configure_admin_queue>> result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+ *   - drivers/nvme/host/pci.c|1766| <<nvme_create_io_queues>> if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+ *
+ * 初始化nvme_dev->queues[qid]并且nvme_dev->ctrl.queue_count++
+ */
 static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
 {
 	struct nvme_queue *nvmeq = &dev->queues[qid];
@@ -1533,6 +1705,11 @@ static int queue_request_irq(struct nvme_queue *nvmeq)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1579| <<nvme_create_queue>> nvme_init_queue(nvmeq, qid);
+ *   - drivers/nvme/host/pci.c|1730| <<nvme_pci_configure_admin_queue>> nvme_init_queue(nvmeq, 0);
+ */
 static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1548,6 +1725,10 @@ static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
 	wmb(); /* ensure the first interrupt sees the initialization */
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|1764| <<nvme_create_io_queues>> ret = nvme_create_queue(&dev->queues[i], i, polled);
+ */
 static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
 {
 	struct nvme_dev *dev = nvmeq->dev;
@@ -1668,6 +1849,12 @@ static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
 	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1870| <<nvme_pci_configure_admin_queue>> result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+ *   - drivers/nvme/host/pci.c|2383| <<nvme_setup_io_queues>> result = nvme_remap_bar(dev, size);
+ *   - drivers/nvme/host/pci.c|2948| <<nvme_dev_map>> if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+ */
 static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -1738,12 +1925,29 @@ static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
 	return result;
 }
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|2228| <<nvme_setup_io_queues>> result = nvme_create_io_queues(dev);
+ */
 static int nvme_create_io_queues(struct nvme_dev *dev)
 {
 	unsigned i, max, rw_queues;
 	int ret = 0;
 
+	/*
+	 * 修改queue_count的地方:
+	 *   - drivers/nvme/host/pci.c|1399| <<nvme_free_queues>> dev->ctrl.queue_count--;
+	 *   - drivers/nvme/host/pci.c|1517| <<nvme_alloc_queue>> dev->ctrl.queue_count++;
+	 *
+	 * 有两处nvme_pci_configure_admin_queue()和nvme_create_io_queues()
+	 * 会调用nvme_alloc_queue()来增加dev->ctrl.queue_count
+	 *
+	 * 如果此时admin queue已经有了, dev->ctrl.queue_count是1, 否则是0吧
+	 */
 	for (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {
+		/*
+		 * 初始化nvme_dev->queues[qid]并且nvme_dev->ctrl.queue_count++
+		 */
 		if (nvme_alloc_queue(dev, i, dev->q_depth)) {
 			ret = -ENOMEM;
 			break;
@@ -2072,6 +2276,10 @@ static void nvme_calc_io_queues(struct nvme_dev *dev, unsigned int irq_queues)
 	}
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2246| <<nvme_setup_io_queues>> result = nvme_setup_irqs(dev, nr_io_queues);
+ */
 static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2150,13 +2358,22 @@ static void nvme_disable_io_queues(struct nvme_dev *dev)
 		__nvme_disable_io_queues(dev, nvme_admin_delete_cq);
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/pci.c|2627| <<nvme_reset_work>> result = nvme_setup_io_queues(dev);
+ */
 static int nvme_setup_io_queues(struct nvme_dev *dev)
 {
+	/*
+	 * queues最早在以下分配:
+	 *   - drivers/nvme/host/pci.c|2785| <<nvme_probe>> dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
+	 */
 	struct nvme_queue *adminq = &dev->queues[0];
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
 	int result, nr_io_queues;
 	unsigned long size;
 
+	/* num_possible_cpus() + write_queues + poll_queues */
 	nr_io_queues = max_io_queues();
 	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
 	if (result < 0)
@@ -2312,6 +2529,10 @@ static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
 /*
  * return error value only when tagset allocation failed
  */
+/*
+ * called only by:
+ *   - drivers/nvme/host/pci.c|2700| <<nvme_reset_work>> if (nvme_dev_add(dev))
+ */
 static int nvme_dev_add(struct nvme_dev *dev)
 {
 	int ret;
@@ -2319,6 +2540,13 @@ static int nvme_dev_add(struct nvme_dev *dev)
 	if (!dev->ctrl.tagset) {
 		dev->tagset.ops = &nvme_mq_ops;
 		dev->tagset.nr_hw_queues = dev->online_queues - 1;
+		/*
+		 * nvme设置nr_maps的地方:
+		 *   - drivers/nvme/host/pci.c|2378| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+		 *   - drivers/nvme/host/pci.c|2380| <<nvme_dev_add>> dev->tagset.nr_maps++;
+		 *   - drivers/nvme/host/rdma.c|742| <<nvme_rdma_alloc_tagset>> set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+		 *   - drivers/nvme/host/tcp.c|1441| <<nvme_tcp_alloc_tagset>> set->nr_maps = 2 ;
+		 */
 		dev->tagset.nr_maps = 2; /* default + read */
 		if (dev->io_queues[HCTX_TYPE_POLL])
 			dev->tagset.nr_maps++;
@@ -2435,6 +2663,20 @@ static void nvme_pci_disable(struct nvme_dev *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|1288| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1315| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|1331| <<nvme_timeout>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2530| <<nvme_remove_dead_ctrl>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2552| <<nvme_reset_work>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2839| <<nvme_reset_prepare>> nvme_dev_disable(dev, false);
+ *   - drivers/nvme/host/pci.c|2851| <<nvme_shutdown>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2868| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2875| <<nvme_remove>> nvme_dev_disable(dev, true);
+ *   - drivers/nvme/host/pci.c|2892| <<nvme_suspend>> nvme_dev_disable(ndev, true);
+ *   - drivers/nvme/host/pci.c|2924| <<nvme_error_detected>> nvme_dev_disable(dev, false);
+ */
 static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 {
 	bool dead = true;
@@ -2470,6 +2712,9 @@ static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
 	nvme_suspend_queue(&dev->queues[0]);
 	nvme_pci_disable(dev);
 
+	/*
+	 * 针对每一个tagset->tags[tagset->nr_hw_queues].sb
+	 */
 	blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
 	blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
 
@@ -2533,6 +2778,17 @@ static void nvme_remove_dead_ctrl(struct nvme_dev *dev, int status)
 		nvme_put_ctrl(&dev->ctrl);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/pci.c|2902| <<nvme_probe>> INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+ *
+ * called by:
+ *   - drivers/nvme/host/core.c|151| <<nvme_reset_ctrl>> if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+ *   - drivers/nvme/host/core.c|163| <<nvme_reset_ctrl_sync>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/core.c|178| <<nvme_do_delete_ctrl>> flush_work(&ctrl->reset_work);
+ *   - drivers/nvme/host/pci.c|3002| <<nvme_remove>> flush_work(&dev->ctrl.reset_work);
+ *   - drivers/nvme/host/pci.c|3078| <<nvme_error_resume>> flush_work(&dev->ctrl.reset_work);
+ */
 static void nvme_reset_work(struct work_struct *work)
 {
 	struct nvme_dev *dev =
@@ -2697,6 +2953,10 @@ static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
 	.get_address		= nvme_pci_get_address,
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/pci.c|3020| <<nvme_probe>> result = nvme_dev_map(dev);
+ */
 static int nvme_dev_map(struct nvme_dev *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev->dev);
@@ -2744,6 +3004,10 @@ static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
 	return 0;
 }
 
+/*
+ * used by only:
+ *   - drivers/nvme/host/pci.c|3078| <<nvme_probe>> async_schedule(nvme_async_probe, dev);
+ */
 static void nvme_async_probe(void *data, async_cookie_t cookie)
 {
 	struct nvme_dev *dev = data;
@@ -2768,6 +3032,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	if (!dev)
 		return -ENOMEM;
 
+	/*
+	 * 在hotplug的情况下max_queue_count()的数量就有些多了
+	 */
 	dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
 					GFP_KERNEL, node);
 	if (!dev->queues)
@@ -2776,6 +3043,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	dev->dev = get_device(&pdev->dev);
 	pci_set_drvdata(pdev, dev);
 
+	/* 只在这里被调用 */
 	result = nvme_dev_map(dev);
 	if (result)
 		goto put_pci;
@@ -2784,6 +3052,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
 	mutex_init(&dev->shutdown_lock);
 
+	/* 只在这里被调用 */
 	result = nvme_setup_prp_pools(dev);
 	if (result)
 		goto unmap;
@@ -2798,6 +3067,9 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 						NVME_MAX_SEGS, true);
 	WARN_ON_ONCE(alloc_size > PAGE_SIZE);
 
+	/*
+	 * 这里分配的size是上面最大的NVME_MAX_KB_SZ和NVME_MAX_SEGS
+	 */
 	dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
 						mempool_kfree,
 						(void *) alloc_size,
@@ -2807,6 +3079,7 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 		goto release_pools;
 	}
 
+	/* 在pci上只被这里调用 */
 	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
 			quirks);
 	if (result)
@@ -2815,6 +3088,11 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	dev_info(dev->ctrl.device, "pci function %s\n", dev_name(&pdev->dev));
 
 	nvme_get_ctrl(&dev->ctrl);
+	/*
+	 * schedule a function for asynchronous execution
+	 *
+	 * nvme_async_probe()只在这里被调用
+	 */
 	async_schedule(nvme_async_probe, dev);
 
 	return 0;
@@ -2833,12 +3111,18 @@ static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
 	return result;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_prepare = nvme_reset_prepare()
+ */
 static void nvme_reset_prepare(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
 	nvme_dev_disable(dev, false);
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.reset_done = nvme_reset_done()
+ */
 static void nvme_reset_done(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2856,6 +3140,17 @@ static void nvme_shutdown(struct pci_dev *pdev)
  * state. This function must not have any dependencies on the device state in
  * order to proceed.
  */
+/*
+ * 调用的一个例子:
+ * [0] nvme_remove
+ * [0] pci_device_remove
+ * [0] device_release_driver_internal
+ * [0] nvme_remove_dead_ctrl_work
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 static void nvme_remove(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2905,6 +3200,9 @@ static int nvme_resume(struct device *dev)
 
 static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
 
+/*
+ * struct pci_error_handlers nvme_err_handler.error_detected = nvme_error_detected()
+ */
 static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 						pci_channel_state_t state)
 {
@@ -2931,6 +3229,9 @@ static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
 	return PCI_ERS_RESULT_NEED_RESET;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.slot_reset = nvme_slot_reset()
+ */
 static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2941,6 +3242,9 @@ static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
 	return PCI_ERS_RESULT_RECOVERED;
 }
 
+/*
+ * struct pci_error_handlers nvme_err_handler.error_resume = nvme_error_resume()
+ */
 static void nvme_error_resume(struct pci_dev *pdev)
 {
 	struct nvme_dev *dev = pci_get_drvdata(pdev);
@@ -2956,6 +3260,10 @@ static const struct pci_error_handlers nvme_err_handler = {
 	.reset_done	= nvme_reset_done,
 };
 
+/*
+ * 最后一个field是quirk
+ * 在代码中可以用ns->ctrl->quirks & xxx 判断是否有某个quirk
+ */
 static const struct pci_device_id nvme_id_table[] = {
 	{ PCI_VDEVICE(INTEL, 0x0953),
 		.driver_data = NVME_QUIRK_STRIPE_SIZE |
@@ -3010,6 +3318,11 @@ static struct pci_driver nvme_driver = {
 	.driver		= {
 		.pm	= &nvme_dev_pm_ops,
 	},
+	/*
+	 * called by:
+	 *   - drivers/pci/pci-sysfs.c|608| <<sriov_numvfs_store>> ret = pdev->driver->sriov_configure(pdev, 0);
+	 *   - drivers/pci/pci-sysfs.c|620| <<sriov_numvfs_store>> ret = pdev->driver->sriov_configure(pdev, num_vfs);
+	 */
 	.sriov_configure = pci_sriov_configure_simple,
 	.err_handler	= &nvme_err_handler,
 };
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index b0c814b..aeda34a 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -15,39 +15,117 @@ struct blk_flush_queue;
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 在以下插入新元素:
+		 *   - block/blk-mq-sched.c|189| <<blk_mq_sched_dispatch_requests>> list_splice_init(&hctx->dispatch, &rq_list);
+		 *   - block/blk-mq-sched.c|365| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1303| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1663| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2235| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
 
 	struct delayed_work	run_work;
+	/*
+	 * 只在以下修改数据cpumask:
+	 *   - block/blk-mq.c|2990| <<blk_mq_map_swqueue>> cpumask_clear(hctx->cpumask);
+	 *   - block/blk-mq.c|3040| <<blk_mq_map_swqueue>> cpumask_set_cpu(i, hctx->cpumask);
+	 */
 	cpumask_var_t		cpumask;
 	int			next_cpu;
 	int			next_cpu_batch;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
+	/*
+	 * 设置sched_data的地方:
+	 *   - block/kyber-iosched.c|515| <<kyber_init_hctx>> hctx->sched_data = khd;
+	 */
 	void			*sched_data;
 	struct request_queue	*queue;
 	struct blk_flush_queue	*fq;
 
 	void			*driver_data;
 
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-debugfs.c|467| <<hctx_ctx_map_show>> sbitmap_bitmap_show(&hctx->ctx_map, m);
+	 *   - block/blk-mq-sched.c|142| <<blk_mq_do_dispatch_ctx>> if (!sbitmap_any_bit_set(&hctx->ctx_map))
+	 *   - block/blk-mq.c|67| <<blk_mq_hctx_has_pending>> sbitmap_any_bit_set(&hctx->ctx_map) ||
+	 *   - block/blk-mq.c|79| <<blk_mq_hctx_mark_pending>> if (!sbitmap_test_bit(&hctx->ctx_map, bit))
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|991| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+	 *   - block/blk-mq.c|1029| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+	 *   - block/blk-mq.c|2267| <<blk_mq_exit_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2313| <<blk_mq_init_hctx>> if (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),
+	 *   - block/blk-mq.c|2346| <<blk_mq_init_hctx>> sbitmap_free(&hctx->ctx_map);
+	 *   - block/blk-mq.c|2512| <<blk_mq_map_swqueue>> sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
+	 *
+	 * 在以下被设置:
+	 *   - block/blk-mq.c|80| <<blk_mq_hctx_mark_pending>> sbitmap_set_bit(&hctx->ctx_map, bit);
+	 *   - block/blk-mq.c|88| <<blk_mq_hctx_clear_pending>> sbitmap_clear_bit(&hctx->ctx_map, bit);
+	 *
+	 * 搜索的时候要用sbitmap_clear_bit()也试一下
+	 */
 	struct sbitmap		ctx_map;
 
+	/*
+	 * 用到dispatch_from的地方:
+	 *   - block/blk-mq-sched.c|137| <<blk_mq_do_dispatch_ctx>> struct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);
+	 *   - block/blk-mq-sched.c|166| <<blk_mq_do_dispatch_ctx>> WRITE_ONCE(hctx->dispatch_from, ctx);
+	 *   - block/blk-mq.c|2428| <<blk_mq_map_swqueue>> hctx->dispatch_from = NULL;
+	 */
 	struct blk_mq_ctx	*dispatch_from;
+	/*
+	 * 修改dispatch_busy的地方:
+	 *   - block/blk-mq.c|1190| <<blk_mq_update_dispatch_busy>> hctx->dispatch_busy = ewma;
+	 */
 	unsigned int		dispatch_busy;
 
 	unsigned short		type;
+	/*
+	 * 在以下被修改:
+	 *   - block/blk-mq.c|2471| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	unsigned short		nr_ctx;
+	/*
+	 * 分配ctxs数组指针的地方:
+	 *   - block/blk-mq.c|2308| <<blk_mq_init_hctx>> hctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),
+	 *
+	 * 设置每个指针元素的地方:
+	 *   - block/blk-mq.c|2471| <<blk_mq_map_swqueue>> hctx->ctxs[hctx->nr_ctx++] = ctx;
+	 */
 	struct blk_mq_ctx	**ctxs;
 
 	spinlock_t		dispatch_wait_lock;
 	wait_queue_entry_t	dispatch_wait;
+	/*
+	 * used by only:
+	 *   - block/blk-mq-tag.h|80| <<bt_wait_ptr>> return sbq_wait_ptr(bt, &hctx->wait_index);
+	 */
 	atomic_t		wait_index;
 
+	/*
+	 * 在以下设置了tags:
+	 *   - block/blk-mq.c|2302| <<blk_mq_init_hctx>> hctx->tags = set->tags[hctx_idx];
+	 *   - block/blk-mq.c|2504| <<blk_mq_map_swqueue>> hctx->tags = set->tags[i];
+	 */
 	struct blk_mq_tags	*tags;
+	/*
+	 * 在以下分配:
+	 *   - block/blk-mq-sched.c|453| <<blk_mq_sched_alloc_tags>> hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
+	 */
 	struct blk_mq_tags	*sched_tags;
 
+	/*
+	 * 在以下进行使用:
+	 *   - block/blk-mq-debugfs.c|607| <<hctx_queued_show>> seq_printf(m, "%lu\n", hctx->queued);
+	 *   - block/blk-mq-debugfs.c|616| <<hctx_queued_write>> hctx->queued = 0;
+	 *   - block/blk-mq.c|418| <<blk_mq_get_request>> data->hctx->queued++;
+	 */
 	unsigned long		queued;
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
@@ -56,14 +134,48 @@ struct blk_mq_hw_ctx {
 	unsigned int		numa_node;
 	unsigned int		queue_num;
 
+	/*
+	 * 是伴随着RQF_MQ_INFLIGHT的:
+	 *   - block/blk-mq-debugfs.c|641| <<hctx_active_show>> seq_printf(m, "%d\n", atomic_read(&hctx->nr_active));
+	 *   - block/blk-mq-tag.c|98| <<hctx_may_queue>> return atomic_read(&hctx->nr_active) < depth;
+	 *   - block/blk-mq.c|511| <<blk_mq_rq_ctx_init>> atomic_inc(&data->hctx->nr_active);
+	 *   - block/blk-mq.c|751| <<blk_mq_free_request>> atomic_dec(&hctx->nr_active);
+	 *   - block/blk-mq.c|1428| <<blk_mq_get_driver_tag>> atomic_inc(&data.hctx->nr_active);
+	 *   - block/blk-mq.c|3205| <<blk_mq_alloc_and_init_hctx>> atomic_set(&hctx->nr_active, 0);
+	 *   - block/blk-mq.h|290| <<__blk_mq_put_driver_tag>> atomic_dec(&hctx->nr_active);
+	 */
 	atomic_t		nr_active;
 	unsigned int		nr_expired;
 
+	/*
+	 * called by:
+	 *   - block/blk-mq.c|2662| <<blk_mq_hctx_notify_dead>> hctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);
+	 *   - block/blk-mq.c|2687| <<blk_mq_remove_cpuhp>> &hctx->cpuhp_dead);
+	 *   - block/blk-mq.c|2742| <<blk_mq_init_hctx>> cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);
+	 */
 	struct hlist_node	cpuhp_dead;
 	struct kobject		kobj;
 
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|560| <<hctx_io_poll_show>> seq_printf(m, "considered=%lu\n", hctx->poll_considered);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3992| <<blk_poll>> hctx->poll_considered++;
+	 */
 	unsigned long		poll_considered;
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|561| <<hctx_io_poll_show>> seq_printf(m, "invoked=%lu\n", hctx->poll_invoked);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|3998| <<blk_poll>> hctx->poll_invoked++;
+	 */
 	unsigned long		poll_invoked;
+	/*
+	 * used by:
+	 *   - block/blk-mq-debugfs.c|562| <<hctx_io_poll_show>> seq_printf(m, "success=%lu\n", hctx->poll_success);
+	 *   - block/blk-mq-debugfs.c|571| <<hctx_io_poll_write>> hctx->poll_considered = hctx->poll_invoked = hctx->poll_success = 0;
+	 *   - block/blk-mq.c|4002| <<blk_poll>> hctx->poll_success++;
+	 */
 	unsigned long		poll_success;
 
 #ifdef CONFIG_BLK_DEBUG_FS
@@ -76,6 +188,9 @@ struct blk_mq_hw_ctx {
 };
 
 struct blk_mq_queue_map {
+	/*
+	 * 猜测是每一个sw queue对应的hw queue???
+	 */
 	unsigned int *mq_map;
 	unsigned int nr_queues;
 	unsigned int queue_offset;
@@ -97,8 +212,24 @@ struct blk_mq_tag_set {
 	 * share maps between types.
 	 */
 	struct blk_mq_queue_map	map[HCTX_MAX_TYPES];
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|2713| <<blk_mq_init_sq_queue>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3058| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - block/blk-mq.c|3069| <<blk_mq_alloc_tag_set>> set->nr_maps = 1;
+	 *   - drivers/nvme/host/pci.c|2322| <<nvme_dev_add>> dev->tagset.nr_maps = 2;
+	 *   - drivers/nvme/host/pci.c|2324| <<nvme_dev_add>> dev->tagset.nr_maps++;
+	 */
 	unsigned int		nr_maps;	/* nr entries in map[] */
 	const struct blk_mq_ops	*ops;
+	/*
+	 * set->nr_hw_queues在非驱动更新的地方:
+	 *   - block/blk-mq.c|3189| <<blk_mq_init_sq_queue>> set->nr_hw_queues = 1;
+	 *   - block/blk-mq.c|3630| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = 1; --> kdump情况下
+	 *   - block/blk-mq.c|3639| <<blk_mq_alloc_tag_set>> set->nr_hw_queues = nr_cpu_ids; --> set->nr_maps==1的情况下
+	 *   - block/blk-mq.c|3874| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = nr_hw_queues;
+	 *   - block/blk-mq.c|3882| <<__blk_mq_update_nr_hw_queues>> set->nr_hw_queues = prev_nr_hw_queues;
+	 */
 	unsigned int		nr_hw_queues;	/* nr hw queues across maps */
 	unsigned int		queue_depth;	/* max hw supported */
 	unsigned int		reserved_tags;
@@ -168,6 +299,10 @@ struct blk_mq_ops {
 	/*
 	 * Called on request timeout
 	 */
+	/*
+	 * 几个例子:
+	 *   - nvme_timeout()
+	 */
 	timeout_fn		*timeout;
 
 	/*
@@ -175,6 +310,12 @@ struct blk_mq_ops {
 	 */
 	poll_fn			*poll;
 
+	/*
+	 * 几个例子:
+	 *   - virtblk_request_done()
+	 *   - scsi_softirq_done()
+	 *   - nvme_pci_complete_rq()
+	 */
 	complete_fn		*complete;
 
 	/*
@@ -216,24 +357,149 @@ struct blk_mq_ops {
 };
 
 enum {
+	/*
+	 * 下面的BLK_MQ_F_xxx只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用
+	 * blk_mq_init_hctx()中把blk_mq_tag_set->flags拷贝到blk_mq_hw_hctx->flags:
+	 *   - block/blk-mq.c|2402| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 */
+	/*
+	 * 主要使用的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - block/blk-mq-sched.c|334| <<__blk_mq_sched_bio_merge>> if ((hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
+	 *
+	 * 某些设置的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - drivers/block/loop.c|1954| <<loop_add>> lo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/null_blk_main.c|1553| <<null_init_tag_set>> set->flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/virtio_blk.c|787| <<virtblk_probe>> vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/block/xen-blkfront.c|980| <<xlvbd_init_blk_queue>> info->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/nvme/host/pci.c|2334| <<nvme_dev_add>> dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
+	 *   - drivers/scsi/scsi_lib.c|1902| <<scsi_mq_setup_tags>> shost->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	 */
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x2):
+	 *   - block/blk-mq-tag.c|79| <<hctx_may_queue>> if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|88| <<blk_mq_tag_busy>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq-tag.h|96| <<blk_mq_tag_idle>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
+	 *   - block/blk-mq.c|305| <<blk_mq_rq_ctx_init>> if (data->hctx->flags & BLK_MQ_F_TAG_SHARED) {
+	 *   - block/blk-mq.c|1109| <<blk_mq_mark_tag_wait>> if (!(hctx->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|1239| <<blk_mq_dispatch_rq_list>> if (hctx->flags & BLK_MQ_F_TAG_SHARED)
+	 *   - block/blk-mq.c|2298| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2533| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2535| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2561| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2578| <<blk_mq_add_queue_tag_set>> !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+	 *   - block/blk-mq.c|2579| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2583| <<blk_mq_add_queue_tag_set>> if (set->flags & BLK_MQ_F_TAG_SHARED)
+	 *
+	 * 设置和删除的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x1)
+	 *   - block/blk-mq.c|2544| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2590| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2309| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2546| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|2572| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
+	/*
+	 * 如果设置了BLK_MQ_F_BLOCKING则有可能会睡眠用srcu, 否则用rcu
+	 *
+	 * 会设置到BLK_MQ_F_BLOCKING的地方, 非常少: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x20)
+	 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1559| <<null_init_tag_set>> set->flags |= BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/paride/pd.c|910| <<pd_probe_drive>> disk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/cdrom/gdrom.c|795| <<probe_gdrom>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 *   - drivers/ide/ide-probe.c|785| <<ide_init_queue>> set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mmc/core/queue.c|413| <<mmc_init_queue>> mq->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING;
+	 *   - drivers/mtd/mtd_blkdevs.c|448| <<add_mtd_blktrans_dev>> BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_BLOCKING);
+	 */
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 使用BLK_MQ_F_NO_SCHED的地方, 主要是第一个: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40)
+	 *   - block/blk-mq.c|2891| <<blk_mq_init_allocated_queue>> if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
+	 *   - block/elevator.c|691| <<elv_support_iosched>> if (q->tag_set && (q->tag_set->flags & BLK_MQ_F_NO_SCHED))
+	 *
+	 * 设置BLK_MQ_F_NO_SCHED的地方: 只被blk_mq_tag_set->flags和blk_mq_hw_hctx->flags使用 (0x40)
+	 *   - block/bsg-lib.c|361| <<bsg_setup_queue>> set->flags = BLK_MQ_F_NO_SCHED | BLK_MQ_F_BLOCKING;
+	 *   - drivers/block/null_blk_main.c|1555| <<null_init_tag_set>> set->flags |= BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/fc.c|3057| <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/pci.c|1643| <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/host/rdma.c|728| <<nvme_rdma_alloc_tagset>> set->flags = BLK_MQ_F_NO_SCHED;
+	 *   - drivers/nvme/target/loop.c|363| <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|394| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
+	 *   - include/linux/blk-mq.h|398| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> << BLK_MQ_F_ALLOC_POLICY_START_BIT)
+	 */
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
+	/*
+	 * 只在下面使用:
+	 *   - include/linux/blk-mq.h|395| <<BLK_MQ_FLAG_TO_ALLOC_POLICY>> ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+	 *   - include/linux/blk-mq.h|397| <<BLK_ALLOC_POLICY_TO_MQ_FLAG>> ((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
+	 */
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
+	/*
+	 * BLK_MQ_S_xxx只用在blk_mq_hw_ctx->state
+	 */
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq.c|1550| <<blk_mq_stop_hw_queue>> set_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1575| <<blk_mq_start_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1596| <<blk_mq_start_stopped_hw_queue>> clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 *   - block/blk-mq.c|1620| <<blk_mq_run_work_fn>> if (test_bit(BLK_MQ_S_STOPPED, &hctx->state))
+	 *   - block/blk-mq.h|184| <<blk_mq_hctx_stopped>> return test_bit(BLK_MQ_S_STOPPED, &hctx->state);
+	 */
 	BLK_MQ_S_STOPPED	= 0,
+	/*
+	 * 在以下被使用:
+	 *   - block/blk-mq-tag.c|32| <<__blk_mq_tag_busy>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
+	 *   - block/blk-mq-tag.c|33| <<__blk_mq_tag_busy>> !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|57| <<__blk_mq_tag_idle>> if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 *   - block/blk-mq-tag.c|76| <<hctx_may_queue>> if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+	 */
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq-sched.c|66| <<blk_mq_sched_mark_restart_hctx>> if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|69| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.c|75| <<blk_mq_sched_restart>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq-sched.c|77| <<blk_mq_sched_restart>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq-sched.h|91| <<blk_mq_sched_needs_restart>> return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1110| <<blk_mq_mark_tag_wait>> if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
+	 *   - block/blk-mq.c|1111| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *
+	 * 用来触发blk_mq_run_hw_queue()
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
+	/*
+	 * 使用的一个例子:
+	 *   - block/blk-mq.c|3051| <<blk_mq_alloc_tag_set>> if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
+	 */
 	BLK_MQ_MAX_DEPTH	= 10240,
 
+	/*
+	 * 在以下使用:
+	 *   - block/blk-mq.c|1456| <<blk_mq_hctx_next_cpu>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 *   - block/blk-mq.c|2555| <<blk_mq_map_swqueue>> hctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;
+	 */
 	BLK_MQ_CPU_WORK_BATCH	= 8,
 };
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|260| <<hctx_flags_show>> const int alloc_policy = BLK_MQ_FLAG_TO_ALLOC_POLICY(hctx->flags);
+ *   - block/blk-mq.c|2124| <<blk_mq_alloc_rq_map>> BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
+ */
 #define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \
 	((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \
 		((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))
+/*
+ * used by:
+ *   - block/blk-mq-debugfs.c|270| <<hctx_flags_show>> hctx->flags ^ BLK_ALLOC_POLICY_TO_MQ_FLAG(alloc_policy),
+ *   - drivers/block/skd_main.c|2846| <<skd_cons_disk>> BLK_ALLOC_POLICY_TO_MQ_FLAG(BLK_TAG_ALLOC_FIFO);
+ *   - drivers/scsi/scsi_lib.c|1904| <<scsi_mq_setup_tags>> BLK_ALLOC_POLICY_TO_MQ_FLAG(shost->hostt->tag_alloc_policy);
+ */
 #define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \
 	((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \
 		<< BLK_MQ_F_ALLOC_POLICY_START_BIT)
@@ -259,13 +525,60 @@ bool blk_mq_can_queue(struct blk_mq_hw_ctx *);
 bool blk_mq_queue_inflight(struct request_queue *q);
 
 enum {
+	/*
+	 * BLK_MQ_REQ_xxx只被blk_mq_alloc_data->flags使用
+	 */
 	/* return when out of requests */
+	/*
+	 * 设置的地方:return when out of requests
+	 *   - block/blk-core.c|1024| <<generic_make_request>> flags = BLK_MQ_REQ_NOWAIT;
+	 *   - block/blk-core.c|1079| <<generic_make_request>> flags = BLK_MQ_REQ_NOWAIT;
+	 *   - block/blk-core.c|1145| <<direct_make_request>> if (unlikely(blk_queue_enter(q, nowait ? BLK_MQ_REQ_NOWAIT : 0))) {
+	 *   - block/blk-mq.c|583| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_NOWAIT;
+	 *   - block/blk-mq.c|1409| <<blk_mq_get_driver_tag>> .flags = BLK_MQ_REQ_NOWAIT,
+	 *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/ide/ide-park.c|50| <<issue_park_cmd>> rq = blk_get_request(q, REQ_OP_DRV_IN, BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/md/dm-mpath.c|517| <<multipath_clone_and_map>> BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 *   - drivers/nvme/host/pci.c|1354| <<nvme_timeout>> BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+	 *   - drivers/nvme/host/pci.c|2265| <<nvme_delete_queue>> req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+	 *   - drivers/scsi/fnic/fnic_scsi.c|2272| <<fnic_scsi_host_start_tag>> dummy = blk_mq_alloc_request(q, REQ_OP_WRITE, BLK_MQ_REQ_NOWAIT);
+	 */
 	BLK_MQ_REQ_NOWAIT	= (__force blk_mq_req_flags_t)(1 << 0),
 	/* allocate from reserved pool */
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|1418| <<blk_mq_get_driver_tag>> data.flags |= BLK_MQ_REQ_RESERVED;
+	 *   - drivers/block/mtip32xx/mtip32xx.c|994| <<mtip_exec_internal_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+	 *   - drivers/ide/ide-atapi.c|202| <<ide_prep_sense>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
+	 *   - drivers/nvme/host/core.c|935| <<nvme_keep_alive>> rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+	 *   - drivers/nvme/host/fabrics.c|401| <<nvmf_connect_admin_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	 *   - drivers/nvme/host/fabrics.c|464| <<nvmf_connect_io_queue>> BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	 */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
+	/*
+	 * 在以下被设置:
+	 *   - block/blk-mq.c|372| <<blk_mq_get_request>> data->flags |= BLK_MQ_REQ_INTERNAL;
+	 *
+	 * 在以下被使用:
+	 *   - block/blk-mq-tag.c|104| <<__blk_mq_get_tag>> if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
+	 *   - block/blk-mq.c|297| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_INTERNAL) {
+	 *   - block/blk-mq.h|176| <<blk_mq_tags_from_data>> if (data->flags & BLK_MQ_REQ_INTERNAL)
+	 */
 	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
 	/* set RQF_PREEMPT */
+	/*
+	 * 在以下使用:
+	 *   - block/blk-core.c|400| <<blk_queue_enter>> const bool pm = flags & BLK_MQ_REQ_PREEMPT;
+	 *   - block/blk-core.c|583| <<blk_get_request>> WARN_ON_ONCE(flags & ~(BLK_MQ_REQ_NOWAIT | BLK_MQ_REQ_PREEMPT));
+	 *   - block/blk-mq.c|568| <<blk_mq_rq_ctx_init>> if (data->flags & BLK_MQ_REQ_PREEMPT)
+	 *
+	 * 在以下设置:
+	 *   - drivers/ide/ide-pm.c|80| <<generic_ide_resume>> rq = blk_get_request(drive->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_PREEMPT);
+	 *   - drivers/scsi/scsi_lib.c|262| <<__scsi_execute>> REQ_OP_SCSI_OUT : REQ_OP_SCSI_IN, BLK_MQ_REQ_PREEMPT);
+	 */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
 };
 
@@ -348,6 +661,15 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/*
+ * 遍历每一个request_queue->queue_hw_ctx[i]
+ *
+ * q->queue_hw_ctx第一维在以下分配:
+ *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+ *
+ * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+ *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+ */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index d66bf5f..3e92106 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -347,6 +347,13 @@ enum req_flag_bits {
 #define REQ_BACKGROUND		(1ULL << __REQ_BACKGROUND)
 #define REQ_NOWAIT		(1ULL << __REQ_NOWAIT)
 #define REQ_NOUNMAP		(1ULL << __REQ_NOUNMAP)
+/*
+ * 设置和取消REQ_HIPRI的地方:
+ *   - drivers/nvme/host/core.c|756| <<nvme_execute_rq_polled>> rq->cmd_flags |= REQ_HIPRI;
+ *   - fs/direct-io.c|1272| <<do_blockdev_direct_IO>> dio->op_flags |= REQ_HIPRI;
+ *   - include/linux/bio.h|838| <<bio_set_polled>> bio->bi_opf |= REQ_HIPRI;
+ *   - mm/page_io.c|405| <<swap_readpage>> bio->bi_opf |= REQ_HIPRI;
+ */
 #define REQ_HIPRI		(1ULL << __REQ_HIPRI)
 
 #define REQ_DRV			(1ULL << __REQ_DRV)
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index faed9d9..35ae6db 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -62,6 +62,9 @@ typedef void (rq_end_io_fn)(struct request *, blk_status_t);
  * request flags */
 typedef __u32 __bitwise req_flags_t;
 
+/*
+ * RQF_xxx用在request->rq_flags
+ */
 /* elevator knows about this request */
 #define RQF_SORTED		((__force req_flags_t)(1 << 0))
 /* drive already may have started this one */
@@ -73,6 +76,15 @@ typedef __u32 __bitwise req_flags_t;
 /* merge of different types, fail separately */
 #define RQF_MIXED_MERGE		((__force req_flags_t)(1 << 5))
 /* track inflight for MQ */
+/*
+ * 是伴随着hctx->nr_active的, 每次设置的时候, hctx->nr_active就增长
+ * 取消的时候, hctx->nr_active就减少
+ *   - block/blk-mq.c|510| <<blk_mq_rq_ctx_init>> rq_flags = RQF_MQ_INFLIGHT;
+ *   - block/blk-mq.c|750| <<blk_mq_free_request>> if (rq->rq_flags & RQF_MQ_INFLIGHT)
+ *   - block/blk-mq.c|1427| <<blk_mq_get_driver_tag>> rq->rq_flags |= RQF_MQ_INFLIGHT;
+ *   - block/blk-mq.h|288| <<__blk_mq_put_driver_tag>> if (rq->rq_flags & RQF_MQ_INFLIGHT) {
+ *   - block/blk-mq.h|289| <<__blk_mq_put_driver_tag>> rq->rq_flags &= ~RQF_MQ_INFLIGHT;
+ */
 #define RQF_MQ_INFLIGHT		((__force req_flags_t)(1 << 6))
 /* don't call prep for this one */
 #define RQF_DONTPREP		((__force req_flags_t)(1 << 7))
@@ -114,9 +126,37 @@ typedef __u32 __bitwise req_flags_t;
 /*
  * Request state for blk-mq.
  */
+/*
+ * MQ_RQ_xxx用在request->state
+ */
 enum mq_rq_state {
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|536| <<blk_mq_free_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|719| <<__blk_mq_requeue_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|2127| <<blk_mq_init_request>> WRITE_ONCE(rq->state, MQ_RQ_IDLE);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|672| <<blk_mq_request_started>> return blk_mq_rq_state(rq) != MQ_RQ_IDLE;
+	 *   - block/blk-mq.c|693| <<blk_mq_start_request>> WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|696| <<blk_mq_start_request>> WRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|825| <<blk_mq_rq_inflight>> if (rq->state == MQ_RQ_IN_FLIGHT && rq->q == hctx->queue) {
+	 *   - block/blk-mq.c|863| <<blk_mq_req_expired>> if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|593| <<__blk_mq_complete_request>> WRITE_ONCE(rq->state, MQ_RQ_COMPLETE);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-mq.c|3389| <<blk_mq_poll_hybrid_sleep>> if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
+	 */
 	MQ_RQ_COMPLETE		= 2,
 };
 
@@ -128,12 +168,31 @@ enum mq_rq_state {
  */
 struct request {
 	struct request_queue *q;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-flush.c|297| <<blk_kick_flush>> flush_rq->mq_ctx = first_rq->mq_ctx;
+	 *   - block/blk-mq.c|316| <<blk_mq_rq_ctx_init>> rq->mq_ctx = data->ctx;
+	 */
 	struct blk_mq_ctx *mq_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-flush.c|298| <<blk_kick_flush>> flush_rq->mq_hctx = first_rq->mq_hctx;
+	 *   - block/blk-mq.c|317| <<blk_mq_rq_ctx_init>> rq->mq_hctx = data->hctx;
+	 *   - block/blk-mq.c|502| <<__blk_mq_free_request>> rq->mq_hctx = NULL;
+	 */
 	struct blk_mq_hw_ctx *mq_hctx;
 
 	unsigned int cmd_flags;		/* op and common flags */
 	req_flags_t rq_flags;
 
+	/*
+	 * 在以下修改:
+	 *   - block/blk-core.c|116| <<blk_rq_init>> rq->internal_tag = -1;
+	 *   - block/blk-flush.c|224| <<flush_end_io>> flush_rq->internal_tag = -1;
+	 *   - block/blk-flush.c|305| <<blk_kick_flush>> flush_rq->internal_tag = first_rq->internal_tag;
+	 *   - block/blk-mq.c|303| <<blk_mq_rq_ctx_init>> rq->internal_tag = tag;
+	 *   - block/blk-mq.c|310| <<blk_mq_rq_ctx_init>> rq->internal_tag = -1;
+	 */
 	int internal_tag;
 
 	/* the following two fields are internal, NEVER access directly */
@@ -317,9 +376,34 @@ struct queue_limits {
 	unsigned long		seg_boundary_mask;
 	unsigned long		virt_boundary_mask;
 
+	/*
+	 * max_hw_sectors和max_sectors设置的地方主要是blk_queue_max_hw_sectors()
+	 *
+	 * 调用blk_queue_max_hw_sectors()的地方很多, 例子是:
+	 *   - drivers/block/xen-blkfront.c|947| <<blkif_set_queue_limits>> blk_queue_max_hw_sectors(rq, (segments * XEN_PAGE_SIZE) / 512);
+	 *   - drivers/block/virtio_blk.c|827| <<virtblk_probe>> blk_queue_max_hw_sectors(q, -1U);
+	 *   - drivers/nvme/host/core.c|1972| <<nvme_set_queue_limits>> blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+	 *   - drivers/block/loop.c|1968| <<loop_add>> blk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);
+	 */
 	unsigned int		max_hw_sectors;
+	/*
+	 * max_dev_sectors在以下被修改:
+	 *   - block/blk-settings.c|54| <<blk_set_default_limits>> lim->max_dev_sectors = 0;
+	 *   - block/blk-settings.c|90| <<blk_set_stacking_limits>> lim->max_dev_sectors = UINT_MAX;
+	 *   - block/blk-settings.c|526| <<blk_stack_limits>> t->max_dev_sectors = min_not_zero(t->max_dev_sectors, b->max_dev_sectors);
+	 *   - drivers/s390/block/dasd.c|3155| <<dasd_setup_queue>> q->limits.max_dev_sectors = max;
+	 *   - drivers/scsi/sd.c|3120| <<sd_revalidate_disk>> q->limits.max_dev_sectors = logical_to_sectors(sdp, dev_max);
+	 */
 	unsigned int		max_dev_sectors;
 	unsigned int		chunk_sectors;
+	/*
+	 * 设置max_sectors的地方:
+	 *   - block/blk-settings.c|53| <<blk_set_default_limits>> lim->max_sectors = lim->max_hw_sectors = BLK_SAFE_MAX_SECTORS;
+	 *   - block/blk-settings.c|89| <<blk_set_stacking_limits>> lim->max_sectors = UINT_MAX;
+	 *   - block/blk-settings.c|211| <<blk_queue_max_hw_sectors>> limits->max_sectors = max_sectors;
+	 *   - block/blk-settings.c|524| <<blk_stack_limits>> t->max_sectors = min_not_zero(t->max_sectors, b->max_sectors);
+	 *   - block/blk-sysfs.c|239| <<queue_max_sectors_store>> q->limits.max_sectors = max_sectors_kb << 1;
+	 */
 	unsigned int		max_sectors;
 	unsigned int		max_segment_size;
 	unsigned int		physical_block_size;
@@ -404,13 +488,38 @@ struct request_queue {
 	const struct blk_mq_ops	*mq_ops;
 
 	/* sw queues */
+	/*
+	 * 实体保存在struct blk_mq_ctxs结构中
+	 */
 	struct blk_mq_ctx __percpu	*queue_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq.c|3382| <<blk_mq_init_allocated_queue>> q->nr_queues = nr_hw_queues(set);
+	 *
+	 * nr_hw_queues()的实现:
+	 *   if (set->nr_maps == 1)
+	 *       return nr_cpu_ids;
+	 *   return max(set->nr_hw_queues, nr_cpu_ids);
+	 */
 	unsigned int		nr_queues;
 
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
+	/*
+	 * q->queue_hw_ctx第一维在以下分配:
+	 *   - block/blk-mq.c|3386| <<blk_mq_init_allocated_queue>> q->queue_hw_ctx = kcalloc_node(q->nr_queues, sizeof(*(q->queue_hw_ctx)),
+	 *
+	 * 每一个元素也在下面用blk_mq_alloc_and_init_hctx()分配:
+	 *   - block/blk-mq.c|3309| <<blk_mq_realloc_hw_ctxs>> hctx = blk_mq_alloc_and_init_hctx(set, q, i, node);
+	 */
 	struct blk_mq_hw_ctx	**queue_hw_ctx;
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq.c|3325| <<blk_mq_realloc_hw_ctxs>> q->nr_hw_queues = set->nr_hw_queues;
+	 *
+	 * 在blk_mq_alloc_tag_set()中在nr_maps==1的情况下会设置为nr_cpu_ids
+	 */
 	unsigned int		nr_hw_queues;
 
 	struct backing_dev_info	*backing_dev_info;
@@ -468,8 +577,23 @@ struct request_queue {
 	/*
 	 * queue settings
 	 */
+	/*
+	 * 在以下设置:
+	 *   - block/blk-mq-sched.c|563| <<blk_mq_init_sched>> q->nr_requests = q->tag_set->queue_depth;
+	 *   - block/blk-mq-sched.c|572| <<blk_mq_init_sched>> q->nr_requests = 2 * min_t(unsigned int , q->tag_set->queue_depth,
+	 *   - block/blk-mq.c|3416| <<blk_mq_init_allocated_queue>> q->nr_requests = set->queue_depth;
+	 *   - block/blk-mq.c|3723| <<blk_mq_update_nr_requests>> q->nr_requests = nr;
+	 *   - block/blk-settings.c|123| <<blk_queue_make_request>> q->nr_requests = BLKDEV_MAX_RQ;
+	 */
 	unsigned long		nr_requests;	/* Max # of requests */
 
+	/*
+	 * 设置dma_drain_size的地方:
+	 *   - block/blk-settings.c|764| <<blk_queue_dma_drain>> q->dma_drain_size = size;
+	 *   - drivers/ata/libata-scsi.c|1390| <<ata_scsi_slave_destroy>> q->dma_drain_size = 0;
+	 *
+	 * 似乎目前只被libata-scsi使用
+	 */
 	unsigned int		dma_drain_size;
 	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
@@ -481,7 +605,25 @@ struct request_queue {
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
+	/*
+	 * 使用的主要地方:
+	 *   - block/blk-core.c|233| <<blk_sync_queue>> del_timer_sync(&q->timeout);
+	 *   - lock/blk-core.c|512| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+	 *   - block/blk-mq.c|1116| <<blk_mq_timeout_work>> mod_timer(&q->timeout, next);
+	 *   - block/blk-timeout.c|152| <<blk_add_timer>> mod_timer(&q->timeout, expiry);
+	 */
 	struct timer_list	timeout;
+	/*
+	 * 可能初始化为blk_mq_timeout_work()或者blk_timeout_work()
+	 *   - block/blk-core.c|513| <<blk_alloc_queue_node>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 *   - block/blk-mq.c|3160| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 *
+	 * 使用的地方:
+	 *   - block/blk-core.c|234| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 *   - block/blk-core.c|462| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+	 *   - block/blk-mq.c|1064| <<blk_mq_timeout_work>> container_of(work, struct request_queue, timeout_work);
+	 *   - block/blk-timeout.c|88| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 */
 	struct work_struct	timeout_work;
 
 	struct list_head	icq_list;
@@ -537,6 +679,11 @@ struct request_queue {
 
 	struct mutex		sysfs_lock;
 
+	/*
+	 * 增加减少的地方:
+	 *   - block/blk-mq.c|194| <<blk_freeze_queue_start>> freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
+	 *   - block/blk-mq.c|249| <<blk_mq_unfreeze_queue>> freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+	 */
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -548,7 +695,40 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|264| <<blk_clear_pm_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|289| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|435| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-core.c|455| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|527| <<blk_alloc_queue_node>> init_waitqueue_head(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|237| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|244| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|285| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_queue_head_t	mq_freeze_wq;
+	/*
+	 * 使用的例子:
+	 *   - block/blk-core.c|380| <<blk_cleanup_queue>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-core.c|406| <<blk_queue_enter>> if (percpu_ref_tryget_live(&q->q_usage_counter)) {
+	 *   - block/blk-core.c|415| <<blk_queue_enter>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|447| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter);
+	 *   - block/blk-core.c|453| <<blk_queue_usage_counter_release>> container_of(ref, struct request_queue, q_usage_counter);
+	 *   - block/blk-core.c|533| <<blk_alloc_queue_node>> if (percpu_ref_init(&q->q_usage_counter,
+	 *   - block/blk-core.c|544| <<blk_alloc_queue_node>> percpu_ref_exit(&q->q_usage_counter);
+	 *   - block/blk-mq-tag.c|401| <<blk_mq_queue_tag_busy_iter>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-mq.c|305| <<blk_freeze_queue_start>> percpu_ref_kill(&q->q_usage_counter);
+	 *   - block/blk-mq.c|319| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|327| <<blk_mq_freeze_queue_wait_timeout>> percpu_ref_is_zero(&q->q_usage_counter),
+	 *   - block/blk-mq.c|366| <<blk_mq_unfreeze_queue>> percpu_ref_resurrect(&q->q_usage_counter);
+	 *   - block/blk-mq.c|1245| <<blk_mq_timeout_work>> if (!percpu_ref_tryget(&q->q_usage_counter))
+	 *   - block/blk-pm.c|86| <<blk_pre_runtime_suspend>> percpu_ref_switch_to_atomic_sync(&q->q_usage_counter);
+	 *   - block/blk-pm.c|87| <<blk_pre_runtime_suspend>> if (percpu_ref_is_zero(&q->q_usage_counter))
+	 *   - block/blk-sysfs.c|924| <<blk_register_queue>> percpu_ref_switch_to_percpu(&q->q_usage_counter);
+	 *   - block/blk.h|66| <<blk_queue_enter_live>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|617| <<scsi_end_request>> percpu_ref_get(&q->q_usage_counter);
+	 *   - drivers/scsi/scsi_lib.c|627| <<scsi_end_request>> percpu_ref_put(&q->q_usage_counter);
+	 */
 	struct percpu_ref	q_usage_counter;
 	struct list_head	all_q_node;
 
@@ -572,6 +752,13 @@ struct request_queue {
 	u64			write_hints[BLK_MAX_WRITE_HINTS];
 };
 
+/*
+ * QUEUE_FLAG_xxx在request_queue->queue_flags中使用
+ */
+/*
+ * used by:
+ *   - include/linux/blkdev.h|678| <<blk_queue_stopped>> #define blk_queue_stopped(q) test_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)
+ */
 #define QUEUE_FLAG_STOPPED	0	/* queue is stopped */
 #define QUEUE_FLAG_DYING	1	/* queue being torn down */
 #define QUEUE_FLAG_BIDI		2	/* queue supports bidi requests */
@@ -592,10 +779,22 @@ struct request_queue {
 #define QUEUE_FLAG_WC		17	/* Write back caching */
 #define QUEUE_FLAG_FUA		18	/* device supports FUA writes */
 #define QUEUE_FLAG_DAX		19	/* device supports DAX */
+/*
+ * 使用的例子:
+ *   - block/blk-mq.c|791| <<blk_mq_start_request>> if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
+ *   - block/blk-stat.c|185| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|195| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|226| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS	20	/* track IO start and completion times */
 #define QUEUE_FLAG_POLL_STATS	21	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED	22	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 23	/* queue supports SCSI commands */
+/*
+ * used by:
+ *   - block/blk-mq.c|278| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *   - block/blk-mq.c|320| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ */
 #define QUEUE_FLAG_QUIESCED	24	/* queue has been quiesced */
 #define QUEUE_FLAG_PCI_P2PDMA	25	/* device supports PCI p2p requests */
 
diff --git a/include/linux/percpu-refcount.h b/include/linux/percpu-refcount.h
index b297cd1..f9c10d5 100644
--- a/include/linux/percpu-refcount.h
+++ b/include/linux/percpu-refcount.h
@@ -61,8 +61,33 @@ typedef void (percpu_ref_func_t)(struct percpu_ref *);
 
 /* flags set in the lower bits of percpu_ref->percpu_count_ptr */
 enum {
+	/*
+	 * used by:
+	 *   - lib/percpu-refcount.c|74| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|168| <<__percpu_ref_switch_to_atomic>> if (ref->percpu_count_ptr & __PERCPU_REF_ATOMIC) {
+	 *   - lib/percpu-refcount.c|175| <<__percpu_ref_switch_to_atomic>> ref->percpu_count_ptr |= __PERCPU_REF_ATOMIC;
+	 *   - lib/percpu-refcount.c|194| <<__percpu_ref_switch_to_percpu>> if (!(ref->percpu_count_ptr & __PERCPU_REF_ATOMIC))
+	 *   - lib/percpu-refcount.c|209| <<__percpu_ref_switch_to_percpu>> ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC);
+	 */
 	__PERCPU_REF_ATOMIC	= 1LU << 0,	/* operating in atomic mode */
+	/*
+	 * used by:
+	 *   - include/linux/percpu-refcount.h|257| <<percpu_ref_tryget_live>> } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
+	 *   - include/linux/percpu-refcount.h|315| <<percpu_ref_is_dying>> return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|79| <<percpu_ref_init>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|225| <<__percpu_ref_switch_mode>> if (ref->force_atomic || (ref->percpu_count_ptr & __PERCPU_REF_DEAD))
+	 *   - lib/percpu-refcount.c|335| <<percpu_ref_kill_and_confirm>> WARN_ONCE(ref->percpu_count_ptr & __PERCPU_REF_DEAD,
+	 *   - lib/percpu-refcount.c|338| <<percpu_ref_kill_and_confirm>> ref->percpu_count_ptr |= __PERCPU_REF_DEAD;
+	 *   - lib/percpu-refcount.c|386| <<percpu_ref_resurrect>> WARN_ON_ONCE(!(ref->percpu_count_ptr & __PERCPU_REF_DEAD));
+	 *   - lib/percpu-refcount.c|389| <<percpu_ref_resurrect>> ref->percpu_count_ptr &= ~__PERCPU_REF_DEAD;
+	 */
 	__PERCPU_REF_DEAD	= 1LU << 1,	/* (being) killed */
+	/*
+	 * used by:
+	 *   - include/linux/percpu-refcount.h|161| <<__ref_is_percpu>> if (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))
+	 *   - lib/percpu-refcount.c|42| <<percpu_count_ptr>> (ref->percpu_count_ptr & ~__PERCPU_REF_ATOMIC_DEAD);
+	 *   - lib/percpu-refcount.c|109| <<percpu_ref_exit>> ref->percpu_count_ptr = __PERCPU_REF_ATOMIC_DEAD;
+	 */
 	__PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,
 
 	__PERCPU_REF_FLAG_BITS	= 2,
@@ -92,6 +117,10 @@ struct percpu_ref {
 	 * mode; if set, then get/put will manipulate the atomic_t.
 	 */
 	unsigned long		percpu_count_ptr;
+	/*
+	 * release调用的地方:
+	 *   - include/linux/percpu-refcount.h|310| <<percpu_ref_put_many>> ref->release(ref);
+	 */
 	percpu_ref_func_t	*release;
 	percpu_ref_func_t	*confirm_switch;
 	bool			force_atomic:1;
@@ -273,6 +302,11 @@ static inline bool percpu_ref_tryget_live(struct percpu_ref *ref)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * called by:
+ *   - include/linux/cgroup.h|402| <<css_put_many>> percpu_ref_put_many(&css->refcnt, n);
+ *   - include/linux/percpu-refcount.h|326| <<percpu_ref_put>> percpu_ref_put_many(ref, 1);
+ */
 static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
 {
 	unsigned long __percpu *percpu_count;
@@ -296,6 +330,10 @@ static inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
  *
  * This function is safe to call as long as @ref is between init and exit.
  */
+/*
+ * 调用的一个例子:
+ *   - block/blk-core.c|448| <<blk_queue_exit>> percpu_ref_put(&q->q_usage_counter); 
+ */
 static inline void percpu_ref_put(struct percpu_ref *ref)
 {
 	percpu_ref_put_many(ref, 1);
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index 14d5581..0a06f14 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -236,7 +236,26 @@ bool sbitmap_any_bit_set(const struct sbitmap *sb);
  */
 bool sbitmap_any_bit_clear(const struct sbitmap *sb);
 
+/*
+ * called by:
+ *   - include/linux/sbitmap.h|264| <<__sbitmap_for_each_set>> index = SB_NR_TO_INDEX(sb, start);
+ *   - include/linux/sbitmap.h|315| <<__sbitmap_word>> return &sb->map[SB_NR_TO_INDEX(sb, bitnr)].word;
+ *   - include/linux/sbitmap.h|338| <<sbitmap_deferred_clear_bit>> unsigned long *addr = &sb->map[SB_NR_TO_INDEX(sb, bitnr)].cleared;
+ *   - lib/sbitmap.c|175| <<sbitmap_get>> index = SB_NR_TO_INDEX(sb, alloc_hint);
+ *   - lib/sbitmap.c|211| <<sbitmap_get_shallow>> index = SB_NR_TO_INDEX(sb, alloc_hint);
+ */
 #define SB_NR_TO_INDEX(sb, bitnr) ((bitnr) >> (sb)->shift)
+/*
+ * called by:
+ *   - include/linux/sbitmap.h|279| <<__sbitmap_for_each_set>> nr = SB_NR_TO_BIT(sb, start);
+ *   - include/linux/sbitmap.h|336| <<sbitmap_set_bit>> set_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - include/linux/sbitmap.h|341| <<sbitmap_clear_bit>> clear_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - include/linux/sbitmap.h|354| <<sbitmap_deferred_clear_bit>> set_bit(SB_NR_TO_BIT(sb, bitnr), addr);
+ *   - include/linux/sbitmap.h|360| <<sbitmap_clear_bit_unlock>> clear_bit_unlock(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - include/linux/sbitmap.h|365| <<sbitmap_test_bit>> return test_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
+ *   - lib/sbitmap.c|183| <<sbitmap_get>> alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
+ *   - lib/sbitmap.c|217| <<sbitmap_get_shallow>> SB_NR_TO_BIT(sb, alloc_hint), true);
+ */
 #define SB_NR_TO_BIT(sb, bitnr) ((bitnr) & ((1U << (sb)->shift) - 1U))
 
 typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);
@@ -251,6 +270,13 @@ typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);
  * This is inline even though it's non-trivial so that the function calls to the
  * callback will hopefully get optimized away.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|1488| <<blk_mq_dequeue_from_ctx>> __sbitmap_for_each_set(&hctx->ctx_map, off,
+ *   - include/linux/sbitmap.h|311| <<sbitmap_for_each_set>> __sbitmap_for_each_set(sb, 0, fn, data);
+ *
+ * 如果fn返回true继续下一个bit, 如果返回false就直接返回了
+ */
 static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 					  unsigned int start,
 					  sb_for_each_fn fn, void *data)
@@ -259,6 +285,7 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 	unsigned int nr;
 	unsigned int scanned = 0;
 
+	/* 因为start不一定是0, 所以会绕回开始 */
 	if (start >= sb->depth)
 		start = 0;
 	index = SB_NR_TO_INDEX(sb, start);
@@ -266,6 +293,9 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 
 	while (scanned < sb->depth) {
 		unsigned long word;
+		/*
+		 * sb->map是struct sbitmap_word *map;
+		 */
 		unsigned int depth = min_t(unsigned int,
 					   sb->map[index].depth - nr,
 					   sb->depth - scanned);
@@ -285,6 +315,7 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 			nr = find_next_bit(&word, depth, nr);
 			if (nr >= depth)
 				break;
+			/* Should return true to continue or false to break early. */
 			if (!fn(sb, (index << sb->shift) + nr, data))
 				return;
 
@@ -303,6 +334,9 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
  * @fn: Callback. Should return true to continue or false to break early.
  * @data: Pointer to pass to callback.
  */
+/*
+ * 如果fn返回true继续下一个bit, 如果返回false就直接返回了
+ */
 static inline void sbitmap_for_each_set(struct sbitmap *sb, sb_for_each_fn fn,
 					void *data)
 {
diff --git a/lib/percpu-refcount.c b/lib/percpu-refcount.c
index 9877682..4d6f165 100644
--- a/lib/percpu-refcount.c
+++ b/lib/percpu-refcount.c
@@ -36,6 +36,9 @@
 static DEFINE_SPINLOCK(percpu_ref_switch_lock);
 static DECLARE_WAIT_QUEUE_HEAD(percpu_ref_switch_waitq);
 
+/*
+ * 去掉percpu_ref->percpu_count_ptr的最后两位并返回
+ */
 static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
 {
 	return (unsigned long __percpu *)
@@ -56,6 +59,12 @@ static unsigned long __percpu *percpu_count_ptr(struct percpu_ref *ref)
  * Note that @release must not sleep - it may potentially be called from RCU
  * callback context by percpu_ref_kill().
  */
+/*
+ * 一个调用的例子block/blk-core.c|534| <<blk_alloc_queue_node>> :
+ *  534         if (percpu_ref_init(&q->q_usage_counter,
+ *  535                                 blk_queue_usage_counter_release,
+ *  536                                 PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
+ */
 int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 		    unsigned int flags, gfp_t gfp)
 {
@@ -82,6 +91,10 @@ int percpu_ref_init(struct percpu_ref *ref, percpu_ref_func_t *release,
 
 	atomic_long_set(&ref->count, start_count);
 
+	/*
+	 * release调用的地方:
+	 *   - include/linux/percpu-refcount.h|310| <<percpu_ref_put_many>> ref->release(ref);
+	 */
 	ref->release = release;
 	ref->confirm_switch = NULL;
 	return 0;
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 5b382c1..9c4ea6b 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -23,6 +23,12 @@
 /*
  * See if we have deferred clears that we can batch move
  */
+/*
+ * called by:
+ *   - lib/sbitmap.c|108| <<sbitmap_resize>> sbitmap_deferred_clear(sb, i);
+ *   - lib/sbitmap.c|163| <<sbitmap_find_bit_in_index>> if (!sbitmap_deferred_clear(sb, index))
+ *   - lib/sbitmap.c|223| <<sbitmap_get_shallow>> if (sbitmap_deferred_clear(sb, index))
+ */
 static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 {
 	unsigned long mask, val;
@@ -448,6 +454,13 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
+/*
+ * called by:
+ *   - block/blk-mq-tag.c|184| <<__blk_mq_get_tag>> return __sbitmap_queue_get(bt);
+ *   - block/kyber-iosched.c|723| <<kyber_get_domain_token>> nr = __sbitmap_queue_get(domain_tokens);
+ *   - block/kyber-iosched.c|740| <<kyber_get_domain_token>> nr = __sbitmap_queue_get(domain_tokens);
+ *   - include/linux/sbitmap.h|477| <<sbitmap_queue_get>> nr = __sbitmap_queue_get(sbq);
+ */
 int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 {
 	unsigned int hint, depth;
@@ -607,6 +620,9 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * 临时记住一点, 会wake_up()!
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
-- 
2.7.4

