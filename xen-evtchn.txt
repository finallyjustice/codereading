Xen: Event Channel

Event channel is an interrupt mechanism in Xen for hypervisor and guests to
notify each other. An event is the Xen equivalent of a hardware interrupt.
Currently, Xen provides two event channel ABIs: 2-Level and FIFO.

Xen provides following types of event channels:

* ECS_INTERDOMAIN
* ECS_PIRQ
* ECS_VIRQ
* ECS_IPI

The objects (struct evtchn) are indexed using a two level scheme of groups and
buckets. Each group is a page of bucket pointers. Each bucket is a page-sized
array of struct evtchn's.

The following event channel operations are supported:

* EVTCHNOP_bind_interdomain
* EVTCHNOP_bind_virq
* EVTCHNOP_bind_pirq
* EVTCHNOP_close
* EVTCHNOP_send
* EVTCHNOP_status
* EVTCHNOP_alloc_unbound
* EVTCHNOP_bind_ipi
* EVTCHNOP_bind_vcpu
* EVTCHNOP_unmask
* EVTCHNOP_reset
* EVTCHNOP_init_control
* EVTCHNOP_expand_array
* EVTCHNOP_set_priority


2-Level
===============================================================================

struct shared_info:
  -> xen_ulong_t evtchn_pending[sizeof(xen_ulong_t) * 8]; // each bit is a port
  -> xen_ulong_t evtchn_mask[sizeof(xen_ulong_t) * 8]; // each bit is a port

struct vcpu_info:
  -> uint8_t evtchn_upcall_pending; // 0 or 1, whether event is pending
  -> uint8_t evtchn_upcall_mask; // 0 or 1, mask all events
  -> xen_ulong_t evtchn_pending_sel; // each bit is 64 ports

pseudocode to set pending:
// set port bit in shared_info->evtchn_pending
// set bucket bit in vcpu_info->evtchn_pending_sel
// set evtchn_upcall_pending


FIFO
===============================================================================

The key data structures are per-domain "event array" and per-vcpu "control
block".

Event array consists of a set of event_word_t and each represents an event.
Control block contains a "head" array and each element represents the head of
each event priority queue.

struct domain
  -> struct evtchn_fifo_domain *evtchn_fifo;
       -> event_word_t *event_array[EVTCHN_FIFO_MAX_EVENT_ARRAY_PAGES];
       -> unsigned int num_evtchns;

struct vcpu
  -> struct evtchn_fifo_vcpu *evtchn_fifo;
       -> struct evtchn_fifo_control_block *control_block;
       -> struct evtchn_fifo_queue queue[EVTCHN_FIFO_MAX_QUEUES];

struct evtchn_fifo_queue
  -> uint32_t *head
  -> uint32_t tail
  -> uint8_t priority
  -> spinlock_t lock

struct evtchn_fifo_control_block
  -> uint32_t ready // each bit indicates if an event is pending in that queue
  -> uint32_t _rsvd
  -> uint32_t head[EVTCHN_FIFO_MAX_QUEUES]

pseudocode to set pending:

// find queue according to priority
q = &v->evtchn_fifo->queue[evtchn->priority];
// put event at tail if queue is not empty
if ( q->tail )
{
    tail_word = evtchn_fifo_word_from_port(d, q->tail);
    linked = evtchn_fifo_set_link(d, tail_word, port);
}
// put the event at head if queue is empty
if ( !linked )
    write_atomic(q->head, port);
q->tail = port;
// set evtchn_upcall_pending only if queue is empty and
// v->evtchn_fifo->control_block->ready is not set


IRQ on PVHVM
===============================================================================

The event channel is emulated as a virtual irq in pvhvm. The guest allocates an
interrupt gate via alloc_intr_gate. The handler for vector
HYPERVISOR_CALLBACK_VECTOR is xen_hvm_callback_vector.


IRQ on PVM
===============================================================================

The event channel handler in pvm is xen_hypervisor_callback
(xen_hypervisor_callback => xen_do_hypervisor_callback =>
 xen_evtchn_do_upcall).

The pvm guest registers the callback fcuntion to xen hypervisor via
CALLBACKOP_register hypervisor of CALLBACKTYPE_event. The xen hypervisor stores
this information in VCPU_event_addr.

The xen hypervisor calls "test_guest_events" in arch/x86/x86_64/entry.S every
time it is going to switch to guest domain context.

