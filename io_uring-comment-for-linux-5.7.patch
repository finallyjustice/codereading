From 5afda9bbe2173b71204eca8745666bc6ceab7cd8 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 14 Jun 2020 11:34:19 -0700
Subject: [PATCH 1/1] io_uring comment for linux-5.7

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 fs/io_uring.c                 | 691 ++++++++++++++++++++++++++++++++++
 include/uapi/linux/io_uring.h |  48 +++
 2 files changed, 739 insertions(+)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index bb25e3997d41..ca5e660a4e3e 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -88,7 +88,30 @@
 #include "internal.h"
 #include "io-wq.h"
 
+/*
+ * root      5808  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wq_manager]
+ * root      5809  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wqe_worker-0]
+ * root      5810  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wqe_worker-0]
+ * root      5811  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wq_manager]
+ * root      5812  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wqe_worker-0]
+ * root      5813  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wqe_worker-0]
+ * root      5815  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wq_manager]
+ * root      5816  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wqe_worker-0]
+ * root      5817  0.0  0.0      0     0 ?        S    10:00   0:00 [io_wqe_worker-0]
+ */
+
+/*
+ * 在以下使用IORING_MAX_ENTRIES:
+ *   - fs/io_uring.c|92| <<IORING_MAX_CQ_ENTRIES>> #define IORING_MAX_CQ_ENTRIES (2 * IORING_MAX_ENTRIES)
+ *   - fs/io_uring.c|7937| <<io_uring_create>> if (entries > IORING_MAX_ENTRIES) {
+ *   - fs/io_uring.c|7940| <<io_uring_create>> entries = IORING_MAX_ENTRIES;
+ */
 #define IORING_MAX_ENTRIES	32768
+/*
+ * 在以下使用IORING_MAX_CQ_ENTRIES:
+ *   - fs/io_uring.c|7960| <<io_uring_create>> if (p->cq_entries > IORING_MAX_CQ_ENTRIES) {
+ *   - fs/io_uring.c|7963| <<io_uring_create>> p->cq_entries = IORING_MAX_CQ_ENTRIES;
+ */
 #define IORING_MAX_CQ_ENTRIES	(2 * IORING_MAX_ENTRIES)
 
 /*
@@ -97,6 +120,10 @@
 #define IORING_FILE_TABLE_SHIFT	9
 #define IORING_MAX_FILES_TABLE	(1U << IORING_FILE_TABLE_SHIFT)
 #define IORING_FILE_TABLE_MASK	(IORING_MAX_FILES_TABLE - 1)
+/*
+ * 在以下使用IORING_MAX_FIXED_FILES:
+ *   - fs/io_uring.c|6597| <<io_sqe_files_register>> if (nr_args > IORING_MAX_FIXED_FILES)
+ */
 #define IORING_MAX_FIXED_FILES	(64 * IORING_MAX_FILES_TABLE)
 
 struct io_uring {
@@ -246,6 +273,16 @@ struct io_ring_ctx {
 		unsigned long		sq_check_overflow;
 
 		struct list_head	defer_list;
+		/*
+		 * 在以下使用io_ring_ctx->timeout_list:
+		 *   - fs/io_uring.c|985| <<io_ring_ctx_alloc>> INIT_LIST_HEAD(&ctx->timeout_list);
+		 *   - fs/io_uring.c|1055| <<io_get_timeout_req>> req = list_first_entry_or_null(&ctx->timeout_list, struct io_kiocb, list);
+		 *   - fs/io_uring.c|1182| <<io_kill_timeouts>> list_for_each_entry_safe(req, tmp, &ctx->timeout_list, list)
+		 *   - fs/io_uring.c|4677| <<io_timeout_fn>> list_for_each_entry_continue_reverse(prev, &ctx->timeout_list, list)
+		 *   - fs/io_uring.c|4697| <<io_timeout_cancel>> list_for_each_entry(req, &ctx->timeout_list, list) {
+		 *   - fs/io_uring.c|4815| <<io_timeout>> entry = ctx->timeout_list.prev;
+		 *   - fs/io_uring.c|4826| <<io_timeout>> list_for_each_prev(entry, &ctx->timeout_list) {
+		 */
 		struct list_head	timeout_list;
 		struct list_head	cq_overflow_list;
 
@@ -259,6 +296,22 @@ struct io_ring_ctx {
 	struct io_wq		*io_wq;
 	struct task_struct	*sqo_thread;	/* if using sq thread polling */
 	struct mm_struct	*sqo_mm;
+	/*
+	 * 在以下使用io_ring_ctx->sqo_wait:
+	 *   - fs/io_uring.c|995| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|1318| <<io_cqring_ev_posted>> if (waitqueue_active(&ctx->sqo_wait))
+	 *   - fs/io_uring.c|1319| <<io_cqring_ev_posted>> wake_up(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|2151| <<io_iopoll_req_issued>> wq_has_sleeper(&ctx->sqo_wait))
+	 *   - fs/io_uring.c|2152| <<io_iopoll_req_issued>> wake_up(&ctx->sqo_wait);
+	 *   - fs/io_uring.c|6273| <<io_sq_thread>> prepare_to_wait(&ctx->sqo_wait, &wait,
+	 *   - fs/io_uring.c|6285| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|6297| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|6302| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|6308| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|6314| <<io_sq_thread>> finish_wait(&ctx->sqo_wait, &wait);
+	 *   - fs/io_uring.c|7740| <<io_ring_ctx_wait_and_kill>> while (ctx->sqo_thread && !wq_has_sleeper(&ctx->sqo_wait))
+	 *   - fs/io_uring.c|7972| <<SYSCALL_DEFINE6(io_uring_enter)>> wake_up(&ctx->sqo_wait);
+	 */
 	wait_queue_head_t	sqo_wait;
 
 	/*
@@ -299,8 +352,29 @@ struct io_ring_ctx {
 		unsigned		cq_mask;
 		atomic_t		cq_timeouts;
 		unsigned long		cq_check_overflow;
+		/*
+		 * 在以下使用io_ring_ctx->io_ring_ctx:
+		 *   - fs/io_uring.c|989| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->cq_wait);
+		 *   - fs/io_uring.c|1094| <<__io_commit_cqring>> if (wq_has_sleeper(&ctx->cq_wait)) {
+		 *   - fs/io_uring.c|1095| <<__io_commit_cqring>> wake_up_interruptible(&ctx->cq_wait);
+		 *   - fs/io_uring.c|7649| <<io_uring_poll>> poll_wait(file, &ctx->cq_wait, wait);
+		 */
 		struct wait_queue_head	cq_wait;
 		struct fasync_struct	*cq_fasync;
+		/*
+		 * 在以下使用io_ring_ctx->cq_ev_fd:
+		 *   - fs/io_uring.c|1308| <<io_should_trigger_evfd>> if (!ctx->cq_ev_fd)
+		 *   - fs/io_uring.c|1337| <<io_cqring_ev_posted>> eventfd_signal(ctx->cq_ev_fd, 1);
+		 *   - fs/io_uring.c|7599| <<io_eventfd_register>> if (ctx->cq_ev_fd)
+		 *   - fs/io_uring.c|7605| <<io_eventfd_register>> ctx->cq_ev_fd = eventfd_ctx_fdget(fd);
+		 *   - fs/io_uring.c|7606| <<io_eventfd_register>> if (IS_ERR(ctx->cq_ev_fd)) {
+		 *   - fs/io_uring.c|7607| <<io_eventfd_register>> int ret = PTR_ERR(ctx->cq_ev_fd);
+		 *   - fs/io_uring.c|7608| <<io_eventfd_register>> ctx->cq_ev_fd = NULL;
+		 *   - fs/io_uring.c|7622| <<io_eventfd_unregister>> if (ctx->cq_ev_fd) {
+		 *   - fs/io_uring.c|7623| <<io_eventfd_unregister>> eventfd_ctx_put(ctx->cq_ev_fd);
+		 *   - fs/io_uring.c|7624| <<io_eventfd_unregister>> ctx->cq_ev_fd = NULL;
+		 *   - fs/io_uring.c|8735| <<SYSCALL_DEFINE4>> ctx->cq_ev_fd != NULL, ret);
+		 */
 		struct eventfd_ctx	*cq_ev_fd;
 	} ____cacheline_aligned_in_smp;
 
@@ -869,12 +943,33 @@ static struct kmem_cache *req_cachep;
 
 static const struct file_operations io_uring_fops;
 
+/*
+ * called by:
+ *   - net/unix/scm.c|38| <<unix_get_socket>> u_sock = io_uring_get_socket(filp);
+ */
 struct sock *io_uring_get_socket(struct file *file)
 {
 #if defined(CONFIG_UNIX)
+	/*
+	 * 在以下使用io_uring_fops:
+	 *   - fs/io_uring.c|875| <<io_uring_get_socket>> if (file->f_op == &io_uring_fops) {
+	 *   - fs/io_uring.c|2038| <<io_file_supports_async>> if (S_ISREG(mode) && file->f_op != &io_uring_fops)
+	 *   - fs/io_uring.c|3399| <<io_close_prep>> if (req->file->f_op == &io_uring_fops ||
+	 *   - fs/io_uring.c|6582| <<io_sqe_files_register>> if (file->f_op == &io_uring_fops) {
+	 *   - fs/io_uring.c|6745| <<__io_sqe_files_update>> if (file->f_op == &io_uring_fops) {
+	 *   - fs/io_uring.c|6826| <<io_init_wq_offload>> if (f.file->f_op != &io_uring_fops) {
+	 *   - fs/io_uring.c|7511| <<SYSCALL_DEFINE6(io_uring_enter)>> if (f.file->f_op != &io_uring_fops)
+	 *   - fs/io_uring.c|7731| <<io_uring_get_fd>> file = anon_inode_getfile("[io_uring]", &io_uring_fops, ctx,
+	 *   - fs/io_uring.c|8098| <<SYSCALL_DEFINE4(io_uring_register)>> if (f.file->f_op != &io_uring_fops)
+	 */
 	if (file->f_op == &io_uring_fops) {
 		struct io_ring_ctx *ctx = file->private_data;
 
+		/*
+		 * struct io_ring_ctx:
+		 *  -> struct socket *ring_sock;
+		 *      -> struct sock *sk;
+		 */
 		return ctx->ring_sock->sk;
 	}
 #endif
@@ -882,6 +977,10 @@ struct sock *io_uring_get_socket(struct file *file)
 }
 EXPORT_SYMBOL(io_uring_get_socket);
 
+/*
+ * 在以下使用io_ring_ctx_ref_free():
+ *   - fs/io_uring.c|928| <<io_ring_ctx_alloc>> if (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,
+ */
 static void io_ring_ctx_ref_free(struct percpu_ref *ref)
 {
 	struct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);
@@ -889,6 +988,15 @@ static void io_ring_ctx_ref_free(struct percpu_ref *ref)
 	complete(&ctx->completions[0]);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7930| <<io_uring_create>> ctx = io_ring_ctx_alloc(p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)
+ *   -> io_uring_setup()
+ *       -> io_uring_create()
+ *           -> io_ring_ctx_alloc()
+ */
 static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 {
 	struct io_ring_ctx *ctx;
@@ -927,6 +1035,13 @@ static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 
 	ctx->flags = p->flags;
 	init_waitqueue_head(&ctx->sqo_wait);
+	/*
+	 * 在以下使用io_ring_ctx:
+	 *   - fs/io_uring.c|989| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->cq_wait);
+	 *   - fs/io_uring.c|1094| <<__io_commit_cqring>> if (wq_has_sleeper(&ctx->cq_wait)) {
+	 *   - fs/io_uring.c|1095| <<__io_commit_cqring>> wake_up_interruptible(&ctx->cq_wait);
+	 *   - fs/io_uring.c|7649| <<io_uring_poll>> poll_wait(file, &ctx->cq_wait, wait);
+	 */
 	init_waitqueue_head(&ctx->cq_wait);
 	INIT_LIST_HEAD(&ctx->cq_overflow_list);
 	init_completion(&ctx->completions[0]);
@@ -952,14 +1067,29 @@ static struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|978| <<req_need_defer>> return __req_need_defer(req);
+ *   - fs/io_uring.c|1004| <<io_get_timeout_req>> if (!__req_need_defer(req)) {
+ */
 static inline bool __req_need_defer(struct io_kiocb *req)
 {
+	/*
+	 * struct io_kiocb:
+	 *  -> struct io_ring_ctx *ctx;
+	 */
 	struct io_ring_ctx *ctx = req->ctx;
 
 	return req->sequence != ctx->cached_cq_tail
 				+ atomic_read(&ctx->cached_cq_overflow);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|988| <<io_get_deferred_req>> if (req && !req_need_defer(req)) {
+ *   - fs/io_uring.c|5046| <<io_req_defer>> if (!req_need_defer(req) && list_empty_careful(&ctx->defer_list))
+ *   - fs/io_uring.c|5058| <<io_req_defer>> if (!req_need_defer(req) && list_empty(&ctx->defer_list)) {
+ */
 static inline bool req_need_defer(struct io_kiocb *req)
 {
 	if (unlikely(req->flags & REQ_F_IO_DRAIN))
@@ -968,6 +1098,10 @@ static inline bool req_need_defer(struct io_kiocb *req)
 	return false;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1137| <<io_commit_cqring>> while ((req = io_get_deferred_req(ctx)) != NULL)
+ */
 static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
 {
 	struct io_kiocb *req;
@@ -981,6 +1115,10 @@ static struct io_kiocb *io_get_deferred_req(struct io_ring_ctx *ctx)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1132| <<io_commit_cqring>> while ((req = io_get_timeout_req(ctx)) != NULL)
+ */
 static struct io_kiocb *io_get_timeout_req(struct io_ring_ctx *ctx)
 {
 	struct io_kiocb *req;
@@ -998,6 +1136,10 @@ static struct io_kiocb *io_get_timeout_req(struct io_ring_ctx *ctx)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1135| <<io_commit_cqring>> __io_commit_cqring(ctx);
+ */
 static void __io_commit_cqring(struct io_ring_ctx *ctx)
 {
 	struct io_rings *rings = ctx->rings;
@@ -1005,6 +1147,13 @@ static void __io_commit_cqring(struct io_ring_ctx *ctx)
 	/* order cqe stores with ring update */
 	smp_store_release(&rings->cq.tail, ctx->cached_cq_tail);
 
+	/*
+	 * 在以下使用io_ring_ctx:
+	 *   - fs/io_uring.c|989| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->cq_wait);
+	 *   - fs/io_uring.c|1094| <<__io_commit_cqring>> if (wq_has_sleeper(&ctx->cq_wait)) {
+	 *   - fs/io_uring.c|1095| <<__io_commit_cqring>> wake_up_interruptible(&ctx->cq_wait);
+	 *   - fs/io_uring.c|7649| <<io_uring_poll>> poll_wait(file, &ctx->cq_wait, wait);
+	 */
 	if (wq_has_sleeper(&ctx->cq_wait)) {
 		wake_up_interruptible(&ctx->cq_wait);
 		kill_fasync(&ctx->cq_fasync, SIGIO, POLL_IN);
@@ -1056,6 +1205,10 @@ static inline void io_req_work_drop_env(struct io_kiocb *req)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1153| <<io_queue_async_work>> io_prep_async_work(req, &link);
+ */
 static inline void io_prep_async_work(struct io_kiocb *req,
 				      struct io_kiocb **link)
 {
@@ -1074,6 +1227,15 @@ static inline void io_prep_async_work(struct io_kiocb *req,
 	*link = io_prep_linked_timeout(req);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1197| <<io_commit_cqring>> io_queue_async_work(req);
+ *   - fs/io_uring.c|1625| <<io_free_req>> io_queue_async_work(nxt);
+ *   - fs/io_uring.c|1806| <<io_iopoll_queue>> io_queue_async_work(req);
+ *   - fs/io_uring.c|3532| <<io_close>> io_queue_async_work(req);
+ *   - fs/io_uring.c|5666| <<__io_queue_sqe>> io_queue_async_work(req);
+ *   - fs/io_uring.c|5727| <<io_queue_sqe>> io_queue_async_work(req);
+ */
 static inline void io_queue_async_work(struct io_kiocb *req)
 {
 	struct io_ring_ctx *ctx = req->ctx;
@@ -1089,6 +1251,11 @@ static inline void io_queue_async_work(struct io_kiocb *req)
 		io_queue_linked_timeout(link);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1183| <<io_kill_timeouts>> io_kill_timeout(req);
+ *   - fs/io_uring.c|1192| <<io_commit_cqring>> io_kill_timeout(req);
+ */
 static void io_kill_timeout(struct io_kiocb *req)
 {
 	int ret;
@@ -1103,6 +1270,10 @@ static void io_kill_timeout(struct io_kiocb *req)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7496| <<io_ring_ctx_wait_and_kill>> io_kill_timeouts(ctx);
+ */
 static void io_kill_timeouts(struct io_ring_ctx *ctx)
 {
 	struct io_kiocb *req, *tmp;
@@ -1113,6 +1284,20 @@ static void io_kill_timeouts(struct io_ring_ctx *ctx)
 	spin_unlock_irq(&ctx->completion_lock);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1280| <<io_cqring_overflow_flush>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|1342| <<__io_cqring_add_event>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|1507| <<io_link_cancel_timeout>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|1581| <<io_fail_links>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|1792| <<io_iopoll_complete>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|4277| <<io_async_task_func>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|4461| <<io_poll_remove_one>> io_commit_cqring(req->ctx);
+ *   - fs/io_uring.c|4547| <<io_poll_complete>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|4683| <<io_timeout_fn>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|4746| <<io_timeout_remove>> io_commit_cqring(ctx);
+ *   - fs/io_uring.c|4919| <<io_async_find_and_cancel>> io_commit_cqring(ctx);
+ */
 static void io_commit_cqring(struct io_ring_ctx *ctx)
 {
 	struct io_kiocb *req;
@@ -1144,6 +1329,10 @@ static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
 	return &rings->cqes[tail & ctx->cq_mask];
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1336| <<io_cqring_ev_posted>> if (io_should_trigger_evfd(ctx))
+ */
 static inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)
 {
 	if (!ctx->cq_ev_fd)
@@ -1153,6 +1342,21 @@ static inline bool io_should_trigger_evfd(struct io_ring_ctx *ctx)
 	return io_wq_current_is_worker();
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1286| <<io_cqring_overflow_flush>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1345| <<__io_cqring_add_event>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1551| <<io_req_link_next>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1583| <<io_fail_links>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|1794| <<io_iopoll_complete>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4287| <<io_async_task_func>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4486| <<io_poll_remove_all>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4566| <<io_poll_task_handler>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4646| <<io_poll_add>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4686| <<io_timeout_fn>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4748| <<io_timeout_remove>> io_cqring_ev_posted(ctx);
+ *   - fs/io_uring.c|4921| <<io_async_find_and_cancel>> io_cqring_ev_posted(ctx);
+ */
 static void io_cqring_ev_posted(struct io_ring_ctx *ctx)
 {
 	if (waitqueue_active(&ctx->wait))
@@ -1358,6 +1562,12 @@ static void __io_req_aux_free(struct io_kiocb *req)
 	io_req_work_drop_env(req);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1683| <<io_free_req>> __io_free_req(req);
+ *   - fs/io_uring.c|1722| <<io_put_req_find_next>> __io_free_req(req);
+ *   - fs/io_uring.c|1760| <<__io_double_put_req>> __io_free_req(req);
+ */
 static void __io_free_req(struct io_kiocb *req)
 {
 	__io_req_aux_free(req);
@@ -1837,6 +2047,10 @@ static void io_iopoll_reap_events(struct io_ring_ctx *ctx)
 	mutex_unlock(&ctx->uring_lock);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7617| <<SYSCALL_DEFINE6(io_uring_enter)>> ret = io_iopoll_check(ctx, &nr_events, min_complete);
+ */
 static int io_iopoll_check(struct io_ring_ctx *ctx, unsigned *nr_events,
 			   long min)
 {
@@ -2415,6 +2629,11 @@ static ssize_t io_import_iovec(int rw, struct io_kiocb *req,
  * For files that don't have ->read_iter() and ->write_iter(), handle them
  * by looping over ->read() or ->write() manually.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|2592| <<io_read>> ret2 = loop_rw_iter(READ, req->file, kiocb, &iter);
+ *   - fs/io_uring.c|2707| <<io_write>> ret2 = loop_rw_iter(WRITE, req->file, kiocb, &iter);
+ */
 static ssize_t loop_rw_iter(int rw, struct file *file, struct kiocb *kiocb,
 			   struct iov_iter *iter)
 {
@@ -2500,6 +2719,11 @@ static int io_alloc_async_ctx(struct io_kiocb *req)
 	return  __io_alloc_async_ctx(req);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2743| <<io_read>> ret = io_setup_async_rw(req, io_size, iovec,
+ *   - fs/io_uring.c|2866| <<io_write>> ret = io_setup_async_rw(req, io_size, iovec,
+ */
 static int io_setup_async_rw(struct io_kiocb *req, ssize_t io_size,
 			     struct iovec *iovec, struct iovec *fast_iov,
 			     struct iov_iter *iter)
@@ -2637,6 +2861,10 @@ static int io_write_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 	return 0;
 }
 
+/*
+ * 处理IORING_OP_WRITEV/IORING_OP_WRITE_FIXED/IORING_OP_WRITE:
+ *   - fs/io_uring.c|5276| <<io_issue_sqe>> ret = io_write(req, force_nonblock);
+ */
 static int io_write(struct io_kiocb *req, bool force_nonblock)
 {
 	struct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;
@@ -3127,6 +3355,10 @@ static int io_provide_buffers_prep(struct io_kiocb *req,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|3178| <<io_provide_buffers>> ret = io_add_buffers(p, &head);
+ */
 static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)
 {
 	struct io_buffer *buf;
@@ -4534,6 +4766,10 @@ static int io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5131| <<io_issue_sqe>> ret = io_poll_add(req);
+ */
 static int io_poll_add(struct io_kiocb *req)
 {
 	struct io_poll_iocb *poll = &req->poll;
@@ -4701,6 +4937,10 @@ static int io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 	return 0;
 }
 
+/*
+ * 在以下使用io_timeout():
+ *   - fs/io_uring.c|5179| <<io_issue_sqe>> ret = io_timeout(req);
+ */
 static int io_timeout(struct io_kiocb *req)
 {
 	struct io_ring_ctx *ctx = req->ctx;
@@ -5076,6 +5316,11 @@ static void io_cleanup_req(struct io_kiocb *req)
 	req->flags &= ~REQ_F_NEED_CLEANUP;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5351| <<io_wq_submit_work>> ret = io_issue_sqe(req, NULL, false);
+ *   - fs/io_uring.c|5548| <<__io_queue_sqe>> ret = io_issue_sqe(req, sqe, true);
+ */
 static int io_issue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 			bool force_nonblock)
 {
@@ -5370,6 +5615,11 @@ static inline struct file *io_file_from_index(struct io_ring_ctx *ctx,
 	return table->files[index & IORING_FILE_TABLE_MASK];;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|2756| <<io_splice_prep>> ret = io_file_get(NULL, req, READ_ONCE(sqe->splice_fd_in), &sp->file_in,
+ *   - fs/io_uring.c|5417| <<io_req_set_file>> return io_file_get(state, req, fd, &req->file, fixed);
+ */
 static int io_file_get(struct io_submit_state *state, struct io_kiocb *req,
 			int fd, struct file **out_file, bool fixed)
 {
@@ -5477,6 +5727,13 @@ static enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|1183| <<io_queue_async_work>> io_queue_linked_timeout(link);
+ *   - fs/io_uring.c|1695| <<io_link_work_cb>> io_queue_linked_timeout(link);
+ *   - fs/io_uring.c|5713| <<__io_queue_sqe>> io_queue_linked_timeout(linked_timeout);
+ *   - fs/io_uring.c|5738| <<__io_queue_sqe>> io_queue_linked_timeout(linked_timeout);
+ */
 static void io_queue_linked_timeout(struct io_kiocb *req)
 {
 	struct io_ring_ctx *ctx = req->ctx;
@@ -5518,6 +5775,12 @@ static struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)
 	return nxt;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|4356| <<io_async_task_func>> __io_queue_sqe(req, NULL);
+ *   - fs/io_uring.c|4640| <<io_poll_task_func>> __io_queue_sqe(nxt, NULL);
+ *   - fs/io_uring.c|5790| <<io_queue_sqe>> __io_queue_sqe(req, sqe);
+ */
 static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
 	struct io_kiocb *linked_timeout;
@@ -5595,6 +5858,11 @@ static void __io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 		revert_creds(old_creds);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5800| <<io_queue_link_head>> io_queue_sqe(req, NULL);
+ *   - fs/io_uring.c|5868| <<io_submit_sqe>> io_queue_sqe(req, sqe);
+ */
 static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 {
 	int ret;
@@ -5628,6 +5896,11 @@ static void io_queue_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5848| <<io_submit_sqe>> io_queue_link_head(head);
+ *   - fs/io_uring.c|6100| <<io_submit_sqes>> io_queue_link_head(link);
+ */
 static inline void io_queue_link_head(struct io_kiocb *req)
 {
 	if (unlikely(req->flags & REQ_F_FAIL_LINK)) {
@@ -5637,6 +5910,10 @@ static inline void io_queue_link_head(struct io_kiocb *req)
 		io_queue_sqe(req, NULL);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5905| <<io_submit_sqes>> err = io_submit_sqe(req, sqe, statep, &link);
+ */
 static int io_submit_sqe(struct io_kiocb *req, const struct io_uring_sqe *sqe,
 			  struct io_submit_state *state, struct io_kiocb **link)
 {
@@ -5719,6 +5996,10 @@ static void io_submit_state_end(struct io_submit_state *state)
 /*
  * Start submission side cache.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|6085| <<io_submit_sqes>> io_submit_state_start(&state, nr);
+ */
 static void io_submit_state_start(struct io_submit_state *state,
 				  unsigned int max_ios)
 {
@@ -5728,6 +6009,10 @@ static void io_submit_state_start(struct io_submit_state *state,
 	state->ios_left = max_ios;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6101| <<io_submit_sqes>> io_commit_sqring(ctx);
+ */
 static void io_commit_sqring(struct io_ring_ctx *ctx)
 {
 	struct io_rings *rings = ctx->rings;
@@ -5748,6 +6033,10 @@ static void io_commit_sqring(struct io_ring_ctx *ctx)
  * used, it's important that those reads are done through READ_ONCE() to
  * prevent a re-load down the line.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|5900| <<io_submit_sqes>> sqe = io_get_sqe(ctx);
+ */
 static const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)
 {
 	u32 *sq_array = ctx->sq_array;
@@ -5771,6 +6060,11 @@ static const struct io_uring_sqe *io_get_sqe(struct io_ring_ctx *ctx)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6099| <<io_submit_sqes>> io_consume_sqe(ctx);
+ *   - fs/io_uring.c|6110| <<io_submit_sqes>> io_consume_sqe(ctx);
+ */
 static inline void io_consume_sqe(struct io_ring_ctx *ctx)
 {
 	ctx->cached_sq_head++;
@@ -5780,6 +6074,10 @@ static inline void io_consume_sqe(struct io_ring_ctx *ctx)
 				IOSQE_IO_HARDLINK | IOSQE_ASYNC | \
 				IOSQE_BUFFER_SELECT)
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5899| <<io_submit_sqes>> err = io_init_req(ctx, req, sqe, statep, async);
+ */
 static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,
 		       const struct io_uring_sqe *sqe,
 		       struct io_submit_state *state, bool async)
@@ -5833,6 +6131,11 @@ static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,
 	}
 
 	/* same numerical values with corresponding REQ_F_*, safe to copy */
+	/*
+	 * 在以下使用IOSQE_IO_LINK:
+	 *   - fs/io_uring.c|6061| <<SQE_VALID_FLAGS>> #define SQE_VALID_FLAGS (IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK| \
+	 *   - fs/io_uring.c|6124| <<io_init_req>> IOSQE_BUFFER_SELECT | IOSQE_IO_LINK);
+	 */
 	req->flags |= sqe_flags & (IOSQE_IO_DRAIN | IOSQE_IO_HARDLINK |
 					IOSQE_ASYNC | IOSQE_FIXED_FILE |
 					IOSQE_BUFFER_SELECT | IOSQE_IO_LINK);
@@ -5843,6 +6146,11 @@ static int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,
 	return io_req_set_file(state, req, READ_ONCE(sqe->fd));
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6052| <<io_sq_thread>> ret = io_submit_sqes(ctx, to_submit, NULL, -1, true);
+ *   - fs/io_uring.c|7555| <<SYSCALL_DEFINE6(io_uring_enter)>> submitted = io_submit_sqes(ctx, to_submit, f.file, fd, false);
+ */
 static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr,
 			  struct file *ring_file, int ring_fd, bool async)
 {
@@ -5858,6 +6166,9 @@ static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr,
 	}
 
 	/* make sure SQ entry isn't read before tail */
+	/*
+	 * unsigned sq_entries;
+	 */
 	nr = min3(nr, ctx->sq_entries, io_sqring_entries(ctx));
 
 	if (!percpu_ref_tryget_many(&ctx->refs, nr))
@@ -5876,11 +6187,22 @@ static int io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr,
 		struct io_kiocb *req;
 		int err;
 
+		/*
+		 * Fetch an sqe, if one is available. Note that sqe_ptr will point to memory
+		 * that is mapped by userspace. This means that care needs to be taken to
+		 * ensure that reads are stable, as we cannot rely on userspace always
+		 * being a good citizen. If members of the sqe are validated and then later
+		 * used, it's important that those reads are done through READ_ONCE() to
+		 * prevent a re-load down the line.
+		 */
 		sqe = io_get_sqe(ctx);
 		if (unlikely(!sqe)) {
 			io_consume_sqe(ctx);
 			break;
 		}
+		/*
+		 * struct io_kiocb *req;
+		 */
 		req = io_alloc_req(ctx, statep);
 		if (unlikely(!req)) {
 			if (!submitted)
@@ -5933,6 +6255,11 @@ static inline void io_sq_thread_drop_mm(struct io_ring_ctx *ctx)
 	}
 }
 
+/*
+ * 在以下使用io_sq_thread():
+ *   - fs/io_uring.c|6910| <<io_sq_offload_start>> ctx->sqo_thread = kthread_create_on_cpu(io_sq_thread,
+ *   - fs/io_uring.c|6914| <<io_sq_offload_start>> ctx->sqo_thread = kthread_create(io_sq_thread, ctx,
+ */
 static int io_sq_thread(void *data)
 {
 	struct io_ring_ctx *ctx = data;
@@ -6065,6 +6392,11 @@ struct io_wait_queue {
 	unsigned nr_timeouts;
 };
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6273| <<io_wake_function>> if (!io_should_wake(iowq, true))
+ *   - fs/io_uring.c|6330| <<io_cqring_wait>> if (io_should_wake(&iowq, false))
+ */
 static inline bool io_should_wake(struct io_wait_queue *iowq, bool noflush)
 {
 	struct io_ring_ctx *ctx = iowq->ctx;
@@ -6078,6 +6410,10 @@ static inline bool io_should_wake(struct io_wait_queue *iowq, bool noflush)
 			atomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;
 }
 
+/*
+ * 在以下使用io_wake_function():
+ *   - fs/io_uring.c|6293| <<io_cqring_wait>> .func = io_wake_function,
+ */
 static int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,
 			    int wake_flags, void *key)
 {
@@ -6095,6 +6431,10 @@ static int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,
  * Wait until events become available, if we don't already have some. The
  * application must reap them itself, as they reside on the shared cq ring.
  */
+/*
+ * called by:
+ *   - fs/io_uring.c|7619| <<SYSCALL_DEFINE6(io_uring_enter)>> ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
+ */
 static int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,
 			  const sigset_t __user *sig, size_t sigsz)
 {
@@ -6153,6 +6493,10 @@ static int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,
 	return READ_ONCE(rings->cq.head) == READ_ONCE(rings->cq.tail) ? ret : 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6406| <<io_sqe_files_unregister>> __io_sqe_files_unregister(ctx);
+ */
 static void __io_sqe_files_unregister(struct io_ring_ctx *ctx)
 {
 #if defined(CONFIG_UNIX)
@@ -6176,6 +6520,10 @@ static void __io_sqe_files_unregister(struct io_ring_ctx *ctx)
 #endif
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6738| <<io_sqe_files_register>> if (percpu_ref_init(&ctx->file_data->refs, io_file_ref_kill,
+ */
 static void io_file_ref_kill(struct percpu_ref *ref)
 {
 	struct fixed_file_data *data;
@@ -6184,6 +6532,13 @@ static void io_file_ref_kill(struct percpu_ref *ref)
 	complete(&data->done);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6625| <<io_sqe_files_register>> io_sqe_files_unregister(ctx);
+ *   - fs/io_uring.c|6631| <<io_sqe_files_register>> io_sqe_files_unregister(ctx);
+ *   - fs/io_uring.c|7253| <<io_ring_ctx_free>> io_sqe_files_unregister(ctx);
+ *   - fs/io_uring.c|8123| <<__io_uring_register(处理IORING_UNREGISTER_FILES)>> ret = io_sqe_files_unregister(ctx);
+ */
 static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
 {
 	struct fixed_file_data *data = ctx->file_data;
@@ -6219,6 +6574,10 @@ static int io_sqe_files_unregister(struct io_ring_ctx *ctx)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6435| <<io_finish_async>> io_sq_thread_stop(ctx);
+ */
 static void io_sq_thread_stop(struct io_ring_ctx *ctx)
 {
 	if (ctx->sqo_thread) {
@@ -6234,6 +6593,11 @@ static void io_sq_thread_stop(struct io_ring_ctx *ctx)
 	}
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7178| <<io_sq_offload_start>> io_finish_async(ctx);
+ *   - fs/io_uring.c|7528| <<io_ring_ctx_free>> io_finish_async(ctx);
+ */
 static void io_finish_async(struct io_ring_ctx *ctx)
 {
 	io_sq_thread_stop(ctx);
@@ -6499,6 +6863,10 @@ static void destroy_fixed_file_ref_node(struct fixed_file_ref_node *ref_node)
 	kfree(ref_node);
 }
 
+/*
+ * 处理IORING_REGISTER_FILES:
+ *   - fs/io_uring.c|8117| <<__io_uring_register>> ret = io_sqe_files_register(ctx, arg, nr_args);
+ */
 static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
 				 unsigned nr_args)
 {
@@ -6623,6 +6991,10 @@ static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6787| <<__io_sqe_files_update>> err = io_sqe_file_register(ctx, file, i);
+ */
 static int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,
 				int index)
 {
@@ -6666,6 +7038,10 @@ static int io_sqe_file_register(struct io_ring_ctx *ctx, struct file *file,
 #endif
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|6929| <<__io_sqe_files_update>> err = io_queue_file_removal(data, file);
+ */
 static int io_queue_file_removal(struct fixed_file_data *data,
 				 struct file *file)
 {
@@ -6684,6 +7060,11 @@ static int io_queue_file_removal(struct fixed_file_data *data,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|5037| <<io_files_update>> ret = __io_sqe_files_update(ctx, &up, req->files_update.nr_args);
+ *   - fs/io_uring.c|6995| <<io_sqe_files_update>> return __io_sqe_files_update(ctx, &up, nr_args);
+ */
 static int __io_sqe_files_update(struct io_ring_ctx *ctx,
 				 struct io_uring_files_update *up,
 				 unsigned nr_args)
@@ -6770,6 +7151,10 @@ static int __io_sqe_files_update(struct io_ring_ctx *ctx,
 	return done ? done : err;
 }
 
+/*
+ * 处理IORING_REGISTER_FILES_UPDATE:
+ *   - fs/io_uring.c|8143| <<__io_uring_register>> ret = io_sqe_files_update(ctx, arg, nr_args);
+ */
 static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
 			       unsigned nr_args)
 {
@@ -6787,6 +7172,10 @@ static int io_sqe_files_update(struct io_ring_ctx *ctx, void __user *arg,
 	return __io_sqe_files_update(ctx, &up, nr_args);
 }
 
+/*
+ * 在以下使用io_free_work():
+ *   - fs/io_uring.c|7016| <<io_init_wq_offload>> data.free_work = io_free_work;
+ */
 static void io_free_work(struct io_wq_work *work)
 {
 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
@@ -6795,6 +7184,16 @@ static void io_free_work(struct io_wq_work *work)
 	io_put_req(req);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7097| <<io_sq_offload_start>> ret = io_init_wq_offload(ctx, p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)
+ *   -> io_uring_setup()
+ *       -> io_uring_create()
+ *           -> io_sq_offload_start()
+ *               -> io_init_wq_offload()
+ */
 static int io_init_wq_offload(struct io_ring_ctx *ctx,
 			      struct io_uring_params *p)
 {
@@ -6841,6 +7240,15 @@ static int io_init_wq_offload(struct io_ring_ctx *ctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|8119| <<io_uring_create>> ret = io_sq_offload_start(ctx, p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)
+ *   -> io_uring_setup()
+ *       -> io_uring_create()
+ *           -> io_sq_offload_start()
+ */
 static int io_sq_offload_start(struct io_ring_ctx *ctx,
 			       struct io_uring_params *p)
 {
@@ -6849,6 +7257,7 @@ static int io_sq_offload_start(struct io_ring_ctx *ctx,
 	mmgrab(current->mm);
 	ctx->sqo_mm = current->mm;
 
+	/* SQ poll thread */
 	if (ctx->flags & IORING_SETUP_SQPOLL) {
 		ret = -EPERM;
 		if (!capable(CAP_SYS_ADMIN))
@@ -6858,6 +7267,9 @@ static int io_sq_offload_start(struct io_ring_ctx *ctx,
 		if (!ctx->sq_thread_idle)
 			ctx->sq_thread_idle = HZ;
 
+		/*
+		 * sq_thread_cpu is valid
+		 */
 		if (p->flags & IORING_SETUP_SQ_AFF) {
 			int cpu = p->sq_thread_cpu;
 
@@ -6898,11 +7310,25 @@ static int io_sq_offload_start(struct io_ring_ctx *ctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7288| <<io_sqe_buffer_unregister>> io_unaccount_mem(ctx->user, imu->nr_bvecs);
+ *   - fs/io_uring.c|7393| <<io_sqe_buffer_register>> io_unaccount_mem(ctx->user, nr_pages);
+ *   - fs/io_uring.c|7404| <<io_sqe_buffer_register>> io_unaccount_mem(ctx->user, nr_pages);
+ *   - fs/io_uring.c|7436| <<io_sqe_buffer_register>> io_unaccount_mem(ctx->user, nr_pages);
+ *   - fs/io_uring.c|7551| <<io_ring_ctx_free>> io_unaccount_mem(ctx->user,
+ *   - fs/io_uring.c|8180| <<io_uring_create>> io_unaccount_mem(user, ring_pages(p->sq_entries,
+ */
 static void io_unaccount_mem(struct user_struct *user, unsigned long nr_pages)
 {
 	atomic_long_sub(nr_pages, &user->locked_vm);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7376| <<io_sqe_buffer_register>> ret = io_account_mem(ctx->user, nr_pages);
+ *   - fs/io_uring.c|8169| <<io_uring_create>> ret = io_account_mem(user,
+ */
 static int io_account_mem(struct user_struct *user, unsigned long nr_pages)
 {
 	unsigned long page_limit, cur_pages, new_pages;
@@ -6921,6 +7347,13 @@ static int io_account_mem(struct user_struct *user, unsigned long nr_pages)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7546| <<io_ring_ctx_free>> io_mem_free(ctx->rings);
+ *   - fs/io_uring.c|7547| <<io_ring_ctx_free>> io_mem_free(ctx->sq_sqes);
+ *   - fs/io_uring.c|8050| <<io_allocate_scq_urings>> io_mem_free(ctx->rings);
+ *   - fs/io_uring.c|8057| <<io_allocate_scq_urings>> io_mem_free(ctx->rings);
+ */
 static void io_mem_free(void *ptr)
 {
 	struct page *page;
@@ -6933,6 +7366,11 @@ static void io_mem_free(void *ptr)
 		free_compound_page(page);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|8033| <<io_allocate_scq_urings>> rings = io_mem_alloc(size);
+ *   - fs/io_uring.c|8055| <<io_allocate_scq_urings>> ctx->sq_sqes = io_mem_alloc(size);
+ */
 static void *io_mem_alloc(size_t size)
 {
 	gfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP |
@@ -6982,6 +7420,12 @@ static unsigned long ring_pages(unsigned sq_entries, unsigned cq_entries)
 	return pages;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7194| <<io_sqe_buffer_register>> io_sqe_buffer_unregister(ctx);
+ *   - fs/io_uring.c|7252| <<io_ring_ctx_free>> io_sqe_buffer_unregister(ctx);
+ *   - fs/io_uring.c|8114| <<__io_uring_register(处理IORING_UNREGISTER_BUFFERS)>> ret = io_sqe_buffer_unregister(ctx);
+ */
 static int io_sqe_buffer_unregister(struct io_ring_ctx *ctx)
 {
 	int i, j;
@@ -7032,6 +7476,10 @@ static int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,
 	return 0;
 }
 
+/*
+ * 处理IORING_REGISTER_BUFFERS:
+ *   - fs/io_uring.c|8079| <<__io_uring_register>> ret = io_sqe_buffer_register(ctx, arg, nr_args);
+ */
 static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 				  unsigned nr_args)
 {
@@ -7174,6 +7622,10 @@ static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
 	return ret;
 }
 
+/*
+ * 处理IORING_REGISTER_EVENTFD或者IORING_REGISTER_EVENTFD_ASYNC:
+ *   - fs/io_uring.c|8150| <<__io_uring_register>> ret = io_eventfd_register(ctx, arg);
+ */
 static int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)
 {
 	__s32 __user *fds = arg;
@@ -7195,6 +7647,11 @@ static int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7271| <<io_ring_ctx_free>> io_eventfd_unregister(ctx);
+ *   - fs/io_uring.c|8162| <<__io_uring_register(处理IORING_UNREGISTER_EVENTFD)>> ret = io_eventfd_unregister(ctx);
+ */
 static int io_eventfd_unregister(struct io_ring_ctx *ctx)
 {
 	if (ctx->cq_ev_fd) {
@@ -7261,6 +7718,13 @@ static __poll_t io_uring_poll(struct file *file, poll_table *wait)
 	struct io_ring_ctx *ctx = file->private_data;
 	__poll_t mask = 0;
 
+	/*
+	 * 在以下使用io_ring_ctx:
+	 *   - fs/io_uring.c|989| <<io_ring_ctx_alloc>> init_waitqueue_head(&ctx->cq_wait);
+	 *   - fs/io_uring.c|1094| <<__io_commit_cqring>> if (wq_has_sleeper(&ctx->cq_wait)) {
+	 *   - fs/io_uring.c|1095| <<__io_commit_cqring>> wake_up_interruptible(&ctx->cq_wait);
+	 *   - fs/io_uring.c|7649| <<io_uring_poll>> poll_wait(file, &ctx->cq_wait, wait);
+	 */
 	poll_wait(file, &ctx->cq_wait, wait);
 	/*
 	 * synchronizes with barrier from wq_has_sleeper call in
@@ -7306,6 +7770,11 @@ static void io_ring_exit_work(struct work_struct *work)
 	io_ring_ctx_free(ctx);
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7353| <<io_uring_release>> io_ring_ctx_wait_and_kill(ctx);
+ *   - fs/io_uring.c|7889| <<io_uring_create>> io_ring_ctx_wait_and_kill(ctx);
+ */
 static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 {
 	mutex_lock(&ctx->uring_lock);
@@ -7337,6 +7806,9 @@ static void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)
 	queue_work(system_wq, &ctx->exit_work);
 }
 
+/*
+ * struct file_operations io_uring_fops.release = io_uring_release()
+ */
 static int io_uring_release(struct inode *inode, struct file *file)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -7346,6 +7818,10 @@ static int io_uring_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7480| <<io_uring_flush>> io_uring_cancel_files(ctx, data);
+ */
 static void io_uring_cancel_files(struct io_ring_ctx *ctx,
 				  struct files_struct *files)
 {
@@ -7403,6 +7879,9 @@ static void io_uring_cancel_files(struct io_ring_ctx *ctx,
 	}
 }
 
+/*
+ * struct file_operations io_uring_fops.flush = io_uring_flush()
+ */
 static int io_uring_flush(struct file *file, void *data)
 {
 	struct io_ring_ctx *ctx = file->private_data;
@@ -7418,6 +7897,11 @@ static int io_uring_flush(struct file *file, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7521| <<io_uring_mmap>> ptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);
+ *   - fs/io_uring.c|7547| <<io_uring_nommu_get_unmapped_area>> ptr = io_uring_validate_mmap_request(file, pgoff, len);
+ */
 static void *io_uring_validate_mmap_request(struct file *file,
 					    loff_t pgoff, size_t sz)
 {
@@ -7447,6 +7931,9 @@ static void *io_uring_validate_mmap_request(struct file *file,
 
 #ifdef CONFIG_MMU
 
+/*
+ * struct file_operations io_uring_fops.mmap = io_uring_mmap()
+ */
 static int io_uring_mmap(struct file *file, struct vm_area_struct *vma)
 {
 	size_t sz = vma->vm_end - vma->vm_start;
@@ -7513,6 +8000,7 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 
 	ret = -ENXIO;
 	ctx = f.file->private_data;
+	/* ctx就在上一行 */
 	if (!percpu_ref_tryget(&ctx->refs))
 		goto out_fput;
 
@@ -7522,20 +8010,34 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 	 * we were asked to.
 	 */
 	ret = 0;
+	/*
+	 * IORING_SETUP_SQPOLL: SQ poll thread
+	 */
 	if (ctx->flags & IORING_SETUP_SQPOLL) {
 		if (!list_empty_careful(&ctx->cq_overflow_list))
 			io_cqring_overflow_flush(ctx, false);
+		/*
+		 * io_uring_enter(2) flags
+		 */
 		if (flags & IORING_ENTER_SQ_WAKEUP)
 			wake_up(&ctx->sqo_wait);
 		submitted = to_submit;
 	} else if (to_submit) {
 		mutex_lock(&ctx->uring_lock);
+		/*
+		 * 在以下调用io_submit_sqes():
+		 *   - fs/io_uring.c|6052| <<io_sq_thread>> ret = io_submit_sqes(ctx, to_submit, NULL, -1, true);
+		 *   - fs/io_uring.c|7555| <<SYSCALL_DEFINE6(io_uring_enter)>> submitted = io_submit_sqes(ctx, to_submit, f.file, fd, false);
+		 */
 		submitted = io_submit_sqes(ctx, to_submit, f.file, fd, false);
 		mutex_unlock(&ctx->uring_lock);
 
 		if (submitted != to_submit)
 			goto out;
 	}
+	/*
+	 * io_uring_enter(2) flags
+	 */
 	if (flags & IORING_ENTER_GETEVENTS) {
 		unsigned nr_events = 0;
 
@@ -7549,8 +8051,14 @@ SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
 		 */
 		if (ctx->flags & IORING_SETUP_IOPOLL &&
 		    !(ctx->flags & IORING_SETUP_SQPOLL)) {
+			/*
+			 * 只在这里调用
+			 */
 			ret = io_iopoll_check(ctx, &nr_events, min_complete);
 		} else {
+			/*
+			 * 只在这里调用
+			 */
 			ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
 		}
 	}
@@ -7649,6 +8157,18 @@ static void io_uring_show_fdinfo(struct seq_file *m, struct file *f)
 }
 #endif
 
+/*
+ * 在以下使用io_uring_fops:
+ *   - fs/io_uring.c|875| <<io_uring_get_socket>> if (file->f_op == &io_uring_fops) {
+ *   - fs/io_uring.c|2038| <<io_file_supports_async>> if (S_ISREG(mode) && file->f_op != &io_uring_fops)
+ *   - fs/io_uring.c|3399| <<io_close_prep>> if (req->file->f_op == &io_uring_fops ||
+ *   - fs/io_uring.c|6582| <<io_sqe_files_register>> if (file->f_op == &io_uring_fops) {
+ *   - fs/io_uring.c|6745| <<__io_sqe_files_update>> if (file->f_op == &io_uring_fops) {
+ *   - fs/io_uring.c|6826| <<io_init_wq_offload>> if (f.file->f_op != &io_uring_fops) {
+ *   - fs/io_uring.c|7511| <<SYSCALL_DEFINE6(io_uring_enter)>> if (f.file->f_op != &io_uring_fops)
+ *   - fs/io_uring.c|7731| <<io_uring_get_fd>> file = anon_inode_getfile("[io_uring]", &io_uring_fops, ctx,
+ *   - fs/io_uring.c|8098| <<SYSCALL_DEFINE4(io_uring_register)>> if (f.file->f_op != &io_uring_fops)
+ */
 static const struct file_operations io_uring_fops = {
 	.release	= io_uring_release,
 	.flush		= io_uring_flush,
@@ -7664,6 +8184,15 @@ static const struct file_operations io_uring_fops = {
 #endif
 };
 
+/*
+ * called by:
+ *   - fs/io_uring.c|7911| <<io_uring_create>> ret = io_allocate_scq_urings(ctx, p);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)
+ *   -> io_uring_setup()
+ *       -> io_uring_create()
+ *           -> io_allocate_scq_urings()
+ */
 static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 				  struct io_uring_params *p)
 {
@@ -7696,6 +8225,9 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
 		return -EOVERFLOW;
 	}
 
+	/*
+	 * struct io_uring_sqe *sq_sqes;
+	 */
 	ctx->sq_sqes = io_mem_alloc(size);
 	if (!ctx->sq_sqes) {
 		io_mem_free(ctx->rings);
@@ -7712,6 +8244,15 @@ static int io_allocate_scq_urings(struct io_ring_ctx *ctx,
  * fd to gain access to the SQ/CQ ring details. If UNIX sockets are enabled,
  * we have to tie this fd to a socket for file garbage collection purposes.
  */
+/*
+ * SYSCALL_DEFINE2(io_uring_setup)
+ *  -> io_uring_setup()
+ *      -> io_uring_create()
+ *          -> io_uring_get_fd()
+ *
+ * called by:
+ *   - fs/io_uring.c|7905| <<io_uring_create>> ret = io_uring_get_fd(ctx);
+ */
 static int io_uring_get_fd(struct io_ring_ctx *ctx)
 {
 	struct file *file;
@@ -7749,6 +8290,15 @@ static int io_uring_get_fd(struct io_ring_ctx *ctx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - include/trace/events/io_uring.h|24| <<global>> TRACE_EVENT(io_uring_create,
+ *   - fs/io_uring.c|7890| <<io_uring_setup>> return io_uring_create(entries, &p, params);
+ *
+ * SYSCALL_DEFINE2(io_uring_setup)
+ *   -> io_uring_setup()
+ *       -> io_uring_create()
+ */
 static int io_uring_create(unsigned entries, struct io_uring_params *p,
 			   struct io_uring_params __user *params)
 {
@@ -7774,6 +8324,7 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p,
 	 * of CQ ring entries manually.
 	 */
 	p->sq_entries = roundup_pow_of_two(entries);
+	/* app defines CQ size */
 	if (p->flags & IORING_SETUP_CQSIZE) {
 		/*
 		 * If IORING_SETUP_CQSIZE is set, we do the same roundup
@@ -7804,6 +8355,12 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p,
 		}
 	}
 
+	/*
+	 * SYSCALL_DEFINE2(io_uring_setup)
+	 *   -> io_uring_setup()
+	 *       -> io_uring_create()
+	 *           -> io_ring_ctx_alloc()
+	 */
 	ctx = io_ring_ctx_alloc(p);
 	if (!ctx) {
 		if (account_mem)
@@ -7817,14 +8374,31 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p,
 	ctx->user = user;
 	ctx->creds = get_current_cred();
 
+	/*
+	 * SYSCALL_DEFINE2(io_uring_setup)
+	 *   -> io_uring_setup()
+	 *       -> io_uring_create()
+	 *           -> io_allocate_scq_urings()
+	 */
 	ret = io_allocate_scq_urings(ctx, p);
 	if (ret)
 		goto err;
 
+	/*
+	 * SYSCALL_DEFINE2(io_uring_setup)
+	 *   -> io_uring_setup()
+	 *       -> io_uring_create()
+	 *           -> io_sq_offload_start()
+	 */
 	ret = io_sq_offload_start(ctx, p);
 	if (ret)
 		goto err;
 
+	/*
+	 * struct io_uring_params:
+	 *   -> struct io_sqring_offsets sq_off;
+	 *   -> struct io_cqring_offsets cq_off;
+	 */
 	memset(&p->sq_off, 0, sizeof(p->sq_off));
 	p->sq_off.head = offsetof(struct io_rings, sq.head);
 	p->sq_off.tail = offsetof(struct io_rings, sq.tail);
@@ -7854,6 +8428,12 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p,
 	 * Install ring fd as the very last thing, so we don't risk someone
 	 * having closed it before we finish setup
 	 */
+	/*
+	 * SYSCALL_DEFINE2(io_uring_setup)
+	 *   -> io_uring_setup()
+	 *       -> io_uring_create()
+	 *           -> io_sq_offload_start()
+	 */
 	ret = io_uring_get_fd(ctx);
 	if (ret < 0)
 		goto err;
@@ -7861,6 +8441,11 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p,
 	trace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);
 	return ret;
 err:
+	/*
+	 * 在以下调用io_ring_ctx_wait_and_kill():
+	 *   - fs/io_uring.c|7353| <<io_uring_release>> io_ring_ctx_wait_and_kill(ctx);
+	 *   - fs/io_uring.c|7889| <<io_uring_create>> io_ring_ctx_wait_and_kill(ctx);
+	 */
 	io_ring_ctx_wait_and_kill(ctx);
 	return ret;
 }
@@ -7870,6 +8455,11 @@ static int io_uring_create(unsigned entries, struct io_uring_params *p,
  * ring size, we return the actual sq/cq ring sizes (among other things) in the
  * params structure passed in.
  */
+/*
+ * called by:
+ *   - kernel/sys_ni.c|51| <<global>> COND_SYSCALL(io_uring_setup);
+ *   - fs/io_uring.c|7948| <<SYSCALL_DEFINE2(io_uring_setup)>> return io_uring_setup(entries, params);
+ */
 static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 {
 	struct io_uring_params p;
@@ -7877,6 +8467,10 @@ static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 
 	if (copy_from_user(&p, params, sizeof(p)))
 		return -EFAULT;
+	/*
+	 * struct io_uring_params:
+	 *   -> __u32 resv[3];
+	 */
 	for (i = 0; i < ARRAY_SIZE(p.resv); i++) {
 		if (p.resv[i])
 			return -EINVAL;
@@ -7893,9 +8487,18 @@ static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
 SYSCALL_DEFINE2(io_uring_setup, u32, entries,
 		struct io_uring_params __user *, params)
 {
+	/*
+	 * Sets up an aio uring context, and returns the fd. Applications asks for a
+	 * ring size, we return the actual sq/cq ring sizes (among other things) in the
+	 * params structure passed in.
+	 */
 	return io_uring_setup(entries, params);
 }
 
+/*
+ * 处理IORING_REGISTER_PROBE:
+ *   - fs/io_uring.c|8058| <<__io_uring_register>> ret = io_probe(ctx, arg, nr_args);
+ */
 static int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)
 {
 	struct io_uring_probe *p;
@@ -7935,6 +8538,10 @@ static int io_probe(struct io_ring_ctx *ctx, void __user *arg, unsigned nr_args)
 	return ret;
 }
 
+/*
+ * 处理IORING_REGISTER_PERSONALITY:
+ *   - fs/io_uring.c|8187| <<__io_uring_register>> ret = io_register_personality(ctx);
+ */
 static int io_register_personality(struct io_ring_ctx *ctx)
 {
 	const struct cred *creds = get_current_cred();
@@ -7947,6 +8554,10 @@ static int io_register_personality(struct io_ring_ctx *ctx)
 	return id;
 }
 
+/*
+ * 处理IORING_UNREGISTER_PERSONALITY:
+ *   - fs/io_uring.c|8193| <<__io_uring_register>> ret = io_unregister_personality(ctx, nr_args);
+ */
 static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)
 {
 	const struct cred *old_creds;
@@ -7960,6 +8571,18 @@ static int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - fs/io_uring.c|8056| <<__io_uring_register>> if (io_register_op_must_quiesce(opcode)) {
+ *   - fs/io_uring.c|8141| <<__io_uring_register>> if (io_register_op_must_quiesce(opcode)) {
+ *
+ * 以下的这些不需要quiesce返回false
+ *  - IORING_UNREGISTER_FILES:
+ *  - IORING_REGISTER_FILES_UPDATE:
+ *  - IORING_REGISTER_PROBE:
+ *  - IORING_REGISTER_PERSONALITY:
+ *  - IORING_UNREGISTER_PERSONALITY:
+ */
 static bool io_register_op_must_quiesce(int op)
 {
 	switch (op) {
@@ -7974,6 +8597,9 @@ static bool io_register_op_must_quiesce(int op)
 	}
 }
 
+/*
+ * 被SYSCALL_DEFINE4(io_uring_register)调用
+ */
 static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 			       void __user *arg, unsigned nr_args)
 	__releases(ctx->uring_lock)
@@ -7989,6 +8615,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 	if (percpu_ref_is_dying(&ctx->refs))
 		return -ENXIO;
 
+	/*
+	 * 以下的这些不需要quiesce返回false
+	 *  - IORING_UNREGISTER_FILES:
+	 *  - IORING_REGISTER_FILES_UPDATE:
+	 *  - IORING_REGISTER_PROBE:
+	 *  - IORING_REGISTER_PERSONALITY:
+	 *  - IORING_UNREGISTER_PERSONALITY:
+	 */
 	if (io_register_op_must_quiesce(opcode)) {
 		percpu_ref_kill(&ctx->refs);
 
@@ -8074,6 +8708,14 @@ static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,
 		break;
 	}
 
+	/*
+	 * 以下的这些不需要quiesce返回false
+	 *  - IORING_UNREGISTER_FILES:
+	 *  - IORING_REGISTER_FILES_UPDATE:
+	 *  - IORING_REGISTER_PROBE:
+	 *  - IORING_REGISTER_PERSONALITY:
+	 *  - IORING_UNREGISTER_PERSONALITY:
+	 */
 	if (io_register_op_must_quiesce(opcode)) {
 		/* bring the ctx back to life */
 		percpu_ref_reinit(&ctx->refs);
@@ -8090,17 +8732,38 @@ SYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,
 	long ret = -EBADF;
 	struct fd f;
 
+	/* f是struct fd */
 	f = fdget(fd);
 	if (!f.file)
 		return -EBADF;
 
 	ret = -EOPNOTSUPP;
+	/*
+	 * 在以下使用io_uring_fops:
+	 *   - fs/io_uring.c|875| <<io_uring_get_socket>> if (file->f_op == &io_uring_fops) {
+	 *   - fs/io_uring.c|2038| <<io_file_supports_async>> if (S_ISREG(mode) && file->f_op != &io_uring_fops)
+	 *   - fs/io_uring.c|3399| <<io_close_prep>> if (req->file->f_op == &io_uring_fops ||
+	 *   - fs/io_uring.c|6582| <<io_sqe_files_register>> if (file->f_op == &io_uring_fops) {
+	 *   - fs/io_uring.c|6745| <<__io_sqe_files_update>> if (file->f_op == &io_uring_fops) {
+	 *   - fs/io_uring.c|6826| <<io_init_wq_offload>> if (f.file->f_op != &io_uring_fops) {
+	 *   - fs/io_uring.c|7511| <<SYSCALL_DEFINE6(io_uring_enter)>> if (f.file->f_op != &io_uring_fops)
+	 *   - fs/io_uring.c|7731| <<io_uring_get_fd>> file = anon_inode_getfile("[io_uring]", &io_uring_fops, ctx,
+	 *   - fs/io_uring.c|8098| <<SYSCALL_DEFINE4(io_uring_register)>> if (f.file->f_op != &io_uring_fops)
+	 *
+	 * SYSCALL_DEFINE2(io_uring_setup)
+	 *  -> io_uring_setup()
+	 *      -> io_uring_create()
+	 *          -> io_uring_get_fd()
+	 */
 	if (f.file->f_op != &io_uring_fops)
 		goto out_fput;
 
 	ctx = f.file->private_data;
 
 	mutex_lock(&ctx->uring_lock);
+	/*
+	 * 只在这里被调用
+	 */
 	ret = __io_uring_register(ctx, opcode, arg, nr_args);
 	mutex_unlock(&ctx->uring_lock);
 	trace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,
@@ -8150,7 +8813,35 @@ static int __init io_uring_init(void)
 
 	BUILD_BUG_ON(ARRAY_SIZE(io_op_defs) != IORING_OP_LAST);
 	BUILD_BUG_ON(__REQ_F_LAST_BIT >= 8 * sizeof(int));
+	/*
+	 * 在以下使用req_cachep:
+	 *   - fs/io_uring.c|901| <<io_ring_ctx_alloc>> ctx->fallback_req = kmem_cache_alloc(req_cachep, GFP_KERNEL);
+	 *   - fs/io_uring.c|948| <<io_ring_ctx_alloc>> kmem_cache_free(req_cachep, ctx->fallback_req);
+	 *   - fs/io_uring.c|1306| <<io_alloc_req>> req = kmem_cache_alloc(req_cachep, gfp);
+	 *   - fs/io_uring.c|1314| <<io_alloc_req>> ret = kmem_cache_alloc_bulk(req_cachep, gfp, sz, state->reqs);
+	 *   - fs/io_uring.c|1321| <<io_alloc_req>> state->reqs[0] = kmem_cache_alloc(req_cachep, gfp);
+	 *   - fs/io_uring.c|1378| <<__io_free_req>> kmem_cache_free(req_cachep, req);
+	 *   - fs/io_uring.c|1423| <<io_free_req_many>> kmem_cache_free_bulk(req_cachep, rb->to_free, rb->reqs);
+	 *   - fs/io_uring.c|5716| <<io_submit_state_end>> kmem_cache_free_bulk(req_cachep, state->free_reqs, state->reqs);
+	 *   - fs/io_uring.c|7255| <<io_ring_ctx_free>> kmem_cache_free(req_cachep, ctx->fallback_req);
+	 *   - fs/io_uring.c|8153| <<io_uring_init>> req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);
+	 */
 	req_cachep = KMEM_CACHE(io_kiocb, SLAB_HWCACHE_ALIGN | SLAB_PANIC);
 	return 0;
 };
 __initcall(io_uring_init);
+
+/*
+ * SYSCALL_DEFINE6(io_uring_enter)
+ * SYSCALL_DEFINE2(io_uring_setup)
+ * SYSCALL_DEFINE4(io_uring_register)
+ */
+
+/*
+ * The application creates one or more SQ entries (SQE), and then updates the
+ * SQ tail. The kernel consumes the SQEs, and updates the SQ head.
+ *
+ * The kernel creates CQ entries (CQE) for one or more completed requests, and
+ * updates the CQ tail. The application consumes the CQEs and updates the CQ
+ * head.
+ */
diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index e48d746b8e2a..a9215911d7a4 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -77,6 +77,11 @@ enum {
 /* issue after inflight IO */
 #define IOSQE_IO_DRAIN		(1U << IOSQE_IO_DRAIN_BIT)
 /* links next sqe */
+/*
+ * 在以下使用IOSQE_IO_LINK:
+ *   - fs/io_uring.c|6061| <<SQE_VALID_FLAGS>> #define SQE_VALID_FLAGS (IOSQE_FIXED_FILE|IOSQE_IO_DRAIN|IOSQE_IO_LINK| \
+ *   - fs/io_uring.c|6124| <<io_init_req>> IOSQE_BUFFER_SELECT | IOSQE_IO_LINK);
+ */
 #define IOSQE_IO_LINK		(1U << IOSQE_IO_LINK_BIT)
 /* like LINK, but stronger */
 #define IOSQE_IO_HARDLINK	(1U << IOSQE_IO_HARDLINK_BIT)
@@ -89,6 +94,17 @@ enum {
  * io_uring_setup() flags
  */
 #define IORING_SETUP_IOPOLL	(1U << 0)	/* io_context is polled */
+/*
+ * 在以下使用IORING_SETUP_SQPOLL:
+ *   - fs/io_uring.c|1726| <<io_iopoll_complete>> if (ctx->flags & IORING_SETUP_SQPOLL)
+ *   - fs/io_uring.c|1986| <<io_iopoll_req_issued>> if ((ctx->flags & IORING_SETUP_SQPOLL) &&
+ *   - fs/io_uring.c|3929| <<io_accept_prep>> if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ *   - fs/io_uring.c|3989| <<io_connect_prep>> if (unlikely(req->ctx->flags & (IORING_SETUP_IOPOLL|IORING_SETUP_SQPOLL)))
+ *   - fs/io_uring.c|6884| <<io_sq_offload_start>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|7590| <<SYSCALL_DEFINE6(io_uring_enter)>> if (ctx->flags & IORING_SETUP_SQPOLL) {
+ *   - fs/io_uring.c|7616| <<SYSCALL_DEFINE6(io_uring_enter)>> !(ctx->flags & IORING_SETUP_SQPOLL)) {
+ *   - fs/io_uring.c|7985| <<io_uring_setup>> if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
+ */
 #define IORING_SETUP_SQPOLL	(1U << 1)	/* SQ poll thread */
 #define IORING_SETUP_SQ_AFF	(1U << 2)	/* sq_thread_cpu is valid */
 #define IORING_SETUP_CQSIZE	(1U << 3)	/* app defines CQ size */
@@ -173,8 +189,26 @@ enum {
 /*
  * Magic offsets for the application to mmap the data it needs
  */
+/*
+ * 在以下使用IORING_OFF_SQ_RING:
+ *   - fs/io_uring.c|7557| <<io_uring_validate_mmap_request>> case IORING_OFF_SQ_RING:
+ *   - tools/io_uring/io_uring-bench.c|434| <<setup_ring>> IORING_OFF_SQ_RING);
+ *   - tools/io_uring/setup.c|19| <<io_uring_mmap>> MAP_SHARED | MAP_POPULATE, fd, IORING_OFF_SQ_RING);
+ */
 #define IORING_OFF_SQ_RING		0ULL
+/*
+ * 在以下使用IORING_OFF_CQ_RING:
+ *   - fs/io_uring.c|7558| <<io_uring_validate_mmap_request>> case IORING_OFF_CQ_RING:
+ *   - tools/io_uring/io_uring-bench.c|451| <<setup_ring>> IORING_OFF_CQ_RING);
+ *   - tools/io_uring/setup.c|43| <<io_uring_mmap>> MAP_SHARED | MAP_POPULATE, fd, IORING_OFF_CQ_RING);
+ */
 #define IORING_OFF_CQ_RING		0x8000000ULL
+/*
+ * 在以下使用IORING_OFF_SQES:
+ *   - fs/io_uring.c|7561| <<io_uring_validate_mmap_request>> case IORING_OFF_SQES:
+ *   - tools/io_uring/io_uring-bench.c|446| <<setup_ring>> IORING_OFF_SQES);
+ *   - tools/io_uring/setup.c|33| <<io_uring_mmap>> IORING_OFF_SQES);
+ */
 #define IORING_OFF_SQES			0x10000000ULL
 
 /*
@@ -210,7 +244,21 @@ struct io_cqring_offsets {
 /*
  * io_uring_enter(2) flags
  */
+/*
+ * 在以下使用IORING_ENTER_GETEVENTS:
+ *   - fs/io_uring.c|7568| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))
+ *   - fs/io_uring.c|7604| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & IORING_ENTER_GETEVENTS) {
+ *   - tools/io_uring/io_uring-bench.c|313| <<submitter_fn>> flags = IORING_ENTER_GETEVENTS;
+ *   - tools/io_uring/queue.c|37| <<__io_uring_get_cqe>> IORING_ENTER_GETEVENTS, NULL);
+ *   - tools/io_uring/queue.c|127| <<io_uring_submit>> IORING_ENTER_GETEVENTS, NULL);
+ */
 #define IORING_ENTER_GETEVENTS	(1U << 0)
+/*
+ * 在以下使用IORING_ENTER_SQ_WAKEUP:
+ *   - fs/io_uring.c|7568| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP))
+ *   - fs/io_uring.c|7593| <<SYSCALL_DEFINE6(io_uring_enter)>> if (flags & IORING_ENTER_SQ_WAKEUP)
+ *   - tools/io_uring/io_uring-bench.c|315| <<submitter_fn>> flags |= IORING_ENTER_SQ_WAKEUP;
+ */
 #define IORING_ENTER_SQ_WAKEUP	(1U << 1)
 
 /*
-- 
2.17.1

