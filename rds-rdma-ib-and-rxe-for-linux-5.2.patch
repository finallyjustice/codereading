From 2881fb7e5ec839118cd663d5a15f90c1cd59a39a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 15 Sep 2019 22:16:17 +0800
Subject: [PATCH 1/1] rds rdma ib and rxe for linux-5.2

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 drivers/infiniband/core/cma.c         |  16 +++
 drivers/infiniband/sw/rxe/rxe.c       |  23 +++
 drivers/infiniband/sw/rxe/rxe_cq.c    |   5 +
 drivers/infiniband/sw/rxe/rxe_loc.h   |   8 ++
 drivers/infiniband/sw/rxe/rxe_net.c   |  17 +++
 drivers/infiniband/sw/rxe/rxe_qp.c    |   8 ++
 drivers/infiniband/sw/rxe/rxe_recv.c  |   5 +
 drivers/infiniband/sw/rxe/rxe_req.c   |   4 +
 drivers/infiniband/sw/rxe/rxe_task.c  |  12 ++
 drivers/infiniband/sw/rxe/rxe_verbs.c |  15 ++
 drivers/nvme/host/fabrics.c           |   6 +
 drivers/nvme/host/rdma.c              | 258 ++++++++++++++++++++++++++++++++++
 include/rdma/ib_verbs.h               |   5 +
 include/rdma/rdma_netlink.h           |   4 +
 net/rds/ib.c                          |   4 +
 net/rds/transport.c                   |   5 +
 16 files changed, 395 insertions(+)

diff --git a/drivers/infiniband/core/cma.c b/drivers/infiniband/core/cma.c
index 19f1730..82ddbd2 100644
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -947,6 +947,22 @@ static int cma_init_conn_qp(struct rdma_id_private *id_priv, struct ib_qp *qp)
 	return ib_modify_qp(qp, &qp_attr, qp_attr_mask);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iser_verbs.c|476| <<iser_create_ib_conn_res>> ret = rdma_create_qp(ib_conn->cma_id, device->pd, &init_attr);
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|138| <<isert_create_qp>> ret = rdma_create_qp(cma_id, device->pd, &attr);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|593| <<srp_create_ch_ib>> ret = rdma_create_qp(ch->rdma_cm.cm_id, dev->pd, init_attr);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1809| <<srpt_create_ch_ib>> ret = rdma_create_qp(ch->rdma_cm.cm_id, sdev->pd, qp_init);
+ *   - drivers/nvme/host/rdma.c|277| <<nvme_rdma_create_qp>> ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
+ *   - drivers/nvme/target/rdma.c|1023| <<nvmet_rdma_create_queue_ib>> ret = rdma_create_qp(queue->cm_id, ndev->pd, &qp_attr);
+ *   - fs/cifs/smbdirect.c|1685| <<_smbd_get_connection>> rc = rdma_create_qp(info->id, info->pd, &qp_attr);
+ *   - net/9p/trans_rdma.c|711| <<rdma_create_trans>> err = rdma_create_qp(rdma->cm_id, rdma->pd, &qp_attr);
+ *   - net/rds/ib_cm.c|539| <<rds_ib_setup_qp>> ret = rdma_create_qp(ic->i_cm_id, ic->i_pd, &attr);
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|490| <<svc_rdma_accept>> ret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|630| <<rpcrdma_ep_recreate_xprt>> err = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|678| <<rpcrdma_ep_reconnect>> err = rdma_create_qp(id, ia->ri_pd, &ep->rep_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|709| <<rpcrdma_ep_connect>> rc = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);
+ */
 int rdma_create_qp(struct rdma_cm_id *id, struct ib_pd *pd,
 		   struct ib_qp_init_attr *qp_init_attr)
 {
diff --git a/drivers/infiniband/sw/rxe/rxe.c b/drivers/infiniband/sw/rxe/rxe.c
index a8c11b5..6161ed8 100644
--- a/drivers/infiniband/sw/rxe/rxe.c
+++ b/drivers/infiniband/sw/rxe/rxe.c
@@ -41,6 +41,11 @@ MODULE_DESCRIPTION("Soft RDMA transport");
 MODULE_LICENSE("Dual BSD/GPL");
 
 /* free resources for all ports on a device */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe.c|69| <<rxe_dealloc>> rxe_cleanup_ports(rxe);
+ *   - drivers/infiniband/sw/rxe/rxe.c|283| <<rxe_init>> rxe_cleanup_ports(rxe);
+ */
 static void rxe_cleanup_ports(struct rxe_dev *rxe)
 {
 	kfree(rxe->port.pkey_tbl);
@@ -151,6 +156,10 @@ static int rxe_init_port_param(struct rxe_port *port)
 /* initialize port state, note IB convention that HCA ports are always
  * numbered from 1
  */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe.c|265| <<rxe_init>> err = rxe_init_ports(rxe);
+ */
 static int rxe_init_ports(struct rxe_dev *rxe)
 {
 	struct rxe_port *port = &rxe->port;
@@ -255,6 +264,10 @@ static int rxe_init_pools(struct rxe_dev *rxe)
 }
 
 /* initialize rxe device state */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe.c|309| <<rxe_add>> err = rxe_init(rxe);
+ */
 static int rxe_init(struct rxe_dev *rxe)
 {
 	int err;
@@ -302,6 +315,10 @@ void rxe_set_mtu(struct rxe_dev *rxe, unsigned int ndev_mtu)
 /* called by ifc layer to create new rxe device.
  * The caller should allocate memory for rxe by calling ib_alloc_device.
  */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|537| <<rxe_net_add>> err = rxe_add(rxe, ndev->mtu, ibdev_name);
+ */
 int rxe_add(struct rxe_dev *rxe, unsigned int mtu, const char *ibdev_name)
 {
 	int err;
@@ -315,6 +332,12 @@ int rxe_add(struct rxe_dev *rxe, unsigned int mtu, const char *ibdev_name)
 	return rxe_register_device(rxe, ibdev_name);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/nldev.c|1316| <<nldev_newlink>> err = ops ? ops->newlink(ibdev_name, ndev) : -EINVAL;
+ *
+ * struct rdma_link_ops rxe_link_ops.newlink = rxe_newlink()
+ */
 static int rxe_newlink(const char *ibdev_name, struct net_device *ndev)
 {
 	struct rxe_dev *exists;
diff --git a/drivers/infiniband/sw/rxe/rxe_cq.c b/drivers/infiniband/sw/rxe/rxe_cq.c
index ad30901..4ade426 100644
--- a/drivers/infiniband/sw/rxe/rxe_cq.c
+++ b/drivers/infiniband/sw/rxe/rxe_cq.c
@@ -129,6 +129,11 @@ int rxe_cq_resize_queue(struct rxe_cq *cq, int cqe,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_comp.c|450| <<do_complete>> rxe_cq_post(qp->scq, &cqe, 0);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|934| <<do_complete>> if (rxe_cq_post(qp->rcq, &cqe, pkt ? bth_se(pkt) : 1))
+ */
 int rxe_cq_post(struct rxe_cq *cq, struct rxe_cqe *cqe, int solicited)
 {
 	struct ib_event ev;
diff --git a/drivers/infiniband/sw/rxe/rxe_loc.h b/drivers/infiniband/sw/rxe/rxe_loc.h
index 775c23b..e71130a 100644
--- a/drivers/infiniband/sw/rxe/rxe_loc.h
+++ b/drivers/infiniband/sw/rxe/rxe_loc.h
@@ -247,6 +247,14 @@ static inline unsigned int wr_opcode_mask(int opcode, struct rxe_qp *qp)
 	return rxe_wr_opcode_info[opcode].mask[qp->ibqp.qp_type];
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_req.c|732| <<rxe_requester>> ret = rxe_xmit_packet(qp, &pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|737| <<read_reply>> err = rxe_xmit_packet(qp, &ack_pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|962| <<send_ack>> err = rxe_xmit_packet(qp, &ack_pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|1003| <<send_atomic_ack>> rc = rxe_xmit_packet(qp, &ack_pkt, skb);
+ *   - drivers/infiniband/sw/rxe/rxe_resp.c|1133| <<duplicate_request>> rc = rxe_xmit_packet(qp, pkt, res->atomic.skb);
+ */
 static inline int rxe_xmit_packet(struct rxe_qp *qp, struct rxe_pkt_info *pkt,
 				  struct sk_buff *skb)
 {
diff --git a/drivers/infiniband/sw/rxe/rxe_net.c b/drivers/infiniband/sw/rxe/rxe_net.c
index 5a3474f..5209399 100644
--- a/drivers/infiniband/sw/rxe/rxe_net.c
+++ b/drivers/infiniband/sw/rxe/rxe_net.c
@@ -187,6 +187,10 @@ static struct dst_entry *rxe_find_route(struct net_device *ndev,
 	return dst;
 }
 
+/*
+ * used by:
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|258| <<rxe_setup_udp_tunnel>> tnl_cfg.encap_rcv = rxe_udp_encap_recv;
+ */
 static int rxe_udp_encap_recv(struct sock *sk, struct sk_buff *skb)
 {
 	struct udphdr *udph;
@@ -419,6 +423,10 @@ static void rxe_skb_tx_dtor(struct sk_buff *skb)
 	rxe_drop_ref(qp);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_loc.h|268| <<rxe_xmit_packet>> err = rxe_send(pkt, skb);
+ */
 int rxe_send(struct rxe_pkt_info *pkt, struct sk_buff *skb)
 {
 	int err;
@@ -523,6 +531,11 @@ enum rdma_link_layer rxe_link_layer(struct rxe_dev *rxe, unsigned int port_num)
 	return IB_LINK_LAYER_ETHERNET;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe.c|331| <<rxe_newlink>> err = rxe_net_add(ibdev_name, ndev);
+ *   - drivers/infiniband/sw/rxe/rxe_sysfs.c|84| <<rxe_param_set_add>> err = rxe_net_add("rxe%d", ndev);
+ */
 int rxe_net_add(const char *ibdev_name, struct net_device *ndev)
 {
 	int err;
@@ -669,6 +682,10 @@ void rxe_net_exit(void)
 	unregister_netdevice_notifier(&rxe_net_notifier);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe.c|356| <<rxe_module_init>> err = rxe_net_init();
+ */
 int rxe_net_init(void)
 {
 	int err;
diff --git a/drivers/infiniband/sw/rxe/rxe_qp.c b/drivers/infiniband/sw/rxe/rxe_qp.c
index e2c6d1c..37ca2c2 100644
--- a/drivers/infiniband/sw/rxe/rxe_qp.c
+++ b/drivers/infiniband/sw/rxe/rxe_qp.c
@@ -216,6 +216,10 @@ static void rxe_qp_init_misc(struct rxe_dev *rxe, struct rxe_qp *qp,
 	atomic_set(&qp->skb_out, 0);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|360| <<rxe_qp_from_init>> err = rxe_qp_init_req(rxe, qp, init, udata, uresp);
+ */
 static int rxe_qp_init_req(struct rxe_dev *rxe, struct rxe_qp *qp,
 			   struct ib_qp_init_attr *init, struct ib_udata *udata,
 			   struct rxe_create_qp_resp __user *uresp)
@@ -333,6 +337,10 @@ static int rxe_qp_init_resp(struct rxe_dev *rxe, struct rxe_qp *qp,
 }
 
 /* called by the create qp verb */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|447| <<rxe_create_qp>> err = rxe_qp_from_init(rxe, qp, pd, init, uresp, ibpd, udata);
+ */
 int rxe_qp_from_init(struct rxe_dev *rxe, struct rxe_qp *qp, struct rxe_pd *pd,
 		     struct ib_qp_init_attr *init,
 		     struct rxe_create_qp_resp __user *uresp,
diff --git a/drivers/infiniband/sw/rxe/rxe_recv.c b/drivers/infiniband/sw/rxe/rxe_recv.c
index f9a492e..dff2ab7 100644
--- a/drivers/infiniband/sw/rxe/rxe_recv.c
+++ b/drivers/infiniband/sw/rxe/rxe_recv.c
@@ -353,6 +353,11 @@ static int rxe_match_dgid(struct rxe_dev *rxe, struct sk_buff *skb)
 }
 
 /* rxe_rcv is called from the interface driver */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|218| <<rxe_udp_encap_recv>> rxe_rcv(skb);
+ *   - drivers/infiniband/sw/rxe/rxe_net.c|454| <<rxe_loopback>> rxe_rcv(skb);
+ */
 void rxe_rcv(struct sk_buff *skb)
 {
 	int err;
diff --git a/drivers/infiniband/sw/rxe/rxe_req.c b/drivers/infiniband/sw/rxe/rxe_req.c
index c5d9b55..8fd7adb 100644
--- a/drivers/infiniband/sw/rxe/rxe_req.c
+++ b/drivers/infiniband/sw/rxe/rxe_req.c
@@ -580,6 +580,10 @@ static void update_state(struct rxe_qp *qp, struct rxe_send_wqe *wqe,
 			  jiffies + qp->qp_timeout_jiffies);
 }
 
+/*
+ * used by:
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|275| <<rxe_qp_init_req>> rxe_requester, "req");
+ */
 int rxe_requester(void *arg)
 {
 	struct rxe_qp *qp = (struct rxe_qp *)arg;
diff --git a/drivers/infiniband/sw/rxe/rxe_task.c b/drivers/infiniband/sw/rxe/rxe_task.c
index 08f05ac..b823577 100644
--- a/drivers/infiniband/sw/rxe/rxe_task.c
+++ b/drivers/infiniband/sw/rxe/rxe_task.c
@@ -55,6 +55,12 @@ int __rxe_do_task(struct rxe_task *task)
  * a second caller finds the task already running
  * but looks just after the last call to func
  */
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_task.c|126| <<rxe_init_task>> tasklet_init(&task->tasklet, rxe_do_task, (unsigned long )task);
+ *   - drivers/infiniband/sw/rxe/rxe_task.c|162| <<rxe_run_task>> rxe_do_task((unsigned long )task);
+ *   - drivers/infiniband/sw/rxe/rxe_task.h|83| <<rxe_run_task>> void rxe_do_task(unsigned long data);
+ */
 void rxe_do_task(unsigned long data)
 {
 	int cont;
@@ -114,6 +120,12 @@ void rxe_do_task(unsigned long data)
 	task->ret = ret;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|274| <<rxe_qp_init_req>> rxe_init_task(rxe, &qp->req.task, qp,
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|276| <<rxe_qp_init_req>> rxe_init_task(rxe, &qp->comp.task, qp,
+ *   - drivers/infiniband/sw/rxe/rxe_qp.c|325| <<rxe_qp_init_resp>> rxe_init_task(rxe, &qp->resp.task, qp,
+ */
 int rxe_init_task(void *obj, struct rxe_task *task,
 		  void *arg, int (*func)(void *), char *name)
 {
diff --git a/drivers/infiniband/sw/rxe/rxe_verbs.c b/drivers/infiniband/sw/rxe/rxe_verbs.c
index 8c3e2a1..50595e9 100644
--- a/drivers/infiniband/sw/rxe/rxe_verbs.c
+++ b/drivers/infiniband/sw/rxe/rxe_verbs.c
@@ -631,6 +631,10 @@ static int init_send_wqe(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|702| <<rxe_post_send_kernel>> err = post_one_send(qp, wr, mask, length);
+ */
 static int post_one_send(struct rxe_qp *qp, const struct ib_send_wr *ibwr,
 			 unsigned int mask, u32 length)
 {
@@ -715,6 +719,13 @@ static int rxe_post_send_kernel(struct rxe_qp *qp, const struct ib_send_wr *wr,
 	return err;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/uverbs_cmd.c|2171| <<ib_uverbs_post_send>> ret = qp->device->ops.post_send(qp->real_qp, wr, &bad_wr);
+ *   - include/rdma/ib_verbs.h|3727| <<ib_post_send>> return qp->device->ops.post_send(qp, send_wr, bad_send_wr ? : &dummy);
+ *
+ * struct ib_device_ops rxe_dev_ops.post_send = rxe_post_send()
+ */
 static int rxe_post_send(struct ib_qp *ibqp, const struct ib_send_wr *wr,
 			 const struct ib_send_wr **bad_wr)
 {
@@ -1110,6 +1121,10 @@ static int rxe_enable_driver(struct ib_device *ib_dev)
 	return 0;
 }
 
+/*
+ * used by:
+ *   - drivers/infiniband/sw/rxe/rxe_verbs.c|1219| <<rxe_register_device>> ib_set_device_ops(dev, &rxe_dev_ops);
+ */
 static const struct ib_device_ops rxe_dev_ops = {
 	.alloc_hw_stats = rxe_ib_alloc_hw_stats,
 	.alloc_mr = rxe_alloc_mr,
diff --git a/drivers/nvme/host/fabrics.c b/drivers/nvme/host/fabrics.c
index 5838f7c..ecf327b 100644
--- a/drivers/nvme/host/fabrics.c
+++ b/drivers/nvme/host/fabrics.c
@@ -139,6 +139,9 @@ EXPORT_SYMBOL_GPL(nvmf_get_address);
  *	> 0: NVMe error status code
  *	< 0: Linux errno error code
  */
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.reg_read32 = nvmf_reg_read32()
+ */
 int nvmf_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
 {
 	struct nvme_command cmd;
@@ -185,6 +188,9 @@ EXPORT_SYMBOL_GPL(nvmf_reg_read32);
  *	> 0: NVMe error status code
  *	< 0: Linux errno error code
  */
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.reg_read64 = nvmt_reg_read64()
+ */
 int nvmf_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
 {
 	struct nvme_command cmd;
diff --git a/drivers/nvme/host/rdma.c b/drivers/nvme/host/rdma.c
index 97f668a..9c83d57 100644
--- a/drivers/nvme/host/rdma.c
+++ b/drivers/nvme/host/rdma.c
@@ -67,12 +67,25 @@ struct nvme_rdma_request {
 };
 
 enum nvme_rdma_queue_flags {
+	/*
+	 * 在以下使用NVME_RDMA_Q_ALLOCATED:
+	 *   - drivers/nvme/host/rdma.c|622| <<nvme_rdma_alloc_queue>> set_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags);
+	 *   - drivers/nvme/host/rdma.c|643| <<nvme_rdma_free_queue>> if (!test_and_clear_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags))
+	 */
 	NVME_RDMA_Q_ALLOCATED		= 0,
 	NVME_RDMA_Q_LIVE		= 1,
 	NVME_RDMA_Q_TR_READY		= 2,
 };
 
 struct nvme_rdma_queue {
+	/*
+	 * 在以下使用rsp_ring:
+	 *   - drivers/nvme/host/rdma.c|498| <<nvme_rdma_destroy_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|556| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|558| <<nvme_rdma_create_queue_ib>> if (!queue->rsp_ring) {
+	 *   - drivers/nvme/host/rdma.c|579| <<nvme_rdma_create_queue_ib>> nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+	 *   - drivers/nvme/host/rdma.c|1628| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+	 */
 	struct nvme_rdma_qe	*rsp_ring;
 	int			queue_size;
 	size_t			cmnd_capsule_len;
@@ -84,6 +97,13 @@ struct nvme_rdma_queue {
 	unsigned long		flags;
 	struct rdma_cm_id	*cm_id;
 	int			cm_error;
+	/*
+	 * cm_done使用的地方:
+	 *   - drivers/nvme/host/rdma.c|294| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|585| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|1724| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|1759| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	struct completion	cm_done;
 };
 
@@ -114,14 +134,33 @@ struct nvme_rdma_ctrl {
 	u32			io_queues[HCTX_MAX_TYPES];
 };
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|722| <<nvme_rdma_alloc_tagset>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|941| <<nvme_rdma_free_ctrl>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+ *   - drivers/nvme/host/rdma.c|1421| <<nvme_rdma_submit_async_event>> struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+ *   - drivers/nvme/host/rdma.c|1910| <<nvme_rdma_delete_ctrl>> nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+ */
 static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
 {
 	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
 }
 
+/*
+ * 在以下使用device_list:
+ *   - drivers/nvme/host/rdma.c|363| <<nvme_rdma_find_get_device>> list_for_each_entry(ndev, &device_list, entry) {
+ *   - drivers/nvme/host/rdma.c|390| <<nvme_rdma_find_get_device>> list_add(&ndev->entry, &device_list);
+ *   - drivers/nvme/host/rdma.c|2098| <<nvme_rdma_remove_one>> list_for_each_entry(ndev, &device_list, entry) {
+ */
 static LIST_HEAD(device_list);
 static DEFINE_MUTEX(device_list_mutex);
 
+/*
+ * 在以下使用nvme_rdma_ctrl_list:
+ *   - drivers/nvme/host/rdma.c|1969| <<nvme_rdma_existing_controller>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ *   - drivers/nvme/host/rdma.c|2063| <<nvme_rdma_create_ctrl>> list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+ *   - drivers/nvme/host/rdma.c|2111| <<nvme_rdma_remove_one>> list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+ */
 static LIST_HEAD(nvme_rdma_ctrl_list);
 static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
 
@@ -130,6 +169,10 @@ static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
  * unsafe.  With it turned off we will have to register a global rkey that
  * allows read and write access to all physical memory.
  */
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|396| <<nvme_rdma_find_get_device>> register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+ */
 static bool register_always = true;
 module_param(register_always, bool, 0444);
 MODULE_PARM_DESC(register_always,
@@ -162,11 +205,21 @@ static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
 		queue->ctrl->io_queues[HCTX_TYPE_READ];
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1314| <<nvme_rdma_map_data>> nvme_rdma_inline_data_size(queue)) {
+ */
 static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
 {
 	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|220| <<nvme_rdma_free_ring>> nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+ *   - drivers/nvme/host/rdma.c|789| <<nvme_rdma_destroy_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ *   - drivers/nvme/host/rdma.c|871| <<nvme_rdma_configure_admin_queue>> nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ */
 static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -174,6 +227,11 @@ static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 	kfree(qe->data);
 }
 
+/*
+ * callled by:
+ *   - drivers/nvme/host/rdma.c|241| <<nvme_rdma_alloc_ring>> if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+ *   - drivers/nvme/host/rdma.c|815| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+ */
 static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
 		size_t capsule_size, enum dma_data_direction dir)
 {
@@ -202,6 +260,10 @@ static void nvme_rdma_free_ring(struct ib_device *ibdev,
 	kfree(ring);
 }
 
+/*
+ * called by only:
+ *   - drivers/nvme/host/rdma.c|502| <<nvme_rdma_create_queue_ib>> queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+ */
 static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 		size_t ib_queue_size, size_t capsule_size,
 		enum dma_data_direction dir)
@@ -230,6 +292,10 @@ static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
 	return NULL;
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|307| <<nvme_rdma_create_qp>> init_attr.event_handler = nvme_rdma_qp_event;
+ */
 static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 {
 	pr_debug("QP event %s (%d)\n",
@@ -237,10 +303,21 @@ static void nvme_rdma_qp_event(struct ib_event *event, void *context)
 
 }
 
+/*
+ * called only by:
+ *   - drivers/nvme/host/rdma.c|603| <<nvme_rdma_alloc_queue>> ret = nvme_rdma_wait_for_cm(queue);
+ */
 static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 {
 	int ret;
 
+	/*
+	 * cm_done使用的地方:
+	 *   - drivers/nvme/host/rdma.c|294| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+	 *   - drivers/nvme/host/rdma.c|585| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|1724| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 *   - drivers/nvme/host/rdma.c|1759| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+	 */
 	ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
 			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
 	if (ret < 0)
@@ -251,6 +328,10 @@ static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
 	return queue->cm_error;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|471| <<nvme_rdma_create_queue_ib>> ret = nvme_rdma_create_qp(queue, send_wr_factor);
+ */
 static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 {
 	struct nvme_rdma_device *dev = queue->device;
@@ -276,6 +357,10 @@ static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.exit_request = nvme_rdma_exit_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.exit_request = nvme_rdma_request()
+ */
 static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx)
 {
@@ -284,6 +369,10 @@ static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
 	kfree(req->sqe.data);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_request = nvme_rdma_init_request()
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_request = nvme_rdma_init_request()
+ */
 static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 		struct request *rq, unsigned int hctx_idx,
 		unsigned int numa_node)
@@ -303,6 +392,9 @@ static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.init_hctx = nvme_rdma_init_hctx()
+ */
 static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -315,6 +407,9 @@ static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.init_hctx = nvme_rdma_init_admin_hctx()
+ */
 static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
 		unsigned int hctx_idx)
 {
@@ -345,11 +440,19 @@ static void nvme_rdma_dev_put(struct nvme_rdma_device *dev)
 	kref_put(&dev->ref, nvme_rdma_free_dev);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|460| <<nvme_rdma_find_get_device>> nvme_rdma_dev_get(ndev))
+ */
 static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
 {
 	return kref_get_unless_zero(&dev->ref);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|492| <<nvme_rdma_create_queue_ib>> queue->device = nvme_rdma_find_get_device(queue->cm_id);
+ */
 static struct nvme_rdma_device *
 nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
 {
@@ -430,6 +533,10 @@ static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
 		     ibdev->attrs.max_fast_reg_page_list_len);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1545| <<nvme_rdma_addr_resolved>> ret = nvme_rdma_create_queue_ib(queue);
+ */
 static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 {
 	struct ib_device *ibdev;
@@ -506,6 +613,11 @@ static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|699| <<nvme_rdma_alloc_io_queues>> ret = nvme_rdma_alloc_queue(ctrl, i,
+ *   - drivers/nvme/host/rdma.c|777| <<nvme_rdma_configure_admin_queue>> error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+ */
 static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		int idx, size_t queue_size)
 {
@@ -513,6 +625,7 @@ static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 	struct sockaddr *src_addr = NULL;
 	int ret;
 
+	/* struct struct nvme_rdma_queue * */
 	queue = &ctrl->queues[idx];
 	queue->ctrl = ctrl;
 	init_completion(&queue->cm_done);
@@ -524,6 +637,10 @@ static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 
 	queue->queue_size = queue_size;
 
+	/*
+	 * nvme_rdma_cm_handler()是User callback invoked to report events
+	 * associated with the returned rdma_id.
+	 */
 	queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
 			RDMA_PS_TCP, IB_QPT_RC);
 	if (IS_ERR(queue->cm_id)) {
@@ -552,6 +669,11 @@ static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
 		goto out_destroy_cm_id;
 	}
 
+	/*
+	 * 在以下使用NVME_RDMA_Q_ALLOCATED:
+	 *   - drivers/nvme/host/rdma.c|622| <<nvme_rdma_alloc_queue>> set_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags);
+	 *   - drivers/nvme/host/rdma.c|643| <<nvme_rdma_free_queue>> if (!test_and_clear_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags))
+	 */
 	set_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags);
 
 	return 0;
@@ -615,6 +737,10 @@ static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|937| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_start_io_queues(ctrl);
+ */
 static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	int i, ret = 0;
@@ -633,6 +759,10 @@ static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|932| <<nvme_rdma_configure_io_queues>> ret = nvme_rdma_alloc_io_queues(ctrl);
+ */
 static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
 {
 	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
@@ -854,6 +984,10 @@ static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
 	nvme_rdma_free_io_queues(ctrl);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1070| <<nvme_rdma_setup_ctrl>> ret = nvme_rdma_configure_io_queues(ctrl, new);
+ */
 static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret;
@@ -923,6 +1057,9 @@ static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
 	}
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.free_ctrl = nvme_rdma_free_ctrl()
+ */
 static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
@@ -959,6 +1096,12 @@ static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1102| <<nvme_rdma_reconnect_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|1985| <<nvme_rdma_reset_ctrl_work>> if (nvme_rdma_setup_ctrl(ctrl, false))
+ *   - drivers/nvme/host/rdma.c|2111| <<nvme_rdma_create_ctrl>> ret = nvme_rdma_setup_ctrl(ctrl, true);
+ */
 static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
 {
 	int ret = -EINVAL;
@@ -1195,6 +1338,10 @@ static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1436| <<nvme_rdma_map_data>> ret = nvme_rdma_map_sg_fr(queue, req, c, count);
+ */
 static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_request *req, struct nvme_command *c,
 		int count)
@@ -1241,6 +1388,10 @@ static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1949| <<nvme_rdma_queue_rq>> err = nvme_rdma_map_data(queue, rq, c);
+ */
 static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 		struct request *rq, struct nvme_command *c)
 {
@@ -1257,6 +1408,9 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	if (!blk_rq_nr_phys_segments(rq))
 		return nvme_rdma_set_sg_null(c);
 
+	/*
+	 * req是struct nvme_rdma_request
+	 */
 	req->sg_table.sgl = req->first_sgl;
 	ret = sg_alloc_table_chained(&req->sg_table,
 			blk_rq_nr_phys_segments(rq), req->sg_table.sgl);
@@ -1265,6 +1419,9 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 
 	req->nents = blk_rq_map_sg(rq->q, rq, req->sg_table.sgl);
 
+	/*
+	 * req是struct nvme_rdma_request
+	 */
 	count = ib_dma_map_sg(ibdev, req->sg_table.sgl, req->nents,
 		    rq_data_dir(rq) == WRITE ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
 	if (unlikely(count <= 0)) {
@@ -1303,6 +1460,10 @@ static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/cq.c|37| <<__ib_process_cq>> wc->wr_cqe->done(cq, wc);
+ */
 static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1320,6 +1481,11 @@ static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1418| <<nvme_rdma_submit_async_event>> ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
+ *   - drivers/nvme/host/rdma.c|1753| <<nvme_rdma_queue_rq>> err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
+ */
 static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
 		struct ib_send_wr *first)
@@ -1331,6 +1497,9 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	sge->length = sizeof(struct nvme_command),
 	sge->lkey   = queue->device->pd->local_dma_lkey;
 
+	/*
+	 * wr是在堆栈分配的
+	 */
 	wr.next       = NULL;
 	wr.wr_cqe     = &qe->cqe;
 	wr.sg_list    = sge;
@@ -1351,6 +1520,11 @@ static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1620| <<nvme_rdma_recv_done>> nvme_rdma_post_recv(queue, qe);
+ *   - drivers/nvme/host/rdma.c|1628| <<nvme_rdma_conn_established>> ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+ */
 static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
 		struct nvme_rdma_qe *qe)
 {
@@ -1392,6 +1566,9 @@ static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
 		nvme_rdma_wr_error(cq, wc, "ASYNC");
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.submit_async_event()
+ */
 static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 {
 	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
@@ -1419,6 +1596,10 @@ static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
 	WARN_ON_ONCE(ret);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1617| <<nvme_rdma_recv_done>> nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+ */
 static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		struct nvme_completion *cqe, struct ib_wc *wc)
 {
@@ -1463,6 +1644,13 @@ static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
 		nvme_end_request(rq, req->status, req->result);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/cq.c|37| <<__ib_process_cq>> wc->wr_cqe->done(cq, wc);
+ *
+ * used by:
+ *   - drivers/nvme/host/rdma.c|1490| <<nvme_rdma_post_recv>> qe->cqe.done = nvme_rdma_recv_done;
+ */
 static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 {
 	struct nvme_rdma_qe *qe =
@@ -1495,6 +1683,10 @@ static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
 	nvme_rdma_post_recv(queue, qe);
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1765| <<nvme_rdma_cm_handler>> queue->cm_error = nvme_rdma_conn_established(queue);
+ */
 static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
 {
 	int ret, i;
@@ -1538,6 +1730,10 @@ static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
 	return -ECONNRESET;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1626| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_addr_resolved(queue);
+ */
 static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 {
 	int ret;
@@ -1561,6 +1757,10 @@ static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|1809| <<nvme_rdma_cm_handler>> cm_error = nvme_rdma_route_resolved(queue);
+ */
 static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 {
 	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
@@ -1611,6 +1811,10 @@ static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/rdma.c|527| <<nvme_rdma_alloc_queue>> queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+ */
 static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 		struct rdma_cm_event *ev)
 {
@@ -1631,6 +1835,13 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	case RDMA_CM_EVENT_ESTABLISHED:
 		queue->cm_error = nvme_rdma_conn_established(queue);
 		/* complete cm_done regardless of success/failure */
+		/*
+		 * cm_done使用的地方:
+		 *   - drivers/nvme/host/rdma.c|294| <<nvme_rdma_wait_for_cm>> ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+		 *   - drivers/nvme/host/rdma.c|585| <<nvme_rdma_alloc_queue>> init_completion(&queue->cm_done);
+		 *   - drivers/nvme/host/rdma.c|1724| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+		 *   - drivers/nvme/host/rdma.c|1759| <<nvme_rdma_cm_handler>> complete(&queue->cm_done);
+		 */
 		complete(&queue->cm_done);
 		return 0;
 	case RDMA_CM_EVENT_REJECTED:
@@ -1672,6 +1883,10 @@ static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
 	return 0;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.timeout = nvme_rdma_timeout()
+ * struct blk_mq_ops nvme_rdma_mq_ops.timeout = nvme_rdma_timeout()
+ */
 static enum blk_eh_timer_return
 nvme_rdma_timeout(struct request *rq, bool reserved)
 {
@@ -1700,6 +1915,10 @@ nvme_rdma_timeout(struct request *rq, bool reserved)
 	return BLK_EH_RESET_TIMER;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.queue_rq = nvme_rdma_queue_rq()
+ */
 static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		const struct blk_mq_queue_data *bd)
 {
@@ -1707,6 +1926,7 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct nvme_rdma_queue *queue = hctx->driver_data;
 	struct request *rq = bd->rq;
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	/* struct ib_sge sge[1 + NVME_RDMA_MAX_INLINE_SEGMENTS]; */
 	struct nvme_rdma_qe *sqe = &req->sqe;
 	struct nvme_command *c = sqe->data;
 	struct ib_device *dev;
@@ -1721,6 +1941,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 
 	dev = queue->device->dev;
 
+	/*
+	 * req->sqe.dma对应的req->sqe.data是struct nvme_command类型
+	 */
 	req->sqe.dma = ib_dma_map_single(dev, req->sqe.data,
 					 sizeof(struct nvme_command),
 					 DMA_TO_DEVICE);
@@ -1731,6 +1954,11 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	ib_dma_sync_single_for_cpu(dev, sqe->dma,
 			sizeof(struct nvme_command), DMA_TO_DEVICE);
 
+	/*
+	 * ns是 struct nvme_ns
+	 * rq是 struct request
+	 * c 是 struct nvme_command
+	 */
 	ret = nvme_setup_cmd(ns, rq, c);
 	if (ret)
 		goto unmap_qe;
@@ -1745,6 +1973,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 		goto err;
 	}
 
+	/*
+	 * sqe是&req->sqe
+	 */
 	sqe->cqe.done = nvme_rdma_send_done;
 
 	ib_dma_sync_single_for_device(dev, sqe->dma,
@@ -1770,6 +2001,9 @@ static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.poll = nvme_rdma_poll()
+ */
 static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 {
 	struct nvme_rdma_queue *queue = hctx->driver_data;
@@ -1777,6 +2011,10 @@ static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
 	return ib_process_cq_direct(queue->ib_cq, -1);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_admin_mq_ops.complete = nvme_rdma_complete_rq()
+ * struct blk_mq_ops nvme_rdma_mq_ops.complete = nvme_rdma_complete_rq()
+ */
 static void nvme_rdma_complete_rq(struct request *rq)
 {
 	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
@@ -1789,6 +2027,9 @@ static void nvme_rdma_complete_rq(struct request *rq)
 	nvme_complete_rq(rq);
 }
 
+/*
+ * struct blk_mq_ops nvme_rdma_mq_ops.map_queues = nvme_rdma_map_queues()
+ */
 static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
 {
 	struct nvme_rdma_ctrl *ctrl = set->driver_data;
@@ -1869,11 +2110,18 @@ static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
 	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
 }
 
+/*
+ * struct nvme_ctrl_ops nvme_rdma_ctrl_ops.delete_ctrl = nvme_rdma_delete_ctrl()
+ */
 static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
 {
 	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
 }
 
+/*
+ * used by:
+ *   - drivers/nvme/host/rdma.c|2090| <<nvme_rdma_create_ctrl>> INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+ */
 static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
 {
 	struct nvme_rdma_ctrl *ctrl =
@@ -1940,6 +2188,12 @@ nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
 	return found;
 }
 
+/*
+ * called by:
+ *   - drivers/nvme/host/fabrics.c|1032| <<nvmf_create_ctrl>> ctrl = ops->create_ctrl(dev, opts);
+ *
+ * struct nvmf_transport_ops nvme_rdma_transport.create_ctrl = nvme_rdma_create_ctrl()
+ */
 static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		struct nvmf_ctrl_options *opts)
 {
@@ -1963,6 +2217,7 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 		opts->mask |= NVMF_OPT_TRSVCID;
 	}
 
+	/* convert an IPv4/IPv6 and port to socket address */
 	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
 			opts->traddr, opts->trsvcid, &ctrl->addr);
 	if (ret) {
@@ -1991,6 +2246,9 @@ static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
 	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
 	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
 
+	/*
+	 * ctrl->ctrl是struct nvme_ctrl
+	 */
 	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
 				opts->nr_poll_queues + 1;
 	ctrl->ctrl.sqsize = opts->queue_size - 1;
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 5487308..5fc6b49 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -2329,6 +2329,11 @@ struct iw_cm_conn_param;
  * need to define the supported operations, otherwise they will be set to null.
  */
 struct ib_device_ops {
+	/*
+	 * called by:
+	 *   - drivers/infiniband/core/uverbs_cmd.c|2171| <<ib_uverbs_post_send>> ret = qp->device->ops.post_send(qp->real_qp, wr, &bad_wr);
+	 *   - include/rdma/ib_verbs.h|3727| <<ib_post_send>> return qp->device->ops.post_send(qp, send_wr, bad_send_wr ? : &dummy);
+	 */
 	int (*post_send)(struct ib_qp *qp, const struct ib_send_wr *send_wr,
 			 const struct ib_send_wr **bad_send_wr);
 	int (*post_recv)(struct ib_qp *qp, const struct ib_recv_wr *recv_wr,
diff --git a/include/rdma/rdma_netlink.h b/include/rdma/rdma_netlink.h
index 10732ab..00bdf53 100644
--- a/include/rdma/rdma_netlink.h
+++ b/include/rdma/rdma_netlink.h
@@ -103,6 +103,10 @@ bool rdma_nl_chk_listeners(unsigned int group);
 struct rdma_link_ops {
 	struct list_head list;
 	const char *type;
+	/*
+	 * called by:
+	 *   - drivers/infiniband/core/nldev.c|1316| <<nldev_newlink>> err = ops ? ops->newlink(ibdev_name, ndev) : -EINVAL;
+	 */
 	int (*newlink)(const char *ibdev_name, struct net_device *ndev);
 };
 
diff --git a/net/rds/ib.c b/net/rds/ib.c
index b8d581b..9a2706f 100644
--- a/net/rds/ib.c
+++ b/net/rds/ib.c
@@ -553,6 +553,10 @@ struct rds_transport rds_ib_transport = {
 	.t_type			= RDS_TRANS_IB
 };
 
+/*
+ * called by:
+ *   - net/rds/rdma_transport.c|285| <<rds_rdma_init>> ret = rds_ib_init();
+ */
 int rds_ib_init(void)
 {
 	int ret;
diff --git a/net/rds/transport.c b/net/rds/transport.c
index 46f709a..23bb592 100644
--- a/net/rds/transport.c
+++ b/net/rds/transport.c
@@ -41,6 +41,11 @@
 static struct rds_transport *transports[RDS_TRANS_COUNT];
 static DECLARE_RWSEM(rds_trans_sem);
 
+/*
+ * called by:
+ *   - net/rds/ib.c|578| <<rds_ib_init>> rds_trans_register(&rds_ib_transport);
+ *   - net/rds/tcp.c|732| <<rds_tcp_init>> rds_trans_register(&rds_tcp_transport);
+ */
 void rds_trans_register(struct rds_transport *trans)
 {
 	BUG_ON(strlen(trans->t_name) + 1 > TRANSNAMSIZ);
-- 
2.7.4

