From 2586d9ad6336e8cdab4f920e4e327b909008e789 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Wed, 11 Feb 2026 13:39:01 -0800
Subject: [PATCH 1/1] linux-v6.18

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/include/asm/kvm_host.h             |   36 +
 arch/arm64/include/asm/kvm_mmu.h              |   12 +
 arch/arm64/kvm/arm.c                          |    4 +
 arch/arm64/kvm/at.c                           |   12 +
 arch/arm64/kvm/guest.c                        |   12 +
 arch/arm64/kvm/hyp/include/nvhe/mem_protect.h |   12 +
 arch/arm64/kvm/hyp/nvhe/mem_protect.c         |   12 +
 arch/arm64/kvm/hyp/nvhe/switch.c              |   12 +
 arch/arm64/kvm/hyp/nvhe/tlb.c                 |   24 +
 arch/arm64/kvm/hyp/vhe/switch.c               |   12 +
 arch/arm64/kvm/hyp/vhe/tlb.c                  |   24 +
 arch/arm64/kvm/pkvm.c                         |   20 +
 arch/arm64/kvm/pmu-emul.c                     |   26 +
 arch/arm64/kvm/sys_regs.c                     |   54 +
 arch/arm64/kvm/vgic/vgic-init.c               |   10 +
 arch/riscv/kvm/tlb.c                          |    7 +
 arch/x86/events/core.c                        |    3 +
 arch/x86/include/asm/io_apic.h                |   26 +
 arch/x86/include/asm/kvm_host.h               |  372 +++
 arch/x86/include/asm/kvm_para.h               |   12 +
 arch/x86/include/asm/mpspec_def.h             |    8 +
 arch/x86/include/asm/nmi.h                    |   21 +
 arch/x86/kernel/aperture_64.c                 |    9 +
 arch/x86/kernel/apic/apic.c                   |    7 +
 arch/x86/kernel/apic/io_apic.c                |  406 +++
 arch/x86/kernel/apic/x2apic_cluster.c         |   27 +
 arch/x86/kernel/apic/x2apic_phys.c            |   24 +
 arch/x86/kernel/crash.c                       |    8 +
 arch/x86/kernel/kvm.c                         |   50 +
 arch/x86/kernel/kvmclock.c                    |    9 +
 arch/x86/kernel/machine_kexec_32.c            |    8 +
 arch/x86/kernel/machine_kexec_64.c            |    8 +
 arch/x86/kernel/mpparse.c                     |   16 +
 arch/x86/kernel/nmi.c                         |   11 +
 arch/x86/kernel/pci-dma.c                     |   15 +
 arch/x86/kernel/reboot.c                      |    8 +
 arch/x86/kvm/cpuid.c                          |   29 +
 arch/x86/kvm/cpuid.h                          |   39 +
 arch/x86/kvm/hyperv.c                         |   45 +
 arch/x86/kvm/ioapic.c                         |    7 +
 arch/x86/kvm/irq.c                            |   48 +
 arch/x86/kvm/irq.h                            |   24 +
 arch/x86/kvm/lapic.c                          |  127 +
 arch/x86/kvm/mmu/mmu.c                        |   56 +
 arch/x86/kvm/mmu/mmu_internal.h               |    6 +
 arch/x86/kvm/pmu.c                            |   28 +
 arch/x86/kvm/svm/avic.c                       |   47 +
 arch/x86/kvm/svm/hyperv.c                     |    9 +
 arch/x86/kvm/svm/nested.c                     |  298 +-
 arch/x86/kvm/svm/pmu.c                        |   24 +
 arch/x86/kvm/svm/sev.c                        |    8 +
 arch/x86/kvm/svm/svm.c                        |  161 +
 arch/x86/kvm/svm/svm.h                        |   31 +
 arch/x86/kvm/vmx/nested.c                     |   58 +
 arch/x86/kvm/vmx/posted_intr.c                |   47 +
 arch/x86/kvm/vmx/vmx.c                        |  132 +
 arch/x86/kvm/vmx/vmx.h                        |   37 +
 arch/x86/kvm/x86.c                            | 2624 +++++++++++++++++
 arch/x86/kvm/x86.h                            |   78 +
 arch/x86/kvm/xen.c                            |   17 +
 drivers/hv/hv_balloon.c                       |    9 +
 drivers/iommu/amd/amd_iommu_types.h           |   95 +
 drivers/iommu/amd/debugfs.c                   |   67 +
 drivers/iommu/amd/init.c                      |  860 ++++++
 drivers/iommu/amd/iommu.c                     |  521 ++++
 drivers/iommu/amd/quirks.c                    |    7 +
 drivers/iommu/intel/dmar.c                    |    9 +
 drivers/vfio/pci/vfio_pci_intrs.c             |   34 +
 drivers/vfio/vfio_main.c                      |   10 +
 drivers/vhost/vdpa.c                          |   10 +
 drivers/virtio/virtio_balloon.c               |  408 +++
 include/linux/balloon_compaction.h            |    6 +
 include/linux/irqbypass.h                     |   46 +
 include/linux/kvm_host.h                      |   96 +
 include/uapi/linux/vfio.h                     |    5 +
 kernel/irq/irqdomain.c                        |   32 +
 kernel/sched/core.c                           |   63 +
 mm/balloon_compaction.c                       |    6 +
 mm/page_reporting.c                           |    5 +
 virt/kvm/async_pf.c                           |   89 +
 virt/kvm/eventfd.c                            |   89 +
 virt/kvm/irqchip.c                            |   73 +
 virt/kvm/kvm_main.c                           |  192 ++
 virt/lib/irqbypass.c                          |   61 +
 84 files changed, 8109 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 64302c438..e985014b9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -1490,8 +1490,21 @@ static inline void kvm_hyp_reserve(void) { }
 void kvm_arm_vcpu_power_off(struct kvm_vcpu *vcpu);
 bool kvm_arm_vcpu_stopped(struct kvm_vcpu *vcpu);
 
+/*
+ * 在以下使用__vm_id_reg():
+ *   - arch/arm64/include/asm/kvm_host.h|1513| <<kvm_read_vm_id_reg>> ({ u64 __val = *__vm_id_reg(&(kvm)->arch, reg); __val; })
+ *   - arch/arm64/kvm/sys_regs.c|2349| <<kvm_set_vm_id_reg>> u64 *p = __vm_id_reg(&kvm->arch, reg);
+ */
 static inline u64 *__vm_id_reg(struct kvm_arch *ka, u32 reg)
 {
+	/*
+	 * struct kvm_arch *ka:
+	 * -> u64 id_regs[KVM_ARM_ID_REG_NUM];
+	 * -> u64 midr_el1;
+	 * -> u64 revidr_el1;
+	 * -> u64 aidr_el1;
+	 * -> u64 ctr_el0;
+	 */
 	switch (reg) {
 	case sys_reg(3, 0, 0, 1, 0) ... sys_reg(3, 0, 0, 7, 7):
 		return &ka->id_regs[IDREG_IDX(reg)];
@@ -1509,6 +1522,29 @@ static inline u64 *__vm_id_reg(struct kvm_arch *ka, u32 reg)
 	}
 }
 
+/*
+ * 在以下使用kvm_read_vm_id_reg():
+ *   - arch/arm64/include/asm/kvm_host.h|1528| <<get_idreg_field_unsigned>> u64 __val = kvm_read_vm_id_reg((kvm), SYS_##id); \
+ *   - arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h|54| <<ctxt_midr_el1>> return kvm_read_vm_id_reg(kvm, SYS_MIDR_EL1);
+ *   - arch/arm64/kvm/hyp/nvhe/pkvm.c|50| <<pkvm_vcpu_reset_hcr>> kvm_read_vm_id_reg(vcpu->kvm, SYS_CTR_EL0) == read_cpuid(CTR_EL0))
+ *   - arch/arm64/kvm/pmu-emul.c|61| <<kvm_pmu_event_mask>> u64 dfr0 = kvm_read_vm_id_reg(kvm, SYS_ID_AA64DFR0_EL1);
+ *   - arch/arm64/kvm/sys_regs.c|1824| <<read_id_reg>> return kvm_read_vm_id_reg(vcpu->kvm, reg_to_encoding(r));
+ *   - arch/arm64/kvm/sys_regs.c|2378| <<access_ctr>> p->regval = kvm_read_vm_id_reg(vcpu->kvm, SYS_CTR_EL0);
+ *   - arch/arm64/kvm/sys_regs.c|4270| <<trap_dbgdidr>> u64 dfr = kvm_read_vm_id_reg(vcpu->kvm, SYS_ID_AA64DFR0_EL1);
+ *   - arch/arm64/kvm/sys_regs.c|5058| <<idregs_debug_show>> desc->name, kvm_read_vm_id_reg(kvm, reg_to_encoding(desc)));
+ *   - arch/arm64/kvm/sys_regs.c|5553| <<vcpu_set_hcr>> kvm_read_vm_id_reg(kvm, SYS_CTR_EL0) == read_sanitised_ftr_reg(SYS_CTR_EL0))
+ *   - arch/arm64/kvm/sys_regs.c|5619| <<kvm_finalize_sys_regs>> val = kvm_read_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1) & ~ID_AA64PFR0_EL1_GIC;
+ *   - arch/arm64/kvm/sys_regs.c|5621| <<kvm_finalize_sys_regs>> val = kvm_read_vm_id_reg(kvm, SYS_ID_PFR1_EL1) & ~ID_PFR1_EL1_GIC;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|165| <<kvm_vgic_create>> aa64pfr0 = kvm_read_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1) & ~ID_AA64PFR0_EL1_GIC;
+ *   - arch/arm64/kvm/vgic/vgic-init.c|166| <<kvm_vgic_create>> pfr1 = kvm_read_vm_id_reg(kvm, SYS_ID_PFR1_EL1) & ~ID_PFR1_EL1_GIC;
+ *
+ * struct kvm_arch *ka:
+ * -> u64 id_regs[KVM_ARM_ID_REG_NUM];
+ * -> u64 midr_el1;
+ * -> u64 revidr_el1;
+ * -> u64 aidr_el1;
+ * -> u64 ctr_el0;
+ */
 #define kvm_read_vm_id_reg(kvm, reg)					\
 	({ u64 __val = *__vm_id_reg(&(kvm)->arch, reg); __val; })
 
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index e4069f2ce..229f26cf7 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -317,6 +317,18 @@ static __always_inline u64 kvm_get_vttbr(struct kvm_s2_mmu *mmu)
  * Must be called from hyp code running at EL2 with an updated VTTBR
  * and interrupts disabled.
  */
+/*
+ * 在以下使用__load_stage2():
+ *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+ *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+ *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+ *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+ *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+ */
 static __always_inline void __load_stage2(struct kvm_s2_mmu *mmu,
 					  struct kvm_arch *arch)
 {
diff --git a/arch/arm64/kvm/arm.c b/arch/arm64/kvm/arm.c
index 052bf0d4d..e5ae8ee92 100644
--- a/arch/arm64/kvm/arm.c
+++ b/arch/arm64/kvm/arm.c
@@ -828,6 +828,10 @@ static void kvm_init_mpidr_data(struct kvm *kvm)
  * run for the first time, as well as the updates that must be
  * performed each time we get a new thread dealing with this vcpu.
  */
+/*
+ * 处理KVM_RUN:
+ *   - virt/kvm/kvm_main.c|4464| <<kvm_vcpu_ioctl(KVM_RUN)>> r = kvm_arch_vcpu_run_pid_change(vcpu);
+ */
 int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
diff --git a/arch/arm64/kvm/at.c b/arch/arm64/kvm/at.c
index be26d5aa6..be3cec84f 100644
--- a/arch/arm64/kvm/at.c
+++ b/arch/arm64/kvm/at.c
@@ -1347,6 +1347,18 @@ static u64 __kvm_at_s1e01_fast(struct kvm_vcpu *vcpu, u32 op, u64 vaddr)
 		}
 	}
 	write_sysreg_el1(vcpu_read_sys_reg(vcpu, SCTLR_EL1),	SYS_SCTLR);
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	__load_stage2(mmu, mmu->arch);
 
 skip_mmu_switch:
diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
index 1c87699fd..6a9a3653b 100644
--- a/arch/arm64/kvm/guest.c
+++ b/arch/arm64/kvm/guest.c
@@ -1002,6 +1002,18 @@ int kvm_vm_ioctl_mte_copy_tags(struct kvm *kvm,
 
 	mutex_lock(&kvm->slots_lock);
 
+	/*
+	 * 在以下使用kvm->nr_memslots_dirty_logging:
+	 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+	 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 */
 	if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
 		ret = -EBUSY;
 		goto out;
diff --git a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
index 5f9d56754..3ea717648 100644
--- a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
+++ b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
@@ -62,6 +62,18 @@ int refill_memcache(struct kvm_hyp_memcache *mc, unsigned long min_pages,
 
 static __always_inline void __load_host_stage2(void)
 {
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	if (static_branch_likely(&kvm_protected_mode_initialized))
 		__load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
 	else
diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 49db32f3d..948ece6d7 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
@@ -334,6 +334,18 @@ int __pkvm_prot_finalize(void)
 	kvm_flush_dcache_to_poc(params, sizeof(*params));
 
 	write_sysreg_hcr(params->hcr_el2);
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	__load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
 
 	/*
diff --git a/arch/arm64/kvm/hyp/nvhe/switch.c b/arch/arm64/kvm/hyp/nvhe/switch.c
index d3b9ec8a7..50c79307d 100644
--- a/arch/arm64/kvm/hyp/nvhe/switch.c
+++ b/arch/arm64/kvm/hyp/nvhe/switch.c
@@ -299,6 +299,18 @@ int __kvm_vcpu_run(struct kvm_vcpu *vcpu)
 	__sysreg_restore_state_nvhe(guest_ctxt);
 
 	mmu = kern_hyp_va(vcpu->arch.hw_mmu);
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	__load_stage2(mmu, kern_hyp_va(mmu->arch));
 	__activate_traps(vcpu);
 
diff --git a/arch/arm64/kvm/hyp/nvhe/tlb.c b/arch/arm64/kvm/hyp/nvhe/tlb.c
index 48da9ca97..d408b5741 100644
--- a/arch/arm64/kvm/hyp/nvhe/tlb.c
+++ b/arch/arm64/kvm/hyp/nvhe/tlb.c
@@ -107,6 +107,18 @@ static void enter_vmid_context(struct kvm_s2_mmu *mmu,
 	 * ensuring that we always have an ISB, but not two ISBs back
 	 * to back.
 	 */
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	if (vcpu)
 		__load_host_stage2();
 	else
@@ -127,6 +139,18 @@ static void exit_vmid_context(struct tlb_inv_context *cxt)
 	if (!mmu)
 		return;
 
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	if (vcpu)
 		__load_stage2(mmu, kern_hyp_va(mmu->arch));
 	else
diff --git a/arch/arm64/kvm/hyp/vhe/switch.c b/arch/arm64/kvm/hyp/vhe/switch.c
index 9984c4923..a6ee16a7d 100644
--- a/arch/arm64/kvm/hyp/vhe/switch.c
+++ b/arch/arm64/kvm/hyp/vhe/switch.c
@@ -219,6 +219,18 @@ void kvm_vcpu_load_vhe(struct kvm_vcpu *vcpu)
 
 	__vcpu_load_switch_sysregs(vcpu);
 	__vcpu_load_activate_traps(vcpu);
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	__load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
 }
 
diff --git a/arch/arm64/kvm/hyp/vhe/tlb.c b/arch/arm64/kvm/hyp/vhe/tlb.c
index ec2569818..a886e5839 100644
--- a/arch/arm64/kvm/hyp/vhe/tlb.c
+++ b/arch/arm64/kvm/hyp/vhe/tlb.c
@@ -60,6 +60,18 @@ static void enter_vmid_context(struct kvm_s2_mmu *mmu,
 	 * place before clearing TGE. __load_stage2() already
 	 * has an ISB in order to deal with this.
 	 */
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	__load_stage2(mmu, mmu->arch);
 	val = read_sysreg(hcr_el2);
 	val &= ~HCR_TGE;
@@ -76,6 +88,18 @@ static void exit_vmid_context(struct tlb_inv_context *cxt)
 	write_sysreg_hcr(HCR_HOST_VHE_FLAGS);
 	isb();
 
+	/*
+	 * 在以下使用__load_stage2():
+	 *   - arch/arm64/kvm/at.c|1350| <<__kvm_at_s1e01_fast>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/include/nvhe/mem_protect.h|66| <<__load_host_stage2>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/mem_protect.c|337| <<__pkvm_prot_finalize>> __load_stage2(&host_mmu.arch.mmu, &host_mmu.arch);
+	 *   - arch/arm64/kvm/hyp/nvhe/switch.c|302| <<__kvm_vcpu_run>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|113| <<enter_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/nvhe/tlb.c|131| <<exit_vmid_context>> __load_stage2(mmu, kern_hyp_va(mmu->arch));
+	 *   - arch/arm64/kvm/hyp/vhe/switch.c|222| <<kvm_vcpu_load_vhe>> __load_stage2(vcpu->arch.hw_mmu, vcpu->arch.hw_mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|63| <<enter_vmid_context>> __load_stage2(mmu, mmu->arch);
+	 *   - arch/arm64/kvm/hyp/vhe/tlb.c|81| <<exit_vmid_context>> __load_stage2(cxt->mmu, cxt->mmu->arch);
+	 */
 	/* ... and the stage-2 MMU context that we switched away from */
 	if (cxt->mmu)
 		__load_stage2(cxt->mmu, cxt->mmu->arch);
diff --git a/arch/arm64/kvm/pkvm.c b/arch/arm64/kvm/pkvm.c
index 24f0f8a8c..579dde28a 100644
--- a/arch/arm64/kvm/pkvm.c
+++ b/arch/arm64/kvm/pkvm.c
@@ -39,6 +39,10 @@ static int __init register_memblock_regions(void)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_hyp_reserve():
+ *   - arch/arm64/mm/init.c|315| <<bootmem_init>> kvm_hyp_reserve();
+ */
 void __init kvm_hyp_reserve(void)
 {
 	u64 hyp_mem_pages = 0;
@@ -104,6 +108,10 @@ static void __pkvm_destroy_hyp_vm(struct kvm *kvm)
 	free_hyp_memcache(&kvm->arch.pkvm.stage2_teardown_mc);
 }
 
+/*
+ * 在以下使用__pkvm_create_hyp_vcpu():
+ *   - arch/arm64/kvm/pkvm.c|209| <<pkvm_create_hyp_vcpu>> ret = __pkvm_create_hyp_vcpu(vcpu);
+ */
 static int __pkvm_create_hyp_vcpu(struct kvm_vcpu *vcpu)
 {
 	size_t hyp_vcpu_sz = PAGE_ALIGN(PKVM_HYP_VCPU_SIZE);
@@ -136,6 +144,10 @@ static int __pkvm_create_hyp_vcpu(struct kvm_vcpu *vcpu)
  *
  * Return 0 on success, negative error code on failure.
  */
+/*
+ * 在以下使用__pkvm_create_hyp_vm():
+ *   - arch/arm64/kvm/pkvm.c|197| <<pkvm_create_hyp_vm>> ret = __pkvm_create_hyp_vm(kvm);
+ */
 static int __pkvm_create_hyp_vm(struct kvm *kvm)
 {
 	size_t pgd_sz, hyp_vm_sz;
@@ -188,6 +200,10 @@ bool pkvm_hyp_vm_is_created(struct kvm *kvm)
 	return READ_ONCE(kvm->arch.pkvm.is_created);
 }
 
+/*
+ * 在以下使用pkvm_create_hyp_vm():
+ *   - arch/arm64/kvm/arm.c|888| <<kvm_arch_vcpu_run_pid_change>> ret = pkvm_create_hyp_vm(kvm);
+ */
 int pkvm_create_hyp_vm(struct kvm *kvm)
 {
 	int ret = 0;
@@ -200,6 +216,10 @@ int pkvm_create_hyp_vm(struct kvm *kvm)
 	return ret;
 }
 
+/*
+ * 在以下使用pkvm_create_hyp_vcpu():
+ *   - arch/arm64/kvm/arm.c|892| <<kvm_arch_vcpu_run_pid_change>> ret = pkvm_create_hyp_vcpu(vcpu);
+ */
 int pkvm_create_hyp_vcpu(struct kvm_vcpu *vcpu)
 {
 	int ret = 0;
diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index b03dbda7f..8a7f60254 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -40,6 +40,11 @@ static struct kvm_pmc *kvm_vcpu_idx_to_pmc(struct kvm_vcpu *vcpu, int cnt_idx)
 	return &vcpu->arch.pmu.pmc[cnt_idx];
 }
 
+/*
+ * 在以下使用__kvm_pmu_event_mask():
+ *   - arch/arm64/kvm/pmu-emul.c|71| <<kvm_pmu_event_mask>> return __kvm_pmu_event_mask(pmuver);
+ *   - arch/arm64/kvm/pmu-emul.c|1176| <<kvm_arm_pmu_v3_set_attr>> nr_events = __kvm_pmu_event_mask(pmuver) + 1;
+ */
 static u32 __kvm_pmu_event_mask(unsigned int pmuver)
 {
 	switch (pmuver) {
@@ -56,8 +61,29 @@ static u32 __kvm_pmu_event_mask(unsigned int pmuver)
 	}
 }
 
+/*
+ * 在以下使用kvm_pmu_event_mask():
+ *   - arch/arm64/kvm/pmu-emul.c|70| <<kvm_pmu_evtyper_mask>> kvm_pmu_event_mask(kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|498| <<kvm_pmu_counter_increment>> type &= kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|698| <<kvm_pmu_create_perf_event>> eventsel = evtreg & kvm_pmu_event_mask(vcpu->kvm);
+ *   - arch/arm64/kvm/pmu-emul.c|897| <<kvm_pmu_get_pmceid>> nr_events = kvm_pmu_event_mask(vcpu->kvm) + 1;
+ */
 static u32 kvm_pmu_event_mask(struct kvm *kvm)
 {
+	/*
+	 * struct kvm_arch *ka:
+	 * -> u64 id_regs[KVM_ARM_ID_REG_NUM];
+	 * -> u64 midr_el1;
+	 * -> u64 revidr_el1;
+	 * -> u64 aidr_el1;
+	 * -> u64 ctr_el0;
+	 *
+	 * 注释:
+	 * ID_AA64DFR0_EL1 is a 64-bit register, and is part of the
+	 * Identification registers functional group.
+	 * This register is read-only.
+	 * bit 8 - 11 是ver
+	 */
 	u64 dfr0 = kvm_read_vm_id_reg(kvm, SYS_ID_AA64DFR0_EL1);
 	u8 pmuver = SYS_FIELD_GET(ID_AA64DFR0_EL1, PMUVer, dfr0);
 
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index ec3fbe0b8..ceefd30d8 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -2327,6 +2327,16 @@ static int set_id_reg(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,
 	}
 
 	ret = arm64_check_features(vcpu, rd, val);
+	/*
+	 * 在以下使用kvm_set_vm_id_reg():
+	 *   - arch/arm64/kvm/sys_regs.c|2331| <<set_id_reg>> kvm_set_vm_id_reg(vcpu->kvm, id, val);
+	 *   - arch/arm64/kvm/sys_regs.c|3057| <<set_imp_id_reg>> kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
+	 *   - arch/arm64/kvm/sys_regs.c|5088| <<reset_vm_ftr_id_reg>> kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
+	 *   - arch/arm64/kvm/sys_regs.c|5620| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
+	 *   - arch/arm64/kvm/sys_regs.c|5622| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|176| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|177| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
+	 */
 	if (!ret)
 		kvm_set_vm_id_reg(vcpu->kvm, id, val);
 
@@ -2344,6 +2354,16 @@ static int set_id_reg(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,
 	return ret;
 }
 
+/*
+ * 在以下使用kvm_set_vm_id_reg():
+ *   - arch/arm64/kvm/sys_regs.c|2331| <<set_id_reg>> kvm_set_vm_id_reg(vcpu->kvm, id, val);
+ *   - arch/arm64/kvm/sys_regs.c|3057| <<set_imp_id_reg>> kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
+ *   - arch/arm64/kvm/sys_regs.c|5088| <<reset_vm_ftr_id_reg>> kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
+ *   - arch/arm64/kvm/sys_regs.c|5620| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
+ *   - arch/arm64/kvm/sys_regs.c|5622| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|176| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|177| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
+ */
 void kvm_set_vm_id_reg(struct kvm *kvm, u32 reg, u64 val)
 {
 	u64 *p = __vm_id_reg(&kvm->arch, reg);
@@ -3054,6 +3074,16 @@ static int set_imp_id_reg(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r,
 	if ((val & r->val) != val)
 		return -EINVAL;
 
+	/*
+	 * 在以下使用kvm_set_vm_id_reg():
+	 *   - arch/arm64/kvm/sys_regs.c|2331| <<set_id_reg>> kvm_set_vm_id_reg(vcpu->kvm, id, val);
+	 *   - arch/arm64/kvm/sys_regs.c|3057| <<set_imp_id_reg>> kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
+	 *   - arch/arm64/kvm/sys_regs.c|5088| <<reset_vm_ftr_id_reg>> kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
+	 *   - arch/arm64/kvm/sys_regs.c|5620| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
+	 *   - arch/arm64/kvm/sys_regs.c|5622| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|176| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|177| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
+	 */
 	kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
 	return 0;
 }
@@ -5085,6 +5115,16 @@ static void reset_vm_ftr_id_reg(struct kvm_vcpu *vcpu, const struct sys_reg_desc
 	if (test_bit(KVM_ARCH_FLAG_ID_REGS_INITIALIZED, &kvm->arch.flags))
 		return;
 
+	/*
+	 * 在以下使用kvm_set_vm_id_reg():
+	 *   - arch/arm64/kvm/sys_regs.c|2331| <<set_id_reg>> kvm_set_vm_id_reg(vcpu->kvm, id, val);
+	 *   - arch/arm64/kvm/sys_regs.c|3057| <<set_imp_id_reg>> kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
+	 *   - arch/arm64/kvm/sys_regs.c|5088| <<reset_vm_ftr_id_reg>> kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
+	 *   - arch/arm64/kvm/sys_regs.c|5620| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
+	 *   - arch/arm64/kvm/sys_regs.c|5622| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|176| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|177| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
+	 */
 	kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
 }
 
@@ -5603,6 +5643,10 @@ void kvm_calculate_traps(struct kvm_vcpu *vcpu)
  *
  * Because this can be called once per CPU, changes must be idempotent.
  */
+/*
+ * 在以下使用kvm_finalize_sys_regs():
+ *   - arch/arm64/kvm/arm.c|857| <<kvm_arch_vcpu_run_pid_change>> ret = kvm_finalize_sys_regs(vcpu);
+ */
 int kvm_finalize_sys_regs(struct kvm_vcpu *vcpu)
 {
 	struct kvm *kvm = vcpu->kvm;
@@ -5617,6 +5661,16 @@ int kvm_finalize_sys_regs(struct kvm_vcpu *vcpu)
 		u64 val;
 
 		val = kvm_read_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1) & ~ID_AA64PFR0_EL1_GIC;
+		/*
+		 * 在以下使用kvm_set_vm_id_reg():
+		 *   - arch/arm64/kvm/sys_regs.c|2331| <<set_id_reg>> kvm_set_vm_id_reg(vcpu->kvm, id, val);
+		 *   - arch/arm64/kvm/sys_regs.c|3057| <<set_imp_id_reg>> kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
+		 *   - arch/arm64/kvm/sys_regs.c|5088| <<reset_vm_ftr_id_reg>> kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
+		 *   - arch/arm64/kvm/sys_regs.c|5620| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
+		 *   - arch/arm64/kvm/sys_regs.c|5622| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
+		 *   - arch/arm64/kvm/vgic/vgic-init.c|176| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
+		 *   - arch/arm64/kvm/vgic/vgic-init.c|177| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
+		 */
 		kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
 		val = kvm_read_vm_id_reg(kvm, SYS_ID_PFR1_EL1) & ~ID_PFR1_EL1_GIC;
 		kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
diff --git a/arch/arm64/kvm/vgic/vgic-init.c b/arch/arm64/kvm/vgic/vgic-init.c
index da62edbc1..a959b54b4 100644
--- a/arch/arm64/kvm/vgic/vgic-init.c
+++ b/arch/arm64/kvm/vgic/vgic-init.c
@@ -173,6 +173,16 @@ int kvm_vgic_create(struct kvm *kvm, u32 type)
 		pfr1 |= SYS_FIELD_PREP_ENUM(ID_PFR1_EL1, GIC, GICv3);
 	}
 
+	/*
+	 * 在以下使用kvm_set_vm_id_reg():
+	 *   - arch/arm64/kvm/sys_regs.c|2331| <<set_id_reg>> kvm_set_vm_id_reg(vcpu->kvm, id, val);
+	 *   - arch/arm64/kvm/sys_regs.c|3057| <<set_imp_id_reg>> kvm_set_vm_id_reg(kvm, reg_to_encoding(r), val);
+	 *   - arch/arm64/kvm/sys_regs.c|5088| <<reset_vm_ftr_id_reg>> kvm_set_vm_id_reg(kvm, id, reg->reset(vcpu, reg));
+	 *   - arch/arm64/kvm/sys_regs.c|5620| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, val);
+	 *   - arch/arm64/kvm/sys_regs.c|5622| <<kvm_finalize_sys_regs>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, val);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|176| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
+	 *   - arch/arm64/kvm/vgic/vgic-init.c|177| <<kvm_vgic_create>> kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
+	 */
 	kvm_set_vm_id_reg(kvm, SYS_ID_AA64PFR0_EL1, aa64pfr0);
 	kvm_set_vm_id_reg(kvm, SYS_ID_PFR1_EL1, pfr1);
 
diff --git a/arch/riscv/kvm/tlb.c b/arch/riscv/kvm/tlb.c
index 3c5a70a2b..6dbce5dc9 100644
--- a/arch/riscv/kvm/tlb.c
+++ b/arch/riscv/kvm/tlb.c
@@ -328,6 +328,13 @@ static void make_xfence_request(struct kvm *kvm,
 			actual_req = fallback_req;
 	}
 
+	/*
+	 * 在以下使用kvm_make_vcpus_request_mask():
+	 *   - arch/riscv/kvm/tlb.c|331| <<make_xfence_request>> kvm_make_vcpus_request_mask(kvm, actual_req, vcpu_mask);
+	 *   - arch/x86/kvm/hyperv.c|2176| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+	 *   - arch/x86/kvm/hyperv.c|2209| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+	 *   - arch/x86/kvm/x86.c|11028| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
+	 */
 	kvm_make_vcpus_request_mask(kvm, actual_req, vcpu_mask);
 }
 
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index fa6c47b50..62ee2fe1a 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1764,6 +1764,9 @@ perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs)
 		return NMI_DONE;
 
 	start_clock = sched_clock();
+	/*
+	 * amd_pmu_handle_irq()
+	 */
 	ret = static_call(x86_pmu_handle_irq)(regs);
 	finish_clock = sched_clock();
 
diff --git a/arch/x86/include/asm/io_apic.h b/arch/x86/include/asm/io_apic.h
index 0d806513c..df99b2e71 100644
--- a/arch/x86/include/asm/io_apic.h
+++ b/arch/x86/include/asm/io_apic.h
@@ -162,8 +162,34 @@ extern void __init io_apic_init_mappings(void);
 extern unsigned int native_io_apic_read(unsigned int apic, unsigned int reg);
 extern void native_restore_boot_irq_mode(void);
 
+/*
+ * 在以下使用io_apic_read():
+ *   - arch/x86/kernel/apic/io_apic.c|292| <<__ioapic_read_entry>> entry.w1 = io_apic_read(apic, 0x10 + 2 * pin);
+ *   - arch/x86/kernel/apic/io_apic.c|293| <<__ioapic_read_entry>> entry.w2 = io_apic_read(apic, 0x11 + 2 * pin);
+ *   - arch/x86/kernel/apic/io_apic.c|1190| <<print_IO_APIC>> reg_00.raw = io_apic_read(ioapic_idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|1191| <<print_IO_APIC>> reg_01.raw = io_apic_read(ioapic_idx, 1); 
+ *   - arch/x86/kernel/apic/io_apic.c|1193| <<print_IO_APIC>> reg_02.raw = io_apic_read(ioapic_idx, 2); 
+ *   - arch/x86/kernel/apic/io_apic.c|1195| <<print_IO_APIC>> reg_03.raw = io_apic_read(ioapic_idx, 3);
+ *   - arch/x86/kernel/apic/io_apic.c|1395| <<setup_ioapic_ids_from_mpc_nocheck>> reg_00.raw = io_apic_read(ioapic_idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|1452| <<setup_ioapic_ids_from_mpc_nocheck>> reg_00.raw = io_apic_read(ioapic_idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|1610| <<io_apic_level_ack_pending>> e.w1 = io_apic_read(entry->apic, 0x10 + pin*2);
+ *   - arch/x86/kernel/apic/io_apic.c|2323| <<resume_ioapic_id>> reg_00.raw = io_apic_read(ioapic_idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|2359| <<io_apic_get_redir_entries>> reg_01.raw = io_apic_read(ioapic, 1);
+ *   - arch/x86/kernel/apic/io_apic.c|2400| <<io_apic_get_unique_id>> reg_00.raw = io_apic_read(ioapic, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|2429| <<io_apic_get_unique_id>> reg_00.raw = io_apic_read(ioapic, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|2471| <<io_apic_unique_id>> reg_00.raw = io_apic_read(idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|2485| <<io_apic_unique_id>> reg_00.raw = io_apic_read(idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|2499| <<io_apic_get_version>> reg_01.raw = io_apic_read(ioapic, 1);
+ *   - arch/x86/kernel/apic/io_apic.c|2655| <<bad_ioapic_register>> reg_00.raw = io_apic_read(idx, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|2656| <<bad_ioapic_register>> reg_01.raw = io_apic_read(idx, 1);
+ *   - arch/x86/kernel/apic/io_apic.c|2657| <<bad_ioapic_register>> reg_02.raw = io_apic_read(idx, 2);
+ */
 static inline unsigned int io_apic_read(unsigned int apic, unsigned int reg)
 {
+	/*
+	 * native_io_apic_read()
+	 * xen_io_apic_read()
+	 */
 	return x86_apic_ops.io_apic_read(apic, reg);
 }
 
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 48598d017..5cce3a84b 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -88,9 +88,38 @@
 #define KVM_REQ_REPORT_TPR_ACCESS	KVM_ARCH_REQ(1)
 #define KVM_REQ_TRIPLE_FAULT		KVM_ARCH_REQ(2)
 #define KVM_REQ_MMU_SYNC		KVM_ARCH_REQ(3)
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *
+ * 处理的函数kvm_guest_time_update()
+ */
 #define KVM_REQ_CLOCK_UPDATE		KVM_ARCH_REQ(4)
 #define KVM_REQ_LOAD_MMU_PGD		KVM_ARCH_REQ(5)
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
+/*
+ * 在以下使用KVM_REQ_APF_HALT:
+ *   - arch/x86/kvm/mmu/mmu.c|4639| <<__kvm_mmu_faultin_pfn>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+ *   - arch/x86/kvm/x86.c|12575| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
+ *   - arch/x86/kvm/x86.c|15581| <<kvm_arch_async_page_not_present>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+ *
+ * 处理的代码:
+ * vcpu->arch.apf.halted = true;
+ */
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
@@ -99,11 +128,36 @@
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
 #endif
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE():
+ *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理的函数kvm_update_masterclock()
+ */
 #define KVM_REQ_MASTERCLOCK_UPDATE	KVM_ARCH_REQ(13)
+/*
+ * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+ *   - arch/x86/kvm/x86.c|3174| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|3204| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+ */
 #define KVM_REQ_MCLOCK_INPROGRESS \
 	KVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_SCAN_IOAPIC \
 	KVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2466| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5626| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11551| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *
+ * 处理的函数kvm_gen_kvmclock_update()
+ */
 #define KVM_REQ_GLOBAL_CLOCK_UPDATE	KVM_ARCH_REQ(16)
 #define KVM_REQ_APIC_PAGE_RELOAD \
 	KVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
@@ -119,6 +173,15 @@
 #define KVM_REQ_TLB_FLUSH_CURRENT	KVM_ARCH_REQ(26)
 #define KVM_REQ_TLB_FLUSH_GUEST \
 	KVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+/*
+ * 在以下使用KVM_REQ_APF_READY:
+ *   - arch/x86/kvm/lapic.c|478| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+ *   - arch/x86/kvm/lapic.c|2791| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *   - arch/x86/kvm/x86.c|12650| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+ *   - arch/x86/kvm/x86.c|15626| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+ *
+ * 处理的函数kvm_check_async_pf_completion()
+ */
 #define KVM_REQ_APF_READY		KVM_ARCH_REQ(28)
 #define KVM_REQ_RECALC_INTERCEPTS	KVM_ARCH_REQ(29)
 #define KVM_REQ_UPDATE_CPU_DIRTY_LOGGING \
@@ -920,6 +983,18 @@ struct kvm_vcpu_arch {
 	 * NOT enforced/sanitized by KVM, i.e. are taken verbatim from the
 	 * guest CPUID provided by userspace.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpu_caps[NR_KVM_CPU_CAPS]:
+	 *   - arch/x86/kvm/cpuid.c|380| <<kvm_vcpu_after_set_cpuid>> memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|406| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] = kvm_cpu_caps[i] |
+	 *   - arch/x86/kvm/cpuid.c|408| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] &= cpuid_get_reg_unsafe(entry, cpuid.reg);
+	 *   - arch/x86/kvm/cpuid.c|526| <<kvm_set_cpuid>> memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|527| <<kvm_set_cpuid>> BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|569| <<kvm_set_cpuid>> memcpy(vcpu->arch.cpu_caps, vcpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.h|233| <<guest_cpu_cap_set>> vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|241| <<guest_cpu_cap_clear>> vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|267| <<guest_cpu_cap_has>> return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+	 */
 	u32 cpu_caps[NR_KVM_CPU_CAPS];
 
 	u64 reserved_gpa_bits;
@@ -951,16 +1026,99 @@ struct kvm_vcpu_arch {
 
 	u64 l1_tsc_offset;
 	u64 tsc_offset; /* current tsc offset */
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2920| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3886| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|5884| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+	 *                                       vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|12212| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	u64 last_guest_tsc;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|5854| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 : rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|5855| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 : rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|5976| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+	 *   - arch/x86/kvm/x86.c|13850| <<kvm_arch_enable_virtualization_cpu>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+	 *   - arch/x86/kvm/x86.c|13852| <<kvm_arch_enable_virtualization_cpu>> if (vcpu->arch.last_host_tsc > max_tsc)
+	 *   - arch/x86/kvm/x86.c|13853| <<kvm_arch_enable_virtualization_cpu>> max_tsc = vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|13902| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.last_host_tsc = local_tsc;
+	 */
 	u64 last_host_tsc;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_offset_adjustment:
+	 *   - arch/x86/kvm/x86.c|5847| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+	 *   - arch/x86/kvm/x86.c|5848| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+	 *   - arch/x86/kvm/x86.c|5849| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_offset_adjustment = 0;
+	 *   - arch/x86/kvm/x86.c|13901| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.tsc_offset_adjustment += delta_cyc;
+	 */
 	u64 tsc_offset_adjustment;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 注释:
+	 * Keep track of which generation this VCPU has synchronized to
+	 */
 	u64 this_tsc_nsec;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 *
+	 * 注释:
+	 * Keep track of which generation this VCPU has synchronized to
+	 */
 	u64 this_tsc_write;
+	/*
+	 * 在以下使用this_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2892| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2914| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 *
+	 * 注释:
+	 * Keep track of which generation this VCPU has synchronized to
+	 */
 	u64 this_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2554| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3745| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5784| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 */
 	bool tsc_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2555| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10856| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|12131| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	bool tsc_always_catchup;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2623| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2664| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|528| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	s8 virtual_tsc_shift;
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2624| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2614| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|518| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	u32 virtual_tsc_mult;
+	/*
+	 * 很多使用. 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2593| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 */
 	u32 virtual_tsc_khz;
 	s64 ia32_tsc_adjust_msr;
 	u64 msr_ia32_power_ctl;
@@ -1017,15 +1175,42 @@ struct kvm_vcpu_arch {
 	unsigned long last_retry_addr;
 
 	struct {
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.halted:
+		 *   - arch/x86/kvm/x86.c|12577| <<vcpu_enter_guest(KVM_REQ_APF_HALT)>> vcpu->arch.apf.halted = true;
+		 *   - arch/x86/kvm/x86.c|12952| <<kvm_vcpu_running>> return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+		 *                                                            !vcpu->arch.apf.halted);
+		 *   - arch/x86/kvm/x86.c|13067| <<vcpu_block>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|14383| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|15612| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+		 */
 		bool halted;
 		gfn_t gfns[ASYNC_PF_PER_VCPU];
 		struct gfn_to_hva_cache data;
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.msr_en_val:
+		 *   - arch/x86/kvm/x86.c|4790| <<kvm_pv_async_pf_enabled>> return (vcpu->arch.apf.msr_en_val & mask) == mask;
+		 *   - arch/x86/kvm/x86.c|4812| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.msr_en_val = data;
+		 *   - arch/x86/kvm/x86.c|5702| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_EN)>> msr_info->data = vcpu->arch.apf.msr_en_val;
+		 *   - arch/x86/kvm/x86.c|14443| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_en_val = 0;
+		 */
 		u64 msr_en_val; /* MSR_KVM_ASYNC_PF_EN */
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.msr_int_val:
+		 *   - arch/x86/kvm/x86.c|4841| <<kvm_pv_enable_async_pf_int>> vcpu->arch.apf.msr_int_val = data;
+		 *   - arch/x86/kvm/x86.c|5708| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_INT)>> msr_info->data = vcpu->arch.apf.msr_int_val;
+		 *   - arch/x86/kvm/x86.c|14444| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_int_val = 0;
+		 */
 		u64 msr_int_val; /* MSR_KVM_ASYNC_PF_INT */
 		u16 vec;
 		u32 id;
 		u32 host_apf_flags;
 		bool send_always;
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.delivery_as_pf_vmexit:
+		 *   - arch/x86/kvm/x86.c|4807| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
+		 *   - arch/x86/kvm/x86.c|15522| <<kvm_can_deliver_async_pf>> return vcpu->arch.apf.delivery_as_pf_vmexit;
+		 */
 		bool delivery_as_pf_vmexit;
 		bool pageready_pending;
 	} apf;
@@ -1081,6 +1266,16 @@ struct kvm_vcpu_arch {
 
 	/* Protected Guests */
 	bool guest_state_protected;
+	/*
+	 * 在以下使用kvm_vcpu_arch->guest_tsc_protected:
+	 *   - arch/x86/kvm/svm/sev.c|4671| <<sev_vcpu_create>> vcpu->arch.guest_tsc_protected = snp_is_secure_tsc_enabled(vcpu->kvm);
+	 *   - arch/x86/kvm/vmx/tdx.c|689| <<tdx_vcpu_create>> vcpu->arch.guest_tsc_protected = true;
+	 *   - arch/x86/kvm/x86.c|3106| <<kvm_vcpu_write_tsc_offset>> if (vcpu->arch.guest_tsc_protected)
+	 *   - arch/x86/kvm/x86.c|3186| <<__kvm_synchronize_tsc>> if (vcpu->arch.guest_tsc_protected)
+	 *   - arch/x86/kvm/x86.c|5488| <<kvm_set_msr_common(MSR_IA32_TSC)>> } else if (!vcpu->arch.guest_tsc_protected) {
+	 *   - arch/x86/kvm/x86.c|6688| <<kvm_arch_vcpu_load>> if (!vcpu->arch.guest_tsc_protected)
+	 *   - arch/x86/kvm/x86.c|8009| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (vcpu->arch.guest_tsc_protected)
+	 */
 	bool guest_tsc_protected;
 
 	/*
@@ -1431,32 +1626,195 @@ struct kvm_arch {
 
 	u64 disabled_exits;
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	s64 kvmclock_offset;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	/*
 	 * This also protects nr_vcpus_matched_tsc which is read from a
 	 * preemption-disabled region, so it must be a raw spinlock.
 	 */
 	raw_spinlock_t tsc_write_lock;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|14062| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	u64 last_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|3082| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *                                                         nsec_to_cycles(vcpu, elapsed);
+	 *   - arch/x86/kvm/x86.c|14063| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+	 */
 	u64 last_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2917| <<__kvm_synchronize_tsc>>
+	 *          kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|3113| <<kvm_synchronize_tsc>>
+	 *          if (synchronizing && vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|6731| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+	 *          matched = (vcpu->arch.virtual_tsc_khz &&
+         *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+         *                     kvm->arch.last_tsc_offset == offset);
+	 */
 	u32 last_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2918| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|6644| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+	 *          matched = (vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_offset == offset);
+	 */
 	u64 last_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2947| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2999| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 */
 	u64 cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2948| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|3005| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	u64 cur_tsc_write;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2949| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|3078| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+	 */
 	u64 cur_tsc_offset;
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2859| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2864| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2869| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 */
 	u64 cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2642| <<kvm_track_tsc_matching>> bool use_master_clock =
+	 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&vcpu->kvm->online_vcpus)) &&
+	 *         gtod_is_based_on_tsc(gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|2668| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2863| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2865| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3211| <<pvclock_update_vm_gtod_copy>> vcpus_matched =
+	 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&kvm->online_vcpus));
+	 */
 	int nr_vcpus_matched_tsc;
 
+	/*
+	 * 在以下使用kvm_arch->default_tsc_khz:
+	 *   - arch/x86/kvm/svm/sev.c|2218| <<snp_launch_start>> if (WARN_ON_ONCE(!kvm->arch.default_tsc_khz))
+	 *   - arch/x86/kvm/svm/sev.c|2221| <<snp_launch_start>> start.desired_tsc_khz = kvm->arch.default_tsc_khz;
+	 *   - arch/x86/kvm/vmx/tdx.c|2436| <<setup_tdparams>> td_params->tsc_frequency = TDX_TSC_KHZ_TO_25MHZ(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|9104| <<kvm_arch_vm_ioctl(KVM_SET_TSC_KHZ)>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|9111| <<kvm_arch_vm_ioctl(KVM_GET_TSC_KHZ)>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|14512| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|15048| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+	 */
 	u32 default_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->user_set_tsc:
+	 *   - arch/x86/kvm/x86.c|2795| <<__kvm_synchronize_tsc>> vcpu->kvm->arch.user_set_tsc = true;
+	 *   - arch/x86/kvm/x86.c|2858| <<kvm_synchronize_tsc>> } else if (kvm->arch.user_set_tsc) {
+	 */
 	bool user_set_tsc;
 	u64 apic_bus_cycle_ns;
 
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|4010| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4063| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4293| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4300| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|4466| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4505| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|4774| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4840| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|15333| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_t pvclock_sc;
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3159| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 * 在以下使用kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2647| <<kvm_track_tsc_matching>> if ((ka->use_master_clock && new_generation) ||
+	 *   - arch/x86/kvm/x86.c|2648| <<kvm_track_tsc_matching>> (ka->use_master_clock != use_master_clock))
+	 *   - arch/x86/kvm/x86.c|2653| <<kvm_track_tsc_matching>> ka->use_master_clock, gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|3163| <<pvclock_update_vm_gtod_copy>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|3167| <<pvclock_update_vm_gtod_copy>> trace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,
+	 *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> if (ka->use_master_clock &&
+	 *   - arch/x86/kvm/x86.c|3434| <<kvm_guest_time_update>> use_master_clock = ka->use_master_clock;
+	 *   - arch/x86/kvm/x86.c|3572| <<kvm_get_wall_clock_epoch>> if (!ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|5373| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+	 *   - arch/x86/kvm/x86.c|7375| <<kvm_vm_ioctl_set_clock>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/xen.c|228| <<kvm_xen_start_timer>> !vcpu->kvm->arch.use_master_clock)
+	 */
 	bool use_master_clock;
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3156| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|3595| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7376| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	u64 master_kernel_ns;
+	/*
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|3157| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3334| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3436| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3594| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 */
 	u64 master_cycle_now;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3893| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3906| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|13791| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	struct delayed_work kvmclock_update_work;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3907| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13250| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|13790| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	struct delayed_work kvmclock_sync_work;
 
 #ifdef CONFIG_KVM_HYPERV
@@ -1468,11 +1826,25 @@ struct kvm_arch {
 #endif
 
 	bool backwards_tsc_observed;
+	/*
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2477| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2480| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|3500| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 */
 	bool boot_vcpu_runs_old_kvmclock;
 	u32 bsp_vcpu_id;
 
 	u64 disabled_quirks;
 
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|8279| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|8907| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	enum kvm_irqchip_mode irqchip_mode;
 	u8 nr_reserved_ioapic_pins;
 
diff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h
index 4a47c16e2..6d4a6f5c8 100644
--- a/arch/x86/include/asm/kvm_para.h
+++ b/arch/x86/include/asm/kvm_para.h
@@ -129,8 +129,20 @@ bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token);
 
 DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
 
+/*
+ * 处理exec_page_fault:
+ *   - arch/x86/mm/fault.c|1516| <<DEFINE_IDTENTRY_RAW_ERRORCODE(exec_page_fault)>> if (kvm_handle_async_pf(regs, (u32)address))
+ */
 static __always_inline bool kvm_handle_async_pf(struct pt_regs *regs, u32 token)
 {
+	/*
+	 * 在以下使用kvm_async_pf_enabled:
+	 *   - arch/x86/include/asm/kvm_para.h|130| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
+	 *   - arch/x86/kernel/kvm.c|49| <<global>> DEFINE_STATIC_KEY_FALSE_RO(kvm_async_pf_enabled);
+	 *   - arch/x86/include/asm/kvm_para.h|134| <<kvm_handle_async_pf>> if (static_branch_unlikely(&kvm_async_pf_enabled))
+	 *   - arch/x86/kernel/kvm.c|356| <<kvm_guest_cpu_init>> WARN_ON_ONCE(!static_branch_likely(&kvm_async_pf_enabled));
+	 *   - arch/x86/kernel/kvm.c|834| <<kvm_guest_init>> static_branch_enable(&kvm_async_pf_enabled);
+	 */
 	if (static_branch_unlikely(&kvm_async_pf_enabled))
 		return __kvm_handle_async_pf(regs, token);
 	else
diff --git a/arch/x86/include/asm/mpspec_def.h b/arch/x86/include/asm/mpspec_def.h
index 6fb923a34..a426baa92 100644
--- a/arch/x86/include/asm/mpspec_def.h
+++ b/arch/x86/include/asm/mpspec_def.h
@@ -103,6 +103,14 @@ struct mpc_bus {
 
 #define MPC_APIC_USABLE		0x01
 
+/*
+ * 在以下使用mpc_ioapic->apicaddr:
+ *   - arch/x86/kernel/apic/io_apic.c|130| <<mpc_ioapic_addr>> return ioapics[ioapic_idx].mp_config.apicaddr;
+ *   - arch/x86/kernel/apic/io_apic.c|2697| <<mp_register_ioapic>> if (ioapics[ioapic].mp_config.apicaddr == address) {
+ *   - arch/x86/kernel/apic/io_apic.c|2712| <<mp_register_ioapic>> ioapics[idx].mp_config.apicaddr = address;
+ *   - arch/x86/kernel/mpparse.c|112| <<MP_ioapic_info>> mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
+ *   - arch/x86/kernel/mpparse.c|359| <<construct_ioapic_table>> ioapic.apicaddr = IO_APIC_DEFAULT_PHYS_BASE;
+ */
 struct mpc_ioapic {
 	unsigned char type;
 	unsigned char apicid;
diff --git a/arch/x86/include/asm/nmi.h b/arch/x86/include/asm/nmi.h
index 79d88d12c..937c7e0fc 100644
--- a/arch/x86/include/asm/nmi.h
+++ b/arch/x86/include/asm/nmi.h
@@ -75,6 +75,27 @@ struct nmiaction {
  *
  * Return: 0 on success, or an error code on failure.
  */
+/*
+ * 在以下使用register_nmi_handler():
+ *   - arch/x86/events/amd/ibs.c|1488| <<perf_event_ibs_init>> ret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, "perf_ibs");
+ *   - arch/x86/events/core.c|2134| <<init_hw_perf_events>> register_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, "PMI");
+ *   - arch/x86/kernel/apic/hw_nmi.c|56| <<register_nmi_cpu_backtrace_handler>> register_nmi_handler(NMI_LOCAL, nmi_cpu_backtrace_handler,
+ *   - arch/x86/kernel/cpu/mce/inject.c|778| <<inject_init>> register_nmi_handler(NMI_LOCAL, mce_raise_notify, 0, "mce_notify");
+ *   - arch/x86/kernel/cpu/mshyperv.c|559| <<ms_hyperv_init_platform>> register_nmi_handler(NMI_UNKNOWN, hv_nmi_unknown, NMI_FLAG_FIRST,
+ *   - arch/x86/kernel/kgdb.c|605| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_LOCAL, kgdb_nmi_handler,
+ *   - arch/x86/kernel/kgdb.c|610| <<kgdb_arch_init>> retval = register_nmi_handler(NMI_UNKNOWN, kgdb_nmi_handler,
+ *   - arch/x86/kernel/nmi_selftest.c|43| <<init_nmi_testsuite>> register_nmi_handler(NMI_UNKNOWN, nmi_unk_cb, 0, "nmi_selftest_unk",
+ *   - arch/x86/kernel/nmi_selftest.c|66| <<test_nmi_ipi>> if (register_nmi_handler(NMI_LOCAL, test_nmi_ipi_callback,
+ *   - arch/x86/kernel/smp.c|145| <<register_stop_handler>> return register_nmi_handler(NMI_LOCAL, smp_stop_nmi_callback,
+ *   - arch/x86/platform/uv/uv_nmi.c|1032| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, "uv"))
+ *   - arch/x86/platform/uv/uv_nmi.c|1035| <<uv_register_nmi_notifier>> if (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, "uvping"))
+ *   - drivers/acpi/apei/ghes.c|1446| <<ghes_nmi_add>> register_nmi_handler(NMI_LOCAL, ghes_notify_nmi, 0, "ghes");
+ *   - drivers/char/ipmi/ipmi_watchdog.c|1255| <<check_parms>> rv = register_nmi_handler(NMI_UNKNOWN, ipmi_nmi, 0,
+ *   - drivers/edac/igen6_edac.c|1439| <<register_err_handler>> rc = register_nmi_handler(NMI_SERR, ecclog_nmi_handler,
+ *   - drivers/watchdog/hpwdt.c|245| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_UNKNOWN, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|248| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_SERR, hpwdt_pretimeout, 0, "hpwdt");
+ *   - drivers/watchdog/hpwdt.c|251| <<hpwdt_init_nmi_decoding>> retval = register_nmi_handler(NMI_IO_CHECK, hpwdt_pretimeout, 0, "hpwdt");
+ */
 #define register_nmi_handler(t, fn, fg, n, init...)	\
 ({							\
 	static struct nmiaction init fn##_na = {	\
diff --git a/arch/x86/kernel/aperture_64.c b/arch/x86/kernel/aperture_64.c
index 769321185..0a0b2eb95 100644
--- a/arch/x86/kernel/aperture_64.c
+++ b/arch/x86/kernel/aperture_64.c
@@ -426,6 +426,15 @@ void __init gart_iommu_hole_init(void)
 			if (!early_is_amd_nb(read_pci_config(bus, slot, 3, 0x00)))
 				continue;
 
+			/*
+			 * 在以下使用iommu_detected:
+			 *   - arch/x86/kernel/pci-dma.c|38| <<global>> int iommu_detected __read_mostly = 0;
+			 *   - arch/x86/kernel/aperture_64.c|429| <<gart_iommu_hole_init>> iommu_detected = 1;
+			 *   - drivers/iommu/amd/init.c|3650| <<amd_iommu_detect>> if (no_iommu || (iommu_detected && !gart_iommu_aperture))
+			 *   - drivers/iommu/amd/init.c|3661| <<amd_iommu_detect>> iommu_detected = 1;
+			 *   - drivers/iommu/intel/dmar.c|931| <<detect_intel_iommu>> if (!ret && !no_iommu && !iommu_detected &&
+			 *   - drivers/iommu/intel/dmar.c|933| <<detect_intel_iommu>> iommu_detected = 1;
+			 */
 			iommu_detected = 1;
 			gart_iommu_aperture = 1;
 			x86_init.iommu.iommu_init = gart_iommu_init;
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 680d30558..387ff95b1 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -161,6 +161,13 @@ EXPORT_SYMBOL_GPL(local_apic_timer_c2_ok);
 /*
  * Debug level, exported for io_apic.c
  */
+/*
+ * 在以下使用apic_verbosity:
+ *   - arch/x86/include/asm/apic.h|40| <<apic_printk>> if ((v) <= apic_verbosity) \
+ *   - arch/x86/kernel/apic/apic.c|2628| <<apic_set_verbosity>> apic_verbosity = APIC_DEBUG;
+ *   - arch/x86/kernel/apic/apic.c|2630| <<apic_set_verbosity>> apic_verbosity = APIC_VERBOSE;
+ *   - arch/x86/kernel/apic/vector.c|1372| <<print_ICs>> if (apic_verbosity == APIC_QUIET)
+ */
 int apic_verbosity __ro_after_init;
 
 int pic_mode __ro_after_init;
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index 5ba2feb2c..4ceb6fab6 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -68,6 +68,49 @@
 #include <asm/pgtable.h>
 #include <asm/x86_init.h>
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] mp_map_pin_to_irq
+ * [0] acpi_register_gsi_ioapic
+ * [0] acpi_pci_irq_enable
+ * [0] do_pci_enable_device
+ * [0] pci_enable_device_flags
+ * [0] e1000_probe
+ * [0] local_pci_probe
+ * [0] pci_device_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ */
+
 #define	for_each_ioapic(idx)		\
 	for ((idx) = 0; (idx) < nr_ioapics; (idx)++)
 #define	for_each_ioapic_reverse(idx)	\
@@ -127,6 +170,14 @@ int mpc_ioapic_id(int ioapic_idx)
 
 unsigned int mpc_ioapic_addr(int ioapic_idx)
 {
+	/*
+	 * 在以下使用mpc_ioapic->apicaddr:
+	 *   - arch/x86/kernel/apic/io_apic.c|130| <<mpc_ioapic_addr>> return ioapics[ioapic_idx].mp_config.apicaddr;
+	 *   - arch/x86/kernel/apic/io_apic.c|2697| <<mp_register_ioapic>> if (ioapics[ioapic].mp_config.apicaddr == address) {
+	 *   - arch/x86/kernel/apic/io_apic.c|2712| <<mp_register_ioapic>> ioapics[idx].mp_config.apicaddr = address;
+	 *   - arch/x86/kernel/mpparse.c|112| <<MP_ioapic_info>> mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
+	 *   - arch/x86/kernel/mpparse.c|359| <<construct_ioapic_table>> ioapic.apicaddr = IO_APIC_DEFAULT_PHYS_BASE;
+	 */
 	return ioapics[ioapic_idx].mp_config.apicaddr;
 }
 
@@ -165,6 +216,26 @@ u32 gsi_top;
 /* MP IRQ source entries */
 struct mpc_intsrc mp_irqs[MAX_IRQ_SOURCES];
 
+/*
+ * 在以下设置mp_irq_entries:
+ *   - arch/x86/kernel/apic/io_apic.c|265| <<mp_save_irq>> if (++mp_irq_entries == MAX_IRQ_SOURCES)
+ * 在以下使用mp_irq_entries:
+ *   - arch/x86/include/asm/io_apic.h|132| <<io_apic_assign_pci_irqs>> (mp_irq_entries && !ioapic_is_disabled && io_apic_irqs)
+ *   - arch/x86/kernel/acpi/boot.c|1145| <<mp_config_acpi_legacy_irqs>> for (idx = 0; idx < mp_irq_entries; idx++) {
+ *   - arch/x86/kernel/acpi/boot.c|1157| <<mp_config_acpi_legacy_irqs>> if (idx != mp_irq_entries) {
+ *   - arch/x86/kernel/apic/io_apic.c|259| <<mp_save_irq>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/apic/io_apic.c|264| <<mp_save_irq>> memcpy(&mp_irqs[mp_irq_entries], m, sizeof(*m));
+ *   - arch/x86/kernel/apic/io_apic.c|740| <<find_irq_entry>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/apic/io_apic.c|758| <<find_isa_irq_pin>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/apic/io_apic.c|772| <<find_isa_irq_apic>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/apic/io_apic.c|780| <<find_isa_irq_apic>> if (i < mp_irq_entries) {
+ *   - arch/x86/kernel/apic/io_apic.c|1230| <<IO_APIC_get_PCI_irq_vector>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/apic/io_apic.c|1405| <<print_IO_APICs>> apic_dbg("number of MP IRQ sources: %d.\n", mp_irq_entries);
+ *   - arch/x86/kernel/apic/io_apic.c|1606| <<setup_ioapic_ids_from_mpc_nocheck>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/mpparse.c|473| <<check_physptr>> if (!mp_irq_entries) {
+ *   - arch/x86/kernel/mpparse.c|669| <<get_MP_intsrc_index>> for (i = 0; i < mp_irq_entries; i++) {
+ *   - arch/x86/kernel/mpparse.c|778| <<replace_intsrc_all>> for (i = 0; i < mp_irq_entries; i++) {
+ */
 /* # of MP IRQ source entries */
 int mp_irq_entries;
 
@@ -285,6 +356,12 @@ static void io_apic_write(unsigned int apic, unsigned int reg,
 	writel(value, &io_apic->data);
 }
 
+/*
+ * 在以下使用__ioapic_read_entry():
+ *   - arch/x86/kernel/apic/io_apic.c|301| <<ioapic_read_entry>> return __ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|446| <<__eoi_ioapic_pin>> entry = entry1 = __ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|1858| <<ioapic_irq_get_chip_state>> rentry = __ioapic_read_entry(p->apic, p->pin);
+ */
 static struct IO_APIC_route_entry __ioapic_read_entry(int apic, int pin)
 {
 	struct IO_APIC_route_entry entry;
@@ -295,9 +372,25 @@ static struct IO_APIC_route_entry __ioapic_read_entry(int apic, int pin)
 	return entry;
 }
 
+/*
+ * 在以下使用ioapic_read_entry():
+ *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+ *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+ *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+ */
 static struct IO_APIC_route_entry ioapic_read_entry(int apic, int pin)
 {
 	guard(raw_spinlock_irqsave)(&ioapic_lock);
+	/*
+	 * 在以下使用__ioapic_read_entry():
+	 *   - arch/x86/kernel/apic/io_apic.c|301| <<ioapic_read_entry>> return __ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|446| <<__eoi_ioapic_pin>> entry = entry1 = __ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1858| <<ioapic_irq_get_chip_state>> rentry = __ioapic_read_entry(p->apic, p->pin);
+	 */
 	return __ioapic_read_entry(apic, pin);
 }
 
@@ -443,6 +536,12 @@ static void __eoi_ioapic_pin(int apic, int pin, int vector)
 	} else {
 		struct IO_APIC_route_entry entry, entry1;
 
+		/*
+		 * 在以下使用__ioapic_read_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|301| <<ioapic_read_entry>> return __ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|446| <<__eoi_ioapic_pin>> entry = entry1 = __ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1858| <<ioapic_irq_get_chip_state>> rentry = __ioapic_read_entry(p->apic, p->pin);
+		 */
 		entry = entry1 = __ioapic_read_entry(apic, pin);
 
 		/* Mask the entry and change the trigger mode to edge. */
@@ -469,6 +568,16 @@ static void clear_IO_APIC_pin(unsigned int apic, unsigned int pin)
 {
 	struct IO_APIC_route_entry entry;
 
+	/*
+	 * 在以下使用ioapic_read_entry():
+	 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+	 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+	 */
 	/* Check delivery_mode to be sure we're not clearing an SMI pin */
 	entry = ioapic_read_entry(apic, pin);
 	if (entry.delivery_mode == APIC_DELIVERY_MODE_SMI)
@@ -481,6 +590,16 @@ static void clear_IO_APIC_pin(unsigned int apic, unsigned int pin)
 	if (!entry.masked) {
 		entry.masked = true;
 		ioapic_write_entry(apic, pin, entry);
+		/*
+		 * 在以下使用ioapic_read_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+		 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+		 */
 		entry = ioapic_read_entry(apic, pin);
 	}
 
@@ -503,12 +622,30 @@ static void clear_IO_APIC_pin(unsigned int apic, unsigned int pin)
 	 * bit.
 	 */
 	ioapic_mask_entry(apic, pin);
+	/*
+	 * 在以下使用ioapic_read_entry():
+	 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+	 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+	 */
 	entry = ioapic_read_entry(apic, pin);
 	if (entry.irr)
 		pr_err("Unable to reset IRR for apic: %d, pin :%d\n",
 		       mpc_ioapic_id(apic), pin);
 }
 
+/*
+ * 在以下使用clear_IO_APIC():
+ *   - arch/x86/kernel/apic/io_apic.c|1503| <<enable_IO_APIC>> clear_IO_APIC();
+ *   - arch/x86/kernel/crash.c|124| <<native_machine_crash_shutdown>> clear_IO_APIC();
+ *   - arch/x86/kernel/machine_kexec_32.c|187| <<machine_kexec>> clear_IO_APIC();
+ *   - arch/x86/kernel/machine_kexec_64.c|428| <<machine_kexec>> clear_IO_APIC();
+ *   - arch/x86/kernel/reboot.c|741| <<native_machine_shutdown>> clear_IO_APIC();
+ */
 void clear_IO_APIC (void)
 {
 	int apic, pin;
@@ -553,6 +690,11 @@ __setup("pirq=", ioapic_pirq_setup);
 /*
  * Saves all the IO-APIC RTE's
  */
+/*
+ * 在以下使用save_ioapic_entries():
+ *   - arch/x86/kernel/apic/io_apic.c|2555| <<global>> struct syscore_ops ioapic_syscore_ops.suspend = save_ioapic_entries,
+ *   - arch/x86/kernel/apic/apic.c|1916| <<enable_IR_x2apic>> ret = save_ioapic_entries();
+ */
 int save_ioapic_entries(void)
 {
 	int apic, pin;
@@ -564,6 +706,16 @@ int save_ioapic_entries(void)
 			continue;
 		}
 
+		/*
+		 * 在以下使用ioapic_read_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+		 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+		 */
 		for_each_pin(apic, pin)
 			ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
 	}
@@ -574,6 +726,12 @@ int save_ioapic_entries(void)
 /*
  * Mask all IO APIC entries.
  */
+/*
+ * 在以下使用mask_ioapic_entries():
+ *   - arch/x86/kernel/apic/apic.c|1924| <<enable_IR_x2apic>> mask_ioapic_entries();
+ *   - arch/x86/kernel/apic/apic.c|2429| <<lapic_suspend>> mask_ioapic_entries();
+ *   - arch/x86/kernel/apic/apic.c|2456| <<lapic_resume>> mask_ioapic_entries();
+ */
 void mask_ioapic_entries(void)
 {
 	int apic, pin;
@@ -597,6 +755,11 @@ void mask_ioapic_entries(void)
 /*
  * Restore IO APIC entries which was saved in the ioapic structure.
  */
+/*
+ * 在以下使用restore_ioapic_entries():
+ *   - arch/x86/kernel/apic/apic.c|1933| <<enable_IR_x2apic>> restore_ioapic_entries();
+ *   - arch/x86/kernel/apic/io_apic.c|2551| <<ioapic_resume>> restore_ioapic_entries();
+ */
 int restore_ioapic_entries(void)
 {
 	int apic, pin;
@@ -614,6 +777,13 @@ int restore_ioapic_entries(void)
 /*
  * Find the IRQ entry number of a certain pin.
  */
+/*
+ * 在以下使用find_irq_entry():
+ *   - arch/x86/kernel/apic/io_apic.c|899| <<__acpi_get_override_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+ *   - arch/x86/kernel/apic/io_apic.c|1191| <<mp_map_gsi_to_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+ *   - arch/x86/kernel/apic/io_apic.c|1285| <<setup_IO_APIC_irqs>> idx = find_irq_entry(ioapic, pin, mp_INT);
+ *   - arch/x86/kernel/apic/io_apic.c|2329| <<check_timer>> int idx = find_irq_entry(apic1, pin1, mp_INT);
+ */
 static int find_irq_entry(int ioapic_idx, int pin, int type)
 {
 	int i;
@@ -646,6 +816,12 @@ static int __init find_isa_irq_pin(int irq, int type)
 	return -1;
 }
 
+/*
+ * 在以下使用find_isa_irq_apic():
+ *   - arch/x86/kernel/apic/io_apic.c|1490| <<enable_IO_APIC>> i8259_apic = find_isa_irq_apic(0, mp_ExtINT);
+ *   - arch/x86/kernel/apic/io_apic.c|2155| <<unlock_ExtINT_logic>> apic = find_isa_irq_apic(8, mp_INT);
+ *   - arch/x86/kernel/apic/io_apic.c|2295| <<check_timer>> apic1 = find_isa_irq_apic(0, mp_INT);
+ */
 static int __init find_isa_irq_apic(int irq, int type)
 {
 	int i;
@@ -777,6 +953,13 @@ static int __acpi_get_override_irq(u32 gsi, bool *trigger, bool *polarity)
 	if (pin < 0)
 		return -1;
 
+	/*
+	 * 在以下使用find_irq_entry():
+	 *   - arch/x86/kernel/apic/io_apic.c|899| <<__acpi_get_override_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|1191| <<mp_map_gsi_to_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|1285| <<setup_IO_APIC_irqs>> idx = find_irq_entry(ioapic, pin, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2329| <<check_timer>> int idx = find_irq_entry(apic1, pin1, mp_INT);
+	 */
 	idx = find_irq_entry(ioapic, pin, mp_INT);
 	if (idx < 0)
 		return -1;
@@ -795,6 +978,14 @@ int acpi_get_override_irq(u32 gsi, int *is_level, int *active_low)
 }
 #endif
 
+/*
+ * 在以下使用ioapic_set_alloc_attr():
+ *   - arch/x86/kernel/acpi/boot.c|696| <<acpi_register_gsi_ioapic>> ioapic_set_alloc_attr(&info, node, trigger, polarity);
+ *   - arch/x86/kernel/apic/io_apic.c|2323| <<mp_alloc_timer_irq>> ioapic_set_alloc_attr(&info, NUMA_NO_NODE, 0, 0);
+ *   - arch/x86/kernel/devicetree.c|225| <<dt_irqdomain_alloc>> ioapic_set_alloc_attr(&tmp, NUMA_NO_NODE, it->is_level, it->active_low);
+ *   - arch/x86/pci/intel_mid.c|273| <<intel_mid_pci_irq_enable>> ioapic_set_alloc_attr(&info, dev_to_node(&dev->dev), 1, polarity_low);
+ *   - drivers/platform/x86/intel_scu_wdt.c|38| <<tangier_probe>> ioapic_set_alloc_attr(&info, cpu_to_node(0), 1, 0);
+ */
 void ioapic_set_alloc_attr(struct irq_alloc_info *info, int node,
 			   int trigger, int polarity)
 {
@@ -910,6 +1101,22 @@ static int alloc_irq_from_domain(struct irq_domain *domain, int ioapic, u32 gsi,
 				       info, legacy, NULL);
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 /*
  * Need special handling for ISA IRQs because there may be multiple IOAPIC pins
  * sharing the same ISA IRQ number and irqdomain only supports 1:1 mapping
@@ -950,6 +1157,22 @@ static int alloc_isa_irq_from_domain(struct irq_domain *domain, int irq, int ioa
 	return irq;
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 static int mp_map_pin_to_irq(u32 gsi, int idx, int ioapic, int pin,
 			     unsigned int flags, struct irq_alloc_info *info)
 {
@@ -1001,6 +1224,12 @@ static int mp_map_pin_to_irq(u32 gsi, int idx, int ioapic, int pin,
 	return irq;
 }
 
+/*
+ * 在以下使用pin_2_irq():
+ *   - arch/x86/kernel/apic/io_apic.c|1319| <<IO_APIC_get_PCI_irq_vector>> irq = pin_2_irq(i, ioapic_idx, mp_irqs[i].dstirq, 0);
+ *   - arch/x86/kernel/apic/io_apic.c|1342| <<IO_APIC_get_PCI_irq_vector>> return pin_2_irq(best_idx, best_ioapic, mp_irqs[best_idx].dstirq, IOAPIC_MAP_ALLOC);
+ *   - arch/x86/kernel/apic/io_apic.c|1368| <<setup_IO_APIC_irqs>> pin_2_irq(idx, ioapic, pin, ioapic ? 0 : IOAPIC_MAP_ALLOC);
+ */
 static int pin_2_irq(int idx, int ioapic, int pin, unsigned int flags)
 {
 	u32 gsi = mp_pin_to_gsi(ioapic, pin);
@@ -1037,6 +1266,13 @@ int mp_map_gsi_to_irq(u32 gsi, unsigned int flags, struct irq_alloc_info *info)
 		return -ENODEV;
 
 	pin = mp_find_ioapic_pin(ioapic, gsi);
+	/*
+	 * 在以下使用find_irq_entry():
+	 *   - arch/x86/kernel/apic/io_apic.c|899| <<__acpi_get_override_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|1191| <<mp_map_gsi_to_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|1285| <<setup_IO_APIC_irqs>> idx = find_irq_entry(ioapic, pin, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2329| <<check_timer>> int idx = find_irq_entry(apic1, pin1, mp_INT);
+	 */
 	idx = find_irq_entry(ioapic, pin, mp_INT);
 	if ((flags & IOAPIC_MAP_CHECK) && idx < 0)
 		return -ENODEV;
@@ -1131,6 +1367,13 @@ static void __init setup_IO_APIC_irqs(void)
 	apic_pr_verbose("Init IO_APIC IRQs\n");
 
 	for_each_ioapic_pin(ioapic, pin) {
+		/*
+		 * 在以下使用find_irq_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|899| <<__acpi_get_override_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+		 *   - arch/x86/kernel/apic/io_apic.c|1191| <<mp_map_gsi_to_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+		 *   - arch/x86/kernel/apic/io_apic.c|1285| <<setup_IO_APIC_irqs>> idx = find_irq_entry(ioapic, pin, mp_INT);
+		 *   - arch/x86/kernel/apic/io_apic.c|2329| <<check_timer>> int idx = find_irq_entry(apic1, pin1, mp_INT);
+		 */
 		idx = find_irq_entry(ioapic, pin, mp_INT);
 		if (idx < 0) {
 			apic_pr_verbose("apic %d pin %d not connected\n",
@@ -1146,6 +1389,10 @@ void ioapic_zap_locks(void)
 	raw_spin_lock_init(&ioapic_lock);
 }
 
+/*
+ * 在以下使用io_apic_print_entries():
+ *   - arch/x86/kernel/apic/io_apic.c|1222| <<print_IO_APIC>> io_apic_print_entries(ioapic_idx, reg_01.bits.entries);
+ */
 static void io_apic_print_entries(unsigned int apic, unsigned int nr_entries)
 {
 	struct IO_APIC_route_entry entry;
@@ -1154,6 +1401,18 @@ static void io_apic_print_entries(unsigned int apic, unsigned int nr_entries)
 
 	apic_dbg("IOAPIC %d:\n", apic);
 	for (i = 0; i <= nr_entries; i++) {
+		/*
+		 * 在以下使用ioapic_read_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+		 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+		 *
+		 * struct IO_APIC_route_entry entry;
+		 */
 		entry = ioapic_read_entry(apic, i);
 		snprintf(buf, sizeof(buf), " pin%02x, %s, %s, %s, V(%02X), IRR(%1d), S(%1d)",
 			 i, entry.masked ? "disabled" : "enabled ",
@@ -1171,6 +1430,10 @@ static void io_apic_print_entries(unsigned int apic, unsigned int nr_entries)
 	}
 }
 
+/*
+ * 在以下使用print_IO_APIC():
+ *   - arch/x86/kernel/apic/io_apic.c|1243| <<print_IO_APICs>> print_IO_APIC(ioapic_idx);
+ */
 static void __init print_IO_APIC(int ioapic_idx)
 {
 	union IO_APIC_reg_00 reg_00;
@@ -1222,6 +1485,10 @@ static void __init print_IO_APIC(int ioapic_idx)
 	io_apic_print_entries(ioapic_idx, reg_01.bits.entries);
 }
 
+/*
+ * 在以下使用print_IO_APICs():
+ *   - arch/x86/kernel/apic/vector.c|1382| <<print_ICs>> print_IO_APICs();
+ */
 void __init print_IO_APICs(void)
 {
 	int ioapic_idx;
@@ -1280,6 +1547,16 @@ void __init enable_IO_APIC(void)
 		return;
 
 	for_each_ioapic_pin(apic, pin) {
+		/*
+		 * 在以下使用ioapic_read_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+		 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+		 */
 		/* See if any of the pins is in ExtINT mode */
 		struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
 
@@ -1302,6 +1579,12 @@ void __init enable_IO_APIC(void)
 	 * mptable a chance anyway.
 	 */
 	i8259_pin  = find_isa_irq_pin(0, mp_ExtINT);
+	/*
+	 * 在以下使用find_isa_irq_apic():
+	 *   - arch/x86/kernel/apic/io_apic.c|1490| <<enable_IO_APIC>> i8259_apic = find_isa_irq_apic(0, mp_ExtINT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2155| <<unlock_ExtINT_logic>> apic = find_isa_irq_apic(8, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2295| <<check_timer>> apic1 = find_isa_irq_apic(0, mp_INT);
+	 */
 	i8259_apic = find_isa_irq_apic(0, mp_ExtINT);
 	/* Trust the MP table if nothing is setup in the hardware */
 	if ((ioapic_i8259.pin == -1) && (i8259_pin >= 0)) {
@@ -1314,6 +1597,14 @@ void __init enable_IO_APIC(void)
 	    (i8259_pin >= 0) && (ioapic_i8259.pin >= 0))
 		pr_warn("ExtINT in hardware and MP table differ\n");
 
+	/*
+	 * 在以下使用clear_IO_APIC():
+	 *   - arch/x86/kernel/apic/io_apic.c|1503| <<enable_IO_APIC>> clear_IO_APIC();
+	 *   - arch/x86/kernel/crash.c|124| <<native_machine_crash_shutdown>> clear_IO_APIC();
+	 *   - arch/x86/kernel/machine_kexec_32.c|187| <<machine_kexec>> clear_IO_APIC();
+	 *   - arch/x86/kernel/machine_kexec_64.c|428| <<machine_kexec>> clear_IO_APIC();
+	 *   - arch/x86/kernel/reboot.c|741| <<native_machine_shutdown>> clear_IO_APIC();
+	 */
 	/* Do not trust the IO-APIC being empty at bootup */
 	clear_IO_APIC();
 }
@@ -1516,6 +1807,13 @@ static void __init delay_without_tsc(void)
  *	- if this function detects that timer IRQs are defunct, then we fall
  *	  back to ISA timer IRQs
  */
+/*
+ * 在以下使用timer_irq_works():
+ *   - arch/x86/kernel/apic/io_apic.c|2136| <<check_timer>> if (timer_irq_works()) {
+ *   - arch/x86/kernel/apic/io_apic.c|2155| <<check_timer>> if (timer_irq_works()) {
+ *   - arch/x86/kernel/apic/io_apic.c|2173| <<check_timer>> if (timer_irq_works()) {
+ *   - arch/x86/kernel/apic/io_apic.c|2190| <<check_timer>> if (timer_irq_works()) {
+ */
 static int __init timer_irq_works(void)
 {
 	unsigned long t1 = jiffies;
@@ -1836,6 +2134,12 @@ static int ioapic_irq_get_chip_state(struct irq_data *irqd, enum irqchip_irq_sta
 
 	guard(raw_spinlock)(&ioapic_lock);
 	for_each_irq_pin(p, mcd->irq_2_pin) {
+		/*
+		 * 在以下使用__ioapic_read_entry():
+		 *   - arch/x86/kernel/apic/io_apic.c|301| <<ioapic_read_entry>> return __ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|446| <<__eoi_ioapic_pin>> entry = entry1 = __ioapic_read_entry(apic, pin);
+		 *   - arch/x86/kernel/apic/io_apic.c|1858| <<ioapic_irq_get_chip_state>> rentry = __ioapic_read_entry(p->apic, p->pin);
+		 */
 		rentry = __ioapic_read_entry(p->apic, p->pin);
 		/*
 		 * The remote IRR is only valid in level trigger mode. It's
@@ -1954,12 +2258,28 @@ static inline void __init unlock_ExtINT_logic(void)
 		WARN_ON_ONCE(1);
 		return;
 	}
+	/*
+	 * 在以下使用find_isa_irq_apic():
+	 *   - arch/x86/kernel/apic/io_apic.c|1490| <<enable_IO_APIC>> i8259_apic = find_isa_irq_apic(0, mp_ExtINT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2155| <<unlock_ExtINT_logic>> apic = find_isa_irq_apic(8, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2295| <<check_timer>> apic1 = find_isa_irq_apic(0, mp_INT);
+	 */
 	apic = find_isa_irq_apic(8, mp_INT);
 	if (apic == -1) {
 		WARN_ON_ONCE(1);
 		return;
 	}
 
+	/*
+	 * 在以下使用ioapic_read_entry():
+	 *   - arch/x86/kernel/apic/io_apic.c|473| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|484| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|506| <<clear_IO_APIC_pin>> entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|568| <<save_ioapic_entries>> ioapics[apic].saved_registers[pin] = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1161| <<io_apic_print_entries>> entry = ioapic_read_entry(apic, i);
+	 *   - arch/x86/kernel/apic/io_apic.c|1296| <<enable_IO_APIC>> struct IO_APIC_route_entry entry = ioapic_read_entry(apic, pin);
+	 *   - arch/x86/kernel/apic/io_apic.c|1982| <<unlock_ExtINT_logic>> entry0 = ioapic_read_entry(apic, pin);
+	 */
 	entry0 = ioapic_read_entry(apic, pin);
 	clear_IO_APIC_pin(apic, pin);
 
@@ -2048,6 +2368,10 @@ static void __init replace_pin_at_irq_node(struct mp_chip_data *data, int node,
  * is so screwy.  Thanks to Brian Perkins for testing/hacking this beast
  * fanatically on his truly buggy board.
  */
+/*
+ * 在以下使用check_timer():
+ *   - arch/x86/kernel/apic/io_apic.c|2313| <<setup_IO_APIC>> check_timer();
+ */
 static inline void __init check_timer(void)
 {
 	struct irq_data *irq_data = irq_get_irq_data(0);
@@ -2080,6 +2404,12 @@ static inline void __init check_timer(void)
 	legacy_pic->init(1);
 
 	pin1  = find_isa_irq_pin(0, mp_INT);
+	/*
+	 * 在以下使用find_isa_irq_apic():
+	 *   - arch/x86/kernel/apic/io_apic.c|1490| <<enable_IO_APIC>> i8259_apic = find_isa_irq_apic(0, mp_ExtINT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2155| <<unlock_ExtINT_logic>> apic = find_isa_irq_apic(8, mp_INT);
+	 *   - arch/x86/kernel/apic/io_apic.c|2295| <<check_timer>> apic1 = find_isa_irq_apic(0, mp_INT);
+	 */
 	apic1 = find_isa_irq_apic(0, mp_INT);
 	pin2  = ioapic_i8259.pin;
 	apic2 = ioapic_i8259.apic;
@@ -2114,6 +2444,13 @@ static inline void __init check_timer(void)
 			 * so only need to unmask if it is level-trigger
 			 * do we really have level trigger timer?
 			 */
+			/*
+			 * 在以下使用find_irq_entry():
+			 *   - arch/x86/kernel/apic/io_apic.c|899| <<__acpi_get_override_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+			 *   - arch/x86/kernel/apic/io_apic.c|1191| <<mp_map_gsi_to_irq>> idx = find_irq_entry(ioapic, pin, mp_INT);
+			 *   - arch/x86/kernel/apic/io_apic.c|1285| <<setup_IO_APIC_irqs>> idx = find_irq_entry(ioapic, pin, mp_INT);
+			 *   - arch/x86/kernel/apic/io_apic.c|2329| <<check_timer>> int idx = find_irq_entry(apic1, pin1, mp_INT);
+			 */
 			int idx = find_irq_entry(apic1, pin1, mp_INT);
 
 			if (idx != -1 && irq_is_level(idx))
@@ -2210,6 +2547,11 @@ static inline void __init check_timer(void)
  */
 #define PIC_IRQS	(1UL << PIC_CASCADE_IR)
 
+/*
+ * 在以下使用mp_irqdomain_create():
+ *   - arch/x86/kernel/apic/io_apic.c|2304| <<setup_IO_APIC>> BUG_ON(mp_irqdomain_create(ioapic));
+ *   - arch/x86/kernel/apic/io_apic.c|2754| <<mp_register_ioapic>> if (mp_irqdomain_create(idx)) {
+ */
 static int mp_irqdomain_create(int ioapic)
 {
 	struct mp_ioapic_gsi *gsi_cfg = mp_ioapic_gsi_routing(ioapic);
@@ -2271,6 +2613,22 @@ static void ioapic_destroy_irqdomain(int idx)
 	}
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 void __init setup_IO_APIC(void)
 {
 	int ioapic;
@@ -2675,6 +3033,14 @@ int mp_register_ioapic(int id, u32 address, u32 gsi_base, struct ioapic_domain_c
 	}
 
 	for_each_ioapic(ioapic) {
+		/*
+		 * 在以下使用mpc_ioapic->apicaddr:
+		 *   - arch/x86/kernel/apic/io_apic.c|130| <<mpc_ioapic_addr>> return ioapics[ioapic_idx].mp_config.apicaddr;
+		 *   - arch/x86/kernel/apic/io_apic.c|2697| <<mp_register_ioapic>> if (ioapics[ioapic].mp_config.apicaddr == address) {
+		 *   - arch/x86/kernel/apic/io_apic.c|2712| <<mp_register_ioapic>> ioapics[idx].mp_config.apicaddr = address;
+		 *   - arch/x86/kernel/mpparse.c|112| <<MP_ioapic_info>> mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
+		 *   - arch/x86/kernel/mpparse.c|359| <<construct_ioapic_table>> ioapic.apicaddr = IO_APIC_DEFAULT_PHYS_BASE;
+		 */
 		if (ioapics[ioapic].mp_config.apicaddr == address) {
 			pr_warn("address 0x%x conflicts with IOAPIC%d\n", address, ioapic);
 			return -EEXIST;
@@ -2690,6 +3056,14 @@ int mp_register_ioapic(int id, u32 address, u32 gsi_base, struct ioapic_domain_c
 
 	ioapics[idx].mp_config.type = MP_IOAPIC;
 	ioapics[idx].mp_config.flags = MPC_APIC_USABLE;
+	/*
+	 * 在以下使用mpc_ioapic->apicaddr:
+	 *   - arch/x86/kernel/apic/io_apic.c|130| <<mpc_ioapic_addr>> return ioapics[ioapic_idx].mp_config.apicaddr;
+	 *   - arch/x86/kernel/apic/io_apic.c|2697| <<mp_register_ioapic>> if (ioapics[ioapic].mp_config.apicaddr == address) {
+	 *   - arch/x86/kernel/apic/io_apic.c|2712| <<mp_register_ioapic>> ioapics[idx].mp_config.apicaddr = address;
+	 *   - arch/x86/kernel/mpparse.c|112| <<MP_ioapic_info>> mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
+	 *   - arch/x86/kernel/mpparse.c|359| <<construct_ioapic_table>> ioapic.apicaddr = IO_APIC_DEFAULT_PHYS_BASE;
+	 */
 	ioapics[idx].mp_config.apicaddr = address;
 
 	io_apic_set_fixmap(FIX_IO_APIC_BASE_0 + idx, address);
@@ -2833,6 +3207,10 @@ static void mp_irqdomain_get_attr(u32 gsi, struct mp_chip_data *data,
  * RTE. This happens later during activation which will fill in the actual
  * routing information.
  */
+/*
+ * 在以下使用mp_preconfigure_entry():
+ *   - arch/x86/kernel/apic/io_apic.c|2902| <<mp_irqdomain_alloc>> mp_preconfigure_entry(data);
+ */
 static void mp_preconfigure_entry(struct mp_chip_data *data)
 {
 	struct IO_APIC_route_entry *entry = &data->entry;
@@ -2847,6 +3225,22 @@ static void mp_preconfigure_entry(struct mp_chip_data *data)
 	entry->masked		= data->is_level;
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 int mp_irqdomain_alloc(struct irq_domain *domain, unsigned int virq,
 		       unsigned int nr_irqs, void *arg)
 {
@@ -2923,6 +3317,11 @@ void mp_irqdomain_free(struct irq_domain *domain, unsigned int virq,
 	irq_domain_free_irqs_top(domain, virq, nr_irqs);
 }
 
+/*
+ * 在以下使用mp_irqdomain_activate():
+ *   - arch/x86/kernel/apic/io_apic.c|2967| <<global>> .activate = mp_irqdomain_activate,
+ *   - arch/x86/kernel/devicetree.c|235| <<global>> .activate = mp_irqdomain_activate,
+ */
 int mp_irqdomain_activate(struct irq_domain *domain, struct irq_data *irq_data, bool reserve)
 {
 	guard(raw_spinlock_irqsave)(&ioapic_lock);
@@ -2942,6 +3341,13 @@ int mp_irqdomain_ioapic_idx(struct irq_domain *domain)
 	return (int)(long)domain->host_data;
 }
 
+/*
+ * 在以下使用mp_ioapic_irqdomain_ops:
+ *   - arch/x86/kernel/acpi/boot.c|468| <<acpi_parse_ioapic>> .ops = &mp_ioapic_irqdomain_ops,
+ *   - arch/x86/kernel/acpi/boot.c|816| <<acpi_register_ioapic>> .ops = &mp_ioapic_irqdomain_ops,
+ *   - arch/x86/kernel/jailhouse.c|97| <<jailhouse_parse_smp_config>> .ops = &mp_ioapic_irqdomain_ops,
+ *   - arch/x86/kernel/mpparse.c|108| <<MP_ioapic_info>> .ops = &mp_ioapic_irqdomain_ops,
+ */
 const struct irq_domain_ops mp_ioapic_irqdomain_ops = {
 	.alloc		= mp_irqdomain_alloc,
 	.free		= mp_irqdomain_free,
diff --git a/arch/x86/kernel/apic/x2apic_cluster.c b/arch/x86/kernel/apic/x2apic_cluster.c
index 7db83212e..642eaa61a 100644
--- a/arch/x86/kernel/apic/x2apic_cluster.c
+++ b/arch/x86/kernel/apic/x2apic_cluster.c
@@ -16,6 +16,17 @@
  * x86_cpu_to_logical_apicid for all online cpus in a sequential way.
  * Using per cpu variable would cost one cache line per cpu.
  */
+/*
+ * 在以下使用x86_cpu_to_logical_apicid:
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|31| <<x2apic_send_IPI>> u32 dest = x86_cpu_to_logical_apicid[cpu];
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|62| <<__x2apic_send_IPI_mask>> dest |= x86_cpu_to_logical_apicid[clustercpu];
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|88| <<x2apic_calc_apicid>> return x86_cpu_to_logical_apicid[cpu];
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|183| <<x2apic_prepare_cpu>> x86_cpu_to_logical_apicid[cpu] = logical_apicid;
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|212| <<x2apic_cluster_probe>> x86_cpu_to_logical_apicid = kcalloc(slots, sizeof(u32), GFP_KERNEL);
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|213| <<x2apic_cluster_probe>> if (!x86_cpu_to_logical_apicid)
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|219| <<x2apic_cluster_probe>> kfree(x86_cpu_to_logical_apicid);
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|220| <<x2apic_cluster_probe>> x86_cpu_to_logical_apicid = NULL;
+ */
 static u32 *x86_cpu_to_logical_apicid __read_mostly;
 
 static DEFINE_PER_CPU(cpumask_var_t, ipi_mask);
@@ -32,6 +43,14 @@ static void x2apic_send_IPI(int cpu, int vector)
 
 	/* x2apic MSRs are special and need a special fence: */
 	weak_wrmsr_fence();
+	/*
+	 * 在以下使用__x2apic_send_IPI_dest():
+	 *   - arch/x86/kernel/apic/x2apic_cluster.c|35| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+	 *   - arch/x86/kernel/apic/x2apic_cluster.c|67| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+	 *   - arch/x86/kernel/apic/x2apic_phys.c|50| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
+	 *   - arch/x86/kernel/apic/x2apic_phys.c|69| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
+	 *                          vector, APIC_DEST_PHYSICAL);
+	 */
 	__x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
 }
 
@@ -64,6 +83,14 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 		if (!dest)
 			continue;
 
+		/*
+		 * 在以下使用__x2apic_send_IPI_dest():
+		 *   - arch/x86/kernel/apic/x2apic_cluster.c|35| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+		 *   - arch/x86/kernel/apic/x2apic_cluster.c|67| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+		 *   - arch/x86/kernel/apic/x2apic_phys.c|50| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
+		 *   - arch/x86/kernel/apic/x2apic_phys.c|69| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
+		 *                          vector, APIC_DEST_PHYSICAL);
+		 */
 		__x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
 		/* Remove cluster CPUs from tmpmask */
 		cpumask_andnot(tmpmsk, tmpmsk, cmsk);
diff --git a/arch/x86/kernel/apic/x2apic_phys.c b/arch/x86/kernel/apic/x2apic_phys.c
index 12d4c3554..1816ea93d 100644
--- a/arch/x86/kernel/apic/x2apic_phys.c
+++ b/arch/x86/kernel/apic/x2apic_phys.c
@@ -47,6 +47,14 @@ static void x2apic_send_IPI(int cpu, int vector)
 
 	/* x2apic MSRs are special and need a special fence: */
 	weak_wrmsr_fence();
+	/*
+	 * 在以下使用__x2apic_send_IPI_dest():
+	 *   - arch/x86/kernel/apic/x2apic_cluster.c|35| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+	 *   - arch/x86/kernel/apic/x2apic_cluster.c|67| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+	 *   - arch/x86/kernel/apic/x2apic_phys.c|50| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
+	 *   - arch/x86/kernel/apic/x2apic_phys.c|69| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
+	 *                          vector, APIC_DEST_PHYSICAL);
+	 */
 	__x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
 }
 
@@ -66,6 +74,14 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 	for_each_cpu(query_cpu, mask) {
 		if (apic_dest == APIC_DEST_ALLBUT && this_cpu == query_cpu)
 			continue;
+		/*
+		 * 在以下使用__x2apic_send_IPI_dest():
+		 *   - arch/x86/kernel/apic/x2apic_cluster.c|35| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+		 *   - arch/x86/kernel/apic/x2apic_cluster.c|67| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+		 *   - arch/x86/kernel/apic/x2apic_phys.c|50| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
+		 *   - arch/x86/kernel/apic/x2apic_phys.c|69| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
+		 *                          vector, APIC_DEST_PHYSICAL);
+		 */
 		__x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
 				       vector, APIC_DEST_PHYSICAL);
 	}
@@ -107,6 +123,14 @@ void x2apic_send_IPI_self(int vector)
 	apic_write(APIC_SELF_IPI, vector);
 }
 
+/*
+ * 在以下使用__x2apic_send_IPI_dest():
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|35| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+ *   - arch/x86/kernel/apic/x2apic_cluster.c|67| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_LOGICAL);
+ *   - arch/x86/kernel/apic/x2apic_phys.c|50| <<x2apic_send_IPI>> __x2apic_send_IPI_dest(dest, vector, APIC_DEST_PHYSICAL);
+ *   - arch/x86/kernel/apic/x2apic_phys.c|69| <<__x2apic_send_IPI_mask>> __x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
+ *                          vector, APIC_DEST_PHYSICAL);
+ */
 void __x2apic_send_IPI_dest(unsigned int apicid, int vector, unsigned int dest)
 {
 	unsigned long cfg = __prepare_ICR(0, vector, dest);
diff --git a/arch/x86/kernel/crash.c b/arch/x86/kernel/crash.c
index 335fd2ee9..cad7d4c24 100644
--- a/arch/x86/kernel/crash.c
+++ b/arch/x86/kernel/crash.c
@@ -121,6 +121,14 @@ void native_machine_crash_shutdown(struct pt_regs *regs)
 #ifdef CONFIG_X86_IO_APIC
 	/* Prevent crash_kexec() from deadlocking on ioapic_lock. */
 	ioapic_zap_locks();
+	/*
+	 * 在以下使用clear_IO_APIC():
+	 *   - arch/x86/kernel/apic/io_apic.c|1503| <<enable_IO_APIC>> clear_IO_APIC();
+	 *   - arch/x86/kernel/crash.c|124| <<native_machine_crash_shutdown>> clear_IO_APIC();
+	 *   - arch/x86/kernel/machine_kexec_32.c|187| <<machine_kexec>> clear_IO_APIC();
+	 *   - arch/x86/kernel/machine_kexec_64.c|428| <<machine_kexec>> clear_IO_APIC();
+	 *   - arch/x86/kernel/reboot.c|741| <<native_machine_shutdown>> clear_IO_APIC();
+	 */
 	clear_IO_APIC();
 #endif
 	lapic_shutdown();
diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c
index b67d7c59d..e2c3a4b03 100644
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@ -46,8 +46,23 @@
 #include <asm/svm.h>
 #include <asm/e820/api.h>
 
+/*
+ * 在以下使用kvm_async_pf_enabled:
+ *   - arch/x86/include/asm/kvm_para.h|130| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
+ *   - arch/x86/kernel/kvm.c|49| <<global>> DEFINE_STATIC_KEY_FALSE_RO(kvm_async_pf_enabled);
+ *   - arch/x86/include/asm/kvm_para.h|134| <<kvm_handle_async_pf>> if (static_branch_unlikely(&kvm_async_pf_enabled))
+ *   - arch/x86/kernel/kvm.c|356| <<kvm_guest_cpu_init>> WARN_ON_ONCE(!static_branch_likely(&kvm_async_pf_enabled));
+ *   - arch/x86/kernel/kvm.c|834| <<kvm_guest_init>> static_branch_enable(&kvm_async_pf_enabled);
+ */
 DEFINE_STATIC_KEY_FALSE_RO(kvm_async_pf_enabled);
 
+/*
+ * 在以下使用kvmapf:
+ *   - arch/x86/kernel/kvm.c|51| <<global>> static int kvmapf = 1;
+ *   - arch/x86/kernel/kvm.c|55| <<parse_no_kvmapf>> kvmapf = 0;
+ *   - arch/x86/kernel/kvm.c|353| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+ *   - arch/x86/kernel/kvm.c|833| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+ */
 static int kvmapf = 1;
 
 static int __init parse_no_kvmapf(char *arg)
@@ -141,6 +156,11 @@ static bool kvm_async_pf_queue_task(u32 token, struct kvm_task_sleep_node *n)
  * Invoked from the async pagefault handling code or from the VM exit page
  * fault handler. In both cases RCU is watching.
  */
+/*
+ * 在以下使用kvm_async_pf_task_wait_schedule():
+ *   - arch/x86/kernel/kvm.c|281| <<__kvm_handle_async_pf>> kvm_async_pf_task_wait_schedule(token);
+ *   - arch/x86/kvm/mmu/mmu.c|4894| <<kvm_handle_page_fault>> kvm_async_pf_task_wait_schedule(fault_address);
+ */
 void kvm_async_pf_task_wait_schedule(u32 token)
 {
 	struct kvm_task_sleep_node n;
@@ -350,9 +370,24 @@ static notrace __maybe_unused void kvm_guest_apic_eoi_write(void)
 
 static void kvm_guest_cpu_init(void)
 {
+	/*
+	 * 在以下使用kvmapf:
+	 *   - arch/x86/kernel/kvm.c|51| <<global>> static int kvmapf = 1;
+	 *   - arch/x86/kernel/kvm.c|55| <<parse_no_kvmapf>> kvmapf = 0;
+	 *   - arch/x86/kernel/kvm.c|353| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+	 *   - arch/x86/kernel/kvm.c|833| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
 		u64 pa;
 
+		/*
+		 * 在以下使用kvm_async_pf_enabled:
+		 *   - arch/x86/include/asm/kvm_para.h|130| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
+		 *   - arch/x86/kernel/kvm.c|49| <<global>> DEFINE_STATIC_KEY_FALSE_RO(kvm_async_pf_enabled);
+		 *   - arch/x86/include/asm/kvm_para.h|134| <<kvm_handle_async_pf>> if (static_branch_unlikely(&kvm_async_pf_enabled))
+		 *   - arch/x86/kernel/kvm.c|356| <<kvm_guest_cpu_init>> WARN_ON_ONCE(!static_branch_likely(&kvm_async_pf_enabled));
+		 *   - arch/x86/kernel/kvm.c|834| <<kvm_guest_init>> static_branch_enable(&kvm_async_pf_enabled);
+		 */
 		WARN_ON_ONCE(!static_branch_likely(&kvm_async_pf_enabled));
 
 		pa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));
@@ -830,7 +865,22 @@ static void __init kvm_guest_init(void)
 	if (kvm_para_has_feature(KVM_FEATURE_PV_EOI))
 		apic_update_callback(eoi, kvm_guest_apic_eoi_write);
 
+	/*
+	 * 在以下使用kvmapf:
+	 *   - arch/x86/kernel/kvm.c|51| <<global>> static int kvmapf = 1;
+	 *   - arch/x86/kernel/kvm.c|55| <<parse_no_kvmapf>> kvmapf = 0;
+	 *   - arch/x86/kernel/kvm.c|353| <<kvm_guest_cpu_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+	 *   - arch/x86/kernel/kvm.c|833| <<kvm_guest_init>> if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {
+		/*
+		 * 在以下使用kvm_async_pf_enabled:
+		 *   - arch/x86/include/asm/kvm_para.h|130| <<global>> DECLARE_STATIC_KEY_FALSE(kvm_async_pf_enabled);
+		 *   - arch/x86/kernel/kvm.c|49| <<global>> DEFINE_STATIC_KEY_FALSE_RO(kvm_async_pf_enabled);
+		 *   - arch/x86/include/asm/kvm_para.h|134| <<kvm_handle_async_pf>> if (static_branch_unlikely(&kvm_async_pf_enabled))
+		 *   - arch/x86/kernel/kvm.c|356| <<kvm_guest_cpu_init>> WARN_ON_ONCE(!static_branch_likely(&kvm_async_pf_enabled));
+		 *   - arch/x86/kernel/kvm.c|834| <<kvm_guest_init>> static_branch_enable(&kvm_async_pf_enabled);
+		 */
 		static_branch_enable(&kvm_async_pf_enabled);
 		sysvec_install(HYPERVISOR_CALLBACK_VECTOR, sysvec_kvm_asyncpf_interrupt);
 	}
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index ca0a49eea..3fd4cb6b0 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -53,6 +53,15 @@ static struct pvclock_vsyscall_time_info *hvclock_mem;
 DEFINE_PER_CPU(struct pvclock_vsyscall_time_info *, hv_clock_per_cpu);
 EXPORT_PER_CPU_SYMBOL_GPL(hv_clock_per_cpu);
 
+/*
+ * [0] kvm_get_wallclock
+ * [0] read_persistent_wall_and_boot_offset
+ * [0] timekeeping_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
  * have elapsed since the hypervisor wrote the data. So we try to account for
diff --git a/arch/x86/kernel/machine_kexec_32.c b/arch/x86/kernel/machine_kexec_32.c
index 1f325304c..6ae475cab 100644
--- a/arch/x86/kernel/machine_kexec_32.c
+++ b/arch/x86/kernel/machine_kexec_32.c
@@ -184,6 +184,14 @@ void machine_kexec(struct kimage *image)
 		 * paths already have calls to restore_boot_irq_mode()
 		 * in one form or other. kexec jump path also need one.
 		 */
+		/*
+		 * 在以下使用clear_IO_APIC():
+		 *   - arch/x86/kernel/apic/io_apic.c|1503| <<enable_IO_APIC>> clear_IO_APIC();
+		 *   - arch/x86/kernel/crash.c|124| <<native_machine_crash_shutdown>> clear_IO_APIC();
+		 *   - arch/x86/kernel/machine_kexec_32.c|187| <<machine_kexec>> clear_IO_APIC();
+		 *   - arch/x86/kernel/machine_kexec_64.c|428| <<machine_kexec>> clear_IO_APIC();
+		 *   - arch/x86/kernel/reboot.c|741| <<native_machine_shutdown>> clear_IO_APIC();
+		 */
 		clear_IO_APIC();
 		restore_boot_irq_mode();
 #endif
diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c
index 201137b98..4a4086189 100644
--- a/arch/x86/kernel/machine_kexec_64.c
+++ b/arch/x86/kernel/machine_kexec_64.c
@@ -425,6 +425,14 @@ void __nocfi machine_kexec(struct kimage *image)
 		 * paths already have calls to restore_boot_irq_mode()
 		 * in one form or other. kexec jump path also need one.
 		 */
+		/*
+		 * 在以下使用clear_IO_APIC():
+		 *   - arch/x86/kernel/apic/io_apic.c|1503| <<enable_IO_APIC>> clear_IO_APIC();
+		 *   - arch/x86/kernel/crash.c|124| <<native_machine_crash_shutdown>> clear_IO_APIC();
+		 *   - arch/x86/kernel/machine_kexec_32.c|187| <<machine_kexec>> clear_IO_APIC();
+		 *   - arch/x86/kernel/machine_kexec_64.c|428| <<machine_kexec>> clear_IO_APIC();
+		 *   - arch/x86/kernel/reboot.c|741| <<native_machine_shutdown>> clear_IO_APIC();
+		 */
 		clear_IO_APIC();
 		restore_boot_irq_mode();
 #endif
diff --git a/arch/x86/kernel/mpparse.c b/arch/x86/kernel/mpparse.c
index 4a1b1b28a..9e0638cc2 100644
--- a/arch/x86/kernel/mpparse.c
+++ b/arch/x86/kernel/mpparse.c
@@ -108,6 +108,14 @@ static void __init MP_ioapic_info(struct mpc_ioapic *m)
 		.ops = &mp_ioapic_irqdomain_ops,
 	};
 
+	/*
+	 * 在以下使用mpc_ioapic->apicaddr:
+	 *   - arch/x86/kernel/apic/io_apic.c|130| <<mpc_ioapic_addr>> return ioapics[ioapic_idx].mp_config.apicaddr;
+	 *   - arch/x86/kernel/apic/io_apic.c|2697| <<mp_register_ioapic>> if (ioapics[ioapic].mp_config.apicaddr == address) {
+	 *   - arch/x86/kernel/apic/io_apic.c|2712| <<mp_register_ioapic>> ioapics[idx].mp_config.apicaddr = address;
+	 *   - arch/x86/kernel/mpparse.c|112| <<MP_ioapic_info>> mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
+	 *   - arch/x86/kernel/mpparse.c|359| <<construct_ioapic_table>> ioapic.apicaddr = IO_APIC_DEFAULT_PHYS_BASE;
+	 */
 	if (m->flags & MPC_APIC_USABLE)
 		mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
 }
@@ -356,6 +364,14 @@ static void __init construct_ioapic_table(int mpc_default_type)
 	ioapic.apicid	= 2;
 	ioapic.apicver	= mpc_default_type > 4 ? 0x10 : 0x01;
 	ioapic.flags	= MPC_APIC_USABLE;
+	/*
+	 * 在以下使用mpc_ioapic->apicaddr:
+	 *   - arch/x86/kernel/apic/io_apic.c|130| <<mpc_ioapic_addr>> return ioapics[ioapic_idx].mp_config.apicaddr;
+	 *   - arch/x86/kernel/apic/io_apic.c|2697| <<mp_register_ioapic>> if (ioapics[ioapic].mp_config.apicaddr == address) {
+	 *   - arch/x86/kernel/apic/io_apic.c|2712| <<mp_register_ioapic>> ioapics[idx].mp_config.apicaddr = address;
+	 *   - arch/x86/kernel/mpparse.c|112| <<MP_ioapic_info>> mp_register_ioapic(m->apicid, m->apicaddr, gsi_top, &cfg);
+	 *   - arch/x86/kernel/mpparse.c|359| <<construct_ioapic_table>> ioapic.apicaddr = IO_APIC_DEFAULT_PHYS_BASE;
+	 */
 	ioapic.apicaddr	= IO_APIC_DEFAULT_PHYS_BASE;
 	MP_ioapic_info(&ioapic);
 
diff --git a/arch/x86/kernel/nmi.c b/arch/x86/kernel/nmi.c
index be93ec725..141786c34 100644
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@ -127,6 +127,13 @@ static void nmi_check_duration(struct nmiaction *action, u64 duration)
 			    action->handler, duration, decimal_msecs);
 }
 
+/*
+ * 在以下使用nmi_handle():
+ *   - arch/x86/kernel/nmi.c|267| <<pci_serr_error>> if (nmi_handle(NMI_SERR, regs))
+ *   - arch/x86/kernel/nmi.c|290| <<io_check_error>> if (nmi_handle(NMI_IO_CHECK, regs))
+ *   - arch/x86/kernel/nmi.c|334| <<unknown_nmi_error>> handled = nmi_handle(NMI_UNKNOWN, regs);
+ *   - arch/x86/kernel/nmi.c|392| <<default_do_nmi>> handled = nmi_handle(NMI_LOCAL, regs);
+ */
 static int nmi_handle(unsigned int type, struct pt_regs *regs)
 {
 	struct nmi_desc *desc = nmi_to_desc(type);
@@ -321,6 +328,10 @@ io_check_error(unsigned char reason, struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(io_check_error);
 
+/*
+ * 在以下使用unknown_nmi_error():
+ *   - arch/x86/kernel/nmi.c|475| <<default_do_nmi>> unknown_nmi_error(reason, regs);
+ */
 static void
 unknown_nmi_error(unsigned char reason, struct pt_regs *regs)
 {
diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c
index 6267363e0..cbdbe7bd4 100644
--- a/arch/x86/kernel/pci-dma.c
+++ b/arch/x86/kernel/pci-dma.c
@@ -33,7 +33,22 @@ int force_iommu __read_mostly = 0;
 
 int iommu_merge __read_mostly = 0;
 
+/*
+ * 在以下设置no_iommu:
+ *   - arch/x86/kernel/pci-dma.c|120| <<iommu_setup>> if (!strncmp(p, "off", 3)) no_iommu = 1;
+ *   - drivers/iommu/intel/iommu.c|2973| <<platform_optin_force_iommu>> no_iommu = 0;
+ *   - drivers/iommu/intel/iommu.c|3024| <<tboot_force_iommu>> no_iommu = 0;
+ */
 int no_iommu __read_mostly;
+/*
+ * 在以下使用iommu_detected:
+ *   - arch/x86/kernel/pci-dma.c|38| <<global>> int iommu_detected __read_mostly = 0;
+ *   - arch/x86/kernel/aperture_64.c|429| <<gart_iommu_hole_init>> iommu_detected = 1;
+ *   - drivers/iommu/amd/init.c|3650| <<amd_iommu_detect>> if (no_iommu || (iommu_detected && !gart_iommu_aperture))
+ *   - drivers/iommu/amd/init.c|3661| <<amd_iommu_detect>> iommu_detected = 1;
+ *   - drivers/iommu/intel/dmar.c|931| <<detect_intel_iommu>> if (!ret && !no_iommu && !iommu_detected &&
+ *   - drivers/iommu/intel/dmar.c|933| <<detect_intel_iommu>> iommu_detected = 1;
+ */
 /* Set this to 1 if there is a HW IOMMU in the system */
 int iommu_detected __read_mostly = 0;
 
diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c
index 964f6b0a3..f50e11af2 100644
--- a/arch/x86/kernel/reboot.c
+++ b/arch/x86/kernel/reboot.c
@@ -738,6 +738,14 @@ void native_machine_shutdown(void)
 	 * Even without the erratum, it still makes sense to quiet IO APIC
 	 * before disabling Local APIC.
 	 */
+	/*
+	 * 在以下使用clear_IO_APIC():
+	 *   - arch/x86/kernel/apic/io_apic.c|1503| <<enable_IO_APIC>> clear_IO_APIC();
+	 *   - arch/x86/kernel/crash.c|124| <<native_machine_crash_shutdown>> clear_IO_APIC();
+	 *   - arch/x86/kernel/machine_kexec_32.c|187| <<machine_kexec>> clear_IO_APIC();
+	 *   - arch/x86/kernel/machine_kexec_64.c|428| <<machine_kexec>> clear_IO_APIC();
+	 *   - arch/x86/kernel/reboot.c|741| <<native_machine_shutdown>> clear_IO_APIC();
+	 */
 	clear_IO_APIC();
 #endif
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index 52524e0ca..cbf142b67 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -377,6 +377,18 @@ void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 	bool allow_gbpages;
 	int i;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpu_caps[NR_KVM_CPU_CAPS]:
+	 *   - arch/x86/kvm/cpuid.c|380| <<kvm_vcpu_after_set_cpuid>> memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|406| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] = kvm_cpu_caps[i] |
+	 *   - arch/x86/kvm/cpuid.c|408| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] &= cpuid_get_reg_unsafe(entry, cpuid.reg);
+	 *   - arch/x86/kvm/cpuid.c|526| <<kvm_set_cpuid>> memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|527| <<kvm_set_cpuid>> BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|569| <<kvm_set_cpuid>> memcpy(vcpu->arch.cpu_caps, vcpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.h|233| <<guest_cpu_cap_set>> vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|241| <<guest_cpu_cap_clear>> vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|267| <<guest_cpu_cap_has>> return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+	 */
 	memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
 	BUILD_BUG_ON(ARRAY_SIZE(reverse_cpuid) != NR_KVM_CPU_CAPS);
 
@@ -523,6 +535,18 @@ static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
 	swap(vcpu->arch.cpuid_entries, e2);
 	swap(vcpu->arch.cpuid_nent, nent);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpu_caps[NR_KVM_CPU_CAPS]:
+	 *   - arch/x86/kvm/cpuid.c|380| <<kvm_vcpu_after_set_cpuid>> memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|406| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] = kvm_cpu_caps[i] |
+	 *   - arch/x86/kvm/cpuid.c|408| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] &= cpuid_get_reg_unsafe(entry, cpuid.reg);
+	 *   - arch/x86/kvm/cpuid.c|526| <<kvm_set_cpuid>> memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|527| <<kvm_set_cpuid>> BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|569| <<kvm_set_cpuid>> memcpy(vcpu->arch.cpu_caps, vcpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.h|233| <<guest_cpu_cap_set>> vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|241| <<guest_cpu_cap_clear>> vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|267| <<guest_cpu_cap_has>> return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+	 */
 	memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
 	BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
 
@@ -2049,6 +2073,11 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 				*edx &= ~feature_bit(CONSTANT_TSC);
 		} else if (IS_ENABLED(CONFIG_KVM_XEN) &&
 			   kvm_xen_is_tsc_leaf(vcpu, function)) {
+			/*
+			 * 在以下使用kvm_guest_time_update():
+			 *   - arch/x86/kvm/cpuid.c|2058| <<kvm_cpuid>> kvm_guest_time_update(vcpu);
+			 *   - arch/x86/kvm/x86.c|11448| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+			 */
 			/*
 			 * Update guest TSC frequency information if necessary.
 			 * Ignore failures, there is no sane value that can be
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index d3f5ae15a..251c6823c 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -230,6 +230,18 @@ static __always_inline void guest_cpu_cap_set(struct kvm_vcpu *vcpu,
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpu_caps[NR_KVM_CPU_CAPS]:
+	 *   - arch/x86/kvm/cpuid.c|380| <<kvm_vcpu_after_set_cpuid>> memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|406| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] = kvm_cpu_caps[i] |
+	 *   - arch/x86/kvm/cpuid.c|408| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] &= cpuid_get_reg_unsafe(entry, cpuid.reg);
+	 *   - arch/x86/kvm/cpuid.c|526| <<kvm_set_cpuid>> memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|527| <<kvm_set_cpuid>> BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|569| <<kvm_set_cpuid>> memcpy(vcpu->arch.cpu_caps, vcpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.h|233| <<guest_cpu_cap_set>> vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|241| <<guest_cpu_cap_clear>> vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|267| <<guest_cpu_cap_has>> return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+	 */
 	vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
 }
 
@@ -238,6 +250,18 @@ static __always_inline void guest_cpu_cap_clear(struct kvm_vcpu *vcpu,
 {
 	unsigned int x86_leaf = __feature_leaf(x86_feature);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpu_caps[NR_KVM_CPU_CAPS]:
+	 *   - arch/x86/kvm/cpuid.c|380| <<kvm_vcpu_after_set_cpuid>> memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|406| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] = kvm_cpu_caps[i] |
+	 *   - arch/x86/kvm/cpuid.c|408| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] &= cpuid_get_reg_unsafe(entry, cpuid.reg);
+	 *   - arch/x86/kvm/cpuid.c|526| <<kvm_set_cpuid>> memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|527| <<kvm_set_cpuid>> BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|569| <<kvm_set_cpuid>> memcpy(vcpu->arch.cpu_caps, vcpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.h|233| <<guest_cpu_cap_set>> vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|241| <<guest_cpu_cap_clear>> vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|267| <<guest_cpu_cap_has>> return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+	 */
 	vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
 }
 
@@ -251,6 +275,9 @@ static __always_inline void guest_cpu_cap_change(struct kvm_vcpu *vcpu,
 		guest_cpu_cap_clear(vcpu, x86_feature);
 }
 
+/*
+ * 很多的调用
+ */
 static __always_inline bool guest_cpu_cap_has(struct kvm_vcpu *vcpu,
 					      unsigned int x86_feature)
 {
@@ -264,6 +291,18 @@ static __always_inline bool guest_cpu_cap_has(struct kvm_vcpu *vcpu,
 		     x86_feature == X86_FEATURE_OSXSAVE ||
 		     x86_feature == X86_FEATURE_OSPKE);
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->cpu_caps[NR_KVM_CPU_CAPS]:
+	 *   - arch/x86/kvm/cpuid.c|380| <<kvm_vcpu_after_set_cpuid>> memset(vcpu->arch.cpu_caps, 0, sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|406| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] = kvm_cpu_caps[i] |
+	 *   - arch/x86/kvm/cpuid.c|408| <<kvm_vcpu_after_set_cpuid>> vcpu->arch.cpu_caps[i] &= cpuid_get_reg_unsafe(entry, cpuid.reg);
+	 *   - arch/x86/kvm/cpuid.c|526| <<kvm_set_cpuid>> memcpy(vcpu_caps, vcpu->arch.cpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|527| <<kvm_set_cpuid>> BUILD_BUG_ON(sizeof(vcpu_caps) != sizeof(vcpu->arch.cpu_caps));
+	 *   - arch/x86/kvm/cpuid.c|569| <<kvm_set_cpuid>> memcpy(vcpu->arch.cpu_caps, vcpu_caps, sizeof(vcpu_caps));
+	 *   - arch/x86/kvm/cpuid.h|233| <<guest_cpu_cap_set>> vcpu->arch.cpu_caps[x86_leaf] |= __feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|241| <<guest_cpu_cap_clear>> vcpu->arch.cpu_caps[x86_leaf] &= ~__feature_bit(x86_feature);
+	 *   - arch/x86/kvm/cpuid.h|267| <<guest_cpu_cap_has>> return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
+	 */
 	return vcpu->arch.cpu_caps[x86_leaf] & __feature_bit(x86_feature);
 }
 
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 38595ecb9..147fa4f9a 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -579,6 +579,20 @@ static u64 get_time_ref_counter(struct kvm *kvm)
 	 * Fall back to get_kvmclock_ns() when TSC page hasn't been set up,
 	 * is broken, disabled or being updated.
 	 */
+	/*
+	 * 在以下使用get_kvmclock_ns():
+	 *   - arch/x86/kvm/hyperv.c|583| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+	 *   - arch/x86/kvm/x86.c|3753| <<kvm_get_wall_clock_epoch>> return ktime_get_real_ns() - get_kvmclock_ns(kvm);
+	 *   - arch/x86/kvm/xen.c|284| <<kvm_xen_start_timer>> guest_now = get_kvmclock_ns(vcpu->kvm);
+	 *   - arch/x86/kvm/xen.c|602| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+	 *   - arch/x86/kvm/xen.c|1051| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 *   - arch/x86/kvm/xen.c|1092| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 *
+	 * 调用get_kvmclock()
+	 * 用struct kvm_clock_data返回当前VM启动了的时间
+	 * 包括guest_time, tsc, 和realtime
+	 * 这个函数get_kvmclock_ns()只有一个guest time的ns
+	 */
 	if (hv->hv_tsc_page_status != HV_TSC_PAGE_SET)
 		return div_u64(get_kvmclock_ns(kvm), 100);
 
@@ -1234,6 +1248,11 @@ void kvm_hv_setup_tsc_page(struct kvm *kvm,
 	hv->hv_tsc_page_status = HV_TSC_PAGE_BROKEN;
 }
 
+/*
+ * 在以下使用kvm_hv_request_tsc_page_update():
+ *   -  arch/x86/kvm/x86.c|3178| <<kvm_update_masterclock>> kvm_hv_request_tsc_page_update(kvm);
+ *   - arch/x86/kvm/x86.c|7226| <<kvm_vm_ioctl_set_clock>> kvm_hv_request_tsc_page_update(kvm); 
+ */
 void kvm_hv_request_tsc_page_update(struct kvm *kvm)
 {
 	struct kvm_hv *hv = to_kvm_hv(kvm);
@@ -1437,6 +1456,18 @@ static int kvm_hv_set_msr_pw(struct kvm_vcpu *vcpu, u32 msr, u64 data,
 				hv->hv_tsc_page_status = HV_TSC_PAGE_GUEST_CHANGED;
 			else
 				hv->hv_tsc_page_status = HV_TSC_PAGE_HOST_CHANGED;
+			/*
+			 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE():
+			 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+			 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+			 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+			 *
+			 * 处理的函数kvm_update_masterclock()
+			 */
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 		} else {
 			hv->hv_tsc_page_status = HV_TSC_PAGE_UNSET;
@@ -2156,6 +2187,13 @@ static u64 kvm_hv_flush_tlb(struct kvm_vcpu *vcpu, struct kvm_hv_hcall *hc)
 					     tlb_flush_entries, hc->rep_cnt);
 		}
 
+		/*
+		 * 在以下使用kvm_make_vcpus_request_mask():
+		 *   - arch/riscv/kvm/tlb.c|331| <<make_xfence_request>> kvm_make_vcpus_request_mask(kvm, actual_req, vcpu_mask);
+		 *   - arch/x86/kvm/hyperv.c|2176| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+		 *   - arch/x86/kvm/hyperv.c|2209| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+		 *   - arch/x86/kvm/x86.c|11028| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
+		 */
 		kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
 	} else {
 		struct kvm_vcpu_hv *hv_v;
@@ -2189,6 +2227,13 @@ static u64 kvm_hv_flush_tlb(struct kvm_vcpu *vcpu, struct kvm_hv_hcall *hc)
 					     tlb_flush_entries, hc->rep_cnt);
 		}
 
+		/*
+		 * 在以下使用kvm_make_vcpus_request_mask():
+		 *   - arch/riscv/kvm/tlb.c|331| <<make_xfence_request>> kvm_make_vcpus_request_mask(kvm, actual_req, vcpu_mask);
+		 *   - arch/x86/kvm/hyperv.c|2176| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+		 *   - arch/x86/kvm/hyperv.c|2209| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+		 *   - arch/x86/kvm/x86.c|11028| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
+		 */
 		kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
 	}
 
diff --git a/arch/x86/kvm/ioapic.c b/arch/x86/kvm/ioapic.c
index 2c2783296..8a9ba856e 100644
--- a/arch/x86/kvm/ioapic.c
+++ b/arch/x86/kvm/ioapic.c
@@ -325,6 +325,13 @@ void kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,
 	int idx, gsi;
 
 	idx = srcu_read_lock(&kvm->irq_srcu);
+	/*
+	 * 在以下使用kvm_irq_map_chip_pin():
+	 *   - arch/x86/kvm/ioapic.c|328| <<kvm_fire_mask_notifiers>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|571| <<kvm_irq_has_notifier>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|603| <<kvm_notify_acked_irq>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|746| <<kvm_notify_irqfd_resampler>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 */
 	gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
 	if (gsi != -1)
 		hlist_for_each_entry_rcu(kimn, &ioapic->mask_notifier_list, link)
diff --git a/arch/x86/kvm/irq.c b/arch/x86/kvm/irq.c
index 7cc895000..1bf1243d7 100644
--- a/arch/x86/kvm/irq.c
+++ b/arch/x86/kvm/irq.c
@@ -293,6 +293,10 @@ bool kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return irqchip_in_kernel(kvm);
 }
 
+/*
+ * 在以下使用kvm_set_routing_entry():
+ *   - virt/kvm/irqchip.c|148| <<setup_routing_entry>> r = kvm_set_routing_entry(kvm, e, ue);
+ */
 int kvm_set_routing_entry(struct kvm *kvm,
 			  struct kvm_kernel_irq_routing_entry *e,
 			  const struct kvm_irq_routing_entry *ue)
@@ -423,6 +427,12 @@ void kvm_arch_irq_routing_update(struct kvm *kvm)
 		kvm_make_scan_ioapic_request(kvm);
 }
 
+/*
+ * 在以下使用kvm_pi_update_irte():
+ *   - arch/x86/kvm/irq.c|490| <<kvm_arch_irq_bypass_add_producer>> ret = kvm_pi_update_irte(irqfd, &irqfd->irq_entry);
+ *   - arch/x86/kvm/irq.c|518| <<kvm_arch_irq_bypass_del_producer>> ret = kvm_pi_update_irte(irqfd, NULL);
+ *   - arch/x86/kvm/irq.c|543| <<kvm_arch_update_irqfd_routing>> kvm_pi_update_irte(irqfd, new);
+ */
 static int kvm_pi_update_irte(struct kvm_kernel_irqfd *irqfd,
 			      struct kvm_kernel_irq_routing_entry *entry)
 {
@@ -458,6 +468,9 @@ static int kvm_pi_update_irte(struct kvm_kernel_irqfd *irqfd,
 	if (!irqfd->irq_bypass_vcpu && !vcpu)
 		return 0;
 
+	/*
+	 * vmx_pi_update_irte
+	 */
 	r = kvm_x86_call(pi_update_irte)(irqfd, irqfd->kvm, host_irq, irqfd->gsi,
 					 vcpu, irq.vector);
 	if (r) {
@@ -487,6 +500,12 @@ int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
 		kvm_x86_call(pi_start_bypass)(kvm);
 
 	if (irqfd->irq_entry.type == KVM_IRQ_ROUTING_MSI) {
+		/*
+		 * 在以下使用kvm_pi_update_irte():
+		 *   - arch/x86/kvm/irq.c|490| <<kvm_arch_irq_bypass_add_producer>> ret = kvm_pi_update_irte(irqfd, &irqfd->irq_entry);
+		 *   - arch/x86/kvm/irq.c|518| <<kvm_arch_irq_bypass_del_producer>> ret = kvm_pi_update_irte(irqfd, NULL);
+		 *   - arch/x86/kvm/irq.c|543| <<kvm_arch_update_irqfd_routing>> kvm_pi_update_irte(irqfd, new);
+		 */
 		ret = kvm_pi_update_irte(irqfd, &irqfd->irq_entry);
 		if (ret)
 			kvm->arch.nr_possible_bypass_irqs--;
@@ -515,6 +534,12 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	spin_lock_irq(&kvm->irqfds.lock);
 
 	if (irqfd->irq_entry.type == KVM_IRQ_ROUTING_MSI) {
+		/*
+		 * 在以下使用kvm_pi_update_irte():
+		 *   - arch/x86/kvm/irq.c|490| <<kvm_arch_irq_bypass_add_producer>> ret = kvm_pi_update_irte(irqfd, &irqfd->irq_entry);
+		 *   - arch/x86/kvm/irq.c|518| <<kvm_arch_irq_bypass_del_producer>> ret = kvm_pi_update_irte(irqfd, NULL);
+		 *   - arch/x86/kvm/irq.c|543| <<kvm_arch_update_irqfd_routing>> kvm_pi_update_irte(irqfd, new);
+		 */
 		ret = kvm_pi_update_irte(irqfd, NULL);
 		if (ret)
 			pr_info("irq bypass consumer (eventfd %p) unregistration fails: %d\n",
@@ -527,6 +552,10 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 	spin_unlock_irq(&kvm->irqfds.lock);
 }
 
+/*
+ * 在以下使用kvm_arch_update_irqfd_routing():
+ *   - virt/kvm/eventfd.c|727| <<kvm_irq_routing_update>> kvm_arch_update_irqfd_routing(irqfd, &old, &irqfd->irq_entry);
+ */
 void kvm_arch_update_irqfd_routing(struct kvm_kernel_irqfd *irqfd,
 				   struct kvm_kernel_irq_routing_entry *old,
 				   struct kvm_kernel_irq_routing_entry *new)
@@ -540,6 +569,12 @@ void kvm_arch_update_irqfd_routing(struct kvm_kernel_irqfd *irqfd,
 	    !memcmp(&old->msi, &new->msi, sizeof(new->msi)))
 		return;
 
+	/*
+	 * 在以下使用kvm_pi_update_irte():
+	 *   - arch/x86/kvm/irq.c|490| <<kvm_arch_irq_bypass_add_producer>> ret = kvm_pi_update_irte(irqfd, &irqfd->irq_entry);
+	 *   - arch/x86/kvm/irq.c|518| <<kvm_arch_irq_bypass_del_producer>> ret = kvm_pi_update_irte(irqfd, NULL);
+	 *   - arch/x86/kvm/irq.c|543| <<kvm_arch_update_irqfd_routing>> kvm_pi_update_irte(irqfd, new);
+	 */
 	kvm_pi_update_irte(irqfd, new);
 }
 
@@ -570,8 +605,21 @@ static const struct kvm_irq_routing_entry default_routing[] = {
 	ROUTING_ENTRY1(22), ROUTING_ENTRY1(23),
 };
 
+/*
+ * 在以下使用:
+ *   - arch/x86/kvm/x86.c|7314| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> r = kvm_setup_default_ioapic_and_pic_routing(kvm);
+ */
 int kvm_setup_default_ioapic_and_pic_routing(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_set_irq_routing():
+	 *   - arch/arm64/kvm/vgic/vgic-irqfd.c|153| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+	 *   - arch/loongarch/kvm/intc/pch_pic.c|418| <<kvm_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+	 *   - arch/powerpc/kvm/mpic.c|1649| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+	 *   - arch/riscv/kvm/vm.c|108| <<kvm_riscv_setup_default_irq_routing>> rc = kvm_set_irq_routing(kvm, ents, lines, 0);
+	 *   - arch/x86/kvm/irq.c|603| <<kvm_setup_default_ioapic_and_pic_routing>> return kvm_set_irq_routing(kvm, default_routing,
+	 *   - virt/kvm/kvm_main.c|5322| <<kvm_vm_ioctl(KVM_SET_GSI_ROUTING)>> r = kvm_set_irq_routing(kvm, entries, routing.nr,
+	 */
 	return kvm_set_irq_routing(kvm, default_routing,
 				   ARRAY_SIZE(default_routing), 0);
 }
diff --git a/arch/x86/kvm/irq.h b/arch/x86/kvm/irq.h
index 34f4a78a7..57437d1d6 100644
--- a/arch/x86/kvm/irq.h
+++ b/arch/x86/kvm/irq.h
@@ -75,6 +75,14 @@ int kvm_vm_ioctl_set_irqchip(struct kvm *kvm, struct kvm_irqchip *chip);
 
 static inline int irqchip_full(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|8279| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|8907| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
@@ -96,6 +104,14 @@ static inline int pic_in_kernel(struct kvm *kvm)
 
 static inline int irqchip_split(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|8279| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|8907| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
@@ -105,6 +121,14 @@ static inline int irqchip_split(struct kvm *kvm)
 
 static inline int irqchip_in_kernel(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->irqchip_mode:
+	 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+	 *   - arch/x86/kvm/x86.c|8279| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+	 *   - arch/x86/kvm/x86.c|8907| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+	 */
 	int mode = kvm->arch.irqchip_mode;
 
 	/* Matches smp_wmb() when setting irqchip_mode */
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 0ae7f913d..142097c53 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -475,6 +475,15 @@ static inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)
 
 	/* Check if there are APF page ready requests pending */
 	if (enabled) {
+		/*
+		 * 在以下使用KVM_REQ_APF_READY:
+		 *   - arch/x86/kvm/lapic.c|478| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+		 *   - arch/x86/kvm/lapic.c|2791| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+		 *   - arch/x86/kvm/x86.c|12650| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+		 *   - arch/x86/kvm/x86.c|15626| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+		 *
+		 * 处理的函数kvm_check_async_pf_completion()
+		 */
 		kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
 		kvm_xen_sw_enable_lapic(apic->vcpu);
 	}
@@ -770,6 +779,10 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * 在以下使用kvm_apic_update_hwapic_isr():
+ *   - arch/x86/kvm/vmx/nested.c|5282| <<__nested_vmx_vmexit>> kvm_apic_update_hwapic_isr(vcpu);
+ */
 void kvm_apic_update_hwapic_isr(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -1882,6 +1895,21 @@ static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 {
 	u64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;
 
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	/*
 	 * If the guest TSC is running at a different ratio than the host, then
 	 * convert the delay to nanoseconds to achieve an accurate delay.  Note
@@ -1889,6 +1917,26 @@ static inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)
 	 * always for VMX enabled hardware.
 	 */
 	if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+		/*
+		 * 在以下使用nsec_to_cycles():
+		 *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+		 *              nsec_to_cycles(vcpu, timer_advance_ns)));
+		 *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+		 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+		 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+		 *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+		 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+		 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+		 *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+		 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+		 *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+		 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+		 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+		 *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+		 *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+		 *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+		 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+		 */
 		__delay(min(guest_cycles,
 			nsec_to_cycles(vcpu, timer_advance_ns)));
 	} else {
@@ -2068,6 +2116,26 @@ static void update_target_expiration(struct kvm_lapic *apic, uint32_t old_diviso
 	ns_remaining_new = mul_u64_u32_div(ns_remaining_old,
 	                                   apic->divide_count, old_divisor);
 
+	/*
+	 * 在以下使用nsec_to_cycles():
+	 *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+	 *              nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+	 *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+	 *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+	 *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+	 */
 	apic->lapic_timer.tscdeadline +=
 		nsec_to_cycles(apic->vcpu, ns_remaining_new) -
 		nsec_to_cycles(apic->vcpu, ns_remaining_old);
@@ -2117,6 +2185,26 @@ static bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)
 		}
 	}
 
+	/*
+	 * 在以下使用nsec_to_cycles():
+	 *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+	 *              nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+	 *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+	 *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+	 *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+	 */
 	apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
 		nsec_to_cycles(apic->vcpu, deadline);
 	apic->lapic_timer.target_expiration = ktime_add_ns(now, deadline);
@@ -2141,6 +2229,26 @@ static void advance_periodic_target_expiration(struct kvm_lapic *apic)
 		ktime_add_ns(apic->lapic_timer.target_expiration,
 				apic->lapic_timer.period);
 	delta = ktime_sub(apic->lapic_timer.target_expiration, now);
+	/*
+	 * 在以下使用nsec_to_cycles():
+	 *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+	 *              nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+	 *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+	 *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+	 *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+	 */
 	apic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +
 		nsec_to_cycles(apic->vcpu, delta);
 }
@@ -2692,6 +2800,15 @@ static void __kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value)
 		if (value & MSR_IA32_APICBASE_ENABLE) {
 			kvm_apic_set_xapic_id(apic, vcpu->vcpu_id);
 			static_branch_slow_dec_deferred(&apic_hw_disabled);
+			/*
+			 * 在以下使用KVM_REQ_APF_READY:
+			 *   - arch/x86/kvm/lapic.c|478| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+			 *   - arch/x86/kvm/lapic.c|2791| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+			 *   - arch/x86/kvm/x86.c|12650| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+			 *   - arch/x86/kvm/x86.c|15626| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+			 *
+			 * 处理的函数kvm_check_async_pf_completion()
+			 */
 			/* Check if there are APF page ready requests pending */
 			kvm_make_request(KVM_REQ_APF_READY, vcpu);
 		} else {
@@ -2748,6 +2865,12 @@ int kvm_apic_set_base(struct kvm_vcpu *vcpu, u64 value, bool host_initiated)
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(kvm_apic_set_base);
 
+/*
+ * 在以下使用kvm_apic_update_apicv():
+ *   - arch/x86/kvm/lapic.c|2895| <<kvm_lapic_reset>> kvm_apic_update_apicv(vcpu);
+ *   - arch/x86/kvm/lapic.c|3216| <<kvm_apic_set_state>> kvm_apic_update_apicv(vcpu);
+ *   - arch/x86/kvm/x86.c|10872| <<__kvm_vcpu_update_apicv>> kvm_apic_update_apicv(vcpu);
+ */
 void kvm_apic_update_apicv(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -3184,6 +3307,10 @@ int kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 	return kvm_apic_state_fixup(vcpu, s, false);
 }
 
+/*
+ * 在以下使用kvm_apic_set_state():
+ *   - arch/x86/kvm/x86.c|5357| <<kvm_vcpu_ioctl_set_lapic>> r = kvm_apic_set_state(vcpu, s);
+ */
 int kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 667d66cf7..bc2e9b769 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -4501,6 +4501,10 @@ static void shadow_page_table_clear_flood(struct kvm_vcpu *vcpu, gva_t addr)
 	walk_shadow_page_lockless_end(vcpu);
 }
 
+/*
+ * 在以下使用alloc_apf_token():
+ *   - arch/x86/kvm/mmu/mmu.c|4520| <<kvm_arch_setup_async_pf>> arch.token = alloc_apf_token(vcpu);
+ */
 static u32 alloc_apf_token(struct kvm_vcpu *vcpu)
 {
 	/* make sure the token value is not 0 */
@@ -4512,6 +4516,10 @@ static u32 alloc_apf_token(struct kvm_vcpu *vcpu)
 	return (vcpu->arch.apf.id++ << 12) | vcpu->vcpu_id;
 }
 
+/*
+ * 在以下使用kvm_arch_setup_async_pf():
+ *   - arch/x86/kvm/mmu/mmu.c|4637| <<__kvm_mmu_faultin_pfn>> } else if (kvm_arch_setup_async_pf(vcpu, fault)) {
+ */
 static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu,
 				    struct kvm_page_fault *fault)
 {
@@ -4523,10 +4531,19 @@ static bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu,
 	arch.direct_map = vcpu->arch.mmu->root_role.direct;
 	arch.cr3 = kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu);
 
+	/*
+	 * 在以下使用kvm_setup_async_pf():
+	 *  - arch/s390/kvm/kvm-s390.c|4773| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu,
+	 *  - arch/x86/kvm/mmu/mmu.c|4526| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, fault->addr,
+	 */
 	return kvm_setup_async_pf(vcpu, fault->addr,
 				  kvm_vcpu_gfn_to_hva(vcpu, fault->gfn), &arch);
 }
 
+/*
+ * 在以下使用kvm_arch_async_page_ready():
+ *   - virt/kvm/async_pf.c|165| <<kvm_check_async_pf_completion>> kvm_arch_async_page_ready(vcpu, work);
+ */
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
@@ -4546,6 +4563,12 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 	      work->arch.cr3 != kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu))
 		return;
 
+	/*
+	 * 在以下使用kvm_mmu_do_page_fault():
+	 *   - arch/x86/kvm/mmu/mmu.c|4549| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+	 *   - arch/x86/kvm/mmu/mmu.c|4946| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+	 *   - arch/x86/kvm/mmu/mmu.c|6353| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+	 */
 	r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
 				  true, NULL, NULL);
 
@@ -4613,6 +4636,15 @@ static int __kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 		trace_kvm_try_async_get_page(fault->addr, fault->gfn);
 		if (kvm_find_async_pf_gfn(vcpu, fault->gfn)) {
 			trace_kvm_async_pf_repeated_fault(fault->addr, fault->gfn);
+			/*
+			 * 在以下使用KVM_REQ_APF_HALT:
+			 *   - arch/x86/kvm/mmu/mmu.c|4639| <<__kvm_mmu_faultin_pfn>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+			 *   - arch/x86/kvm/x86.c|12575| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
+			 *   - arch/x86/kvm/x86.c|15581| <<kvm_arch_async_page_not_present>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+			 *
+			 * 处理的代码:
+			 * vcpu->arch.apf.halted = true;
+			 */
 			kvm_make_request(KVM_REQ_APF_HALT, vcpu);
 			return RET_PF_RETRY;
 		} else if (kvm_arch_setup_async_pf(vcpu, fault)) {
@@ -4943,6 +4975,12 @@ int kvm_tdp_map_page(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code, u8 *level
 			return -EIO;
 
 		cond_resched();
+		/*
+		 * 在以下使用kvm_mmu_do_page_fault():
+		 *   - arch/x86/kvm/mmu/mmu.c|4549| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+		 *   - arch/x86/kvm/mmu/mmu.c|4946| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+		 *   - arch/x86/kvm/mmu/mmu.c|6353| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
 	} while (r == RET_PF_RETRY);
 
@@ -6350,6 +6388,12 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (r == RET_PF_INVALID) {
 		vcpu->stat.pf_taken++;
 
+		/*
+		 * 在以下使用kvm_mmu_do_page_fault():
+		 *   - arch/x86/kvm/mmu/mmu.c|4549| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+		 *   - arch/x86/kvm/mmu/mmu.c|4946| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+		 *   - arch/x86/kvm/mmu/mmu.c|6353| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
 					  &emulation_type, NULL);
 		if (KVM_BUG_ON(r == RET_PF_INVALID, vcpu->kvm))
@@ -7631,6 +7675,18 @@ static bool kvm_mmu_sp_dirty_logging_enabled(struct kvm *kvm,
 	 * nonzero, the page will be zapped unnecessarily.  Either way, this
 	 * only affects efficiency in racy situations, and not correctness.
 	 */
+	/*
+	 * 在以下使用kvm->nr_memslots_dirty_logging:
+	 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+	 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 */
 	if (!atomic_read(&kvm->nr_memslots_dirty_logging))
 		return false;
 
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index ed5c01df2..10b04fd5d 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -347,6 +347,12 @@ static inline void kvm_mmu_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 				      fault->is_private);
 }
 
+/*
+ * 在以下使用kvm_mmu_do_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|4549| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+ *   - arch/x86/kvm/mmu/mmu.c|4946| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+ *   - arch/x86/kvm/mmu/mmu.c|6353| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u64 err, bool prefetch,
 					int *emulation_type, u8 *level)
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 487ad19a2..b5d7e1585 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -103,6 +103,10 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * 在以下使用kvm_init_pmu_capability():
+ *   - arch/x86/kvm/x86.c|11420| <<kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ */
 void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -664,6 +668,16 @@ static int kvm_pmu_rdpmc_vmware(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 		ctr_val = ktime_get_boottime_ns();
 		break;
 	case VMWARE_BACKDOOR_PMC_APPARENT_TIME:
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+		 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		ctr_val = ktime_get_boottime_ns() +
 			vcpu->kvm->arch.kvmclock_offset;
 		break;
@@ -708,6 +722,13 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用kvm_pmu_is_valid_msr():
+ *   - arch/x86/kvm/x86.c|5301| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|5386| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|5483| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ *   - arch/x86/kvm/x86.c|5739| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ */
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	switch (msr) {
@@ -718,6 +739,10 @@ bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 	default:
 		break;
 	}
+	/*
+	 * amd_msr_idx_to_pmc()
+	 * amd_is_valid_msr()
+	 */
 	return kvm_pmu_call(msr_idx_to_pmc)(vcpu, msr) ||
 	       kvm_pmu_call(is_valid_msr)(vcpu, msr);
 }
@@ -751,6 +776,9 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		msr_info->data = 0;
 		break;
 	default:
+		/*
+		 * amd_pmu_get_msr()
+		 */
 		return kvm_pmu_call(get_msr)(vcpu, msr_info);
 	}
 
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index fef00546c..51ae4b331 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -29,6 +29,53 @@
 #include "irq.h"
 #include "svm.h"
 
+/*
+ * 在以下使用irq_bypass_producer->add_consumer:
+ *   - virt/lib/irqbypass.c|40| <<__connect>> if (prod->add_consumer)
+ *   - virt/lib/irqbypass.c|41| <<__connect>> ret = prod->add_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->del_consumer:
+ *   - virt/lib/irqbypass.c|45| <<__connect>> if (ret && prod->del_consumer)
+ *   - virt/lib/irqbypass.c|46| <<__connect>> prod->del_consumer(prod, cons);
+ *   - virt/lib/irqbypass.c|72| <<__disconnect>> if (prod->del_consumer)
+ *   - virt/lib/irqbypass.c|73| <<__disconnect>> prod->del_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->stop:
+ *   - virt/lib/irqbypass.c|35| <<__connect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|36| <<__connect>> prod->stop(prod);
+ *   - virt/lib/irqbypass.c|65| <<__disconnect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|66| <<__disconnect>> prod->stop(prod);
+ *
+ * 在以下使用irq_bypass_producer->start:
+ *   - virt/lib/irqbypass.c|51| <<__connect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|52| <<__connect>> prod->start(prod);
+ *   - virt/lib/irqbypass.c|77| <<__disconnect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|78| <<__disconnect>> prod->start(prod);
+ *
+ *
+ * 在以下使用irq_bypass_consumer->add_producer:
+ *   - virt/kvm/eventfd.c|478| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ *   - virt/lib/irqbypass.c|44| <<__connect>> ret = cons->add_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->del_producer:
+ *   - virt/kvm/eventfd.c|479| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ *   - virt/lib/irqbypass.c|70| <<__disconnect>> cons->del_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->stop:
+ *   - virt/lib/irqbypass.c|37| <<__connect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|38| <<__connect>> cons->stop(cons);
+ *   - virt/lib/irqbypass.c|67| <<__disconnect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|68| <<__disconnect>> cons->stop(cons);
+ *
+ * 在以下使用irq_bypass_consumer->start:
+ *   - virt/lib/irqbypass.c|49| <<__connect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|50| <<__connect>> cons->start(cons);
+ *   - virt/lib/irqbypass.c|75| <<__disconnect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|76| <<__disconnect>> cons->start(cons);
+ */
+
 /*
  * Encode the arbitrary VM ID and the vCPU's _index_ into the GATag so that
  * KVM can retrieve the correct vCPU from a GALog entry if an interrupt can't
diff --git a/arch/x86/kvm/svm/hyperv.c b/arch/x86/kvm/svm/hyperv.c
index 088f6429b..91f498bd6 100644
--- a/arch/x86/kvm/svm/hyperv.c
+++ b/arch/x86/kvm/svm/hyperv.c
@@ -14,5 +14,14 @@ void svm_hv_inject_synthetic_vmexit_post_tlb_flush(struct kvm_vcpu *vcpu)
 	svm->vmcb->control.exit_code_hi = 0;
 	svm->vmcb->control.exit_info_1 = HV_SVM_ENL_EXITCODE_TRAP_AFTER_FLUSH;
 	svm->vmcb->control.exit_info_2 = 0;
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index da6e80b3a..d6ef90804 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -53,6 +53,15 @@ static void nested_svm_inject_npf_exit(struct kvm_vcpu *vcpu,
 	vmcb->control.exit_info_1 &= ~0xffffffffULL;
 	vmcb->control.exit_info_1 |= fault->error_code;
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
 
@@ -403,6 +412,11 @@ static bool nested_vmcb_check_controls(struct kvm_vcpu *vcpu)
 	return __nested_vmcb_check_controls(vcpu, ctl);
 }
 
+/*
+ * 在以下使用__nested_copy_vmcb_control_to_cache():
+ *   - arch/x86/kvm/svm/nested.c|465| <<nested_copy_vmcb_control_to_cache>> __nested_copy_vmcb_control_to_cache(&svm->vcpu, &svm->nested.ctl, control);
+ *   - arch/x86/kvm/svm/nested.c|2046| <<svm_set_nested_state>> __nested_copy_vmcb_control_to_cache(vcpu, &ctl_cached, ctl);
+ */
 static
 void __nested_copy_vmcb_control_to_cache(struct kvm_vcpu *vcpu,
 					 struct vmcb_ctrl_area_cached *to,
@@ -410,6 +424,11 @@ void __nested_copy_vmcb_control_to_cache(struct kvm_vcpu *vcpu,
 {
 	unsigned int i;
 
+	/*
+	 * 代码
+	 * 124 struct __attribute__ ((__packed__)) vmcb_control_area {
+	 * 125         u32 intercepts[MAX_INTERCEPT];
+	 */
 	for (i = 0; i < MAX_INTERCEPT; i++)
 		to->intercepts[i] = from->intercepts[i];
 
@@ -575,6 +594,12 @@ static void nested_svm_transition_tlb_flush(struct kvm_vcpu *vcpu)
  * Load guest's/host's cr3 on nested vmentry or vmexit. @nested_npt is true
  * if we are emulating VM-Entry into a guest with NPT enabled.
  */
+/*
+ * 在以下使用nested_svm_load_cr3():
+ *   - arch/x86/kvm/svm/nested.c|953| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+ *   - arch/x86/kvm/svm/nested.c|1334| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
+ *   - arch/x86/kvm/svm/nested.c|2046| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3, nested_npt_enabled(svm), false);
+ */
 static int nested_svm_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,
 			       bool nested_npt, bool reload_pdptrs)
 {
@@ -605,6 +630,10 @@ void nested_vmcb02_compute_g_pat(struct vcpu_svm *svm)
 	svm->nested.vmcb02.ptr->save.g_pat = svm->vmcb01.ptr->save.g_pat;
 }
 
+/*
+ * 在以下使用nested_vmcb02_prepare_save():
+ *   - arch/x86/kvm/svm/nested.c|951| <<enter_svm_guest_mode>> nested_vmcb02_prepare_save(svm, vmcb12);
+ */
 static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12)
 {
 	bool new_vmcb12 = false;
@@ -680,6 +709,12 @@ static void nested_vmcb02_prepare_save(struct vcpu_svm *svm, struct vmcb *vmcb12
 	} else {
 		svm_copy_lbrs(vmcb02, vmcb01);
 	}
+	/*
+	 * 在以下使用svm_update_lbrv():
+	 *   - arch/x86/kvm/svm/nested.c|707| <<nested_vmcb02_prepare_save>> svm_update_lbrv(&svm->vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<nested_svm_vmexit>> svm_update_lbrv(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr(MSR_IA32_DEBUGCTLMSR)>> svm_update_lbrv(vcpu);
+	 */
 	svm_update_lbrv(&svm->vcpu);
 }
 
@@ -707,6 +742,11 @@ static bool is_evtinj_nmi(u32 evtinj)
 	return type == SVM_EVTINJ_TYPE_NMI;
 }
 
+/*
+ * 在以下使用nested_vmcb02_prepare_control():
+ *   - arch/x86/kvm/svm/nested.c|950| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+ *   - arch/x86/kvm/svm/nested.c|2037| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+ */
 static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 					  unsigned long vmcb12_rip,
 					  unsigned long vmcb12_csbase)
@@ -795,6 +835,21 @@ static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 
 	vmcb02->control.tsc_offset = vcpu->arch.tsc_offset;
 
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	if (guest_cpu_cap_has(vcpu, X86_FEATURE_TSCRATEMSR) &&
 	    svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
 		nested_svm_update_tsc_ratio_msr(vcpu);
@@ -871,6 +926,11 @@ static void nested_vmcb02_prepare_control(struct vcpu_svm *svm,
 	recalc_intercepts(svm);
 }
 
+/*
+ * 在以下使用nested_svm_copy_common_state():
+ *   - arch/x86/kvm/svm/nested.c|914| <<enter_svm_guest_mode>> nested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);
+ *   - arch/x86/kvm/svm/nested.c|1158| <<nested_svm_vmexit>> nested_svm_copy_common_state(svm->nested.vmcb02.ptr, svm->vmcb01.ptr);
+ */
 static void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to_vmcb)
 {
 	/*
@@ -883,6 +943,11 @@ static void nested_svm_copy_common_state(struct vmcb *from_vmcb, struct vmcb *to
 	to_vmcb->save.spec_ctrl = from_vmcb->save.spec_ctrl;
 }
 
+/*
+ * 在以下使用enter_svm_guest_mode():
+ *   - arch/x86/kvm/svm/nested.c|1007| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+ *   - arch/x86/kvm/svm/svm.c|4763| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+ */
 int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 			 struct vmcb *vmcb12, bool from_vmrun)
 {
@@ -911,12 +976,40 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 
 	WARN_ON(svm->vmcb == svm->nested.vmcb02.ptr);
 
+	/*
+	 * 在以下使用nested_svm_copy_common_state():
+	 *   - arch/x86/kvm/svm/nested.c|914| <<enter_svm_guest_mode>> nested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);
+	 *   - arch/x86/kvm/svm/nested.c|1158| <<nested_svm_vmexit>> nested_svm_copy_common_state(svm->nested.vmcb02.ptr, svm->vmcb01.ptr);
+	 */
 	nested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|916| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1162| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1359| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1872| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1234| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	/*
+	 * 在以下使用nested_vmcb02_prepare_control():
+	 *   - arch/x86/kvm/svm/nested.c|950| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	 *   - arch/x86/kvm/svm/nested.c|2037| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+	 */
 	nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	/*
+	 * 只在这里调用
+	 */
 	nested_vmcb02_prepare_save(svm, vmcb12);
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|953| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1334| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2046| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 */
 	ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3,
 				  nested_npt_enabled(svm), from_vmrun);
 	if (ret)
@@ -925,6 +1018,16 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 	if (!from_vmrun)
 		kvm_make_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);
 
+	/*
+	 * 在以下使用svm_set_gif():
+	 *   - arch/x86/kvm/svm/nested.c|984| <<enter_svm_guest_mode>> svm_set_gif(svm, true);
+	 *   - arch/x86/kvm/svm/nested.c|1313| <<nested_svm_vmexit>> svm_set_gif(svm, false);
+	 *   - arch/x86/kvm/svm/nested.c|1984| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+	 *   - arch/x86/kvm/svm/nested.c|2040| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+	 *   - arch/x86/kvm/svm/svm.c|218| <<svm_set_efer>> svm_set_gif(svm, true);
+	 *   - arch/x86/kvm/svm/svm.c|2300| <<stgi_interception>> svm_set_gif(to_svm(vcpu), true);
+	 *   - arch/x86/kvm/svm/svm.c|2312| <<clgi_interception>> svm_set_gif(to_svm(vcpu), false);
+	 */
 	svm_set_gif(svm, true);
 
 	if (kvm_vcpu_apicv_active(vcpu))
@@ -935,6 +1038,10 @@ int enter_svm_guest_mode(struct kvm_vcpu *vcpu, u64 vmcb12_gpa,
 	return 0;
 }
 
+/*
+ * 在以下使用nested_svm_vmrun():
+ *   - arch/x86/kvm/svm/svm.c|2122| <<vmrun_interception>> return nested_svm_vmrun(vcpu);
+ */
 int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -961,6 +1068,10 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 		return ret;
 	}
 
+	/*
+	 * struct vcpu_svm *svm:
+	 * -> struct vmcb *vmcb;
+	 */
 	vmcb12_gpa = svm->vmcb->save.rax;
 	ret = kvm_vcpu_map(vcpu, gpa_to_gfn(vmcb12_gpa), &map);
 	if (ret == -EINVAL) {
@@ -1004,6 +1115,11 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 
 	svm->nested.nested_run_pending = 1;
 
+	/*
+	 * 在以下使用enter_svm_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1007| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+	 *   - arch/x86/kvm/svm/svm.c|4763| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+	 */
 	if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
 		goto out_exit_err;
 
@@ -1020,6 +1136,15 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 	svm->vmcb->control.exit_info_1  = 0;
 	svm->vmcb->control.exit_info_2  = 0;
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 
 out:
@@ -1028,6 +1153,12 @@ int nested_svm_vmrun(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+/*
+ * 在以下使用svm_copy_vmrun_state():
+ *   - arch/x86/kvm/svm/nested.c|2134| <<svm_set_nested_state>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
+ *   - arch/x86/kvm/svm/svm.c|4837| <<svm_enter_smm>> svm_copy_vmrun_state(map_save.hva + 0x400,
+ *   - arch/x86/kvm/svm/svm.c|4881| <<svm_leave_smm>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, map_save.hva + 0x400);
+ */
 /* Copy state save area fields which are handled by VMRUN */
 void svm_copy_vmrun_state(struct vmcb_save_area *to_save,
 			  struct vmcb_save_area *from_save)
@@ -1071,6 +1202,15 @@ void svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb)
 	to_vmcb->save.sysenter_eip = from_vmcb->save.sysenter_eip;
 }
 
+/*
+ * 在以下使用nested_svm_vmexit():
+ *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+ *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+ */
 int nested_svm_vmexit(struct vcpu_svm *svm)
 {
 	struct kvm_vcpu *vcpu = &svm->vcpu;
@@ -1155,10 +1295,24 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	if (!vmcb02->control.bus_lock_counter)
 		svm->nested.ctl.bus_lock_rip = INVALID_GPA;
 
+	/*
+	 * 在以下使用nested_svm_copy_common_state():
+	 *   - arch/x86/kvm/svm/nested.c|914| <<enter_svm_guest_mode>> nested_svm_copy_common_state(svm->vmcb01.ptr, svm->nested.vmcb02.ptr);
+	 *   - arch/x86/kvm/svm/nested.c|1158| <<nested_svm_vmexit>> nested_svm_copy_common_state(svm->nested.vmcb02.ptr, svm->vmcb01.ptr);
+	 */
 	nested_svm_copy_common_state(svm->nested.vmcb02.ptr, svm->vmcb01.ptr);
 
 	kvm_nested_vmexit_handle_ibrs(vcpu);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|916| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1162| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1359| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1872| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1234| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	/*
@@ -1189,6 +1343,12 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	else
 		svm_copy_lbrs(vmcb01, vmcb02);
 
+	/*
+	 * 在以下使用svm_update_lbrv():
+	 *   - arch/x86/kvm/svm/nested.c|707| <<nested_vmcb02_prepare_save>> svm_update_lbrv(&svm->vcpu);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<nested_svm_vmexit>> svm_update_lbrv(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr(MSR_IA32_DEBUGCTLMSR)>> svm_update_lbrv(vcpu);
+	 */
 	svm_update_lbrv(vcpu);
 
 	if (vnmi) {
@@ -1209,6 +1369,16 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 	 * On vmexit the  GIF is set to false and
 	 * no event can be injected in L1.
 	 */
+	/*
+	 * 在以下使用svm_set_gif():
+	 *   - arch/x86/kvm/svm/nested.c|984| <<enter_svm_guest_mode>> svm_set_gif(svm, true);
+	 *   - arch/x86/kvm/svm/nested.c|1313| <<nested_svm_vmexit>> svm_set_gif(svm, false);
+	 *   - arch/x86/kvm/svm/nested.c|1984| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+	 *   - arch/x86/kvm/svm/nested.c|2040| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+	 *   - arch/x86/kvm/svm/svm.c|218| <<svm_set_efer>> svm_set_gif(svm, true);
+	 *   - arch/x86/kvm/svm/svm.c|2300| <<stgi_interception>> svm_set_gif(to_svm(vcpu), true);
+	 *   - arch/x86/kvm/svm/svm.c|2312| <<clgi_interception>> svm_set_gif(to_svm(vcpu), false);
+	 */
 	svm_set_gif(svm, false);
 	vmcb01->control.exit_int_info = 0;
 
@@ -1253,6 +1423,12 @@ int nested_svm_vmexit(struct vcpu_svm *svm)
 
 	nested_svm_uninit_mmu_context(vcpu);
 
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|953| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1334| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2046| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 */
 	rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
 	if (rc)
 		return 1;
@@ -1292,6 +1468,16 @@ static void nested_svm_triple_fault(struct kvm_vcpu *vcpu)
 		return;
 
 	kvm_clear_request(KVM_REQ_TRIPLE_FAULT, vcpu);
+	/*
+	 * 在以下使用nested_svm_simple_vmexit():
+	 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+	 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+	 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+	 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+	 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+	 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+	 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+	 */
 	nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
 }
 
@@ -1325,6 +1511,15 @@ void svm_free_nested(struct vcpu_svm *svm)
 	if (!svm->nested.initialized)
 		return;
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|916| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1162| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1359| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1872| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1234| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	if (WARN_ON_ONCE(svm->vmcb != svm->vmcb01.ptr))
 		svm_switch_vmcb(svm, &svm->vmcb01);
 
@@ -1359,6 +1554,14 @@ void svm_leave_nested(struct kvm_vcpu *vcpu)
 		svm_switch_vmcb(svm, &svm->vmcb01);
 
 		nested_svm_uninit_mmu_context(vcpu);
+		/*
+		 * 在以下使用vmcb_mark_all_dirty():
+		 *   - arch/x86/kvm/svm/nested.c|1535| <<svm_leave_nested>> vmcb_mark_all_dirty(svm->vmcb);
+		 *   - arch/x86/kvm/svm/sev.c|3981| <<sev_snp_init_protected_guest_state>> vmcb_mark_all_dirty(svm->vmcb);
+		 *   - arch/x86/kvm/svm/svm.c|1173| <<init_vmcb>> vmcb_mark_all_dirty(vmcb);
+		 *   - arch/x86/kvm/svm/svm.c|3633| <<pre_svm_run>> vmcb_mark_all_dirty(svm->vmcb);
+		 *   - arch/x86/kvm/svm/svm.c|4835| <<svm_leave_smm>> vmcb_mark_all_dirty(svm->vmcb01.ptr);
+		 */
 		vmcb_mark_all_dirty(svm->vmcb);
 
 		if (kvm_apicv_activated(vcpu->kvm))
@@ -1468,6 +1671,15 @@ int nested_svm_exit_handled(struct vcpu_svm *svm)
 
 	vmexit = nested_svm_intercept(svm);
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	if (vmexit == NESTED_EXIT_DONE)
 		nested_svm_vmexit(svm);
 
@@ -1530,6 +1742,15 @@ static void nested_svm_inject_exception_vmexit(struct kvm_vcpu *vcpu)
 		WARN_ON(ex->has_payload);
 	}
 
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	nested_svm_vmexit(svm);
 }
 
@@ -1563,6 +1784,16 @@ static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 			return -EBUSY;
 		if (!nested_exit_on_init(svm))
 			return 0;
+		/*
+		 * 在以下使用nested_svm_simple_vmexit():
+		 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+		 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+		 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+		 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+		 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+		 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+		 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+		 */
 		nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
 		return 0;
 	}
@@ -1586,6 +1817,16 @@ static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 			return -EBUSY;
 		if (!nested_exit_on_smi(svm))
 			return 0;
+		/*
+		 * 在以下使用nested_svm_simple_vmexit():
+		 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+		 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+		 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+		 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+		 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+		 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+		 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+		 */
 		nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
 		return 0;
 	}
@@ -1596,6 +1837,16 @@ static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 			return -EBUSY;
 		if (!nested_exit_on_nmi(svm))
 			return 0;
+		/*
+		 * 在以下使用nested_svm_simple_vmexit():
+		 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+		 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+		 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+		 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+		 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+		 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+		 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+		 */
 		nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
 		return 0;
 	}
@@ -1606,6 +1857,16 @@ static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 		if (!nested_exit_on_intr(svm))
 			return 0;
 		trace_kvm_nested_intr_vmexit(svm->vmcb->save.rip);
+		/*
+		 * 在以下使用nested_svm_simple_vmexit():
+		 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+		 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+		 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+		 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+		 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+		 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+		 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+		 */
 		nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
 		return 0;
 	}
@@ -1803,6 +2064,16 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 
 	if (!(kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE)) {
 		svm_leave_nested(vcpu);
+		/*
+		 * 在以下使用svm_set_gif():
+		 *   - arch/x86/kvm/svm/nested.c|984| <<enter_svm_guest_mode>> svm_set_gif(svm, true);
+		 *   - arch/x86/kvm/svm/nested.c|1313| <<nested_svm_vmexit>> svm_set_gif(svm, false);
+		 *   - arch/x86/kvm/svm/nested.c|1984| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+		 *   - arch/x86/kvm/svm/nested.c|2040| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+		 *   - arch/x86/kvm/svm/svm.c|218| <<svm_set_efer>> svm_set_gif(svm, true);
+		 *   - arch/x86/kvm/svm/svm.c|2300| <<stgi_interception>> svm_set_gif(to_svm(vcpu), true);
+		 *   - arch/x86/kvm/svm/svm.c|2312| <<clgi_interception>> svm_set_gif(to_svm(vcpu), false);
+		 */
 		svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
 		return 0;
 	}
@@ -1866,10 +2137,30 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 
 	svm->nested.vmcb12_gpa = kvm_state->hdr.svm.vmcb_pa;
 
+	/*
+	 * 在以下使用svm_copy_vmrun_state():
+	 *   - arch/x86/kvm/svm/nested.c|2134| <<svm_set_nested_state>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
+	 *   - arch/x86/kvm/svm/svm.c|4837| <<svm_enter_smm>> svm_copy_vmrun_state(map_save.hva + 0x400,
+	 *   - arch/x86/kvm/svm/svm.c|4881| <<svm_leave_smm>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, map_save.hva + 0x400);
+	 */
 	svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
 	nested_copy_vmcb_control_to_cache(svm, ctl);
 
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|916| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1162| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1359| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1872| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1234| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	/*
+	 * 在以下使用nested_vmcb02_prepare_control():
+	 *   - arch/x86/kvm/svm/nested.c|950| <<enter_svm_guest_mode>> nested_vmcb02_prepare_control(svm, vmcb12->save.rip, vmcb12->save.cs.base);
+	 *   - arch/x86/kvm/svm/nested.c|2037| <<svm_set_nested_state>> nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
+	 */
 	nested_vmcb02_prepare_control(svm, svm->vmcb->save.rip, svm->vmcb->save.cs.base);
 
 	/*
@@ -1878,7 +2169,12 @@ static int svm_set_nested_state(struct kvm_vcpu *vcpu,
 	 * thus MMU might not be initialized correctly.
 	 * Set it again to fix this.
 	 */
-
+	/*
+	 * 在以下使用nested_svm_load_cr3():
+	 *   - arch/x86/kvm/svm/nested.c|953| <<enter_svm_guest_mode>> ret = nested_svm_load_cr3(&svm->vcpu, svm->nested.save.cr3, nested_npt_enabled(svm), from_vmrun);
+	 *   - arch/x86/kvm/svm/nested.c|1334| <<nested_svm_vmexit>> rc = nested_svm_load_cr3(vcpu, vmcb01->save.cr3, false, true);
+	 *   - arch/x86/kvm/svm/nested.c|2046| <<svm_set_nested_state>> ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3, nested_npt_enabled(svm), false);
+	 */
 	ret = nested_svm_load_cr3(&svm->vcpu, vcpu->arch.cr3,
 				  nested_npt_enabled(svm), false);
 	if (WARN_ON_ONCE(ret))
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index bc062285f..fa4a82340 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -25,6 +25,11 @@ enum pmu_type {
 	PMU_TYPE_EVNTSEL,
 };
 
+/*
+ * 在以下使用amd_pmu_get_pmc():
+ *   - arch/x86/kvm/svm/pmu.c|73| <<get_gp_pmc_amd>> return amd_pmu_get_pmc(pmu, idx);
+ *   - arch/x86/kvm/svm/pmu.c|90| <<amd_rdpmc_ecx_to_pmc>> return amd_pmu_get_pmc(vcpu_to_pmu(vcpu), idx);
+ */
 static struct kvm_pmc *amd_pmu_get_pmc(struct kvm_pmu *pmu, int pmc_idx)
 {
 	unsigned int num_counters = pmu->nr_arch_gp_counters;
@@ -35,6 +40,15 @@ static struct kvm_pmc *amd_pmu_get_pmc(struct kvm_pmu *pmu, int pmc_idx)
 	return &pmu->gp_counters[array_index_nospec(pmc_idx, num_counters)];
 }
 
+/*
+ * 在以下使用get_gp_pmc_amd():
+ *   - arch/x86/kvm/svm/pmu.c|98| <<amd_msr_idx_to_pmc>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|99| <<amd_msr_idx_to_pmc>> pmc = pmc ? pmc : get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|135| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|141| <<amd_pmu_get_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ *   - arch/x86/kvm/svm/pmu.c|158| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
+ *   - arch/x86/kvm/svm/pmu.c|164| <<amd_pmu_set_msr>> pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_EVNTSEL);
+ */
 static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 					     enum pmu_type type)
 {
@@ -70,6 +84,11 @@ static inline struct kvm_pmc *get_gp_pmc_amd(struct kvm_pmu *pmu, u32 msr,
 		return NULL;
 	}
 
+        /*
+	 * 在以下使用amd_pmu_get_pmc():
+         *   - arch/x86/kvm/svm/pmu.c|73| <<get_gp_pmc_amd>> return amd_pmu_get_pmc(pmu, idx);
+         *   - arch/x86/kvm/svm/pmu.c|90| <<amd_rdpmc_ecx_to_pmc>> return amd_pmu_get_pmc(vcpu_to_pmu(vcpu), idx);
+	 */
 	return amd_pmu_get_pmc(pmu, idx);
 }
 
@@ -87,6 +106,11 @@ static int amd_check_rdpmc_early(struct kvm_vcpu *vcpu, unsigned int idx)
 static struct kvm_pmc *amd_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,
 	unsigned int idx, u64 *mask)
 {
+        /*
+	 * 在以下使用amd_pmu_get_pmc():
+         *   - arch/x86/kvm/svm/pmu.c|73| <<get_gp_pmc_amd>> return amd_pmu_get_pmc(pmu, idx);
+         *   - arch/x86/kvm/svm/pmu.c|90| <<amd_rdpmc_ecx_to_pmc>> return amd_pmu_get_pmc(vcpu_to_pmu(vcpu), idx);
+	 */
 	return amd_pmu_get_pmc(vcpu_to_pmu(vcpu), idx);
 }
 
diff --git a/arch/x86/kvm/svm/sev.c b/arch/x86/kvm/svm/sev.c
index 0835c664f..6320b3f7b 100644
--- a/arch/x86/kvm/svm/sev.c
+++ b/arch/x86/kvm/svm/sev.c
@@ -3978,6 +3978,14 @@ static void sev_snp_init_protected_guest_state(struct kvm_vcpu *vcpu)
 	 * When replacing the VMSA during SEV-SNP AP creation,
 	 * mark the VMCB dirty so that full state is always reloaded.
 	 */
+	/*
+	 * 在以下使用vmcb_mark_all_dirty():
+	 *   - arch/x86/kvm/svm/nested.c|1535| <<svm_leave_nested>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/sev.c|3981| <<sev_snp_init_protected_guest_state>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|1173| <<init_vmcb>> vmcb_mark_all_dirty(vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|3633| <<pre_svm_run>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|4835| <<svm_leave_smm>> vmcb_mark_all_dirty(svm->vmcb01.ptr);
+	 */
 	vmcb_mark_all_dirty(svm->vmcb);
 
 	if (!VALID_PAGE(svm->sev_es.snp_vmsa_gpa))
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 9d29b2e7e..67b4af242 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -215,6 +215,16 @@ int svm_set_efer(struct kvm_vcpu *vcpu, u64 efer)
 	if ((old_efer & EFER_SVME) != (efer & EFER_SVME)) {
 		if (!(efer & EFER_SVME)) {
 			svm_leave_nested(vcpu);
+			/*
+			 * 在以下使用svm_set_gif():
+			 *   - arch/x86/kvm/svm/nested.c|984| <<enter_svm_guest_mode>> svm_set_gif(svm, true);
+			 *   - arch/x86/kvm/svm/nested.c|1313| <<nested_svm_vmexit>> svm_set_gif(svm, false);
+			 *   - arch/x86/kvm/svm/nested.c|1984| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+			 *   - arch/x86/kvm/svm/nested.c|2040| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+			 *   - arch/x86/kvm/svm/svm.c|218| <<svm_set_efer>> svm_set_gif(svm, true);
+			 *   - arch/x86/kvm/svm/svm.c|2300| <<stgi_interception>> svm_set_gif(to_svm(vcpu), true);
+			 *   - arch/x86/kvm/svm/svm.c|2312| <<clgi_interception>> svm_set_gif(to_svm(vcpu), false);
+			 */
 			svm_set_gif(svm, true);
 			/* #GP intercept is still needed for vmware backdoor */
 			if (!enable_vmware_backdoor)
@@ -703,6 +713,12 @@ void *svm_alloc_permissions_map(unsigned long size, gfp_t gfp_mask)
 	return pm;
 }
 
+/*
+ * 在以下使用svm_recalc_lbr_msr_intercepts():
+ *   - arch/x86/kvm/svm/svm.c|757| <<svm_recalc_msr_intercepts>> svm_recalc_lbr_msr_intercepts(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|833| <<svm_enable_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|861| <<svm_update_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+ */
 static void svm_recalc_lbr_msr_intercepts(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -743,6 +759,12 @@ static void svm_recalc_msr_intercepts(struct kvm_vcpu *vcpu)
 	svm_disable_intercept_for_msr(vcpu, MSR_SYSCALL_MASK, MSR_TYPE_RW);
 #endif
 
+	/*
+	 * 在以下使用svm_recalc_lbr_msr_intercepts():
+	 *   - arch/x86/kvm/svm/svm.c|757| <<svm_recalc_msr_intercepts>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|833| <<svm_enable_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|861| <<svm_update_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 */
 	if (lbrv)
 		svm_recalc_lbr_msr_intercepts(vcpu);
 
@@ -820,6 +842,12 @@ static void __svm_enable_lbrv(struct kvm_vcpu *vcpu)
 void svm_enable_lbrv(struct kvm_vcpu *vcpu)
 {
 	__svm_enable_lbrv(vcpu);
+	/*
+	 * 在以下使用svm_recalc_lbr_msr_intercepts():
+	 *   - arch/x86/kvm/svm/svm.c|757| <<svm_recalc_msr_intercepts>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|833| <<svm_enable_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|861| <<svm_update_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 */
 	svm_recalc_lbr_msr_intercepts(vcpu);
 }
 
@@ -829,6 +857,12 @@ static void __svm_disable_lbrv(struct kvm_vcpu *vcpu)
 	to_svm(vcpu)->vmcb->control.virt_ext &= ~LBR_CTL_ENABLE_MASK;
 }
 
+/*
+ * 在以下使用svm_update_lbrv():
+ *   - arch/x86/kvm/svm/nested.c|707| <<nested_vmcb02_prepare_save>> svm_update_lbrv(&svm->vcpu);
+ *   - arch/x86/kvm/svm/nested.c|1329| <<nested_svm_vmexit>> svm_update_lbrv(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr(MSR_IA32_DEBUGCTLMSR)>> svm_update_lbrv(vcpu);
+ */
 void svm_update_lbrv(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -848,6 +882,12 @@ void svm_update_lbrv(struct kvm_vcpu *vcpu)
 	 * In this case, even though LBR_CTL does not need an update, intercepts
 	 * do, so always recalculate the intercepts here.
 	 */
+	/*
+	 * 在以下使用svm_recalc_lbr_msr_intercepts():
+	 *   - arch/x86/kvm/svm/svm.c|757| <<svm_recalc_msr_intercepts>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|833| <<svm_enable_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 *   - arch/x86/kvm/svm/svm.c|861| <<svm_update_lbrv>> svm_recalc_lbr_msr_intercepts(vcpu);
+	 */
 	svm_recalc_lbr_msr_intercepts(vcpu);
 }
 
@@ -1160,6 +1200,14 @@ static void init_vmcb(struct kvm_vcpu *vcpu, bool init_event)
 
 	kvm_make_request(KVM_REQ_RECALC_INTERCEPTS, vcpu);
 
+	/*
+	 * 在以下使用vmcb_mark_all_dirty():
+	 *   - arch/x86/kvm/svm/nested.c|1535| <<svm_leave_nested>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/sev.c|3981| <<sev_snp_init_protected_guest_state>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|1173| <<init_vmcb>> vmcb_mark_all_dirty(vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|3633| <<pre_svm_run>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|4835| <<svm_leave_smm>> vmcb_mark_all_dirty(svm->vmcb01.ptr);
+	 */
 	vmcb_mark_all_dirty(vmcb);
 
 	enable_gif(svm);
@@ -1173,6 +1221,21 @@ static void __svm_vcpu_reset(struct kvm_vcpu *vcpu)
 
 	if (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_STUFF_FEATURE_MSRS))
 		vcpu->arch.microcode_version = 0x01000065;
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
 
 	svm->nmi_masked = false;
@@ -1192,6 +1255,15 @@ static void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 		__svm_vcpu_reset(vcpu);
 }
 
+/*
+ * 在以下使用svm_switch_vmcb():
+ *   - arch/x86/kvm/svm/nested.c|916| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/nested.c|1162| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1329| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1359| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+ *   - arch/x86/kvm/svm/nested.c|1872| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+ *   - arch/x86/kvm/svm/svm.c|1234| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+ */
 void svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)
 {
 	svm->current_vmcb = target_vmcb;
@@ -1231,6 +1303,15 @@ static int svm_vcpu_create(struct kvm_vcpu *vcpu)
 
 	svm->vmcb01.ptr = page_address(vmcb01_page);
 	svm->vmcb01.pa = __sme_set(page_to_pfn(vmcb01_page) << PAGE_SHIFT);
+	/*
+	 * 在以下使用svm_switch_vmcb():
+	 *   - arch/x86/kvm/svm/nested.c|916| <<enter_svm_guest_mode>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/nested.c|1162| <<nested_svm_vmexit>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1329| <<svm_free_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1359| <<svm_leave_nested>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 *   - arch/x86/kvm/svm/nested.c|1872| <<svm_set_nested_state>> svm_switch_vmcb(svm, &svm->nested.vmcb02);
+	 *   - arch/x86/kvm/svm/svm.c|1234| <<svm_vcpu_create>> svm_switch_vmcb(svm, &svm->vmcb01);
+	 */
 	svm_switch_vmcb(svm, &svm->vmcb01);
 
 	svm->guest_state_loaded = false;
@@ -2151,6 +2232,10 @@ static int svm_instr_opcode(struct kvm_vcpu *vcpu)
 	return NONE_SVM_INSTR;
 }
 
+/*
+ * 在以下使用emulate_svm_instr():
+ *   - arch/x86/kvm/svm/svm.c|2219| <<gp_interception>> return emulate_svm_instr(vcpu, opcode);
+ */
 static int emulate_svm_instr(struct kvm_vcpu *vcpu, int opcode)
 {
 	const int guest_mode_exit_codes[] = {
@@ -2167,6 +2252,16 @@ static int emulate_svm_instr(struct kvm_vcpu *vcpu, int opcode)
 	int ret;
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_svm_simple_vmexit():
+		 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+		 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+		 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+		 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+		 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+		 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+		 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+		 */
 		/* Returns '1' or -errno on failure, '0' on success. */
 		ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
 		if (ret)
@@ -2224,6 +2319,16 @@ static int gp_interception(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用svm_set_gif():
+ *   - arch/x86/kvm/svm/nested.c|984| <<enter_svm_guest_mode>> svm_set_gif(svm, true);
+ *   - arch/x86/kvm/svm/nested.c|1313| <<nested_svm_vmexit>> svm_set_gif(svm, false);
+ *   - arch/x86/kvm/svm/nested.c|1984| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+ *   - arch/x86/kvm/svm/nested.c|2040| <<svm_set_nested_state>> svm_set_gif(svm, !!(kvm_state->flags & KVM_STATE_NESTED_GIF_SET));
+ *   - arch/x86/kvm/svm/svm.c|218| <<svm_set_efer>> svm_set_gif(svm, true);
+ *   - arch/x86/kvm/svm/svm.c|2300| <<stgi_interception>> svm_set_gif(to_svm(vcpu), true);
+ *   - arch/x86/kvm/svm/svm.c|2312| <<clgi_interception>> svm_set_gif(to_svm(vcpu), false);
+ */
 void svm_set_gif(struct vcpu_svm *svm, bool value)
 {
 	if (value) {
@@ -2999,6 +3104,12 @@ static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 
 		svm->vmcb->save.dbgctl = data;
 		vmcb_mark_dirty(svm->vmcb, VMCB_LBR);
+		/*
+		 * 在以下使用svm_update_lbrv():
+		 *   - arch/x86/kvm/svm/nested.c|707| <<nested_vmcb02_prepare_save>> svm_update_lbrv(&svm->vcpu);
+		 *   - arch/x86/kvm/svm/nested.c|1329| <<nested_svm_vmexit>> svm_update_lbrv(vcpu);
+		 *   - arch/x86/kvm/svm/svm.c|3069| <<svm_set_msr(MSR_IA32_DEBUGCTLMSR)>> svm_update_lbrv(vcpu);
+		 */
 		svm_update_lbrv(vcpu);
 		break;
 	case MSR_VM_HSAVE_PA:
@@ -3563,6 +3674,14 @@ static int pre_svm_run(struct kvm_vcpu *vcpu)
 	 */
 	if (unlikely(svm->current_vmcb->cpu != vcpu->cpu)) {
 		svm->current_vmcb->asid_generation = 0;
+		/*
+		 * 在以下使用vmcb_mark_all_dirty():
+		 *   - arch/x86/kvm/svm/nested.c|1535| <<svm_leave_nested>> vmcb_mark_all_dirty(svm->vmcb);
+		 *   - arch/x86/kvm/svm/sev.c|3981| <<sev_snp_init_protected_guest_state>> vmcb_mark_all_dirty(svm->vmcb);
+		 *   - arch/x86/kvm/svm/svm.c|1173| <<init_vmcb>> vmcb_mark_all_dirty(vmcb);
+		 *   - arch/x86/kvm/svm/svm.c|3633| <<pre_svm_run>> vmcb_mark_all_dirty(svm->vmcb);
+		 *   - arch/x86/kvm/svm/svm.c|4835| <<svm_leave_smm>> vmcb_mark_all_dirty(svm->vmcb01.ptr);
+		 */
 		vmcb_mark_all_dirty(svm->vmcb);
 		svm->current_vmcb->cpu = vcpu->cpu;
         }
@@ -4684,6 +4803,16 @@ static int svm_enter_smm(struct kvm_vcpu *vcpu, union kvm_smram *smram)
 	svm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];
 	svm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];
 
+	/*
+	 * 在以下使用nested_svm_simple_vmexit():
+	 *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+	 *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+	 *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+	 *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+	 *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+	 *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+	 *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+	 */
 	ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
 	if (ret)
 		return ret;
@@ -4705,6 +4834,12 @@ static int svm_enter_smm(struct kvm_vcpu *vcpu, union kvm_smram *smram)
 
 	BUILD_BUG_ON(offsetof(struct vmcb, save) != 0x400);
 
+	/*
+	 * 在以下使用svm_copy_vmrun_state():
+	 *   - arch/x86/kvm/svm/nested.c|2134| <<svm_set_nested_state>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
+	 *   - arch/x86/kvm/svm/svm.c|4837| <<svm_enter_smm>> svm_copy_vmrun_state(map_save.hva + 0x400,
+	 *   - arch/x86/kvm/svm/svm.c|4881| <<svm_leave_smm>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, map_save.hva + 0x400);
+	 */
 	svm_copy_vmrun_state(map_save.hva + 0x400,
 			     &svm->vmcb01.ptr->save);
 
@@ -4749,17 +4884,36 @@ static int svm_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)
 	 * used during SMM (see svm_enter_smm())
 	 */
 
+	/*
+	 * 在以下使用svm_copy_vmrun_state():
+	 *   - arch/x86/kvm/svm/nested.c|2134| <<svm_set_nested_state>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, save);
+	 *   - arch/x86/kvm/svm/svm.c|4837| <<svm_enter_smm>> svm_copy_vmrun_state(map_save.hva + 0x400,
+	 *   - arch/x86/kvm/svm/svm.c|4881| <<svm_leave_smm>> svm_copy_vmrun_state(&svm->vmcb01.ptr->save, map_save.hva + 0x400);
+	 */
 	svm_copy_vmrun_state(&svm->vmcb01.ptr->save, map_save.hva + 0x400);
 
 	/*
 	 * Enter the nested guest now
 	 */
 
+	/*
+	 * 在以下使用vmcb_mark_all_dirty():
+	 *   - arch/x86/kvm/svm/nested.c|1535| <<svm_leave_nested>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/sev.c|3981| <<sev_snp_init_protected_guest_state>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|1173| <<init_vmcb>> vmcb_mark_all_dirty(vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|3633| <<pre_svm_run>> vmcb_mark_all_dirty(svm->vmcb);
+	 *   - arch/x86/kvm/svm/svm.c|4835| <<svm_leave_smm>> vmcb_mark_all_dirty(svm->vmcb01.ptr);
+	 */
 	vmcb_mark_all_dirty(svm->vmcb01.ptr);
 
 	vmcb12 = map.hva;
 	nested_copy_vmcb_control_to_cache(svm, &vmcb12->control);
 	nested_copy_vmcb_save_to_cache(svm, &vmcb12->save);
+	/*
+	 * 在以下使用enter_svm_guest_mode():
+	 *   - arch/x86/kvm/svm/nested.c|1007| <<nested_svm_vmrun>> if (enter_svm_guest_mode(vcpu, vmcb12_gpa, vmcb12, true))
+	 *   - arch/x86/kvm/svm/svm.c|4763| <<svm_leave_smm>> ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
+	 */
 	ret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);
 
 	if (ret)
@@ -5298,6 +5452,13 @@ static __init int svm_hardware_setup(void)
 			kvm_caps.has_tsc_control = true;
 		}
 	}
+	/*
+	 * 在以下使用kvm_caps.max_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/svm/svm.c|5455| <<svm_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;
+	 *   - arch/x86/kvm/x86.c|2748| <<set_tsc_khz>> if (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/x86.c|12116| <<kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+	 */
 	kvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;
 	kvm_caps.tsc_scaling_ratio_frac_bits = 32;
 
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index dd78e6402..ad10649f6 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -401,11 +401,23 @@ static inline bool ghcb_gpa_is_registered(struct vcpu_svm *svm, u64 val)
 	return svm->sev_es.ghcb_registered_gpa == val;
 }
 
+/*
+ * 在以下使用vmcb_mark_all_dirty():
+ *   - arch/x86/kvm/svm/nested.c|1535| <<svm_leave_nested>> vmcb_mark_all_dirty(svm->vmcb);
+ *   - arch/x86/kvm/svm/sev.c|3981| <<sev_snp_init_protected_guest_state>> vmcb_mark_all_dirty(svm->vmcb);
+ *   - arch/x86/kvm/svm/svm.c|1173| <<init_vmcb>> vmcb_mark_all_dirty(vmcb);
+ *   - arch/x86/kvm/svm/svm.c|3633| <<pre_svm_run>> vmcb_mark_all_dirty(svm->vmcb);
+ *   - arch/x86/kvm/svm/svm.c|4835| <<svm_leave_smm>> vmcb_mark_all_dirty(svm->vmcb01.ptr);
+ */
 static inline void vmcb_mark_all_dirty(struct vmcb *vmcb)
 {
 	vmcb->control.clean = 0;
 }
 
+/*
+ * 在以下使用vmcb_mark_all_clean():
+ *   - arch/x86/kvm/svm/svm.c|4387| <<svm_vcpu_run>> vmcb_mark_all_clean(svm->vmcb);
+ */
 static inline void vmcb_mark_all_clean(struct vmcb *vmcb)
 {
 	vmcb->control.clean = VMCB_ALL_CLEAN_MASK
@@ -762,11 +774,30 @@ void svm_copy_vmrun_state(struct vmcb_save_area *to_save,
 void svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb);
 int nested_svm_vmexit(struct vcpu_svm *svm);
 
+/*
+ * 在以下使用nested_svm_simple_vmexit():
+ *   - arch/x86/kvm/svm/nested.c|1295| <<nested_svm_triple_fault>> nested_svm_simple_vmexit(to_svm(vcpu), SVM_EXIT_SHUTDOWN);
+ *   - arch/x86/kvm/svm/nested.c|1566| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INIT);
+ *   - arch/x86/kvm/svm/nested.c|1589| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
+ *   - arch/x86/kvm/svm/nested.c|1599| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_NMI);
+ *   - arch/x86/kvm/svm/nested.c|1609| <<svm_check_nested_events>> nested_svm_simple_vmexit(svm, SVM_EXIT_INTR);
+ *   - arch/x86/kvm/svm/svm.c|2171| <<emulate_svm_instr>> ret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);
+ *   - arch/x86/kvm/svm/svm.c|4687| <<svm_enter_smm>> ret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);
+ */
 static inline int nested_svm_simple_vmexit(struct vcpu_svm *svm, u32 exit_code)
 {
 	svm->vmcb->control.exit_code   = exit_code;
 	svm->vmcb->control.exit_info_1 = 0;
 	svm->vmcb->control.exit_info_2 = 0;
+	/*
+	 * 在以下使用nested_svm_vmexit():
+	 *   - arch/x86/kvm/svm/hyperv.c|17| <<svm_hv_inject_synthetic_vmexit_post_tlb_flush>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|56| <<nested_svm_inject_npf_exit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1023| <<nested_svm_vmrun>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1472| <<nested_svm_exit_handled>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/nested.c|1533| <<nested_svm_inject_exception_vmexit>> nested_svm_vmexit(svm);
+	 *   - arch/x86/kvm/svm/svm.h|770| <<nested_svm_simple_vmexit>> return nested_svm_vmexit(svm);
+	 */
 	return nested_svm_vmexit(svm);
 }
 
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index bcea087b6..a20697110 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -302,6 +302,13 @@ static void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)
 	cpu = get_cpu();
 	prev = vmx->loaded_vmcs;
 	vmx->loaded_vmcs = vmcs;
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|305| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4663| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4668| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1475| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 	vmx_sync_vmcs_host_state(vmx, prev);
 	put_cpu();
@@ -2407,6 +2414,13 @@ static void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs0
 	exec_control &= ~CPU_BASED_TPR_SHADOW;
 	exec_control |= vmcs12->cpu_based_vm_exec_control;
 
+	/*
+	 * 在以下使用nested_vmx->l1_tpr_threshold:
+	 *   - arch/x86/kvm/vmx/nested.c|2410| <<prepare_vmcs02_early>> vmx->nested.l1_tpr_threshold = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|5197| <<__nested_vmx_vmexit>> if (vmx->nested.l1_tpr_threshold != -1)
+	 *   - arch/x86/kvm/vmx/nested.c|5198| <<__nested_vmx_vmexit>> vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
+	 *   - arch/x86/kvm/vmx/vmx.c|6758| <<vmx_update_cr8_intercept>> to_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;
+	 */
 	vmx->nested.l1_tpr_threshold = -1;
 	if (exec_control & CPU_BASED_TPR_SHADOW)
 		vmcs_write32(TPR_THRESHOLD, vmcs12->tpr_threshold);
@@ -4660,6 +4674,13 @@ static void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,
 
 	cpu = get_cpu();
 	vmx->loaded_vmcs = &vmx->nested.vmcs02;
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|305| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4663| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4668| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1475| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 
 	sync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);
@@ -5194,14 +5215,33 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	if (kvm_caps.has_tsc_control)
 		vmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);
 
+	/*
+	 * 在以下使用nested_vmx->l1_tpr_threshold:
+	 *   - arch/x86/kvm/vmx/nested.c|2410| <<prepare_vmcs02_early>> vmx->nested.l1_tpr_threshold = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|5197| <<__nested_vmx_vmexit>> if (vmx->nested.l1_tpr_threshold != -1)
+	 *   - arch/x86/kvm/vmx/nested.c|5198| <<__nested_vmx_vmexit>> vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
+	 *   - arch/x86/kvm/vmx/vmx.c|6758| <<vmx_update_cr8_intercept>> to_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;
+	 */
 	if (vmx->nested.l1_tpr_threshold != -1)
 		vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
 
+	/*
+	 * 在以下使用nested_vmx->change_vmcs01_virtual_apic_mode:
+	 *   - arch/x86/kvm/vmx/nested.c|5200| <<__nested_vmx_vmexit>> if (vmx->nested.change_vmcs01_virtual_apic_mode) {
+	 *   - arch/x86/kvm/vmx/nested.c|5201| <<__nested_vmx_vmexit>> vmx->nested.change_vmcs01_virtual_apic_mode = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6777| <<vmx_set_virtual_apic_mode>> vmx->nested.change_vmcs01_virtual_apic_mode = true;
+	 */
 	if (vmx->nested.change_vmcs01_virtual_apic_mode) {
 		vmx->nested.change_vmcs01_virtual_apic_mode = false;
 		vmx_set_virtual_apic_mode(vcpu);
 	}
 
+	/*
+	 * 在以下使用update_vmcs01_cpu_dirty_logging:
+	 *   - arch/x86/kvm/vmx/nested.c|5205| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_cpu_dirty_logging) {
+	 *   - arch/x86/kvm/vmx/nested.c|5206| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_cpu_dirty_logging = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|8241| <<vmx_update_cpu_dirty_logging>> vmx->nested.update_vmcs01_cpu_dirty_logging = true;
+	 */
 	if (vmx->nested.update_vmcs01_cpu_dirty_logging) {
 		vmx->nested.update_vmcs01_cpu_dirty_logging = false;
 		vmx_update_cpu_dirty_logging(vcpu);
@@ -5209,16 +5249,34 @@ void __nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 
 	nested_put_vmcs12_pages(vcpu);
 
+	/*
+	 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page():
+	 *   - arch/x86/kvm/vmx/nested.c|5218| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5219| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6830| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	if (vmx->nested.reload_vmcs01_apic_access_page) {
 		vmx->nested.reload_vmcs01_apic_access_page = false;
 		kvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_apicv_status:
+	 *   - arch/x86/kvm/vmx/nested.c|5223| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+	 *   - arch/x86/kvm/vmx/nested.c|5224| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4418| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+	 */
 	if (vmx->nested.update_vmcs01_apicv_status) {
 		vmx->nested.update_vmcs01_apicv_status = false;
 		kvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);
 	}
 
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+	 *   - arch/x86/kvm/vmx/nested.c|5228| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5229| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6910| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+	 */
 	if (vmx->nested.update_vmcs01_hwapic_isr) {
 		vmx->nested.update_vmcs01_hwapic_isr = false;
 		kvm_apic_update_hwapic_isr(vcpu);
diff --git a/arch/x86/kvm/vmx/posted_intr.c b/arch/x86/kvm/vmx/posted_intr.c
index 4a6d9a17d..a16d364ac 100644
--- a/arch/x86/kvm/vmx/posted_intr.c
+++ b/arch/x86/kvm/vmx/posted_intr.c
@@ -14,6 +14,53 @@
 #include "vmx.h"
 #include "tdx.h"
 
+/*
+ * 在以下使用irq_bypass_producer->add_consumer:
+ *   - virt/lib/irqbypass.c|40| <<__connect>> if (prod->add_consumer)
+ *   - virt/lib/irqbypass.c|41| <<__connect>> ret = prod->add_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->del_consumer:
+ *   - virt/lib/irqbypass.c|45| <<__connect>> if (ret && prod->del_consumer)
+ *   - virt/lib/irqbypass.c|46| <<__connect>> prod->del_consumer(prod, cons);
+ *   - virt/lib/irqbypass.c|72| <<__disconnect>> if (prod->del_consumer)
+ *   - virt/lib/irqbypass.c|73| <<__disconnect>> prod->del_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->stop:
+ *   - virt/lib/irqbypass.c|35| <<__connect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|36| <<__connect>> prod->stop(prod);
+ *   - virt/lib/irqbypass.c|65| <<__disconnect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|66| <<__disconnect>> prod->stop(prod);
+ *
+ * 在以下使用irq_bypass_producer->start:
+ *   - virt/lib/irqbypass.c|51| <<__connect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|52| <<__connect>> prod->start(prod);
+ *   - virt/lib/irqbypass.c|77| <<__disconnect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|78| <<__disconnect>> prod->start(prod);
+ *
+ *
+ * 在以下使用irq_bypass_consumer->add_producer:
+ *   - virt/kvm/eventfd.c|478| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ *   - virt/lib/irqbypass.c|44| <<__connect>> ret = cons->add_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->del_producer:
+ *   - virt/kvm/eventfd.c|479| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ *   - virt/lib/irqbypass.c|70| <<__disconnect>> cons->del_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->stop:
+ *   - virt/lib/irqbypass.c|37| <<__connect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|38| <<__connect>> cons->stop(cons);
+ *   - virt/lib/irqbypass.c|67| <<__disconnect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|68| <<__disconnect>> cons->stop(cons);
+ *
+ * 在以下使用irq_bypass_consumer->start:
+ *   - virt/lib/irqbypass.c|49| <<__connect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|50| <<__connect>> cons->start(cons);
+ *   - virt/lib/irqbypass.c|75| <<__disconnect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|76| <<__disconnect>> cons->start(cons);
+ */
+
 /*
  * Maintain a per-CPU list of vCPUs that need to be awakened by wakeup_handler()
  * when a WAKEUP_VECTOR interrupted is posted.  vCPUs are added to the list when
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 91b6f2f3e..d165da53d 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1407,6 +1407,13 @@ static void shrink_ple_window(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用vmx_vcpu_load_vmcs():
+ *   - arch/x86/kvm/vmx/nested.c|305| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/nested.c|4663| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/nested.c|4668| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ *   - arch/x86/kvm/vmx/vmx.c|1475| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+ */
 void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -1472,6 +1479,13 @@ void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (vcpu->scheduled_out && !kvm_pause_in_guest(vcpu->kvm))
 		shrink_ple_window(vcpu);
 
+	/*
+	 * 在以下使用vmx_vcpu_load_vmcs():
+	 *   - arch/x86/kvm/vmx/nested.c|305| <<vmx_switch_vmcs>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4663| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/nested.c|4668| <<copy_vmcs02_to_vmcs12_rare>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|1475| <<vmx_vcpu_load>> vmx_vcpu_load_vmcs(vcpu, cpu);
+	 */
 	vmx_vcpu_load_vmcs(vcpu, cpu);
 
 	vmx_vcpu_pi_load(vcpu, cpu);
@@ -1902,6 +1916,21 @@ u64 vmx_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu)
 	    nested_cpu_has2(vmcs12, SECONDARY_EXEC_TSC_SCALING))
 		return vmcs12->tsc_multiplier;
 
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	return kvm_caps.default_tsc_scaling_ratio;
 }
 
@@ -4415,6 +4444,12 @@ void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->update_vmcs01_apicv_status:
+		 *   - arch/x86/kvm/vmx/nested.c|5223| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+		 *   - arch/x86/kvm/vmx/nested.c|5224| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|4418| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+		 */
 		vmx->nested.update_vmcs01_apicv_status = true;
 		return;
 	}
@@ -4597,6 +4632,18 @@ static u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)
 	*/
 	exec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;
 
+	/*
+	 * 在以下使用kvm->nr_memslots_dirty_logging:
+	 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+	 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 */
 	/*
 	 * PML is enabled/disabled when dirty logging of memsmlots changes, but
 	 * it needs to be set here when dirty logging is already active, e.g.
@@ -6742,6 +6789,13 @@ void vmx_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
 		return;
 
 	tpr_threshold = (irr == -1 || tpr < irr) ? 0 : irr;
+	/*
+	 * 在以下使用nested_vmx->l1_tpr_threshold
+	 *   - arch/x86/kvm/vmx/nested.c|2410| <<prepare_vmcs02_early>> vmx->nested.l1_tpr_threshold = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|5197| <<__nested_vmx_vmexit>> if (vmx->nested.l1_tpr_threshold != -1)
+	 *   - arch/x86/kvm/vmx/nested.c|5198| <<__nested_vmx_vmexit>> vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
+	 *   - arch/x86/kvm/vmx/vmx.c|6758| <<vmx_update_cr8_intercept>> to_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;
+	 */
 	if (is_guest_mode(vcpu))
 		to_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;
 	else
@@ -6762,6 +6816,12 @@ void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)
 
 	/* Postpone execution until vmcs01 is the current VMCS. */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->change_vmcs01_virtual_apic_mode:
+		 *   - arch/x86/kvm/vmx/nested.c|5200| <<__nested_vmx_vmexit>> if (vmx->nested.change_vmcs01_virtual_apic_mode) {
+		 *   - arch/x86/kvm/vmx/nested.c|5201| <<__nested_vmx_vmexit>> vmx->nested.change_vmcs01_virtual_apic_mode = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6777| <<vmx_set_virtual_apic_mode>> vmx->nested.change_vmcs01_virtual_apic_mode = true;
+		 */
 		vmx->nested.change_vmcs01_virtual_apic_mode = true;
 		return;
 	}
@@ -6815,6 +6875,12 @@ void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)
 
 	/* Defer reload until vmcs01 is the current VMCS. */
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page():
+		 *   - arch/x86/kvm/vmx/nested.c|5218| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+		 *   - arch/x86/kvm/vmx/nested.c|5219| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6830| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+		 */
 		to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
 		return;
 	}
@@ -6895,6 +6961,12 @@ void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)
 		 */
 		WARN_ON_ONCE(vcpu->wants_to_run &&
 			     nested_cpu_has_vid(get_vmcs12(vcpu)));
+		/*
+		 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+		 *   - arch/x86/kvm/vmx/nested.c|5228| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+		 *   - arch/x86/kvm/vmx/nested.c|5229| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|6910| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+		 */
 		to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
 		return;
 	}
@@ -8195,6 +8267,26 @@ int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,
 	tscl = rdtsc();
 	guest_tscl = kvm_read_l1_tsc(vcpu, tscl);
 	delta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;
+	/*
+	 * 在以下使用nsec_to_cycles():
+	 *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+	 *              nsec_to_cycles(vcpu, timer_advance_ns)));
+	 *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+	 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+	 *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+	 *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+	 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+	 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+	 *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+	 *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+	 *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+	 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+	 */
 	lapic_timer_advance_cycles = nsec_to_cycles(vcpu,
 						    ktimer->timer_advance_ns);
 
@@ -8203,6 +8295,21 @@ int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,
 	else
 		delta_tsc = 0;
 
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	/* Convert to host delta tsc if tsc scaling is enabled */
 	if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
 	    delta_tsc && u64_shl_div_u64(delta_tsc,
@@ -8238,10 +8345,28 @@ void vmx_update_cpu_dirty_logging(struct kvm_vcpu *vcpu)
 		return;
 
 	if (is_guest_mode(vcpu)) {
+		/*
+		 * 在以下使用update_vmcs01_cpu_dirty_logging:
+		 *   - arch/x86/kvm/vmx/nested.c|5205| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_cpu_dirty_logging) {
+		 *   - arch/x86/kvm/vmx/nested.c|5206| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_cpu_dirty_logging = false;
+		 *   - arch/x86/kvm/vmx/vmx.c|8241| <<vmx_update_cpu_dirty_logging>> vmx->nested.update_vmcs01_cpu_dirty_logging = true;
+		 */
 		vmx->nested.update_vmcs01_cpu_dirty_logging = true;
 		return;
 	}
 
+	/*
+	 * 在以下使用kvm->nr_memslots_dirty_logging:
+	 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+	 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 */
 	/*
 	 * Note, nr_memslots_dirty_logging can be changed concurrent with this
 	 * code, but in that case another update request will be made and so
@@ -8562,6 +8687,13 @@ __init int vmx_hardware_setup(void)
 	if (cpu_has_vmx_tsc_scaling())
 		kvm_caps.has_tsc_control = true;
 
+	/*
+	 * 在以下使用kvm_caps.max_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/svm/svm.c|5455| <<svm_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;
+	 *   - arch/x86/kvm/x86.c|2748| <<set_tsc_khz>> if (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/x86.c|12116| <<kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+	 */
 	kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;
 	kvm_caps.tsc_scaling_ratio_frac_bits = 48;
 	kvm_caps.has_bus_lock_exit = cpu_has_vmx_bus_lock_detection();
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index ea9312102..68186c1b2 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -131,10 +131,40 @@ struct nested_vmx {
 	 */
 	bool vmcs02_initialized;
 
+	/*
+	 * 在以下使用nested_vmx->change_vmcs01_virtual_apic_mode:
+	 *   - arch/x86/kvm/vmx/nested.c|5200| <<__nested_vmx_vmexit>> if (vmx->nested.change_vmcs01_virtual_apic_mode) {
+	 *   - arch/x86/kvm/vmx/nested.c|5201| <<__nested_vmx_vmexit>> vmx->nested.change_vmcs01_virtual_apic_mode = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6777| <<vmx_set_virtual_apic_mode>> vmx->nested.change_vmcs01_virtual_apic_mode = true;
+	 */
 	bool change_vmcs01_virtual_apic_mode;
+	/*
+	 * 在以下使用nested_vmx->reload_vmcs01_apic_access_page():
+	 *   - arch/x86/kvm/vmx/nested.c|5218| <<__nested_vmx_vmexit>> if (vmx->nested.reload_vmcs01_apic_access_page) {
+	 *   - arch/x86/kvm/vmx/nested.c|5219| <<__nested_vmx_vmexit>> vmx->nested.reload_vmcs01_apic_access_page = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6830| <<vmx_set_apic_access_page_addr>> to_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;
+	 */
 	bool reload_vmcs01_apic_access_page;
+	/*
+	 * 在以下使用update_vmcs01_cpu_dirty_logging:
+	 *   - arch/x86/kvm/vmx/nested.c|5205| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_cpu_dirty_logging) {
+	 *   - arch/x86/kvm/vmx/nested.c|5206| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_cpu_dirty_logging = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|8241| <<vmx_update_cpu_dirty_logging>> vmx->nested.update_vmcs01_cpu_dirty_logging = true;
+	 */
 	bool update_vmcs01_cpu_dirty_logging;
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_apicv_status:
+	 *   - arch/x86/kvm/vmx/nested.c|5223| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_apicv_status) {
+	 *   - arch/x86/kvm/vmx/nested.c|5224| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_apicv_status = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|4418| <<vmx_refresh_apicv_exec_ctrl>> vmx->nested.update_vmcs01_apicv_status = true;
+	 */
 	bool update_vmcs01_apicv_status;
+	/*
+	 * 在以下使用nested_vmx->update_vmcs01_hwapic_isr:
+	 *   - arch/x86/kvm/vmx/nested.c|5228| <<__nested_vmx_vmexit>> if (vmx->nested.update_vmcs01_hwapic_isr) {
+	 *   - arch/x86/kvm/vmx/nested.c|5229| <<__nested_vmx_vmexit>> vmx->nested.update_vmcs01_hwapic_isr = false;
+	 *   - arch/x86/kvm/vmx/vmx.c|6910| <<vmx_hwapic_isr_update>> to_vmx(vcpu)->nested.update_vmcs01_hwapic_isr = true;
+	 */
 	bool update_vmcs01_hwapic_isr;
 
 	/*
@@ -185,6 +215,13 @@ struct nested_vmx {
 	u64 pre_vmenter_ssp;
 	u64 pre_vmenter_ssp_tbl;
 
+	/*
+	 * 在以下使用nested_vmx->l1_tpr_threshold:
+	 *   - arch/x86/kvm/vmx/nested.c|2410| <<prepare_vmcs02_early>> vmx->nested.l1_tpr_threshold = -1;
+	 *   - arch/x86/kvm/vmx/nested.c|5197| <<__nested_vmx_vmexit>> if (vmx->nested.l1_tpr_threshold != -1)
+	 *   - arch/x86/kvm/vmx/nested.c|5198| <<__nested_vmx_vmexit>> vmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);
+	 *   - arch/x86/kvm/vmx/vmx.c|6758| <<vmx_update_cr8_intercept>> to_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;
+	 */
 	/* to migrate it to L1 if L2 writes to L1's CR8 directly */
 	int l1_tpr_threshold;
 
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c9c2aa6f4..d96dde3ef 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -326,6 +326,13 @@ static struct kmem_cache *x86_emulator_cache;
  * msrs_to_save and emulated_msrs.
  */
 
+/*
+ * 在以下使用msrs_to_save_base[]
+ *   - arch/x86/kvm/x86.c|329| <<global>> static const u32 msrs_to_save_base[] = {
+ *   - arch/x86/kvm/x86.c|385| <<global>> static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_base) +
+ *   - arch/x86/kvm/x86.c|9017| <<kvm_init_msr_lists>> for (i = 0; i < ARRAY_SIZE(msrs_to_save_base); i++)
+ *   - arch/x86/kvm/x86.c|9018| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_base[i]);
+ */
 static const u32 msrs_to_save_base[] = {
 	MSR_IA32_SYSENTER_CS, MSR_IA32_SYSENTER_ESP, MSR_IA32_SYSENTER_EIP,
 	MSR_STAR,
@@ -382,6 +389,13 @@ static const u32 msrs_to_save_pmu[] = {
 	MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_SET,
 };
 
+/*
+ * 在以下使用msrs_to_save_base[]
+ *   - arch/x86/kvm/x86.c|329| <<global>> static const u32 msrs_to_save_base[] = {
+ *   - arch/x86/kvm/x86.c|385| <<global>> static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_base) +
+ *   - arch/x86/kvm/x86.c|9017| <<kvm_init_msr_lists>> for (i = 0; i < ARRAY_SIZE(msrs_to_save_base); i++)
+ *   - arch/x86/kvm/x86.c|9018| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_base[i]);
+ */
 static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_base) +
 			ARRAY_SIZE(msrs_to_save_pmu)];
 static unsigned num_msrs_to_save;
@@ -493,6 +507,10 @@ static bool kvm_is_immutable_feature_msr(u32 msr)
 	return false;
 }
 
+/*
+ * 在以下使用kvm_is_advertised_msr():
+ *   - arch/x86/kvm/x86.c|552| <<kvm_do_msr_access>> if (host_initiated && !*data && kvm_is_advertised_msr(msr))
+ */
 static bool kvm_is_advertised_msr(u32 msr_index)
 {
 	unsigned int i;
@@ -513,6 +531,15 @@ static bool kvm_is_advertised_msr(u32 msr_index)
 typedef int (*msr_access_t)(struct kvm_vcpu *vcpu, u32 index, u64 *data,
 			    bool host_initiated);
 
+/*
+ * 在以下使用kvm_do_msr_access():
+ *   - arch/x86/kvm/x86.c|1720| <<do_get_feature_msr>> return kvm_do_msr_access(vcpu,
+ *              index, data, true, MSR_TYPE_R, kvm_get_feature_msr);
+ *   - arch/x86/kvm/x86.c|1955| <<kvm_set_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+ *              index, &data, host_initiated, MSR_TYPE_W, _kvm_set_msr);
+ *   - arch/x86/kvm/x86.c|2019| <<kvm_get_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+ *              index, data, host_initiated, MSR_TYPE_R, __kvm_get_msr);
+ */
 static __always_inline int kvm_do_msr_access(struct kvm_vcpu *vcpu, u32 msr,
 					     u64 *data, bool host_initiated,
 					     enum kvm_msr_access rw,
@@ -823,6 +850,12 @@ void kvm_deliver_exception_payload(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(kvm_deliver_exception_payload);
 
+/*
+ * 在以下使用kvm_queue_exception_vmexit():
+ *   - arch/x86/kvm/x86.c|883| <<kvm_multiple_exception>> kvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,
+ *   - arch/x86/kvm/x86.c|1011| <<kvm_inject_page_fault>> kvm_queue_exception_vmexit(vcpu, PF_VECTOR,
+ *   - arch/x86/kvm/x86.c|13411| <<kvm_arch_vcpu_ioctl_run>> kvm_queue_exception_vmexit(vcpu, ex->vector,
+ */
 static void kvm_queue_exception_vmexit(struct kvm_vcpu *vcpu, unsigned int vector,
 				       bool has_error_code, u32 error_code,
 				       bool has_payload, unsigned long payload)
@@ -853,6 +886,12 @@ static void kvm_multiple_exception(struct kvm_vcpu *vcpu, unsigned int nr,
 	 */
 	if (is_guest_mode(vcpu) &&
 	    kvm_x86_ops.nested_ops->is_exception_vmexit(vcpu, nr, error_code)) {
+		/*
+		 * 在以下使用kvm_queue_exception_vmexit():
+		 *   - arch/x86/kvm/x86.c|883| <<kvm_multiple_exception>> kvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,
+		 *   - arch/x86/kvm/x86.c|1011| <<kvm_inject_page_fault>> kvm_queue_exception_vmexit(vcpu, PF_VECTOR,
+		 *   - arch/x86/kvm/x86.c|13411| <<kvm_arch_vcpu_ioctl_run>> kvm_queue_exception_vmexit(vcpu, ex->vector,
+		 */
 		kvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,
 					   has_payload, payload);
 		return;
@@ -972,10 +1011,23 @@ static int complete_emulated_insn_gp(struct kvm_vcpu *vcpu, int err)
 				       EMULTYPE_COMPLETE_USER_EXIT);
 }
 
+/*
+ * 在以下使用kvm_inject_page_fault():
+ *   - arch/x86/kvm/mmu/mmu.c|5765| <<init_kvm_tdp_mmu>> context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5914| <<init_kvm_softmmu>> context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/mmu/mmu.c|5928| <<init_kvm_nested_mmu>> g_context->inject_page_fault = kvm_inject_page_fault;
+ *   - arch/x86/kvm/x86.c|15741| <<kvm_arch_async_page_not_present>> kvm_inject_page_fault(vcpu, &fault);
+ */
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 {
 	++vcpu->stat.pf_guest;
 
+	/*
+	 * 在以下使用kvm_queue_exception_vmexit():
+	 *   - arch/x86/kvm/x86.c|883| <<kvm_multiple_exception>> kvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,
+	 *   - arch/x86/kvm/x86.c|1011| <<kvm_inject_page_fault>> kvm_queue_exception_vmexit(vcpu, PF_VECTOR,
+	 *   - arch/x86/kvm/x86.c|13411| <<kvm_arch_vcpu_ioctl_run>> kvm_queue_exception_vmexit(vcpu, ex->vector,
+	 */
 	/*
 	 * Async #PF in L2 is always forwarded to L1 as a VM-Exit regardless of
 	 * whether or not L1 wants to intercept "regular" #PF.
@@ -1717,6 +1769,15 @@ static int kvm_get_feature_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,
 
 static int do_get_feature_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
+	/*
+	 *  在以下使用kvm_do_msr_access():
+	 *   - arch/x86/kvm/x86.c|1720| <<do_get_feature_msr>> return kvm_do_msr_access(vcpu,
+	 *              index, data, true, MSR_TYPE_R, kvm_get_feature_msr);
+	 *   - arch/x86/kvm/x86.c|1955| <<kvm_set_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+	 *              index, &data, host_initiated, MSR_TYPE_W, _kvm_set_msr);
+	 *   - arch/x86/kvm/x86.c|2019| <<kvm_get_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+	 *              index, data, host_initiated, MSR_TYPE_R, __kvm_get_msr);
+	 */
 	return kvm_do_msr_access(vcpu, index, data, true, MSR_TYPE_R,
 				 kvm_get_feature_msr);
 }
@@ -1952,6 +2013,15 @@ static int _kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data,
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
+	/*
+	 *  在以下使用kvm_do_msr_access():
+	 *   - arch/x86/kvm/x86.c|1720| <<do_get_feature_msr>> return kvm_do_msr_access(vcpu,
+	 *              index, data, true, MSR_TYPE_R, kvm_get_feature_msr);
+	 *   - arch/x86/kvm/x86.c|1955| <<kvm_set_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+	 *              index, &data, host_initiated, MSR_TYPE_W, _kvm_set_msr);
+	 *   - arch/x86/kvm/x86.c|2019| <<kvm_get_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+	 *              index, data, host_initiated, MSR_TYPE_R, __kvm_get_msr);
+	 */
 	return kvm_do_msr_access(vcpu, index, &data, host_initiated, MSR_TYPE_W,
 				 _kvm_set_msr);
 }
@@ -2013,15 +2083,34 @@ int kvm_msr_read(struct kvm_vcpu *vcpu, u32 index, u64 *data)
 	return __kvm_get_msr(vcpu, index, data, true);
 }
 
+/*
+ * 在以下使用kvm_get_msr_ignored_check():
+ *   - arch/x86/kvm/x86.c|2031| <<__kvm_emulate_msr_read>> return kvm_get_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|2319| <<do_get_msr>> return kvm_get_msr_ignored_check(vcpu, index, data, true);
+ */
 static int kvm_get_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 *data, bool host_initiated)
 {
+	/*
+	 *  在以下使用kvm_do_msr_access():
+	 *   - arch/x86/kvm/x86.c|1720| <<do_get_feature_msr>> return kvm_do_msr_access(vcpu,
+	 *              index, data, true, MSR_TYPE_R, kvm_get_feature_msr);
+	 *   - arch/x86/kvm/x86.c|1955| <<kvm_set_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+	 *              index, &data, host_initiated, MSR_TYPE_W, _kvm_set_msr);
+	 *   - arch/x86/kvm/x86.c|2019| <<kvm_get_msr_ignored_check>> return kvm_do_msr_access(vcpu,
+	 *              index, data, host_initiated, MSR_TYPE_R, __kvm_get_msr);
+	 */
 	return kvm_do_msr_access(vcpu, index, data, host_initiated, MSR_TYPE_R,
 				 __kvm_get_msr);
 }
 
 int __kvm_emulate_msr_read(struct kvm_vcpu *vcpu, u32 index, u64 *data)
 {
+	/*
+	 * 在以下使用kvm_get_msr_ignored_check():
+	 *   - arch/x86/kvm/x86.c|2031| <<__kvm_emulate_msr_read>> return kvm_get_msr_ignored_check(vcpu, index, data, false);
+	 *   - arch/x86/kvm/x86.c|2319| <<do_get_msr>> return kvm_get_msr_ignored_check(vcpu, index, data, true);
+	 */
 	return kvm_get_msr_ignored_check(vcpu, index, data, false);
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(__kvm_emulate_msr_read);
@@ -2310,6 +2399,11 @@ EXPORT_SYMBOL_FOR_KVM_INTERNAL(handle_fastpath_wrmsr_imm);
  */
 static int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
+	/*
+	 * 在以下使用kvm_get_msr_ignored_check():
+	 *   - arch/x86/kvm/x86.c|2031| <<__kvm_emulate_msr_read>> return kvm_get_msr_ignored_check(vcpu, index, data, false);
+	 *   - arch/x86/kvm/x86.c|2319| <<do_get_msr>> return kvm_get_msr_ignored_check(vcpu, index, data, true);
+	 */
 	return kvm_get_msr_ignored_check(vcpu, index, data, true);
 }
 
@@ -2354,6 +2448,10 @@ struct pvclock_gtod_data {
 
 static struct pvclock_gtod_data pvclock_gtod_data;
 
+/*
+ * 在以下使用update_pvclock_gtod():
+ *   - arch/x86/kvm/x86.c|10569| <<pvclock_gtod_notify>> update_pvclock_gtod(tk);
+ */
 static void update_pvclock_gtod(struct timekeeper *tk)
 {
 	struct pvclock_gtod_data *vdata = &pvclock_gtod_data;
@@ -2384,6 +2482,15 @@ static void update_pvclock_gtod(struct timekeeper *tk)
 	write_seqcount_end(&vdata->seq);
 }
 
+/*
+ * 在以下使用get_kvmclock_base_ns():
+ *   - arch/x86/kvm/x86.c|3066| <<kvm_synchronize_tsc>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|3666| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+ *   - arch/x86/kvm/x86.c|3868| <<kvm_guest_time_update>> kernel_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|6735| <<kvm_arch_tsc_set_attr>> ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|8044| <<kvm_vm_ioctl_set_clock>> now_raw_ns = get_kvmclock_base_ns();
+ *   - arch/x86/kvm/x86.c|14139| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+ */
 static s64 get_kvmclock_base_ns(void)
 {
 	/* Count up from boot time, but with the frequency of the raw clock.  */
@@ -2420,6 +2527,11 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	if (kvm_write_guest(kvm, wall_clock, &version, sizeof(version)))
 		return;
 
+	/*
+	 * 在以下使用kvm_get_wall_clock_epoch():
+	 *   - arch/x86/kvm/x86.c|2423| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 */
 	wall_nsec = kvm_get_wall_clock_epoch(kvm);
 
 	wc.nsec = do_div(wall_nsec, NSEC_PER_SEC);
@@ -2438,19 +2550,50 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 	kvm_write_guest(kvm, wall_clock, &version, sizeof(version));
 }
 
+/*
+ * 在以下使用kvm_write_system_time():
+ *   - arch/x86/kvm/x86.c|4910| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME_NEW)>> kvm_write_system_time(vcpu, data, false, msr_info->host_initiated);
+ *   - arch/x86/kvm/x86.c|4916| <<kvm_set_msr_common(MSR_KVM_SYSTEM_TIME)>> kvm_write_system_time(vcpu, data, true, msr_info->host_initiated);
+ */
 static void kvm_write_system_time(struct kvm_vcpu *vcpu, gpa_t system_time,
 				  bool old_msr, bool host_initiated)
 {
 	struct kvm_arch *ka = &vcpu->kvm->arch;
 
 	if (vcpu->vcpu_id == 0 && !host_initiated) {
+		/*
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE():
+		 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time()>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_update_masterclock()
+		 */
 		if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 
+		/*
+		 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+		 *   - arch/x86/kvm/x86.c|2477| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+		 *   - arch/x86/kvm/x86.c|2480| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+		 *   - arch/x86/kvm/x86.c|3500| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+		 */
 		ka->boot_vcpu_runs_old_kvmclock = old_msr;
 	}
 
 	vcpu->arch.time = system_time;
+	/*
+	 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/x86.c|2466| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5626| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11551| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+	 *
+	 * 处理的函数kvm_gen_kvmclock_update()
+	 */
 	kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
 
 	/* we verify if the enable bit is set... */
@@ -2469,6 +2612,13 @@ static uint32_t div_frac(uint32_t dividend, uint32_t divisor)
 	return dividend;
 }
 
+/*
+ * 在以下使用kvm_get_time_scale():
+ *   - arch/x86/kvm/x86.c|2630| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+ *   - arch/x86/kvm/x86.c|3651| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+ *   - arch/x86/kvm/x86.c|3908| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+ *   - arch/x86/kvm/x86.c|4102| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC,
+ */
 static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 			       s8 *pshift, u32 *pmultiplier)
 {
@@ -2498,10 +2648,23 @@ static void kvm_get_time_scale(uint64_t scaled_hz, uint64_t base_hz,
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用kvm_guest_has_master_clock:
+ *   - arch/x86/kvm/x86.c|2526| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+ *   - arch/x86/kvm/x86.c|3299| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+ *   - arch/x86/kvm/x86.c|10542| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+ *   - arch/x86/kvm/x86.c|10577| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+ */
 static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
 #endif
 
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+/*
+ * 在以下使用max_tsc_khz:
+ *   - arch/x86/kvm/x86.c|11786| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+ *   - arch/x86/kvm/x86.c|11796| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+ *   - arch/x86/kvm/x86.c|15440| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+ */
 static unsigned long max_tsc_khz;
 
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
@@ -2513,20 +2676,69 @@ static u32 adjust_tsc_khz(u32 khz, s32 ppm)
 
 static void kvm_vcpu_write_tsc_multiplier(struct kvm_vcpu *vcpu, u64 l1_multiplier);
 
+/*
+ * 在以下使用set_tsc_khz():
+ *   - arch/x86/kvm/x86.c|2648| <<kvm_set_tsc_khz>> return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
+ *
+ * 一定会执行到这个函数, 就看scale了(use_scaling)
+ */
 static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 {
 	u64 ratio;
 
 	/* Guest TSC same frequency as host TSC? */
 	if (!scale) {
+		/*
+		 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+		 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+		 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+		 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+		 */
 		kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
 		return 0;
 	}
 
 	/* TSC scaling supported? */
 	if (!kvm_caps.has_tsc_control) {
+		/*
+		 * 如果guest的freq比host大
+		 * 也就是guest的频率更大
+		 * 比如guest的是1000, host是100.
+		 *
+		 * 现在开始讨论, tick了1000次.
+		 * guest认为是1s, host认为过了10s
+		 * 当前guest_tsc是1000, 根据guest_freq, 实际是1000*10
+		 * 就要通过增加offset来增大当前guest_tsc的value
+		 * 这样guest就觉得赶上来了
+		 *
+		 * 反过来没法支持. 我觉得是因为不能减少tsc_offset
+		 * 这样guest_tsc会减少, 时钟倒退了.
+		 * host是1000， guest是100
+		 * "user requested TSC rate below hardware speed"
+		 */
 		if (user_tsc_khz > tsc_khz) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+			 *   - arch/x86/kvm/x86.c|2554| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|3745| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+			 *   - arch/x86/kvm/x86.c|5784| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+			 */
 			vcpu->arch.tsc_catchup = 1;
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+			 *   - arch/x86/kvm/x86.c|2555| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|10856| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+			 *   - arch/x86/kvm/x86.c|12131| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+			 */
 			vcpu->arch.tsc_always_catchup = 1;
 			return 0;
 		} else {
@@ -2539,6 +2751,13 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	ratio = mul_u64_u32_div(1ULL << kvm_caps.tsc_scaling_ratio_frac_bits,
 				user_tsc_khz, tsc_khz);
 
+	/*
+	 * 在以下使用kvm_caps.max_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/svm/svm.c|5455| <<svm_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;
+	 *   - arch/x86/kvm/x86.c|2748| <<set_tsc_khz>> if (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/x86.c|12116| <<kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+	 */
 	if (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {
 		pr_warn_ratelimited("Invalid TSC scaling ratio - virtual-tsc-khz=%u\n",
 			            user_tsc_khz);
@@ -2549,6 +2768,28 @@ static int set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz, bool scale)
 	return 0;
 }
 
+/*
+ * [  826.720965] KVM_GET_TSC_KHZ vcpu=0 tsc_khz=3392424
+ * [  826.722307] KVM_GET_TSC_KHZ vcpu=1 tsc_khz=3392424
+ * [  826.723495] KVM_GET_TSC_KHZ vcpu=2 tsc_khz=3392424
+ * [  826.724815] KVM_GET_TSC_KHZ vcpu=3 tsc_khz=3392424
+ * [  826.725845] KVM_GET_CLOCK ns=8020741 flags=0
+ * [  826.739406] KVM_GET_TSC_KHZ vcpu=0 tsc_khz=3392424
+ * [  826.740213] KVM_SET_TSC_KHZ vcpu=0 tsc_khz=3392424
+ * [  826.740990] set MSR_IA32_TSC vcpu=0 data=0 host=1
+ * [  826.742171] KVM_GET_TSC_KHZ vcpu=1 tsc_khz=3392424
+ * [  826.743138] KVM_SET_TSC_KHZ vcpu=1 tsc_khz=3392424
+ * [  826.744096] set MSR_IA32_TSC vcpu=1 data=0 host=1
+ * [  826.745113] KVM_GET_TSC_KHZ vcpu=2 tsc_khz=3392424
+ * [  826.745967] KVM_SET_TSC_KHZ vcpu=2 tsc_khz=3392424
+ * [  826.746835] set MSR_IA32_TSC vcpu=2 data=0 host=1
+ * [  826.747812] KVM_GET_TSC_KHZ vcpu=3 tsc_khz=3392424
+ * [  826.748739] KVM_SET_TSC_KHZ vcpu=3 tsc_khz=3392424
+ *
+ * 在以下使用kvm_set_tsc_khz():
+ *   - arch/x86/kvm/x86.c|7423| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+ *   - arch/x86/kvm/x86.c|13805| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+ */
 static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 {
 	u32 thresh_lo, thresh_hi;
@@ -2556,15 +2797,55 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 
 	/* tsc_khz can be zero if TSC calibration fails */
 	if (user_tsc_khz == 0) {
+		/*
+		 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+		 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+		 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+		 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+		 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+		 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+		 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+		 */
 		/* set tsc_scaling_ratio to a safe value */
 		kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
 		return -1;
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2624| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2614| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|518| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2623| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2664| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|528| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *
+	 * 在这里计算:
+	 * vcpu->arch.virtual_tsc_shift
+	 * vcpu->arch.virtual_tsc_mult
+	 */
 	/* Compute a scale to convert nanoseconds in TSC cycles */
 	kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
 			   &vcpu->arch.virtual_tsc_shift,
 			   &vcpu->arch.virtual_tsc_mult);
+	/*
+	 * 很多使用. 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2593| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 */
 	vcpu->arch.virtual_tsc_khz = user_tsc_khz;
 
 	/*
@@ -2573,6 +2854,9 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 	 * rate being applied is within that bounds of the hardware
 	 * rate.  If so, no scaling or compensation need be done.
 	 */
+	/*
+	 * 这里用的是tsc_khz!
+	 */
 	thresh_lo = adjust_tsc_khz(tsc_khz, -tsc_tolerance_ppm);
 	thresh_hi = adjust_tsc_khz(tsc_khz, tsc_tolerance_ppm);
 	if (user_tsc_khz < thresh_lo || user_tsc_khz > thresh_hi) {
@@ -2580,14 +2864,77 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
 			 user_tsc_khz, thresh_lo, thresh_hi);
 		use_scaling = 1;
 	}
+	/*
+	 * 只在这里调用
+	 */
 	return set_tsc_khz(vcpu, user_tsc_khz, use_scaling);
 }
 
+/*
+ * [RFC PATCH v3 10/21] KVM: x86: Fix software TSC upscaling in kvm_update_guest_time()
+ * https://lore.kernel.org/all/20240522001817.619072-11-dwmw2@infradead.org/
+ *
+ * @@ -2491,10 +2491,19 @@ static int kvm_set_tsc_khz(struct kvm_vcpu *vcpu, u32 user_tsc_khz)
+ *
+ *  static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
+ *  {
+ * -	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ * -				      vcpu->arch.virtual_tsc_mult,
+ * -				      vcpu->arch.virtual_tsc_shift);
+ * -	tsc += vcpu->arch.this_tsc_write;
+ * +	s64 delta = kernel_ns - vcpu->arch.this_tsc_nsec;
+ * +	u64 tsc = vcpu->arch.this_tsc_write;
+ * +
+ * +	// pvclock_scale_delta cannot cope with negative deltas
+ * +	if (delta >= 0)
+ * +		tsc += pvclock_scale_delta(delta,
+ * +					   vcpu->arch.virtual_tsc_mult,
+ * +					   vcpu->arch.virtual_tsc_shift);
+ * +	else
+ * +		tsc -= pvclock_scale_delta(-delta,
+ * +					   vcpu->arch.virtual_tsc_mult,
+ * +					   vcpu->arch.virtual_tsc_shift);
+ * +
+ *  	return tsc;
+ *  }
+ */
+/*
+ * 在以下使用compute_guest_tsc():
+ *   - arch/x86/kvm/x86.c|3746| <<kvm_guest_time_update>> u64 tsc = compute_guest_tsc(v, kernel_ns);
+ */
 static u64 compute_guest_tsc(struct kvm_vcpu *vcpu, s64 kernel_ns)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2624| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2614| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|518| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2623| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2664| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|528| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *
+	 * virtual_tsc_mult和virtual_tsc_shift是由user tsc khz计算的
+	 */
 	u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
 				      vcpu->arch.virtual_tsc_mult,
 				      vcpu->arch.virtual_tsc_shift);
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 */
 	tsc += vcpu->arch.this_tsc_write;
 	return tsc;
 }
@@ -2599,6 +2946,10 @@ static inline bool gtod_is_based_on_tsc(int mode)
 }
 #endif
 
+/*
+ * 在以下使用kvm_track_tsc_matching():
+ *   - arch/x86/kvm/x86.c|2834| <<__kvm_synchronize_tsc>> kvm_track_tsc_matching(vcpu, !matched);
+ */
 static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 {
 #ifdef CONFIG_X86_64
@@ -2610,10 +2961,33 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
 	 * and all vCPUs must have matching TSCs.  Note, the count for matching
 	 * vCPUs doesn't include the reference vCPU, hence "+1".
 	 */
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2642| <<kvm_track_tsc_matching>> bool use_master_clock =
+	 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&vcpu->kvm->online_vcpus)) &&
+	 *         gtod_is_based_on_tsc(gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|2668| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2863| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2865| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3211| <<pvclock_update_vm_gtod_copy>> vcpus_matched =
+	 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&kvm->online_vcpus));
+	 */
 	bool use_master_clock = (ka->nr_vcpus_matched_tsc + 1 ==
 				 atomic_read(&vcpu->kvm->online_vcpus)) &&
 				gtod_is_based_on_tsc(gtod->clock.vclock_mode);
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE():
+	 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_update_masterclock()
+	 */
 	/*
 	 * Request a masterclock update if the masterclock needs to be toggled
 	 * on/off, or when starting a new generation and the masterclock is
@@ -2640,6 +3014,11 @@ static void kvm_track_tsc_matching(struct kvm_vcpu *vcpu, bool new_generation)
  *
  * N equals to kvm_caps.tsc_scaling_ratio_frac_bits.
  */
+/*
+ * 在以下使用__scale_tsc():
+ *   - arch/x86/kvm/x86.c|3026| <<kvm_scale_tsc>> _tsc = __scale_tsc(ratio, tsc);
+ *   - arch/x86/kvm/x86.c|12116| <<kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+ */
 static inline u64 __scale_tsc(u64 ratio, u64 tsc)
 {
 	return mul_u64_u64_shr(tsc, ratio, kvm_caps.tsc_scaling_ratio_frac_bits);
@@ -2649,12 +3028,34 @@ u64 kvm_scale_tsc(u64 tsc, u64 ratio)
 {
 	u64 _tsc = tsc;
 
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	if (ratio != kvm_caps.default_tsc_scaling_ratio)
 		_tsc = __scale_tsc(ratio, tsc);
 
 	return _tsc;
 }
 
+/*
+ * 在以下使用kvm_compute_l1_tsc_offset():
+ *   - arch/x86/kvm/x86.c|3395| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|3498| <<kvm_synchronize_tsc>> offset = kvm_compute_l1_tsc_offset(vcpu, data);
+ *   - arch/x86/kvm/x86.c|5492| <<kvm_set_msr_common(MSR_IA32_TSC)>> u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+ *   - arch/x86/kvm/x86.c|6676| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu, vcpu->arch.last_guest_tsc);
+ */
 static u64 kvm_compute_l1_tsc_offset(struct kvm_vcpu *vcpu, u64 target_tsc)
 {
 	u64 tsc;
@@ -2675,6 +3076,21 @@ u64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier)
 {
 	u64 nested_offset;
 
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
 		nested_offset = l1_offset;
 	else
@@ -2688,6 +3104,21 @@ EXPORT_SYMBOL_FOR_KVM_INTERNAL(kvm_calc_nested_tsc_offset);
 
 u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 {
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
 		return mul_u64_u64_shr(l1_multiplier, l2_multiplier,
 				       kvm_caps.tsc_scaling_ratio_frac_bits);
@@ -2696,6 +3127,12 @@ u64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier)
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(kvm_calc_nested_tsc_multiplier);
 
+/*
+ * 在以下使用kvm_vcpu_write_tsc_offset():
+ *   - arch/x86/kvm/x86.c|2808| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ *   - arch/x86/kvm/x86.c|2909| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+ *   - arch/x86/kvm/x86.c|5361| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+ */
 static void kvm_vcpu_write_tsc_offset(struct kvm_vcpu *vcpu, u64 l1_offset)
 {
 	if (vcpu->arch.guest_tsc_protected)
@@ -2757,16 +3194,50 @@ static inline bool kvm_check_tsc_unstable(void)
  * offset for the vcpu and tracks the TSC matching generation that the vcpu
  * participates in.
  */
+/*
+ * 关注的!!!!
+ * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+ *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+ * 在以下使用kvm_vcpu_arch->this_tsc_write:
+ *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+ *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+ *
+ *
+ * 在以下使用__kvm_synchronize_tsc():
+ *   - arch/x86/kvm/x86.c|2901| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+ *   - arch/x86/kvm/x86.c|6106| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+ */
 static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 				  u64 ns, bool matched, bool user_set_tsc)
 {
 	struct kvm *kvm = vcpu->kvm;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	lockdep_assert_held(&kvm->arch.tsc_write_lock);
 
 	if (vcpu->arch.guest_tsc_protected)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->user_set_tsc:
+	 *   - arch/x86/kvm/x86.c|2795| <<__kvm_synchronize_tsc>> vcpu->kvm->arch.user_set_tsc = true;
+	 *   - arch/x86/kvm/x86.c|2858| <<kvm_synchronize_tsc>> } else if (kvm->arch.user_set_tsc) {
+	 */
 	if (user_set_tsc)
 		vcpu->kvm->arch.user_set_tsc = true;
 
@@ -2774,13 +3245,59 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 	 * We also track th most recent recorded KHZ, write and time to
 	 * allow the matching interval to be extended at each write.
 	 */
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|14062| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	kvm->arch.last_tsc_nsec = ns;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|3082| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+	 *                                                         nsec_to_cycles(vcpu, elapsed);
+	 *   - arch/x86/kvm/x86.c|14063| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+	 */
 	kvm->arch.last_tsc_write = tsc;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2917| <<__kvm_synchronize_tsc>>
+	 *          kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|3113| <<kvm_synchronize_tsc>>
+	 *          if (synchronizing && vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|6731| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+	 *          matched = (vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_offset == offset);
+	 */
 	kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	/*
+	 * 在以下使用kvm_arch->last_tsc_offset:
+	 *   - arch/x86/kvm/x86.c|2918| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+	 *   - arch/x86/kvm/x86.c|6644| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+	 *          matched = (vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_offset == offset);
+	 */
 	kvm->arch.last_tsc_offset = offset;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2920| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3886| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|5884| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+	 *                                       vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|12212| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	vcpu->arch.last_guest_tsc = tsc;
 
+	/*
+	 * 在以下使用kvm_vcpu_write_tsc_offset():
+	 *   - arch/x86/kvm/x86.c|2808| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2909| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|5361| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, offset);
 
 	if (!matched) {
@@ -2793,23 +3310,125 @@ static void __kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 offset, u64 tsc,
 		 *
 		 * These values are tracked in kvm->arch.cur_xxx variables.
 		 */
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2859| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+		 *   - arch/x86/kvm/x86.c|2864| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		 *   - arch/x86/kvm/x86.c|2869| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 */
 		kvm->arch.cur_tsc_generation++;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_nsec:
+		 *   - arch/x86/kvm/x86.c|2947| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+		 *   - arch/x86/kvm/x86.c|2999| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+		 */
 		kvm->arch.cur_tsc_nsec = ns;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_write:
+		 *   - arch/x86/kvm/x86.c|2948| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = tsc;
+		 *   - arch/x86/kvm/x86.c|3005| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+		 */
 		kvm->arch.cur_tsc_write = tsc;
+		/*
+		 * 在以下使用kvm_arch->cur_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2949| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+		 *   - arch/x86/kvm/x86.c|3078| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+		 */
 		kvm->arch.cur_tsc_offset = offset;
+		/*
+		 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2642| <<kvm_track_tsc_matching>> bool use_master_clock =
+		 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&vcpu->kvm->online_vcpus)) &&
+		 *         gtod_is_based_on_tsc(gtod->clock.vclock_mode);
+		 *   - arch/x86/kvm/x86.c|2668| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2863| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2865| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|3211| <<pvclock_update_vm_gtod_copy>> vcpus_matched =
+		 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&kvm->online_vcpus));
+		 */
 		kvm->arch.nr_vcpus_matched_tsc = 0;
 	} else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		/*
+		 * 在以下使用this_tsc_generation:
+		 *   - arch/x86/kvm/x86.c|2892| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+		 *   - arch/x86/kvm/x86.c|2914| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+		 */
+		/*
+		 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+		 *   - arch/x86/kvm/x86.c|2642| <<kvm_track_tsc_matching>> bool use_master_clock =
+		 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&vcpu->kvm->online_vcpus)) &&
+		 *         gtod_is_based_on_tsc(gtod->clock.vclock_mode);
+		 *   - arch/x86/kvm/x86.c|2668| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+		 *   - arch/x86/kvm/x86.c|2863| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+		 *   - arch/x86/kvm/x86.c|2865| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+		 *   - arch/x86/kvm/x86.c|3211| <<pvclock_update_vm_gtod_copy>> vcpus_matched =
+		 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&kvm->online_vcpus));
+		 */
 		kvm->arch.nr_vcpus_matched_tsc++;
 	}
 
 	/* Keep track of which generation this VCPU has synchronized to */
+	/*
+	 * 在以下使用kvm_arch->cur_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2859| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_generation++;
+	 *   - arch/x86/kvm/x86.c|2864| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2869| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 *
+	 * 在以下使用this_tsc_generation:
+	 *   - arch/x86/kvm/x86.c|2892| <<__kvm_synchronize_tsc>> } else if (vcpu->arch.this_tsc_generation != kvm->arch.cur_tsc_generation) {
+	 *   - arch/x86/kvm/x86.c|2914| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	 *
+	 * 注释:
+	 * Keep track of which generation this VCPU has synchronized to
+	 */
 	vcpu->arch.this_tsc_generation = kvm->arch.cur_tsc_generation;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 在以下使用kvm_arch->cur_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2947| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|2999| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	 *
+	 * 注释:
+	 * Keep track of which generation this VCPU has synchronized to
+	 */
 	vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+	/*
+	 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+	 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 *
+	 * 在以下使用kvm_arch->cur_tsc_write:
+	 *   - arch/x86/kvm/x86.c|2948| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_write = tsc;
+	 *   - arch/x86/kvm/x86.c|3005| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+	 * 
+	 * 注释:
+	 * Keep track of which generation this VCPU has synchronized to
+	 */
 	vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
 
+	/*
+	 * 只在这里调用
+	 */
 	kvm_track_tsc_matching(vcpu, !matched);
 }
 
+/*
+ * 注意的地方!!!
+ * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+ *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+ * 在以下使用kvm_vcpu_arch->this_tsc_write:
+ *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+ *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+ *
+ *
+ * 在以下使用kvm_synchronize_tsc():
+ *   - arch/x86/kvm/x86.c|4229| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, &data);
+ *   - arch/x86/kvm/x86.c|12937| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, NULL);
+ */
 static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 {
 	u64 data = user_value ? *user_value : 0;
@@ -2819,11 +3438,36 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 	bool matched = false;
 	bool synchronizing = false;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
 	offset = kvm_compute_l1_tsc_offset(vcpu, data);
 	ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->last_tsc_nsec:
+	 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+	 *   - arch/x86/kvm/x86.c|3067| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+	 *   - arch/x86/kvm/x86.c|14062| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+	 */
 	elapsed = ns - kvm->arch.last_tsc_nsec;
 
+	/*
+	 * 很多使用. 在以下设置kvm_vcpu_arch->virtual_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2593| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+	 */
 	if (vcpu->arch.virtual_tsc_khz) {
 		if (data == 0) {
 			/*
@@ -2832,6 +3476,18 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 			 */
 			synchronizing = true;
 		} else if (kvm->arch.user_set_tsc) {
+			/*
+			 * 在以下使用kvm_arch->user_set_tsc:
+			 *   - arch/x86/kvm/x86.c|2795| <<__kvm_synchronize_tsc>> vcpu->kvm->arch.user_set_tsc = true;
+			 *   - arch/x86/kvm/x86.c|2858| <<kvm_synchronize_tsc>> } else if (kvm->arch.user_set_tsc) {
+			 *
+			 *
+			 * 在以下使用kvm_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+			 *   - arch/x86/kvm/x86.c|3082| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *                                                         nsec_to_cycles(vcpu, elapsed);
+			 *   - arch/x86/kvm/x86.c|14063| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+			 */
 			u64 tsc_exp = kvm->arch.last_tsc_write +
 						nsec_to_cycles(vcpu, elapsed);
 			u64 tsc_hz = vcpu->arch.virtual_tsc_khz * 1000LL;
@@ -2862,11 +3518,47 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 	 * compensation code attempt to catch up if we fall behind, but
 	 * it's better to try to match offsets from the beginning.
          */
+	/*
+	 * 在以下使用kvm_arch->last_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|2917| <<__kvm_synchronize_tsc>>
+	 *          kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+	 *   - arch/x86/kvm/x86.c|3113| <<kvm_synchronize_tsc>>
+	 *          if (synchronizing && vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+	 *   - arch/x86/kvm/x86.c|6731| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+	 *          matched = (vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+	 *                     kvm->arch.last_tsc_offset == offset);
+	 */
 	if (synchronizing &&
 	    vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
 		if (!kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_arch->cur_tsc_offset:
+			 *   - arch/x86/kvm/x86.c|2949| <<__kvm_synchronize_tsc>> kvm->arch.cur_tsc_offset = offset;
+			 *   - arch/x86/kvm/x86.c|3078| <<kvm_synchronize_tsc>> offset = kvm->arch.cur_tsc_offset;
+			 */
 			offset = kvm->arch.cur_tsc_offset;
 		} else {
+			/*
+			 * 在以下使用nsec_to_cycles():
+			 *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+			 *              nsec_to_cycles(vcpu, timer_advance_ns)));
+			 *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+			 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+			 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+			 *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+			 *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+			 *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+			 *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+			 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+			 *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+			 *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+			 *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+			 *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+			 *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+			 *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+			 *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+			 */
 			u64 delta = nsec_to_cycles(vcpu, elapsed);
 			data += delta;
 			offset = kvm_compute_l1_tsc_offset(vcpu, data);
@@ -2874,28 +3566,79 @@ static void kvm_synchronize_tsc(struct kvm_vcpu *vcpu, u64 *user_value)
 		matched = true;
 	}
 
+	/*
+	 * 在以下使用__kvm_synchronize_tsc():
+	 *   - arch/x86/kvm/x86.c|2901| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+	 *   - arch/x86/kvm/x86.c|6106| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+	 *
+	 * ns是get_kvmclock_base_ns(), 每次都不一样
+	 * data可能是0, 1, 也可能是offset
+	 */
 	__kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 }
 
+/*
+ * 在以下使用adjust_tsc_offset_guest():
+ *   - arch/x86/kvm/x86.c|3375| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+ *   - arch/x86/kvm/x86.c|4157| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+ *   - arch/x86/kvm/x86.c|5013| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+ *   - arch/x86/kvm/x86.c|5064| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+ */
 static inline void adjust_tsc_offset_guest(struct kvm_vcpu *vcpu,
 					   s64 adjustment)
 {
 	u64 tsc_offset = vcpu->arch.l1_tsc_offset;
+	/*
+	 * 在以下使用kvm_vcpu_write_tsc_offset():
+	 *   - arch/x86/kvm/x86.c|2808| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 *   - arch/x86/kvm/x86.c|2909| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+	 *   - arch/x86/kvm/x86.c|5361| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+	 */
 	kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
 }
 
+/*
+ * 只在以下调用adjust_tsc_offset_host():
+ *   - arch/x86/kvm/x86.c|6186| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+ */
 static inline void adjust_tsc_offset_host(struct kvm_vcpu *vcpu, s64 adjustment)
 {
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
 		WARN_ON(adjustment < 0);
 	adjustment = kvm_scale_tsc((u64) adjustment,
 				   vcpu->arch.l1_tsc_scaling_ratio);
+	/*
+	 * 在以下使用adjust_tsc_offset_guest():
+	 *   - arch/x86/kvm/x86.c|3375| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+	 *   - arch/x86/kvm/x86.c|4157| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+	 *   - arch/x86/kvm/x86.c|5013| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+	 *   - arch/x86/kvm/x86.c|5064| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+	 */
 	adjust_tsc_offset_guest(vcpu, adjustment);
 }
 
 #ifdef CONFIG_X86_64
 
+/*
+ * 在以下使用read_tsc():
+ *   - arch/x86/kvm/x86.c|3421| <<vgettsc>> *tsc_timestamp = read_tsc();
+ */
 static u64 read_tsc(void)
 {
 	u64 ret = (u64)rdtsc_ordered();
@@ -2916,6 +3659,12 @@ static u64 read_tsc(void)
 	return last;
 }
 
+/*
+ * 在以下使用vgettsc():
+ *   - arch/x86/kvm/x86.c|3449| <<do_kvmclock_base>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|3472| <<do_monotonic>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ *   - arch/x86/kvm/x86.c|3492| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+ */
 static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 			  int *mode)
 {
@@ -2937,6 +3686,9 @@ static inline u64 vgettsc(struct pvclock_clock *clock, u64 *tsc_timestamp,
 		break;
 	case VDSO_CLOCKMODE_TSC:
 		*mode = VDSO_CLOCKMODE_TSC;
+		/*
+		 * 只在这里调用read_ysc()
+		 */
 		*tsc_timestamp = read_tsc();
 		v = (*tsc_timestamp - clock->cycle_last) &
 			clock->mask;
@@ -2965,6 +3717,12 @@ static int do_kvmclock_base(s64 *t, u64 *tsc_timestamp)
 	do {
 		seq = read_seqcount_begin(&gtod->seq);
 		ns = gtod->raw_clock.base_cycles;
+		/*
+		 * 在以下使用vgettsc():
+		 *   - arch/x86/kvm/x86.c|3449| <<do_kvmclock_base>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+		 *   - arch/x86/kvm/x86.c|3472| <<do_monotonic>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+		 *   - arch/x86/kvm/x86.c|3492| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+		 */
 		ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
 		ns >>= gtod->raw_clock.shift;
 		ns += ktime_to_ns(ktime_add(gtod->raw_clock.offset, gtod->offs_boot));
@@ -2978,6 +3736,10 @@ static int do_kvmclock_base(s64 *t, u64 *tsc_timestamp)
  * This calculates CLOCK_MONOTONIC at the time of the TSC snapshot, with
  * no boot time offset.
  */
+/*
+ * 在以下使用do_monotonic():
+ *   - arch/x86/kvm/x86.c|3762| <<kvm_get_monotonic_and_clockread>> return gtod_is_based_on_tsc(do_monotonic(kernel_ns,
+ */
 static int do_monotonic(s64 *t, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -2988,6 +3750,12 @@ static int do_monotonic(s64 *t, u64 *tsc_timestamp)
 	do {
 		seq = read_seqcount_begin(&gtod->seq);
 		ns = gtod->clock.base_cycles;
+		/*
+		 * 在以下使用vgettsc():
+		 *   - arch/x86/kvm/x86.c|3449| <<do_kvmclock_base>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+		 *   - arch/x86/kvm/x86.c|3472| <<do_monotonic>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+		 *   - arch/x86/kvm/x86.c|3492| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+		 */
 		ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
 		ns >>= gtod->clock.shift;
 		ns += ktime_to_ns(gtod->clock.offset);
@@ -2997,6 +3765,10 @@ static int do_monotonic(s64 *t, u64 *tsc_timestamp)
 	return mode;
 }
 
+/*
+ * 在以下使用do_realtime():
+ *   - arch/x86/kvm/x86.c|3545| <<kvm_get_walltime_and_clockread>> return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
+ */
 static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 {
 	struct pvclock_gtod_data *gtod = &pvclock_gtod_data;
@@ -3008,6 +3780,12 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
 		seq = read_seqcount_begin(&gtod->seq);
 		ts->tv_sec = gtod->wall_time_sec;
 		ns = gtod->clock.base_cycles;
+		/*
+		 * 在以下使用vgettsc():
+		 *   - arch/x86/kvm/x86.c|3449| <<do_kvmclock_base>> ns += vgettsc(&gtod->raw_clock, tsc_timestamp, &mode);
+		 *   - arch/x86/kvm/x86.c|3472| <<do_monotonic>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+		 *   - arch/x86/kvm/x86.c|3492| <<do_realtime>> ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
+		 */
 		ns += vgettsc(&gtod->clock, tsc_timestamp, &mode);
 		ns >>= gtod->clock.shift;
 	} while (unlikely(read_seqcount_retry(&gtod->seq, seq)));
@@ -3023,6 +3801,10 @@ static int do_realtime(struct timespec64 *ts, u64 *tsc_timestamp)
  * reports the TSC value from which it do so. Returns true if host is
  * using TSC based clocksource.
  */
+/*
+ * 在以下使用kvm_get_time_and_clockread():
+ *   - arch/x86/kvm/x86.c|3639| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+ */
 static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
@@ -3037,12 +3819,19 @@ static bool kvm_get_time_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
  * Calculates CLOCK_MONOTONIC and reports the TSC value from which it did
  * so. Returns true if host is using TSC based clocksource.
  */
+/*
+ * 在以下使用kvm_get_monotonic_and_clockread():
+ *   - arch/x86/kvm/xen.c|252| <<kvm_xen_start_timer>> !kvm_get_monotonic_and_clockread(&kernel_now, &host_tsc)) {
+ */
 bool kvm_get_monotonic_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
 {
 	/* checked again under seqlock below */
 	if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
 		return false;
 
+	/*
+	 * 只在这里调用do_monotonic()
+	 */
 	return gtod_is_based_on_tsc(do_monotonic(kernel_ns,
 						 tsc_timestamp));
 }
@@ -3054,6 +3843,12 @@ bool kvm_get_monotonic_and_clockread(s64 *kernel_ns, u64 *tsc_timestamp)
  * DO NOT USE this for anything related to migration. You want CLOCK_TAI
  * for that.
  */
+/*
+ * 在以下使用kvm_get_walltime_and_clockread():
+ *   - arch/x86/kvm/x86.c|3833| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+ *   - arch/x86/kvm/x86.c|4319| <<kvm_get_wall_clock_epoch>> !kvm_get_walltime_and_clockread(&ts, &host_tsc))
+ *   - arch/x86/kvm/x86.c|11404| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+ */
 static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 					   u64 *tsc_timestamp)
 {
@@ -3061,6 +3856,9 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
 	if (!gtod_is_based_on_tsc(pvclock_gtod_data.clock.vclock_mode))
 		return false;
 
+	/*
+	 * 只在这里调用do_realtime()
+	 */
 	return gtod_is_based_on_tsc(do_realtime(ts, tsc_timestamp));
 }
 #endif
@@ -3106,6 +3904,13 @@ static bool kvm_get_walltime_and_clockread(struct timespec64 *ts,
  *
  */
 
+/*
+ * 在以下使用pvclock_update_vm_gtod_copy():
+ *   - arch/x86/kvm/x86.c|3180| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|7228| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|9751| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+ *   - arch/x86/kvm/x86.c|13164| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+ */
 static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3113,10 +3918,52 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 	int vclock_mode;
 	bool host_tsc_clocksource, vcpus_matched;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->nr_vcpus_matched_tsc:
+	 *   - arch/x86/kvm/x86.c|2642| <<kvm_track_tsc_matching>> bool use_master_clock =
+	 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&vcpu->kvm->online_vcpus)) &&
+	 *         gtod_is_based_on_tsc(gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|2668| <<kvm_track_tsc_matching>> trace_kvm_track_tsc(vcpu->vcpu_id, ka->nr_vcpus_matched_tsc,
+	 *   - arch/x86/kvm/x86.c|2863| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc = 0;
+	 *   - arch/x86/kvm/x86.c|2865| <<__kvm_synchronize_tsc>> kvm->arch.nr_vcpus_matched_tsc++;
+	 *   - arch/x86/kvm/x86.c|3211| <<pvclock_update_vm_gtod_copy>> vcpus_matched =
+	 *         (ka->nr_vcpus_matched_tsc + 1 == atomic_read(&kvm->online_vcpus));
+	 */
 	vcpus_matched = (ka->nr_vcpus_matched_tsc + 1 ==
 			atomic_read(&kvm->online_vcpus));
 
+	/*
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3156| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|3595| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7376| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 *
+	 * 在以下使用kvm_arch->master_cycle_now:
+	 *   - arch/x86/kvm/x86.c|3157| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3334| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3436| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+	 *   - arch/x86/kvm/x86.c|3594| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+	 */
 	/*
 	 * If the host uses TSC clock, then passthrough TSC as stable
 	 * to the guest.
@@ -3125,10 +3972,23 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 					&ka->master_kernel_ns,
 					&ka->master_cycle_now);
 
+	/*
+	 * 在以下使用kvm_arch->boot_vcpu_runs_old_kvmclock:
+	 *   - arch/x86/kvm/x86.c|2477| <<kvm_write_system_time>> if (ka->boot_vcpu_runs_old_kvmclock != old_msr)
+	 *   - arch/x86/kvm/x86.c|2480| <<kvm_write_system_time>> ka->boot_vcpu_runs_old_kvmclock = old_msr;
+	 *   - arch/x86/kvm/x86.c|3500| <<pvclock_update_vm_gtod_copy>> && !ka->boot_vcpu_runs_old_kvmclock;
+	 */
 	ka->use_master_clock = host_tsc_clocksource && vcpus_matched
 				&& !ka->backwards_tsc_observed
 				&& !ka->boot_vcpu_runs_old_kvmclock;
 
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2526| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3299| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|10542| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|10577| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (ka->use_master_clock)
 		atomic_set(&kvm_guest_has_master_clock, 1);
 
@@ -3138,46 +3998,186 @@ static void pvclock_update_vm_gtod_copy(struct kvm *kvm)
 #endif
 }
 
+/*
+ * 在以下使用kvm_make_mclock_inprogress_request():
+ *    - arch/x86/kvm/x86.c|3185| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+ *    - arch/x86/kvm/x86.c|9798| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+ */
 static void kvm_make_mclock_inprogress_request(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+	 *   - arch/x86/kvm/x86.c|3174| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *   - arch/x86/kvm/x86.c|3204| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
 }
 
+/*
+ * 在以下使用__kvm_start_pvclock_update():
+ *   - arch/x86/kvm/x86.c|3188| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9811| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+ */
 static void __kvm_start_pvclock_update(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|4010| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4063| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4293| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4300| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|4466| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4505| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|4774| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4840| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|15333| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *
+	 *
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
 	write_seqcount_begin(&kvm->arch.pvclock_sc);
 }
 
+/*
+ * 在以下使用kvm_start_pvclock_update():
+ *   - arch/x86/kvm/x86.c|3226| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7281| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+ */
 static void kvm_start_pvclock_update(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_make_mclock_inprogress_request():
+	 *    - arch/x86/kvm/x86.c|3185| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+	 *    - arch/x86/kvm/x86.c|9798| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+	 */
 	kvm_make_mclock_inprogress_request(kvm);
 
+	/*
+	 * 在以下使用__kvm_start_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3188| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9811| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+	 */
 	/* no guest entries from this point */
 	__kvm_start_pvclock_update(kvm);
 }
 
+/*
+ * 在以下使用kvm_end_pvclock_update():
+ *   - arch/x86/kvm/x86.c|3235| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|7313| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+ *   - arch/x86/kvm/x86.c|9820| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+ */
 static void kvm_end_pvclock_update(struct kvm *kvm)
 {
 	struct kvm_arch *ka = &kvm->arch;
 	struct kvm_vcpu *vcpu;
 	unsigned long i;
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	write_seqcount_end(&ka->pvclock_sc);
 	raw_spin_unlock_irq(&ka->tsc_write_lock);
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
+	/*
+	 * 在以下使用KVM_REQ_MCLOCK_INPROGRESS:
+	 *   - arch/x86/kvm/x86.c|3174| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+	 *   - arch/x86/kvm/x86.c|3204| <<kvm_end_pvclock_update>> kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
+	 */
 	/* guest entries allowed */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_clear_request(KVM_REQ_MCLOCK_INPROGRESS, vcpu);
 }
 
+/*
+ * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+ *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *
+ * 处理的函数kvm_update_masterclock()
+ *
+ *
+ * 处理KVM_REQ_MASTERCLOCK_UPDATE:
+ *    - arch/x86/kvm/x86.c|11075| <<vcpu_enter_guest>> kvm_update_masterclock(vcpu->kvm);
+ */
 static void kvm_update_masterclock(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_hv_request_tsc_page_update():
+	 *   -  arch/x86/kvm/x86.c|3178| <<kvm_update_masterclock>> kvm_hv_request_tsc_page_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7226| <<kvm_vm_ioctl_set_clock>> kvm_hv_request_tsc_page_update(kvm);
+	 */
 	kvm_hv_request_tsc_page_update(kvm);
+	/*
+	 * 在以下使用kvm_start_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3226| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7281| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+	 */
 	kvm_start_pvclock_update(kvm);
+	/*
+	 * 在以下使用pvclock_update_vm_gtod_copy():
+	 *   - arch/x86/kvm/x86.c|3180| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|7228| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|9751| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|13164| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
+	/*
+	 * 在以下使用kvm_end_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3235| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7313| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9820| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+	 */
 	kvm_end_pvclock_update(kvm);
 }
 
@@ -3189,6 +4189,12 @@ static void kvm_update_masterclock(struct kvm *kvm)
  * notification when calibration completes, but practically speaking calibration
  * will complete before userspace is alive enough to create VMs.
  */
+/*
+ * 在以下使用get_cpu_tsc_khz():
+ *   - arch/x86/kvm/x86.c|3423| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+ *   - arch/x86/kvm/x86.c|3566| <<kvm_guest_time_update>> tgt_tsc_khz = get_cpu_tsc_khz();
+ *   - arch/x86/kvm/x86.c|3704| <<kvm_get_wall_clock_epoch>> local_tsc_khz = get_cpu_tsc_khz();
+ */
 static unsigned long get_cpu_tsc_khz(void)
 {
 	if (static_cpu_has(X86_FEATURE_CONSTANT_TSC))
@@ -3197,6 +4203,27 @@ static unsigned long get_cpu_tsc_khz(void)
 		return __this_cpu_read(cpu_tsc_khz);
 }
 
+/*
+ * 1052 #define KVM_CLOCK_TSC_STABLE            2
+ * 1053 #define KVM_CLOCK_REALTIME              (1 << 2)
+ * 1054 #define KVM_CLOCK_HOST_TSC              (1 << 3)
+ * 1055 
+ * 1056 struct kvm_clock_data {
+ * 1057         __u64 clock;
+ * 1058         __u32 flags;
+ * 1059         __u32 pad0;
+ * 1060         __u64 realtime;
+ * 1061         __u64 host_tsc;
+ * 1062         __u32 pad[4];
+ * 1063 };
+ *
+ *
+ * 在以下使用__get_kvmclock():
+ *   - arch/x86/kvm/x86.c|3354| <<get_kvmclock>> __get_kvmclock(kvm, data);
+ *
+ * 用struct kvm_clock_data返回当前VM启动了的时间
+ * 包括guest_time, tsc, 和realtime
+ */
 /* Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */
 static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
@@ -3212,6 +4239,14 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 #ifdef CONFIG_X86_64
 		struct timespec64 ts;
 
+		/*
+		 * 在以下使用kvm_get_walltime_and_clockread():
+		 *   - arch/x86/kvm/x86.c|3833| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+		 *   - arch/x86/kvm/x86.c|4319| <<kvm_get_wall_clock_epoch>> !kvm_get_walltime_and_clockread(&ts, &host_tsc))
+		 *   - arch/x86/kvm/x86.c|11404| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+		 *
+		 * 同时获得tsc和ns (基于realtime)
+		 */
 		if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
 			data->realtime = ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec;
 			data->flags |= KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC;
@@ -3220,38 +4255,140 @@ static void __get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 		data->host_tsc = rdtsc();
 
 		data->flags |= KVM_CLOCK_TSC_STABLE;
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|3157| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+		 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3334| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3436| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3594| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 */
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
+		/*
+		 * 在以下使用kvm_arch->master_kernel_ns:
+		 *   - arch/x86/kvm/x86.c|3156| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+		 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+		 *   - arch/x86/kvm/x86.c|3595| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7376| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+		 *
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+		 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		/*
+		 * 在以下使用kvm_get_time_scale():
+		 *   - arch/x86/kvm/x86.c|2630| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+		 *   - arch/x86/kvm/x86.c|3651| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+		 *   - arch/x86/kvm/x86.c|3908| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+		 *   - arch/x86/kvm/x86.c|4102| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC,
+		 *
+		 *
+		 * 在以下使用get_cpu_tsc_khz():
+		 *   - arch/x86/kvm/x86.c|3423| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+		 *   - arch/x86/kvm/x86.c|3566| <<kvm_guest_time_update>> tgt_tsc_khz = get_cpu_tsc_khz();
+		 *   - arch/x86/kvm/x86.c|3704| <<kvm_get_wall_clock_epoch>> local_tsc_khz = get_cpu_tsc_khz();
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
 		data->clock = __pvclock_read_cycles(&hv_clock, data->host_tsc);
 	} else {
+		/*
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+		 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
 	}
 
 	put_cpu();
 }
 
+/*
+ * 在以下使用get_kvmclock():
+ *   - arch/x86/kvm/x86.c|3452| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+ *   - arch/x86/kvm/x86.c|7470| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+ *
+ * 用struct kvm_clock_data返回当前VM启动了的时间
+ * 包括guest_time, tsc, 和realtime
+ */
 static void get_kvmclock(struct kvm *kvm, struct kvm_clock_data *data)
 {
 	struct kvm_arch *ka = &kvm->arch;
 	unsigned seq;
 
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|4010| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4063| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4293| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4300| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|4466| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4505| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|4774| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4840| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|15333| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 用struct kvm_clock_data返回当前VM启动了的时间
+		 * 包括guest_time, tsc, 和realtime
+		 * 只在这里调用
+		 */
 		__get_kvmclock(kvm, data);
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 }
 
+/*
+ * 在以下使用get_kvmclock_ns():
+ *   - arch/x86/kvm/hyperv.c|583| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+ *   - arch/x86/kvm/x86.c|3753| <<kvm_get_wall_clock_epoch>> return ktime_get_real_ns() - get_kvmclock_ns(kvm);
+ *   - arch/x86/kvm/xen.c|284| <<kvm_xen_start_timer>> guest_now = get_kvmclock_ns(vcpu->kvm);
+ *   - arch/x86/kvm/xen.c|602| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+ *   - arch/x86/kvm/xen.c|1051| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *   - arch/x86/kvm/xen.c|1092| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+ *
+ * 调用get_kvmclock()
+ * 用struct kvm_clock_data返回当前VM启动了的时间
+ * 包括guest_time, tsc, 和realtime
+ * 这个函数get_kvmclock_ns()只有一个guest time的ns
+ */
 u64 get_kvmclock_ns(struct kvm *kvm)
 {
 	struct kvm_clock_data data;
 
+	/*
+	 * 在以下使用get_kvmclock():
+	 *   - arch/x86/kvm/x86.c|3452| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+	 *   - arch/x86/kvm/x86.c|7470| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+	 *
+	 * 用struct kvm_clock_data返回当前VM启动了的时间
+	 * 包括guest_time, tsc, 和realtime
+	 */
 	get_kvmclock(kvm, &data);
 	return data.clock;
 }
 
+/*
+ * 在以下使用kvm_setup_guest_pvclock():
+ *   - arch/x86/kvm/x86.c|3635| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+ *   - arch/x86/kvm/x86.c|3655| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_info_cache,
+ *   - arch/x86/kvm/x86.c|3658| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_time_info_cache, 0);
+ */
 static void kvm_setup_guest_pvclock(struct pvclock_vcpu_time_info *ref_hv_clock,
 				    struct kvm_vcpu *vcpu,
 				    struct gfn_to_pfn_cache *gpc,
@@ -3300,6 +4437,66 @@ static void kvm_setup_guest_pvclock(struct pvclock_vcpu_time_info *ref_hv_clock,
 	trace_kvm_pvclock_update(vcpu->vcpu_id, &hv_clock);
 }
 
+/*
+ * [RFC PATCH v3 10/21] KVM: x86: Fix software TSC upscaling in kvm_update_guest_time()
+ * Date: Wed, 22 May 2024 01:17:05 +0100
+ *
+ * From: David Woodhouse <dwmw@amazon.co.uk>
+ *
+ * There was some confusion in kvm_update_guest_time() when software needs
+ * to advance the guest TSC.
+ *
+ * In master clock mode, there are two points of time which need to be taken
+ * into account. First there is the master clock reference point, stored in
+ * kvm->arch.master_kernel_ns (and associated host TSC ->master_cycle_now).
+ * Secondly, there is the time *now*, at the point kvm_update_guest_time()
+ * is being called.
+ *
+ * With software TSC upscaling, the guest TSC is getting further and further
+ * ahead of the host TSC as time elapses. So at time "now", the guest TSC
+ * should be further ahead of the host, than it was at master_kernel_ns.
+ *
+ * The adjustment in kvm_update_guest_time() was not taking that into
+ * account, and was only advancing the guest TSC by the appropriate amount
+ * for master_kernel_ns, *not* the current time.
+ *
+ * Fix it to calculate them both correctly.
+ *
+ * Since the KVM clock reference point in master_kernel_ns might actually
+ * be *earlier* than the reference point used for the guest TSC
+ * (vcpu->last_tsc_nsec), this might lead to a negative delta. Fix the
+ * compute_guest_tsc() function to cope with negative numbers, which
+ * then means there is no need to force a master clock update when the
+ * guest TSC is written.
+ *
+ * Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
+ */
+
+/*
+ * 在以下使用KVM_REQ_CLOCK_UPDATE:
+ *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+ *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+ *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+ *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *
+ * 处理的函数kvm_guest_time_update()
+ *
+ *
+ * 在以下使用kvm_guest_time_update():
+ *   - arch/x86/kvm/cpuid.c|2058| <<kvm_cpuid(KVM_REQ_CLOCK_UPDATE)>> kvm_guest_time_update(vcpu);
+ *   - arch/x86/kvm/x86.c|11448| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+ */
 int kvm_guest_time_update(struct kvm_vcpu *v)
 {
 	struct pvclock_vcpu_time_info hv_clock = {};
@@ -3319,22 +4516,94 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	 * to the guest.
 	 */
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|4010| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4063| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4293| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4300| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|4466| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4505| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|4774| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4840| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|15333| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
+		/*
+		 * 在以下设置kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|3159| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+		 * 在以下使用kvm_arch->use_master_clock:
+		 *   - arch/x86/kvm/x86.c|2647| <<kvm_track_tsc_matching>> if ((ka->use_master_clock && new_generation) ||
+		 *   - arch/x86/kvm/x86.c|2648| <<kvm_track_tsc_matching>> (ka->use_master_clock != use_master_clock))
+		 *   - arch/x86/kvm/x86.c|2653| <<kvm_track_tsc_matching>> ka->use_master_clock, gtod->clock.vclock_mode);
+		 *   - arch/x86/kvm/x86.c|3163| <<pvclock_update_vm_gtod_copy>> if (ka->use_master_clock)
+		 *   - arch/x86/kvm/x86.c|3167| <<pvclock_update_vm_gtod_copy>> trace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,
+		 *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> if (ka->use_master_clock &&
+		 *   - arch/x86/kvm/x86.c|3434| <<kvm_guest_time_update>> use_master_clock = ka->use_master_clock;
+		 *   - arch/x86/kvm/x86.c|3572| <<kvm_get_wall_clock_epoch>> if (!ka->use_master_clock)
+		 *   - arch/x86/kvm/x86.c|5373| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+		 *   - arch/x86/kvm/x86.c|7375| <<kvm_vm_ioctl_set_clock>> if (ka->use_master_clock)
+		 *   - arch/x86/kvm/xen.c|228| <<kvm_xen_start_timer>> !vcpu->kvm->arch.use_master_clock)
+		 */
 		use_master_clock = ka->use_master_clock;
 		if (use_master_clock) {
+			/*
+			 * 在以下使用kvm_arch->master_cycle_now:
+			 *   - arch/x86/kvm/x86.c|3157| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+			 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+			 *   - arch/x86/kvm/x86.c|3334| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+			 *   - arch/x86/kvm/x86.c|3436| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+			 *   - arch/x86/kvm/x86.c|3594| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+			 */
 			host_tsc = ka->master_cycle_now;
+			/*
+			 * 在以下使用kvm_arch->master_kernel_ns:
+			 *   - arch/x86/kvm/x86.c|3156| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+			 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+			 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+			 *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+			 *   - arch/x86/kvm/x86.c|3595| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+			 *   - arch/x86/kvm/x86.c|7376| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+			 */
 			kernel_ns = ka->master_kernel_ns;
 		}
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
 
 	/* Keep irq disabled to prevent changes to the clock */
 	local_irq_save(flags);
+	/*
+	 * 在以下使用get_cpu_tsc_khz():
+	 *   - arch/x86/kvm/x86.c|3423| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+	 *   - arch/x86/kvm/x86.c|3566| <<kvm_guest_time_update>> tgt_tsc_khz = get_cpu_tsc_khz();
+	 *   - arch/x86/kvm/x86.c|3704| <<kvm_get_wall_clock_epoch>> local_tsc_khz = get_cpu_tsc_khz();
+	 */
 	tgt_tsc_khz = get_cpu_tsc_khz();
 	if (unlikely(tgt_tsc_khz == 0)) {
 		local_irq_restore(flags);
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 		return 1;
 	}
+	/*
+	 * 如果不用master clock, 这里是"最新"的时候
+	 */
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
 		kernel_ns = get_kvmclock_base_ns();
@@ -3352,9 +4621,71 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	 *      time to disappear, and the guest to stand still or run
 	 *	very slowly.
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+	 *   - arch/x86/kvm/x86.c|2554| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|3745| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+	 *   - arch/x86/kvm/x86.c|5784| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+	 *
+	 * 在kvm_arch_vcpu_load()中是tsc unstable的时候
+	 * 在set_tsc_khz()是没有tsc scaling并且(user_tsc_khz > tsc_khz)的时候
+	 * 也就是只有guest的tsc比host的tsc快的时候
+	 */
 	if (vcpu->tsc_catchup) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+		 *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+		 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+		 * 在以下使用kvm_vcpu_arch->this_tsc_write:
+		 *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+		 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+		 *
+		 *
+		 * master == 1:
+		 *   host_tsc = ka->master_cycle_now;
+		 *   tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+		 *   kernel_ns = ka->master_kernel_ns;
+		 * master == 0:
+		 *   host_tsc = rdtsc();
+		 *   tsc_timestamp = kvm_read_l1_tsc(v, host_tsc);
+		 *   kernel_ns = get_kvmclock_base_ns();
+		 *
+		 * 只在这里调用compute_guest_tsc()
+		 * 根据距离vcpu->arch.this_tsc_nsec和vcpu->arch.this_tsc_write
+		 * 计算当前应该的guest_tsc
+		 */
 		u64 tsc = compute_guest_tsc(v, kernel_ns);
+		/*
+		 * 依据某个reference time point:
+		 * tsc是根据nanosecond推算的guest_tsc应该是多少
+		 * tsc_timestamp是这个时刻guest_tsc的实际值
+		 */
 		if (tsc > tsc_timestamp) {
+			/*
+			 * 如果guest的freq比host大
+			 * 也就是guest的频率更大
+			 * 比如guest的是1000, host是100.
+			 *
+			 * 现在开始讨论, tick了1000次.
+			 * guest认为是1s, host认为过了10s
+			 * 当前guest_tsc是1000, 根据guest_freq, 实际是1000*10
+			 * 就要通过增加offset来增大当前guest_tsc的value
+			 * 这样guest就觉得赶上来了
+			 *
+			 * 反过来没法支持. 我觉得是因为不能减少tsc_offset
+			 * 这样guest_tsc会减少, 时钟倒退了.
+			 * host是1000， guest是100
+			 * "user requested TSC rate below hardware speed"
+			 *
+			 *
+			 * 在以下使用adjust_tsc_offset_guest():
+			 *   - arch/x86/kvm/x86.c|3375| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+			 *   - arch/x86/kvm/x86.c|4157| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+			 *   - arch/x86/kvm/x86.c|5013| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+			 *   - arch/x86/kvm/x86.c|5064| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+			 *
+			 * 加的offset越多, guest_tsc越大
+			 */
 			adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
 			tsc_timestamp = tsc;
 		}
@@ -3365,6 +4696,9 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	/* With all the info we got, fill in the values */
 
 	if (kvm_caps.has_tsc_control) {
+		/*
+		 * 上面的tgt_tsc_khz来自host hardware
+		 */
 		tgt_tsc_khz = kvm_scale_tsc(tgt_tsc_khz,
 					    v->arch.l1_tsc_scaling_ratio);
 		tgt_tsc_khz = tgt_tsc_khz ? : 1;
@@ -3380,7 +4714,25 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	hv_clock.tsc_shift = vcpu->pvclock_tsc_shift;
 	hv_clock.tsc_to_system_mul = vcpu->pvclock_tsc_mul;
 	hv_clock.tsc_timestamp = tsc_timestamp;
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2920| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3886| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|5884| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+	 *                                       vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|12212| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	vcpu->last_guest_tsc = tsc_timestamp;
 
 	/* If the host uses TSC clocksource, then it is stable */
@@ -3398,6 +4750,12 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 			hv_clock.flags |= PVCLOCK_GUEST_STOPPED;
 			vcpu->pvclock_set_guest_stopped_request = false;
 		}
+		/*
+		 * 在以下使用kvm_setup_guest_pvclock():
+		 *   - arch/x86/kvm/x86.c|3635| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+		 *   - arch/x86/kvm/x86.c|3655| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_info_cache,
+		 *   - arch/x86/kvm/x86.c|3658| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_time_info_cache, 0);
+		 */
 		kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
 
 		hv_clock.flags &= ~PVCLOCK_GUEST_STOPPED;
@@ -3417,9 +4775,21 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
 	if (ka->xen.hvm_config.flags & KVM_XEN_HVM_CONFIG_PVCLOCK_TSC_UNSTABLE)
 		hv_clock.flags &= ~PVCLOCK_TSC_STABLE_BIT;
 
+	/*
+	 * 在以下使用kvm_setup_guest_pvclock():
+	 *   - arch/x86/kvm/x86.c|3635| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|3655| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_info_cache,
+	 *   - arch/x86/kvm/x86.c|3658| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_time_info_cache, 0);
+	 */
 	if (vcpu->xen.vcpu_info_cache.active)
 		kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_info_cache,
 					offsetof(struct compat_vcpu_info, time));
+	/*
+	 * 在以下使用kvm_setup_guest_pvclock():
+	 *   - arch/x86/kvm/x86.c|3635| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->pv_time, 0);
+	 *   - arch/x86/kvm/x86.c|3655| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_info_cache,
+	 *   - arch/x86/kvm/x86.c|3658| <<kvm_guest_time_update>> kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_time_info_cache, 0);
+	 */
 	if (vcpu->xen.vcpu_time_info_cache.active)
 		kvm_setup_guest_pvclock(&hv_clock, v, &vcpu->xen.vcpu_time_info_cache, 0);
 #endif
@@ -3445,6 +4815,17 @@ int kvm_guest_time_update(struct kvm_vcpu *v)
  * Fall back to using their values at slightly different moments by
  * calling ktime_get_real_ns() and get_kvmclock_ns() separately.
  */
+/*
+ * 在以下使用kvm_get_wall_clock_epoch():
+ *   - arch/x86/kvm/x86.c|2423| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ *
+ * 注意注释:
+ * The pvclock_wall_clock ABI tells the guest the wall clock time at
+ * which it started (i.e. its epoch, when its kvmclock was zero).
+ *
+ * 返回的是VM启动的timestamp
+ */
 uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -3455,6 +4836,18 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 	uint64_t host_tsc;
 
 	do {
+		/*
+		 * 在以下使用kvm_arch->pvclock_sc:
+		 *   - arch/x86/kvm/x86.c|4010| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4063| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4293| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4300| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|4466| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4505| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|4774| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+		 *   - arch/x86/kvm/x86.c|4840| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+		 *   - arch/x86/kvm/x86.c|15333| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+		 */
 		seq = read_seqcount_begin(&ka->pvclock_sc);
 
 		local_tsc_khz = 0;
@@ -3467,8 +4860,20 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 		 */
 		get_cpu();
 
+		/*
+		 * 在以下使用get_cpu_tsc_khz():
+		 *   - arch/x86/kvm/x86.c|3423| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+		 *   - arch/x86/kvm/x86.c|3566| <<kvm_guest_time_update>> tgt_tsc_khz = get_cpu_tsc_khz();
+		 *   - arch/x86/kvm/x86.c|3704| <<kvm_get_wall_clock_epoch>> local_tsc_khz = get_cpu_tsc_khz();
+		 */
 		local_tsc_khz = get_cpu_tsc_khz();
 
+		/*
+		 * 在以下使用kvm_get_walltime_and_clockread():
+		 *   - arch/x86/kvm/x86.c|3833| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+		 *   - arch/x86/kvm/x86.c|4319| <<kvm_get_wall_clock_epoch>> !kvm_get_walltime_and_clockread(&ts, &host_tsc))
+		 *   - arch/x86/kvm/x86.c|11404| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+		 */
 		if (local_tsc_khz &&
 		    !kvm_get_walltime_and_clockread(&ts, &host_tsc))
 			local_tsc_khz = 0; /* Fall back to old method */
@@ -3480,7 +4885,33 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 		 * After that, it's just mathematics which can happen on any
 		 * CPU at any time.
 		 */
+		/*
+		 * 在以下使用kvm_arch->master_cycle_now:
+		 *   - arch/x86/kvm/x86.c|3157| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+		 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3334| <<__get_kvmclock>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3436| <<kvm_guest_time_update>> host_tsc = ka->master_cycle_now;
+		 *   - arch/x86/kvm/x86.c|3594| <<kvm_get_wall_clock_epoch>> hv_clock.tsc_timestamp = ka->master_cycle_now;
+		 */
 		hv_clock.tsc_timestamp = ka->master_cycle_now;
+		/*
+		 * 在以下使用kvm_arch->master_kernel_ns:
+		 *   - arch/x86/kvm/x86.c|3156| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+		 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+		 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+		 *   - arch/x86/kvm/x86.c|3595| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7376| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+		 *
+		 * 在以下使用kvm_arch->kvmclock_offset:
+		 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+		 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+		 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+		 */
 		hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
 
 	} while (read_seqcount_retry(&ka->pvclock_sc, seq));
@@ -3492,13 +4923,37 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 	 * since 1970-01-01.
 	 */
 	if (local_tsc_khz) {
+		/*
+		 * 在以下使用kvm_get_time_scale():
+		 *   - arch/x86/kvm/x86.c|2630| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+		 *   - arch/x86/kvm/x86.c|3651| <<__get_kvmclock>> kvm_get_time_scale(NSEC_PER_SEC, get_cpu_tsc_khz() * 1000LL,
+		 *   - arch/x86/kvm/x86.c|3908| <<kvm_guest_time_update>> kvm_get_time_scale(NSEC_PER_SEC, tgt_tsc_khz * 1000LL,
+		 *   - arch/x86/kvm/x86.c|4102| <<kvm_get_wall_clock_epoch>> kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC,
+		 */
 		kvm_get_time_scale(NSEC_PER_SEC, local_tsc_khz * NSEC_PER_USEC,
 				   &hv_clock.tsc_shift,
 				   &hv_clock.tsc_to_system_mul);
+		/*
+		 * __pvclock_read_cycles(&hv_clock, host_tsc)应该是VM启动了多久
+		 */
 		return ts.tv_nsec + NSEC_PER_SEC * ts.tv_sec -
 			__pvclock_read_cycles(&hv_clock, host_tsc);
 	}
 #endif
+	/*
+	 * 在以下使用get_kvmclock_ns():
+	 *   - arch/x86/kvm/hyperv.c|583| <<get_time_ref_counter>> return div_u64(get_kvmclock_ns(kvm), 100);
+	 *   - arch/x86/kvm/x86.c|3753| <<kvm_get_wall_clock_epoch>> return ktime_get_real_ns() - get_kvmclock_ns(kvm);
+	 *   - arch/x86/kvm/xen.c|284| <<kvm_xen_start_timer>> guest_now = get_kvmclock_ns(vcpu->kvm);
+	 *   - arch/x86/kvm/xen.c|602| <<kvm_xen_update_runstate>> u64 now = get_kvmclock_ns(v->kvm);
+	 *   - arch/x86/kvm/xen.c|1051| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 *   - arch/x86/kvm/xen.c|1092| <<kvm_xen_vcpu_set_attr>> if (get_kvmclock_ns(vcpu->kvm) <
+	 *
+	 * 调用get_kvmclock()
+	 * 用struct kvm_clock_data返回当前VM启动了的时间
+	 * 包括guest_time, tsc, 和realtime
+	 * 这个函数get_kvmclock_ns()只有一个guest time的ns
+	 */
 	return ktime_get_real_ns() - get_kvmclock_ns(kvm);
 }
 
@@ -3518,6 +4973,19 @@ uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 
 #define KVMCLOCK_UPDATE_DELAY msecs_to_jiffies(100)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_update_work:
+ *   - arch/x86/kvm/x86.c|3893| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+ *   - arch/x86/kvm/x86.c|3906| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+ *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *   - arch/x86/kvm/x86.c|13791| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+ *
+ * 在以下使用kvmclock_update_fn():
+ *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+ *
+ * 给每个vCPU设置KVM_REQ_CLOCK_UPDATE
+ * 等于稍后调用kvm_guest_time_update()
+ */
 static void kvmclock_update_fn(struct work_struct *work)
 {
 	unsigned long i;
@@ -3528,22 +4996,91 @@ static void kvmclock_update_fn(struct work_struct *work)
 	struct kvm_vcpu *vcpu;
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 *
+		 *
+		 * kvm_guest_time_update()
+		 */
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 		kvm_vcpu_kick(vcpu);
 	}
 }
 
+/*
+ * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|2466| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|5626| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+ *   - arch/x86/kvm/x86.c|11551| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+ *                       
+ * 处理的函数kvm_gen_kvmclock_update()
+ *
+ *
+ * 处理KVM_REQ_GLOBAL_CLOCK_UPDATE:
+ *   - arch/x86/kvm/x86.c|11552| <<vcpu_enter_guest(KVM_REQ_GLOBAL_CLOCK_UPDATE)>> kvm_gen_kvmclock_update(vcpu);
+ */
 static void kvm_gen_kvmclock_update(struct kvm_vcpu *v)
 {
 	struct kvm *kvm = v->kvm;
 
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3893| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3906| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|13791| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work,
 					KVMCLOCK_UPDATE_DELAY);
 }
 
 #define KVMCLOCK_SYNC_PERIOD (300 * HZ)
 
+/*
+ * 在以下使用kvm_arch->kvmclock_sync_work:
+ *   - arch/x86/kvm/x86.c|3907| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|13250| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+ *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ *   - arch/x86/kvm/x86.c|13790| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+ *
+ *
+ * 在以下使用kvmclock_sync_fn():
+ *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+ */
 static void kvmclock_sync_fn(struct work_struct *work)
 {
 	struct delayed_work *dwork = to_delayed_work(work);
@@ -3551,7 +5088,21 @@ static void kvmclock_sync_fn(struct work_struct *work)
 					   kvmclock_sync_work);
 	struct kvm *kvm = container_of(ka, struct kvm, arch);
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3893| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3906| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|13791| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3907| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13250| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|13790| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 					KVMCLOCK_SYNC_PERIOD);
 }
@@ -3650,13 +5201,31 @@ static int set_msr_mce(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 0;
 }
 
+/*
+ * 在以下使用kvm_pv_async_pf_enabled():
+ *   - arch/x86/kvm/x86.c|4814| <<kvm_pv_enable_async_pf>> if (!kvm_pv_async_pf_enabled(vcpu)) {
+ *   - arch/x86/kvm/x86.c|15605| <<kvm_can_deliver_async_pf>> if (!kvm_pv_async_pf_enabled(vcpu))
+ *   - arch/x86/kvm/x86.c|15731| <<kvm_arch_async_page_present>> kvm_pv_async_pf_enabled(vcpu) &&
+ *   - arch/x86/kvm/x86.c|15770| <<kvm_arch_can_dequeue_async_page_present>> if (!kvm_pv_async_pf_enabled(vcpu))
+ */
 static inline bool kvm_pv_async_pf_enabled(struct kvm_vcpu *vcpu)
 {
 	u64 mask = KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.msr_en_val:
+	 *   - arch/x86/kvm/x86.c|4790| <<kvm_pv_async_pf_enabled>> return (vcpu->arch.apf.msr_en_val & mask) == mask;
+	 *   - arch/x86/kvm/x86.c|4812| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.msr_en_val = data;
+	 *   - arch/x86/kvm/x86.c|5702| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_EN)>> msr_info->data = vcpu->arch.apf.msr_en_val;
+	 *   - arch/x86/kvm/x86.c|14443| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_en_val = 0;
+	 */
 	return (vcpu->arch.apf.msr_en_val & mask) == mask;
 }
 
+/*
+ * 处理MSR_KVM_ASYNC_PF_EN:
+ *   - arch/x86/kvm/x86.c|5323| <<kvm_set_msr_common>> if (kvm_pv_enable_async_pf(vcpu, data))
+ */
 static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 {
 	gpa_t gpa = data & ~0x3f;
@@ -3676,6 +5245,13 @@ static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 	if (!lapic_in_kernel(vcpu))
 		return data ? 1 : 0;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.msr_en_val:
+	 *   - arch/x86/kvm/x86.c|4790| <<kvm_pv_async_pf_enabled>> return (vcpu->arch.apf.msr_en_val & mask) == mask;
+	 *   - arch/x86/kvm/x86.c|4812| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.msr_en_val = data;
+	 *   - arch/x86/kvm/x86.c|5702| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_EN)>> msr_info->data = vcpu->arch.apf.msr_en_val;
+	 *   - arch/x86/kvm/x86.c|14443| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_en_val = 0;
+	 */
 	vcpu->arch.apf.msr_en_val = data;
 
 	if (!kvm_pv_async_pf_enabled(vcpu)) {
@@ -3689,6 +5265,11 @@ static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 		return 1;
 
 	vcpu->arch.apf.send_always = (data & KVM_ASYNC_PF_SEND_ALWAYS);
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.delivery_as_pf_vmexit:
+	 *   - arch/x86/kvm/x86.c|4807| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
+	 *   - arch/x86/kvm/x86.c|15522| <<kvm_can_deliver_async_pf>> return vcpu->arch.apf.delivery_as_pf_vmexit;
+	 */
 	vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
 
 	kvm_async_pf_wakeup_all(vcpu);
@@ -3696,6 +5277,10 @@ static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * 处理MSR_KVM_ASYNC_PF_INT:
+ *   - arch/x86/kvm/x86.c|5330| <<kvm_set_msr_common>> if (kvm_pv_enable_async_pf_int(vcpu, data))
+ */
 static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 {
 	/* Bits 8-63 are reserved */
@@ -3705,6 +5290,12 @@ static int kvm_pv_enable_async_pf_int(struct kvm_vcpu *vcpu, u64 data)
 	if (!lapic_in_kernel(vcpu))
 		return 1;
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.msr_int_val:
+	 *   - arch/x86/kvm/x86.c|4841| <<kvm_pv_enable_async_pf_int>> vcpu->arch.apf.msr_int_val = data;
+	 *   - arch/x86/kvm/x86.c|5708| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_INT)>> msr_info->data = vcpu->arch.apf.msr_int_val;
+	 *   - arch/x86/kvm/x86.c|14444| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_int_val = 0;
+	 */
 	vcpu->arch.apf.msr_int_val = data;
 
 	vcpu->arch.apf.vec = data & KVM_ASYNC_PF_VEC_MASK;
@@ -4072,6 +5663,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (guest_cpu_cap_has(vcpu, X86_FEATURE_TSC_ADJUST)) {
 			if (!msr_info->host_initiated) {
 				s64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;
+				/*
+				 * 在以下使用adjust_tsc_offset_guest():
+				 *   - arch/x86/kvm/x86.c|3375| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+				 *   - arch/x86/kvm/x86.c|4157| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+				 *   - arch/x86/kvm/x86.c|5013| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+				 *   - arch/x86/kvm/x86.c|5064| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+				 */
 				adjust_tsc_offset_guest(vcpu, adj);
 				/* Before back to guest, tsc_timestamp must be adjusted
 				 * as well, otherwise guest's percpu pvclock time could jump.
@@ -4115,9 +5713,21 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		break;
 	case MSR_IA32_TSC:
 		if (msr_info->host_initiated) {
+			/*
+			 * 在以下使用kvm_synchronize_tsc():
+			 *   - arch/x86/kvm/x86.c|4229| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, &data);
+			 *   - arch/x86/kvm/x86.c|12937| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, NULL);
+			 */
 			kvm_synchronize_tsc(vcpu, &data);
 		} else if (!vcpu->arch.guest_tsc_protected) {
 			u64 adj = kvm_compute_l1_tsc_offset(vcpu, data) - vcpu->arch.l1_tsc_offset;
+			/*
+			 * 在以下使用adjust_tsc_offset_guest():
+			 *   - arch/x86/kvm/x86.c|3375| <<adjust_tsc_offset_host>> adjust_tsc_offset_guest(vcpu, adjustment);
+			 *   - arch/x86/kvm/x86.c|4157| <<kvm_guest_time_update>> adjust_tsc_offset_guest(v, tsc - tsc_timestamp);
+			 *   - arch/x86/kvm/x86.c|5013| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> adjust_tsc_offset_guest(vcpu, adj);
+			 *   - arch/x86/kvm/x86.c|5064| <<kvm_set_msr_common(MSR_IA32_TSC)>> adjust_tsc_offset_guest(vcpu, adj);
+			 */
 			adjust_tsc_offset_guest(vcpu, adj);
 			vcpu->arch.ia32_tsc_adjust_msr += adj;
 		}
@@ -4183,6 +5793,12 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			return 1;
 		if (data & 0x1) {
 			vcpu->arch.apf.pageready_pending = false;
+			/*
+			 * 在以下使用kvm_check_async_pf_completion():
+			 *   - arch/s390/kvm/kvm-s390.c|4785| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+			 *   - arch/x86/kvm/x86.c|5320| <<kvm_set_msr_common>> kvm_check_async_pf_completion(vcpu);
+			 *   - arch/x86/kvm/x86.c|12639| <<vcpu_enter_guest>> kvm_check_async_pf_completion(vcpu);
+			 */
 			kvm_check_async_pf_completion(vcpu);
 		}
 		break;
@@ -4541,12 +6157,25 @@ int kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF))
 			return 1;
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.msr_en_val:
+		 *   - arch/x86/kvm/x86.c|4790| <<kvm_pv_async_pf_enabled>> return (vcpu->arch.apf.msr_en_val & mask) == mask;
+		 *   - arch/x86/kvm/x86.c|4812| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.msr_en_val = data;
+		 *   - arch/x86/kvm/x86.c|5702| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_EN)>> msr_info->data = vcpu->arch.apf.msr_en_val;
+		 *   - arch/x86/kvm/x86.c|14443| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_en_val = 0;
+		 */
 		msr_info->data = vcpu->arch.apf.msr_en_val;
 		break;
 	case MSR_KVM_ASYNC_PF_INT:
 		if (!guest_pv_has(vcpu, KVM_FEATURE_ASYNC_PF_INT))
 			return 1;
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.msr_int_val:
+		 *   - arch/x86/kvm/x86.c|4841| <<kvm_pv_enable_async_pf_int>> vcpu->arch.apf.msr_int_val = data;
+		 *   - arch/x86/kvm/x86.c|5708| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_INT)>> msr_info->data = vcpu->arch.apf.msr_int_val;
+		 *   - arch/x86/kvm/x86.c|14444| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_int_val = 0;
+		 */
 		msr_info->data = vcpu->arch.apf.msr_int_val;
 		break;
 	case MSR_KVM_ASYNC_PF_ACK:
@@ -5184,6 +6813,11 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 
 static DEFINE_PER_CPU(struct kvm_vcpu *, last_vcpu);
 
+/*
+ * 在以下使用kvm_arch_vcpu_load():
+ *   - virt/kvm/kvm_main.c|170| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|6379| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -5203,6 +6837,11 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 			wbinvd_on_cpu(vcpu->cpu);
 	}
 
+	/*
+	 * 只在这里vcpu_load
+	 * vmx_vcpu_load()
+	 * svm_vcpu_load()
+	 */
 	kvm_x86_call(vcpu_load)(vcpu, cpu);
 
 	if (vcpu != per_cpu(last_vcpu, cpu)) {
@@ -5221,23 +6860,65 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	/* Save host pkru register if supported */
 	vcpu->arch.host_pkru = read_pkru();
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_offset_adjustment:
+	 *   - arch/x86/kvm/x86.c|5847| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+	 *   - arch/x86/kvm/x86.c|5848| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+	 *   - arch/x86/kvm/x86.c|5849| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_offset_adjustment = 0;
+	 *   - arch/x86/kvm/x86.c|13901| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.tsc_offset_adjustment += delta_cyc;
+	 */
 	/* Apply any externally detected TSC adjustments (due to suspend) */
 	if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+		/*
+		 * 只在这里调用
+		 */
 		adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
 		vcpu->arch.tsc_offset_adjustment = 0;
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 	}
 
 	if (unlikely(vcpu->cpu != cpu) || kvm_check_tsc_unstable()) {
+		/*
+		 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+		 *   - arch/x86/kvm/x86.c|5854| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+		 *                                     rdtsc() - vcpu->arch.last_host_tsc;
+		 *   - arch/x86/kvm/x86.c|5855| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+		 *                                     rdtsc() - vcpu->arch.last_host_tsc;
+		 *   - arch/x86/kvm/x86.c|5976| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+		 *   - arch/x86/kvm/x86.c|13850| <<kvm_arch_enable_virtualization_cpu>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+		 *   - arch/x86/kvm/x86.c|13852| <<kvm_arch_enable_virtualization_cpu>> if (vcpu->arch.last_host_tsc > max_tsc)
+		 *   - arch/x86/kvm/x86.c|13853| <<kvm_arch_enable_virtualization_cpu>> max_tsc = vcpu->arch.last_host_tsc;
+		 *   - arch/x86/kvm/x86.c|13902| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.last_host_tsc = local_tsc;
+		 */
 		s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
 				rdtsc() - vcpu->arch.last_host_tsc;
 		if (tsc_delta < 0)
 			mark_tsc_unstable("KVM discovered backwards TSC");
 
 		if (kvm_check_tsc_unstable()) {
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+			 *   - arch/x86/kvm/x86.c|2920| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+			 *   - arch/x86/kvm/x86.c|3886| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+			 *   - arch/x86/kvm/x86.c|5884| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+			 *                                       vcpu->arch.last_guest_tsc);
+			 *   - arch/x86/kvm/x86.c|12212| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+			 */
 			u64 offset = kvm_compute_l1_tsc_offset(vcpu,
 						vcpu->arch.last_guest_tsc);
+			/*
+			 * 在以下使用kvm_vcpu_write_tsc_offset():
+			 *   - arch/x86/kvm/x86.c|2808| <<__kvm_synchronize_tsc>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 *   - arch/x86/kvm/x86.c|2909| <<adjust_tsc_offset_guest>> kvm_vcpu_write_tsc_offset(vcpu, tsc_offset + adjustment);
+			 *   - arch/x86/kvm/x86.c|5361| <<kvm_arch_vcpu_load>> kvm_vcpu_write_tsc_offset(vcpu, offset);
+			 */
 			kvm_vcpu_write_tsc_offset(vcpu, offset);
+			/*
+			 * 在以下使用kvm_vcpu_arch->tsc_catchup:
+			 *   - arch/x86/kvm/x86.c|2554| <<set_tsc_khz>> vcpu->arch.tsc_catchup = 1;
+			 *   - arch/x86/kvm/x86.c|3745| <<kvm_guest_time_update>> if (vcpu->tsc_catchup) {
+			 *   - arch/x86/kvm/x86.c|5784| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_catchup = 1;
+			 */
 			if (!vcpu->arch.guest_tsc_protected)
 				vcpu->arch.tsc_catchup = 1;
 		}
@@ -5245,6 +6926,13 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		if (kvm_lapic_hv_timer_in_use(vcpu))
 			kvm_lapic_restart_hv_timer(vcpu);
 
+		/*
+		 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2466| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5626| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11551| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+		 * 处理的函数kvm_gen_kvmclock_update()
+		 */
 		/*
 		 * On a host with synchronized TSC, there is no need to update
 		 * kvmclock on vcpu->cpu migration
@@ -5332,6 +7020,18 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	}
 
 	kvm_x86_call(vcpu_put)(vcpu);
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+	 *   - arch/x86/kvm/x86.c|5854| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+	 *                                     rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|5855| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+	 *                                     rdtsc() - vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|5976| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+	 *   - arch/x86/kvm/x86.c|13850| <<kvm_arch_enable_virtualization_cpu>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+	 *   - arch/x86/kvm/x86.c|13852| <<kvm_arch_enable_virtualization_cpu>> if (vcpu->arch.last_host_tsc > max_tsc)
+	 *   - arch/x86/kvm/x86.c|13853| <<kvm_arch_enable_virtualization_cpu>> max_tsc = vcpu->arch.last_host_tsc;
+	 *   - arch/x86/kvm/x86.c|13902| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.last_host_tsc = local_tsc;
+	 */
 	vcpu->arch.last_host_tsc = rdtsc();
 }
 
@@ -5346,6 +7046,10 @@ static int kvm_vcpu_ioctl_get_lapic(struct kvm_vcpu *vcpu,
 	return kvm_apic_get_state(vcpu, s);
 }
 
+/*
+ * 在以下使用kvm_vcpu_ioctl_set_lapic():
+ *   - arch/x86/kvm/x86.c|6249| <<kvm_arch_vcpu_ioctl()>> r = kvm_vcpu_ioctl_set_lapic(vcpu, u.lapic);
+ */
 static int kvm_vcpu_ioctl_set_lapic(struct kvm_vcpu *vcpu,
 				    struct kvm_lapic_state *s)
 {
@@ -5907,11 +7611,36 @@ static int kvm_vcpu_ioctl_x86_set_xcrs(struct kvm_vcpu *vcpu,
  * EINVAL is returned when the host attempts to set the flag for a guest that
  * does not support pv clocks.
  */
+/*
+ * 在以下使用kvm_set_guest_paused():
+ *   - arch/x86/kvm/x86.c|8050| <<kvm_arch_vcpu_ioctl(KVM_KVMCLOCK_CTRL)>> r = kvm_set_guest_paused(vcpu);
+ *   - arch/x86/kvm/x86.c|8697| <<kvm_arch_suspend_notifier>> (void )kvm_set_guest_paused(vcpu);
+ */
 static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
 {
 	if (!vcpu->arch.pv_time.active)
 		return -EINVAL;
 	vcpu->arch.pvclock_set_guest_stopped_request = true;
+	/*
+	 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+	 *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+	 *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+	 *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_guest_time_update()
+	 */
 	kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 	return 0;
 }
@@ -5952,6 +7681,15 @@ static int kvm_arch_tsc_get_attr(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * 注意的地方!!!
+ * 在以下使用kvm_vcpu_arch->this_tsc_nsec:
+ *   - arch/x86/kvm/x86.c|2613| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+ *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_nsec = kvm->arch.cur_tsc_nsec;
+ * 在以下使用kvm_vcpu_arch->this_tsc_write:
+ *   - arch/x86/kvm/x86.c|2616| <<compute_guest_tsc>> tsc += vcpu->arch.this_tsc_write;
+ *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> vcpu->arch.this_tsc_write = kvm->arch.cur_tsc_write;
+ */
 static int kvm_arch_tsc_set_attr(struct kvm_vcpu *vcpu,
 				 struct kvm_device_attr *attr)
 {
@@ -5969,8 +7707,41 @@ static int kvm_arch_tsc_set_attr(struct kvm_vcpu *vcpu,
 		if (get_user(offset, uaddr))
 			break;
 
+		/*
+		 * 在以下使用kvm_arch->tsc_write_lock:
+		 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+		 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+		 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+		 */
 		raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
 
+		/*
+		 * 在以下使用kvm_arch->last_tsc_offset:
+		 *   - arch/x86/kvm/x86.c|2918| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_offset = offset;
+		 *   - arch/x86/kvm/x86.c|6644| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+		 *          matched = (vcpu->arch.virtual_tsc_khz &&
+		 *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+		 *                     kvm->arch.last_tsc_offset == offset);
+		 *
+		 * 在以下使用kvm_arch->last_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|2917| <<__kvm_synchronize_tsc>>
+		 *          kvm->arch.last_tsc_khz = vcpu->arch.virtual_tsc_khz;
+		 *   - arch/x86/kvm/x86.c|3113| <<kvm_synchronize_tsc>>
+		 *          if (synchronizing && vcpu->arch.virtual_tsc_khz == kvm->arch.last_tsc_khz) {
+		 *   - arch/x86/kvm/x86.c|6731| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>>
+		 *          matched = (vcpu->arch.virtual_tsc_khz &&
+		 *                     kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
+		 *                     kvm->arch.last_tsc_offset == offset);
+		 */
 		matched = (vcpu->arch.virtual_tsc_khz &&
 			   kvm->arch.last_tsc_khz == vcpu->arch.virtual_tsc_khz &&
 			   kvm->arch.last_tsc_offset == offset);
@@ -5978,6 +7749,11 @@ static int kvm_arch_tsc_set_attr(struct kvm_vcpu *vcpu,
 		tsc = kvm_scale_tsc(rdtsc(), vcpu->arch.l1_tsc_scaling_ratio) + offset;
 		ns = get_kvmclock_base_ns();
 
+		/*
+		 * 在以下使用__kvm_synchronize_tsc():
+		 *   - arch/x86/kvm/x86.c|2901| <<kvm_synchronize_tsc>> __kvm_synchronize_tsc(vcpu, offset, data, ns, matched, !!user_value);
+		 *   - arch/x86/kvm/x86.c|6106| <<kvm_arch_tsc_set_attr(KVM_VCPU_TSC_OFFSET)>> __kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
+		 */
 		__kvm_synchronize_tsc(vcpu, offset, tsc, ns, matched, true);
 		raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
@@ -6504,11 +8280,30 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 
 		r = -EINVAL;
 
+		/*
+		 * 在以下使用kvm_vcpu_arch->guest_tsc_protected:
+		 *   - arch/x86/kvm/svm/sev.c|4671| <<sev_vcpu_create>> vcpu->arch.guest_tsc_protected = snp_is_secure_tsc_enabled(vcpu->kvm);
+		 *   - arch/x86/kvm/vmx/tdx.c|689| <<tdx_vcpu_create>> vcpu->arch.guest_tsc_protected = true;
+		 *   - arch/x86/kvm/x86.c|3106| <<kvm_vcpu_write_tsc_offset>> if (vcpu->arch.guest_tsc_protected)
+		 *   - arch/x86/kvm/x86.c|3186| <<__kvm_synchronize_tsc>> if (vcpu->arch.guest_tsc_protected)
+		 *   - arch/x86/kvm/x86.c|5488| <<kvm_set_msr_common(MSR_IA32_TSC)>> } else if (!vcpu->arch.guest_tsc_protected) {
+		 *   - arch/x86/kvm/x86.c|6688| <<kvm_arch_vcpu_load>> if (!vcpu->arch.guest_tsc_protected)
+		 *   - arch/x86/kvm/x86.c|8009| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (vcpu->arch.guest_tsc_protected)
+		 */
 		if (vcpu->arch.guest_tsc_protected)
 			goto out;
 
 		user_tsc_khz = (u32)arg;
 
+		/*
+		 * 在以下使用kvm_caps.max_guest_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|8268| <<kvm_arch_vcpu_ioctl>> if (kvm_caps.has_tsc_control &&
+		 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+		 *   - arch/x86/kvm/x86.c|9390| <<kvm_arch_vm_ioctl>> if (kvm_caps.has_tsc_control &&
+		 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+		 *   - arch/x86/kvm/x86.c|11649| <<kvm_hyperv_tsc_notifier>> kvm_caps.max_guest_tsc_khz = tsc_khz;
+		 *   - arch/x86/kvm/x86.c|12117| <<kvm_x86_vendor_init>> kvm_caps.max_guest_tsc_khz = max;
+		 */
 		if (kvm_caps.has_tsc_control &&
 		    user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
 			goto out;
@@ -6516,12 +8311,22 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		if (user_tsc_khz == 0)
 			user_tsc_khz = tsc_khz;
 
+		/*
+		 * 在以下使用kvm_set_tsc_khz():
+		 *   - arch/x86/kvm/x86.c|7423| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+		 *   - arch/x86/kvm/x86.c|13805| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+		 */
 		if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
 			r = 0;
 
 		goto out;
 	}
 	case KVM_GET_TSC_KHZ: {
+		/*
+		 * 很多使用
+		 * 只在这里设置:
+		 *   - arch/x86/kvm/x86.c|2593| <<kvm_set_tsc_khz>> vcpu->arch.virtual_tsc_khz = user_tsc_khz;
+		 */
 		r = vcpu->arch.virtual_tsc_khz;
 		goto out;
 	}
@@ -6763,6 +8568,14 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 			goto split_irqchip_unlock;
 		if (kvm->created_vcpus)
 			goto split_irqchip_unlock;
+		/*
+		 * 在以下使用kvm_arch->irqchip_mode:
+		 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/x86.c|8279| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+		 *   - arch/x86/kvm/x86.c|8907| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		 */
 		/* Pairs with irqchip_in_kernel. */
 		smp_wmb();
 		kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
@@ -7182,10 +8995,22 @@ int kvm_arch_pm_notifier(struct kvm *kvm, unsigned long state)
 }
 #endif /* CONFIG_HAVE_KVM_PM_NOTIFIER */
 
+/*
+ * 处理KVM_GET_CLOCK:
+ *   - arch/x86/kvm/x86.c|9087| <<kvm_arch_vm_ioctl(KVM_GET_CLOCK)>> r = kvm_vm_ioctl_get_clock(kvm, argp);
+ */
 static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_clock_data data = { 0 };
 
+	/*
+	 * 在以下使用get_kvmclock():
+	 *   - arch/x86/kvm/x86.c|3452| <<get_kvmclock_ns>> get_kvmclock(kvm, &data);
+	 *   - arch/x86/kvm/x86.c|7470| <<kvm_vm_ioctl_get_clock>> get_kvmclock(kvm, &data);
+	 *
+	 * 用struct kvm_clock_data返回当前VM启动了的时间
+	 * 包括guest_time, tsc, 和realtime
+	 */
 	get_kvmclock(kvm, &data);
 	if (copy_to_user(argp, &data, sizeof(data)))
 		return -EFAULT;
@@ -7193,6 +9018,10 @@ static int kvm_vm_ioctl_get_clock(struct kvm *kvm, void __user *argp)
 	return 0;
 }
 
+/*
+ * 处理KVM_SET_CLOCK:
+ *   - arch/x86/kvm/x86.c|9084| <<kvm_arch_vm_ioctl(KVM_SET_CLOCK)>> r = kvm_vm_ioctl_set_clock(kvm, argp);
+ */
 static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 {
 	struct kvm_arch *ka = &kvm->arch;
@@ -7209,8 +9038,25 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 	if (data.flags & ~KVM_CLOCK_VALID_FLAGS)
 		return -EINVAL;
 
+	/*
+	 * 在以下使用kvm_hv_request_tsc_page_update():
+	 *   -  arch/x86/kvm/x86.c|3178| <<kvm_update_masterclock>> kvm_hv_request_tsc_page_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7226| <<kvm_vm_ioctl_set_clock>> kvm_hv_request_tsc_page_update(kvm);
+	 */
 	kvm_hv_request_tsc_page_update(kvm);
+	/*
+	 * 在以下使用kvm_start_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3226| <<kvm_update_masterclock>> kvm_start_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7281| <<kvm_vm_ioctl_set_clock>> kvm_start_pvclock_update(kvm);
+	 */
 	kvm_start_pvclock_update(kvm);
+	/*
+	 * 在以下使用pvclock_update_vm_gtod_copy():
+	 *   - arch/x86/kvm/x86.c|3180| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|7228| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|9751| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|13164| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 
 	/*
@@ -7230,11 +9076,52 @@ static int kvm_vm_ioctl_set_clock(struct kvm *kvm, void __user *argp)
 			data.clock += now_real_ns - data.realtime;
 	}
 
+	/*
+	 * 在以下设置kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|3159| <<pvclock_update_vm_gtod_copy>> ka->use_master_clock = host_tsc_clocksource && vcpus_matched
+	 * 在以下使用kvm_arch->use_master_clock:
+	 *   - arch/x86/kvm/x86.c|2647| <<kvm_track_tsc_matching>> if ((ka->use_master_clock && new_generation) ||
+	 *   - arch/x86/kvm/x86.c|2648| <<kvm_track_tsc_matching>> (ka->use_master_clock != use_master_clock))
+	 *   - arch/x86/kvm/x86.c|2653| <<kvm_track_tsc_matching>> ka->use_master_clock, gtod->clock.vclock_mode);
+	 *   - arch/x86/kvm/x86.c|3163| <<pvclock_update_vm_gtod_copy>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|3167| <<pvclock_update_vm_gtod_copy>> trace_kvm_update_master_clock(ka->use_master_clock, vclock_mode,
+	 *   - arch/x86/kvm/x86.c|3321| <<__get_kvmclock>> if (ka->use_master_clock &&
+	 *   - arch/x86/kvm/x86.c|3434| <<kvm_guest_time_update>> use_master_clock = ka->use_master_clock;
+	 *   - arch/x86/kvm/x86.c|3572| <<kvm_get_wall_clock_epoch>> if (!ka->use_master_clock)
+	 *   - arch/x86/kvm/x86.c|5373| <<kvm_arch_vcpu_load>> if (!vcpu->kvm->arch.use_master_clock || vcpu->cpu == -1)
+	 *   - arch/x86/kvm/x86.c|7375| <<kvm_vm_ioctl_set_clock>> if (ka->use_master_clock)
+	 *   - arch/x86/kvm/xen.c|228| <<kvm_xen_start_timer>> !vcpu->kvm->arch.use_master_clock)
+	 *
+	 *
+	 * 在以下使用kvm_arch->master_kernel_ns:
+	 *   - arch/x86/kvm/x86.c|3156| <<pvclock_update_vm_gtod_copy>> host_tsc_clocksource = kvm_get_time_and_clockread(
+	 *                                    &ka->master_kernel_ns, &ka->master_cycle_now);
+	 *   - arch/x86/kvm/x86.c|3335| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3437| <<kvm_guest_time_update>> kernel_ns = ka->master_kernel_ns;
+	 *   - arch/x86/kvm/x86.c|3595| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7376| <<kvm_vm_ioctl_set_clock>> now_raw_ns = ka->master_kernel_ns;
+	 */
 	if (ka->use_master_clock)
 		now_raw_ns = ka->master_kernel_ns;
 	else
 		now_raw_ns = get_kvmclock_base_ns();
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	ka->kvmclock_offset = data.clock - now_raw_ns;
+	/*
+	 * 在以下使用kvm_end_pvclock_update():
+	 *   - arch/x86/kvm/x86.c|3235| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|7313| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+	 *   - arch/x86/kvm/x86.c|9820| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+	 */
 	kvm_end_pvclock_update(kvm);
 	return 0;
 }
@@ -7317,6 +9204,19 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 			kvm_pic_destroy(kvm);
 			goto create_irqchip_unlock;
 		}
+		/*
+		 * 三种:
+		 * KVM_IRQCHIP_NONE
+		 * KVM_IRQCHIP_KERNEL
+		 * KVM_IRQCHIP_SPLIT
+		 *
+		 * 在以下使用kvm_arch->irqchip_mode:
+		 *   - arch/x86/kvm/irq.h|78| <<irqchip_full>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|99| <<irqchip_split>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/irq.h|108| <<irqchip_in_kernel>> int mode = kvm->arch.irqchip_mode;
+		 *   - arch/x86/kvm/x86.c|8279| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_SPLIT;
+		 *   - arch/x86/kvm/x86.c|8907| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
+		 */
 		/* Write kvm->irq_routing before enabling irqchip_in_kernel. */
 		smp_wmb();
 		kvm->arch.irqchip_mode = KVM_IRQCHIP_KERNEL;
@@ -7509,9 +9409,15 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 	}
 #endif
 	case KVM_SET_CLOCK:
+		/*
+		 * 只在这里调用
+		 */
 		r = kvm_vm_ioctl_set_clock(kvm, argp);
 		break;
 	case KVM_GET_CLOCK:
+		/*
+		 * 只在这里调用
+		 */
 		r = kvm_vm_ioctl_get_clock(kvm, argp);
 		break;
 	case KVM_SET_TSC_KHZ: {
@@ -7520,6 +9426,15 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		r = -EINVAL;
 		user_tsc_khz = (u32)arg;
 
+		/*
+		 * 在以下使用kvm_caps.max_guest_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|8268| <<kvm_arch_vcpu_ioctl>> if (kvm_caps.has_tsc_control &&
+		 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+		 *   - arch/x86/kvm/x86.c|9390| <<kvm_arch_vm_ioctl>> if (kvm_caps.has_tsc_control &&
+		 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+		 *   - arch/x86/kvm/x86.c|11649| <<kvm_hyperv_tsc_notifier>> kvm_caps.max_guest_tsc_khz = tsc_khz;
+		 *   - arch/x86/kvm/x86.c|12117| <<kvm_x86_vendor_init>> kvm_caps.max_guest_tsc_khz = max;
+		 */
 		if (kvm_caps.has_tsc_control &&
 		    user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
 			goto out;
@@ -7529,6 +9444,16 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 
 		mutex_lock(&kvm->lock);
 		if (!kvm->created_vcpus) {
+			/*
+			 * 在以下使用kvm_arch->default_tsc_khz:
+			 *   - arch/x86/kvm/svm/sev.c|2218| <<snp_launch_start>> if (WARN_ON_ONCE(!kvm->arch.default_tsc_khz))
+			 *   - arch/x86/kvm/svm/sev.c|2221| <<snp_launch_start>> start.desired_tsc_khz = kvm->arch.default_tsc_khz;
+			 *   - arch/x86/kvm/vmx/tdx.c|2436| <<setup_tdparams>> td_params->tsc_frequency = TDX_TSC_KHZ_TO_25MHZ(kvm->arch.default_tsc_khz);
+			 *   - arch/x86/kvm/x86.c|9104| <<kvm_arch_vm_ioctl(KVM_SET_TSC_KHZ)>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+			 *   - arch/x86/kvm/x86.c|9111| <<kvm_arch_vm_ioctl(KVM_GET_TSC_KHZ)>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+			 *   - arch/x86/kvm/x86.c|14512| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+			 *   - arch/x86/kvm/x86.c|15048| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+			 */
 			WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
 			r = 0;
 		}
@@ -7536,6 +9461,16 @@ int kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg)
 		goto out;
 	}
 	case KVM_GET_TSC_KHZ: {
+		/*
+		 * 在以下使用kvm_arch->default_tsc_khz:
+		 *   - arch/x86/kvm/svm/sev.c|2218| <<snp_launch_start>> if (WARN_ON_ONCE(!kvm->arch.default_tsc_khz))
+		 *   - arch/x86/kvm/svm/sev.c|2221| <<snp_launch_start>> start.desired_tsc_khz = kvm->arch.default_tsc_khz;
+		 *   - arch/x86/kvm/vmx/tdx.c|2436| <<setup_tdparams>> td_params->tsc_frequency = TDX_TSC_KHZ_TO_25MHZ(kvm->arch.default_tsc_khz);
+		 *   - arch/x86/kvm/x86.c|9104| <<kvm_arch_vm_ioctl(KVM_SET_TSC_KHZ)>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+		 *   - arch/x86/kvm/x86.c|9111| <<kvm_arch_vm_ioctl(KVM_GET_TSC_KHZ)>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+		 *   - arch/x86/kvm/x86.c|14512| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+		 *   - arch/x86/kvm/x86.c|15048| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+		 */
 		r = READ_ONCE(kvm->arch.default_tsc_khz);
 		goto out;
 	}
@@ -7615,6 +9550,11 @@ static void kvm_probe_feature_msr(u32 msr_index)
 	msr_based_features[num_msr_based_features++] = msr_index;
 }
 
+/*
+ * 在以下使用kvm_probe_msr_to_save():
+ *   - arch/x86/kvm/x86.c|9018| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_base[i]);
+ *   - arch/x86/kvm/x86.c|9022| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_pmu[i]);
+ */
 static void kvm_probe_msr_to_save(u32 msr_index)
 {
 	u32 dummy[2];
@@ -7722,6 +9662,10 @@ static void kvm_probe_msr_to_save(u32 msr_index)
 	msrs_to_save[num_msrs_to_save++] = msr_index;
 }
 
+/*
+ * 在以下使用kvm_init_msr_lists():
+ *   - arch/x86/kvm/x86.c|11544| <<kvm_x86_vendor_init>> kvm_init_msr_lists();
+ */
 static void kvm_init_msr_lists(void)
 {
 	unsigned i;
@@ -7733,10 +9677,26 @@ static void kvm_init_msr_lists(void)
 	num_emulated_msrs = 0;
 	num_msr_based_features = 0;
 
+	/*
+	 * 在以下使用msrs_to_save_base[]
+	 *   - arch/x86/kvm/x86.c|329| <<global>> static const u32 msrs_to_save_base[] = {
+	 *   - arch/x86/kvm/x86.c|385| <<global>> static u32 msrs_to_save[ARRAY_SIZE(msrs_to_save_base) +
+	 *   - arch/x86/kvm/x86.c|9017| <<kvm_init_msr_lists>> for (i = 0; i < ARRAY_SIZE(msrs_to_save_base); i++)
+	 *   - arch/x86/kvm/x86.c|9018| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_base[i]);
+	 *
+	 * 在以下使用kvm_probe_msr_to_save():
+	 *   - arch/x86/kvm/x86.c|9018| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_base[i]);
+	 *   - arch/x86/kvm/x86.c|9022| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_pmu[i]);
+	 */
 	for (i = 0; i < ARRAY_SIZE(msrs_to_save_base); i++)
 		kvm_probe_msr_to_save(msrs_to_save_base[i]);
 
 	if (enable_pmu) {
+		/*
+		 * 在以下使用kvm_probe_msr_to_save():
+		 *   - arch/x86/kvm/x86.c|9018| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_base[i]);
+		 *   - arch/x86/kvm/x86.c|9022| <<kvm_init_msr_lists>> kvm_probe_msr_to_save(msrs_to_save_pmu[i]);
+		 */
 		for (i = 0; i < ARRAY_SIZE(msrs_to_save_pmu); i++)
 			kvm_probe_msr_to_save(msrs_to_save_pmu[i]);
 	}
@@ -9719,6 +11679,11 @@ static void kvm_hyperv_tsc_notifier(void)
 	int cpu;
 
 	mutex_lock(&kvm_lock);
+	/*
+	 * 在以下使用kvm_make_mclock_inprogress_request():
+	 *    - arch/x86/kvm/x86.c|3185| <<kvm_start_pvclock_update>> kvm_make_mclock_inprogress_request(kvm);
+	 *    - arch/x86/kvm/x86.c|9798| <<kvm_hyperv_tsc_notifier>> kvm_make_mclock_inprogress_request(kvm);
+	 */
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_make_mclock_inprogress_request(kvm);
 
@@ -9730,11 +11695,38 @@ static void kvm_hyperv_tsc_notifier(void)
 		for_each_present_cpu(cpu)
 			per_cpu(cpu_tsc_khz, cpu) = tsc_khz;
 	}
+	/*
+	 * 在以下使用kvm_caps.max_guest_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|8268| <<kvm_arch_vcpu_ioctl>> if (kvm_caps.has_tsc_control &&
+	 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+	 *   - arch/x86/kvm/x86.c|9390| <<kvm_arch_vm_ioctl>> if (kvm_caps.has_tsc_control &&
+	 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+	 *   - arch/x86/kvm/x86.c|11649| <<kvm_hyperv_tsc_notifier>> kvm_caps.max_guest_tsc_khz = tsc_khz;
+	 *   - arch/x86/kvm/x86.c|12117| <<kvm_x86_vendor_init>> kvm_caps.max_guest_tsc_khz = max;
+	 */
 	kvm_caps.max_guest_tsc_khz = tsc_khz;
 
 	list_for_each_entry(kvm, &vm_list, vm_list) {
+		/*
+		 * 在以下使用__kvm_start_pvclock_update():
+		 *   - arch/x86/kvm/x86.c|3188| <<kvm_start_pvclock_update>> __kvm_start_pvclock_update(kvm);
+		 *   - arch/x86/kvm/x86.c|9811| <<kvm_hyperv_tsc_notifier>> __kvm_start_pvclock_update(kvm);
+		 */
 		__kvm_start_pvclock_update(kvm);
+		/*
+		 * 在以下使用pvclock_update_vm_gtod_copy():
+		 *   - arch/x86/kvm/x86.c|3180| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+		 *   - arch/x86/kvm/x86.c|7228| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+		 *   - arch/x86/kvm/x86.c|9751| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+		 *   - arch/x86/kvm/x86.c|13164| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+		 */
 		pvclock_update_vm_gtod_copy(kvm);
+		/*
+		 * 在以下使用kvm_end_pvclock_update():
+		 *   - arch/x86/kvm/x86.c|3235| <<kvm_update_masterclock>> kvm_end_pvclock_update(kvm);
+		 *   - arch/x86/kvm/x86.c|7313| <<kvm_vm_ioctl_set_clock>> kvm_end_pvclock_update(kvm);
+		 *   - arch/x86/kvm/x86.c|9820| <<kvm_hyperv_tsc_notifier>> kvm_end_pvclock_update(kvm);
+		 */
 		kvm_end_pvclock_update(kvm);
 	}
 
@@ -9849,6 +11841,12 @@ static int kvmclock_cpu_online(unsigned int cpu)
 static void kvm_timer_init(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC)) {
+		/*
+		 * 在以下使用max_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|11786| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+		 *   - arch/x86/kvm/x86.c|11796| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+		 *   - arch/x86/kvm/x86.c|15440| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+		 */
 		max_tsc_khz = tsc_khz;
 
 		if (IS_ENABLED(CONFIG_CPU_FREQ)) {
@@ -9873,20 +11871,51 @@ static void kvm_timer_init(void)
 }
 
 #ifdef CONFIG_X86_64
+/*
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|10546| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|10555| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|10824| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static void pvclock_gtod_update_fn(struct work_struct *work)
 {
 	struct kvm *kvm;
 	struct kvm_vcpu *vcpu;
 	unsigned long i;
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+	 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_update_masterclock()
+	 */
 	mutex_lock(&kvm_lock);
 	list_for_each_entry(kvm, &vm_list, vm_list)
 		kvm_for_each_vcpu(i, vcpu, kvm)
 			kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2526| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3299| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|10542| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|10577| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	atomic_set(&kvm_guest_has_master_clock, 0);
 	mutex_unlock(&kvm_lock);
 }
 
+/*
+ * 在以下使用pvclock_gtod_work:
+ *   - arch/x86/kvm/x86.c|10546| <<global>> static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
+ *   - arch/x86/kvm/x86.c|10555| <<pvclock_irq_work_fn>> queue_work(system_long_wq, &pvclock_gtod_work);
+ *   - arch/x86/kvm/x86.c|10824| <<kvm_x86_vendor_exit>> cancel_work_sync(&pvclock_gtod_work);
+ */
 static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
 
 /*
@@ -9894,6 +11923,12 @@ static DECLARE_WORK(pvclock_gtod_work, pvclock_gtod_update_fn);
  * region to prevent possible deadlocks against time accessors which
  * are invoked with work related locks held.
  */
+/*
+ * 在以下使用pvclock_irq_work:
+ *   - arch/x86/kvm/x86.c|10865| <<global>> static DEFINE_IRQ_WORK(pvclock_irq_work, pvclock_irq_work_fn);
+ *   - arch/x86/kvm/x86.c|10895| <<pvclock_gtod_notify>> irq_work_queue(&pvclock_irq_work);
+ *   - arch/x86/kvm/x86.c|11140| <<kvm_x86_vendor_exit>> irq_work_sync(&pvclock_irq_work);
+ */
 static void pvclock_irq_work_fn(struct irq_work *w)
 {
 	queue_work(system_long_wq, &pvclock_gtod_work);
@@ -9912,17 +11947,32 @@ static int pvclock_gtod_notify(struct notifier_block *nb, unsigned long unused,
 
 	update_pvclock_gtod(tk);
 
+	/*
+	 * 这里主要是disable
+	 */
 	/*
 	 * Disable master clock if host does not trust, or does not use,
 	 * TSC based clocksource. Delegate queue_work() to irq_work as
 	 * this is invoked with tk_core.seq write held.
 	 */
+	/*
+	 * 在以下使用kvm_guest_has_master_clock:
+	 *   - arch/x86/kvm/x86.c|2526| <<global>> static atomic_t kvm_guest_has_master_clock = ATOMIC_INIT(0);
+	 *   - arch/x86/kvm/x86.c|3299| <<pvclock_update_vm_gtod_copy>> atomic_set(&kvm_guest_has_master_clock, 1);
+	 *   - arch/x86/kvm/x86.c|10542| <<pvclock_gtod_update_fn>> atomic_set(&kvm_guest_has_master_clock, 0);
+	 *   - arch/x86/kvm/x86.c|10577| <<pvclock_gtod_notify>> atomic_read(&kvm_guest_has_master_clock) != 0)
+	 */
 	if (!gtod_is_based_on_tsc(gtod->clock.vclock_mode) &&
 	    atomic_read(&kvm_guest_has_master_clock) != 0)
 		irq_work_queue(&pvclock_irq_work);
 	return 0;
 }
 
+/*
+ * 在以下使用pvclock_gtod_notifier:
+ *   - arch/x86/kvm/x86.c|11067| <<kvm_x86_vendor_init>> pvclock_gtod_register_notifier(&pvclock_gtod_notifier);
+ *   - arch/x86/kvm/x86.c|11139| <<kvm_x86_vendor_exit>> pvclock_gtod_unregister_notifier(&pvclock_gtod_notifier);
+ */
 static struct notifier_block pvclock_gtod_notifier = {
 	.notifier_call = pvclock_gtod_notify,
 };
@@ -10120,6 +12170,13 @@ int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 	}
 
 	if (kvm_caps.has_tsc_control) {
+		/*
+		 * 在以下使用kvm_caps.max_tsc_scaling_ratio:
+		 *   - arch/x86/kvm/svm/svm.c|5455| <<svm_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;
+		 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;
+		 *   - arch/x86/kvm/x86.c|2748| <<set_tsc_khz>> if (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {
+		 *   - arch/x86/kvm/x86.c|12116| <<kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+		 */
 		/*
 		 * Make sure the user can only configure tsc_khz values that
 		 * fit into a signed integer.
@@ -10128,8 +12185,32 @@ int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 		 */
 		u64 max = min(0x7fffffffULL,
 			      __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+		/*
+		 * 在以下使用kvm_caps.max_guest_tsc_khz:
+		 *   - arch/x86/kvm/x86.c|8268| <<kvm_arch_vcpu_ioctl>> if (kvm_caps.has_tsc_control &&
+		 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+		 *   - arch/x86/kvm/x86.c|9390| <<kvm_arch_vm_ioctl>> if (kvm_caps.has_tsc_control &&
+		 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+		 *   - arch/x86/kvm/x86.c|11649| <<kvm_hyperv_tsc_notifier>> kvm_caps.max_guest_tsc_khz = tsc_khz;
+		 *   - arch/x86/kvm/x86.c|12117| <<kvm_x86_vendor_init>> kvm_caps.max_guest_tsc_khz = max;
+		 */
 		kvm_caps.max_guest_tsc_khz = max;
 	}
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
 	kvm_init_msr_lists();
 	return 0;
@@ -10197,9 +12278,21 @@ static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 	 * When tsc is in permanent catchup mode guests won't be able to use
 	 * pvclock_read_retry loop to get consistent view of pvclock
 	 */
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2555| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10856| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|12131| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	if (vcpu->arch.tsc_always_catchup)
 		return -KVM_EOPNOTSUPP;
 
+	/*
+	 * 在以下使用kvm_get_walltime_and_clockread():
+	 *   - arch/x86/kvm/x86.c|3833| <<__get_kvmclock>> if (kvm_get_walltime_and_clockread(&ts, &data->host_tsc)) {
+	 *   - arch/x86/kvm/x86.c|4319| <<kvm_get_wall_clock_epoch>> !kvm_get_walltime_and_clockread(&ts, &host_tsc))
+	 *   - arch/x86/kvm/x86.c|11404| <<kvm_pv_clock_pairing>> if (!kvm_get_walltime_and_clockread(&ts, &cycle))
+	 */
 	if (!kvm_get_walltime_and_clockread(&ts, &cycle))
 		return -KVM_EOPNOTSUPP;
 
@@ -10842,6 +12935,13 @@ int kvm_get_nr_pending_nmis(struct kvm_vcpu *vcpu)
 void kvm_make_scan_ioapic_request_mask(struct kvm *kvm,
 				       unsigned long *vcpu_bitmap)
 {
+	/*
+	 * 在以下使用kvm_make_vcpus_request_mask():
+	 *   - arch/riscv/kvm/tlb.c|331| <<make_xfence_request>> kvm_make_vcpus_request_mask(kvm, actual_req, vcpu_mask);
+	 *   - arch/x86/kvm/hyperv.c|2176| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+	 *   - arch/x86/kvm/hyperv.c|2209| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+	 *   - arch/x86/kvm/x86.c|11028| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
+	 */
 	kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
 }
 
@@ -11057,11 +13157,54 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 			kvm_mmu_free_obsolete_roots(vcpu);
 		if (kvm_check_request(KVM_REQ_MIGRATE_TIMER, vcpu))
 			__kvm_migrate_timers(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/cpuid.c|2057| <<kvm_cpuid>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|3302| <<kvm_end_pvclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3569| <<kvm_guest_time_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|3782| <<kvmclock_update_fn>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|3791| <<kvm_gen_kvmclock_update>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
+		 *   - arch/x86/kvm/x86.c|4330| <<kvm_set_msr_common(MSR_IA32_TSC_ADJUST)>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5494| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|6191| <<kvm_set_guest_paused>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|10150| <<__kvmclock_cpufreq_notifier>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11447| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|11828| <<vcpu_enter_guest>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|13405| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|955| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|970| <<kvm_xen_vcpu_set_attr>> kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_guest_time_update()
+		 *
+		 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE:
+		 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+		 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+		 *
+		 * 处理的函数kvm_update_masterclock()
+		 *
+		 * 在以下使用KVM_REQ_GLOBAL_CLOCK_UPDATE:
+		 *   - arch/x86/kvm/x86.c|2466| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|5626| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu);
+		 *   - arch/x86/kvm/x86.c|11551| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
+		 *
+		 * 处理的函数kvm_gen_kvmclock_update()
+		 */
 		if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
 			kvm_update_masterclock(vcpu->kvm);
 		if (kvm_check_request(KVM_REQ_GLOBAL_CLOCK_UPDATE, vcpu))
 			kvm_gen_kvmclock_update(vcpu);
 		if (kvm_check_request(KVM_REQ_CLOCK_UPDATE, vcpu)) {
+			/*
+			 * 在以下使用kvm_guest_time_update():
+			 *   - arch/x86/kvm/cpuid.c|2058| <<kvm_cpuid>> kvm_guest_time_update(vcpu);
+			 *   - arch/x86/kvm/x86.c|11448| <<vcpu_enter_guest(KVM_REQ_CLOCK_UPDATE)>> r = kvm_guest_time_update(vcpu);
+			 */
 			r = kvm_guest_time_update(vcpu);
 			if (unlikely(r))
 				goto out;
@@ -11109,6 +13252,24 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 				goto out;
 			}
 		}
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.halted:
+		 *   - arch/x86/kvm/x86.c|12577| <<vcpu_enter_guest(KVM_REQ_APF_HALT)>> vcpu->arch.apf.halted = true;
+		 *   - arch/x86/kvm/x86.c|12952| <<kvm_vcpu_running>> return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+		 *                                                            !vcpu->arch.apf.halted);
+		 *   - arch/x86/kvm/x86.c|13067| <<vcpu_block>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|14383| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|15612| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+		 *
+		 *
+		 * 在以下使用KVM_REQ_APF_HALT:
+		 *   - arch/x86/kvm/mmu/mmu.c|4639| <<__kvm_mmu_faultin_pfn>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+		 *   - arch/x86/kvm/x86.c|12575| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|15581| <<kvm_arch_async_page_not_present>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+		 *
+		 * 处理的代码:
+		 * vcpu->arch.apf.halted = true;
+		 */
 		if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
 			/* Page is swapped out. Do synthetic halt */
 			vcpu->arch.apf.halted = true;
@@ -11178,6 +13339,20 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 #endif
 		if (kvm_check_request(KVM_REQ_APICV_UPDATE, vcpu))
 			kvm_vcpu_update_apicv(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_APF_READY:
+		 *   - arch/x86/kvm/lapic.c|478| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+		 *   - arch/x86/kvm/lapic.c|2791| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+		 *   - arch/x86/kvm/x86.c|12650| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+		 *   - arch/x86/kvm/x86.c|15626| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+		 *
+		 * 处理的函数kvm_check_async_pf_completion()
+		 *
+		 * 在以下使用kvm_check_async_pf_completion():
+		 *   - arch/s390/kvm/kvm-s390.c|4785| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+		 *   - arch/x86/kvm/x86.c|5320| <<kvm_set_msr_common>> kvm_check_async_pf_completion(vcpu);
+		 *   - arch/x86/kvm/x86.c|12639| <<vcpu_enter_guest>> kvm_check_async_pf_completion(vcpu);
+		 */
 		if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
 			kvm_check_async_pf_completion(vcpu);
 
@@ -11372,6 +13547,14 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		hw_breakpoint_restore();
 
 	vcpu->arch.last_vmentry_cpu = vcpu->cpu;
+	/*
+	 * 在以下使用kvm_vcpu_arch->last_guest_tsc:
+	 *   - arch/x86/kvm/x86.c|2920| <<__kvm_synchronize_tsc>> vcpu->arch.last_guest_tsc = tsc;
+	 *   - arch/x86/kvm/x86.c|3886| <<kvm_guest_time_update>> vcpu->last_guest_tsc = tsc_timestamp;
+	 *   - arch/x86/kvm/x86.c|5884| <<kvm_arch_vcpu_load>> u64 offset = kvm_compute_l1_tsc_offset(vcpu,
+	 *                                       vcpu->arch.last_guest_tsc);
+	 *   - arch/x86/kvm/x86.c|12212| <<vcpu_enter_guest>> vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
+	 */
 	vcpu->arch.last_guest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());
 
 	vcpu->mode = OUTSIDE_GUEST_MODE;
@@ -11441,6 +13624,12 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		profile_hit(KVM_PROFILING, (void *)rip);
 	}
 
+	/*
+	 * 在以下使用kvm_vcpu_arch->tsc_always_catchup:
+	 *   - arch/x86/kvm/x86.c|2555| <<set_tsc_khz>> vcpu->arch.tsc_always_catchup = 1;
+	 *   - arch/x86/kvm/x86.c|10856| <<kvm_pv_clock_pairing>> if (vcpu->arch.tsc_always_catchup)
+	 *   - arch/x86/kvm/x86.c|12131| <<vcpu_enter_guest>> if (unlikely(vcpu->arch.tsc_always_catchup))
+	 */
 	if (unlikely(vcpu->arch.tsc_always_catchup))
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 
@@ -11465,6 +13654,15 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 
 static bool kvm_vcpu_running(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.halted:
+	 *   - arch/x86/kvm/x86.c|12577| <<vcpu_enter_guest(KVM_REQ_APF_HALT)>> vcpu->arch.apf.halted = true;
+	 *   - arch/x86/kvm/x86.c|12952| <<kvm_vcpu_running>> return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+	 *                                                            !vcpu->arch.apf.halted);
+	 *   - arch/x86/kvm/x86.c|13067| <<vcpu_block>> vcpu->arch.apf.halted = false;
+	 *   - arch/x86/kvm/x86.c|14383| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+	 *   - arch/x86/kvm/x86.c|15612| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+	 */
 	return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
 		!vcpu->arch.apf.halted);
 }
@@ -11581,6 +13779,15 @@ static inline int vcpu_block(struct kvm_vcpu *vcpu)
 		kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
 		fallthrough;
 	case KVM_MP_STATE_RUNNABLE:
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.halted:
+		 *   - arch/x86/kvm/x86.c|12577| <<vcpu_enter_guest(KVM_REQ_APF_HALT)>> vcpu->arch.apf.halted = true;
+		 *   - arch/x86/kvm/x86.c|12952| <<kvm_vcpu_running>> return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+		 *                                                            !vcpu->arch.apf.halted);
+		 *   - arch/x86/kvm/x86.c|13067| <<vcpu_block>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|14383| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|15612| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+		 */
 		vcpu->arch.apf.halted = false;
 		break;
 	case KVM_MP_STATE_INIT_RECEIVED:
@@ -11925,6 +14132,12 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.exception_from_userspace && is_guest_mode(vcpu) &&
 	    kvm_x86_ops.nested_ops->is_exception_vmexit(vcpu, ex->vector,
 							ex->error_code)) {
+		/*
+		 * 在以下使用kvm_queue_exception_vmexit():
+		 *   - arch/x86/kvm/x86.c|883| <<kvm_multiple_exception>> kvm_queue_exception_vmexit(vcpu, nr, has_error, error_code,
+		 *   - arch/x86/kvm/x86.c|1011| <<kvm_inject_page_fault>> kvm_queue_exception_vmexit(vcpu, PF_VECTOR,
+		 *   - arch/x86/kvm/x86.c|13411| <<kvm_arch_vcpu_ioctl_run>> kvm_queue_exception_vmexit(vcpu, ex->vector,
+		 */
 		kvm_queue_exception_vmexit(vcpu, ex->vector,
 					   ex->has_error_code, ex->error_code,
 					   ex->has_payload, ex->payload);
@@ -12635,6 +14848,10 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return kvm_x86_call(vcpu_precreate)(kvm);
 }
 
+/*
+ * 在以下使用kvm_arch_vcpu_create():
+ *   - virt/kvm/kvm_main.c|4372| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -12709,6 +14926,20 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	kvm_xen_init_vcpu(vcpu);
 	vcpu_load(vcpu);
 	kvm_vcpu_after_set_cpuid(vcpu);
+	/*
+	 * 在以下使用kvm_arch->default_tsc_khz:
+	 *   - arch/x86/kvm/svm/sev.c|2218| <<snp_launch_start>> if (WARN_ON_ONCE(!kvm->arch.default_tsc_khz))
+	 *   - arch/x86/kvm/svm/sev.c|2221| <<snp_launch_start>> start.desired_tsc_khz = kvm->arch.default_tsc_khz;
+	 *   - arch/x86/kvm/vmx/tdx.c|2436| <<setup_tdparams>> td_params->tsc_frequency = TDX_TSC_KHZ_TO_25MHZ(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|9104| <<kvm_arch_vm_ioctl(KVM_SET_TSC_KHZ)>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|9111| <<kvm_arch_vm_ioctl(KVM_GET_TSC_KHZ)>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|14512| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|15048| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+	 *
+	 * 在以下使用kvm_set_tsc_khz():
+	 *   - arch/x86/kvm/x86.c|7423| <<kvm_arch_vcpu_ioctl(KVM_SET_TSC_KHZ)>> if (!kvm_set_tsc_khz(vcpu, user_tsc_khz))
+	 *   - arch/x86/kvm/x86.c|13805| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	 */
 	kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
 	kvm_vcpu_reset(vcpu, false);
 	kvm_init_mmu(vcpu);
@@ -12739,6 +14970,11 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 	if (mutex_lock_killable(&vcpu->mutex))
 		return;
 	vcpu_load(vcpu);
+	/*
+	 * 在以下使用kvm_synchronize_tsc():
+	 *   - arch/x86/kvm/x86.c|4229| <<kvm_set_msr_common(MSR_IA32_TSC)>> kvm_synchronize_tsc(vcpu, &data);
+	 *   - arch/x86/kvm/x86.c|12937| <<kvm_arch_vcpu_postcreate>> kvm_synchronize_tsc(vcpu, NULL);
+	 */
 	kvm_synchronize_tsc(vcpu, NULL);
 	vcpu_put(vcpu);
 
@@ -12747,6 +14983,13 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 
 	mutex_unlock(&vcpu->mutex);
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3907| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13250| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|13790| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	if (kvmclock_periodic_sync && vcpu->vcpu_idx == 0)
 		schedule_delayed_work(&kvm->arch.kvmclock_sync_work,
 						KVMCLOCK_SYNC_PERIOD);
@@ -12877,7 +15120,20 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 	vcpu->arch.cr2 = 0;
 
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.msr_en_val:
+	 *   - arch/x86/kvm/x86.c|4790| <<kvm_pv_async_pf_enabled>> return (vcpu->arch.apf.msr_en_val & mask) == mask;
+	 *   - arch/x86/kvm/x86.c|4812| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.msr_en_val = data;
+	 *   - arch/x86/kvm/x86.c|5702| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_EN)>> msr_info->data = vcpu->arch.apf.msr_en_val;
+	 *   - arch/x86/kvm/x86.c|14443| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_en_val = 0;
+	 */
 	vcpu->arch.apf.msr_en_val = 0;
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.msr_int_val:
+	 *   - arch/x86/kvm/x86.c|4841| <<kvm_pv_enable_async_pf_int>> vcpu->arch.apf.msr_int_val = data;
+	 *   - arch/x86/kvm/x86.c|5708| <<kvm_get_msr_common(MSR_KVM_ASYNC_PF_INT)>> msr_info->data = vcpu->arch.apf.msr_int_val;
+	 *   - arch/x86/kvm/x86.c|14444| <<kvm_vcpu_reset>> vcpu->arch.apf.msr_int_val = 0;
+	 */
 	vcpu->arch.apf.msr_int_val = 0;
 	vcpu->arch.st.msr_val = 0;
 
@@ -12885,6 +15141,15 @@ void kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)
 
 	kvm_clear_async_pf_completion_queue(vcpu);
 	kvm_async_pf_hash_reset(vcpu);
+	/*
+	 * 在以下使用kvm_vcpu_arch->apf.halted:
+	 *   - arch/x86/kvm/x86.c|12577| <<vcpu_enter_guest(KVM_REQ_APF_HALT)>> vcpu->arch.apf.halted = true;
+	 *   - arch/x86/kvm/x86.c|12952| <<kvm_vcpu_running>> return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+	 *                                                            !vcpu->arch.apf.halted);
+	 *   - arch/x86/kvm/x86.c|13067| <<vcpu_block>> vcpu->arch.apf.halted = false;
+	 *   - arch/x86/kvm/x86.c|14383| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+	 *   - arch/x86/kvm/x86.c|15612| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+	 */
 	vcpu->arch.apf.halted = false;
 
 	kvm_xstate_reset(vcpu, init_event);
@@ -13015,6 +15280,18 @@ int kvm_arch_enable_virtualization_cpu(void)
 		kvm_for_each_vcpu(i, vcpu, kvm) {
 			if (!stable && vcpu->cpu == smp_processor_id())
 				kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
+			/*
+			 * 在以下使用kvm_vcpu_arch->last_host_tsc:
+			 *   - arch/x86/kvm/x86.c|5854| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+			 *                                     rdtsc() - vcpu->arch.last_host_tsc;
+			 *   - arch/x86/kvm/x86.c|5855| <<kvm_arch_vcpu_load>> s64 tsc_delta = !vcpu->arch.last_host_tsc ? 0 :
+			 *                                     rdtsc() - vcpu->arch.last_host_tsc;
+			 *   - arch/x86/kvm/x86.c|5976| <<kvm_arch_vcpu_put>> vcpu->arch.last_host_tsc = rdtsc();
+			 *   - arch/x86/kvm/x86.c|13850| <<kvm_arch_enable_virtualization_cpu>> if (stable && vcpu->arch.last_host_tsc > local_tsc) {
+			 *   - arch/x86/kvm/x86.c|13852| <<kvm_arch_enable_virtualization_cpu>> if (vcpu->arch.last_host_tsc > max_tsc)
+			 *   - arch/x86/kvm/x86.c|13853| <<kvm_arch_enable_virtualization_cpu>> max_tsc = vcpu->arch.last_host_tsc;
+			 *   - arch/x86/kvm/x86.c|13902| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.last_host_tsc = local_tsc;
+			 */
 			if (stable && vcpu->arch.last_host_tsc > local_tsc) {
 				backwards_tsc = true;
 				if (vcpu->arch.last_host_tsc > max_tsc)
@@ -13066,8 +15343,27 @@ int kvm_arch_enable_virtualization_cpu(void)
 		list_for_each_entry(kvm, &vm_list, vm_list) {
 			kvm->arch.backwards_tsc_observed = true;
 			kvm_for_each_vcpu(i, vcpu, kvm) {
+				/*
+				 * 在以下使用kvm_vcpu_arch->tsc_offset_adjustment:
+				 *   - arch/x86/kvm/x86.c|5847| <<kvm_arch_vcpu_load>> if (unlikely(vcpu->arch.tsc_offset_adjustment)) {
+				 *   - arch/x86/kvm/x86.c|5848| <<kvm_arch_vcpu_load>> adjust_tsc_offset_host(vcpu, vcpu->arch.tsc_offset_adjustment);
+				 *   - arch/x86/kvm/x86.c|5849| <<kvm_arch_vcpu_load>> vcpu->arch.tsc_offset_adjustment = 0;
+				 *   - arch/x86/kvm/x86.c|13901| <<kvm_arch_enable_virtualization_cpu>> vcpu->arch.tsc_offset_adjustment += delta_cyc;
+				 */
 				vcpu->arch.tsc_offset_adjustment += delta_cyc;
 				vcpu->arch.last_host_tsc = local_tsc;
+				/*
+				 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE():
+				 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+				 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+				 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+				 *
+				 * 处理的函数kvm_update_masterclock()
+				 */
 				kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
 			}
 
@@ -13077,7 +15373,20 @@ int kvm_arch_enable_virtualization_cpu(void)
 			 * you may have some problem.  Solving this issue is
 			 * left as an exercise to the reader.
 			 */
+			/*
+			 * 在以下使用kvm_arch->last_tsc_nsec:
+			 *   - arch/x86/kvm/x86.c|2915| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_nsec = ns;
+			 *   - arch/x86/kvm/x86.c|3067| <<kvm_synchronize_tsc>> elapsed = ns - kvm->arch.last_tsc_nsec;
+			 *   - arch/x86/kvm/x86.c|14062| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_nsec = 0;
+			 */
 			kvm->arch.last_tsc_nsec = 0;
+			/*
+			 * 在以下使用kvm_arch->last_tsc_write:
+			 *   - arch/x86/kvm/x86.c|2916| <<__kvm_synchronize_tsc>> kvm->arch.last_tsc_write = tsc;
+			 *   - arch/x86/kvm/x86.c|3082| <<kvm_synchronize_tsc>> u64 tsc_exp = kvm->arch.last_tsc_write +
+			 *                                                         nsec_to_cycles(vcpu, elapsed);
+			 *   - arch/x86/kvm/x86.c|14063| <<kvm_arch_enable_virtualization_cpu>> kvm->arch.last_tsc_write = 0;
+			 */
 			kvm->arch.last_tsc_write = 0;
 		}
 
@@ -13111,6 +15420,10 @@ void kvm_arch_free_vm(struct kvm *kvm)
 }
 
 
+/*
+ * 在以下使用kvm_arch_init_vm():
+ *   - virt/kvm/kvm_main.c|1250| <<kvm_create_vm>> r = kvm_arch_init_vm(kvm, type);
+ */
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
 	int ret;
@@ -13141,15 +15454,74 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 
 	atomic_set(&kvm->arch.noncoherent_dma_count, 0);
 
+	/*
+	 * 在以下使用kvm_arch->tsc_write_lock:
+	 *   - arch/x86/kvm/x86.c|3184| <<__kvm_synchronize_tsc>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3394| <<kvm_synchronize_tsc>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3512| <<kvm_synchronize_tsc>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|3855| <<pvclock_update_vm_gtod_copy>> lockdep_assert_held(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3942| <<__kvm_start_pvclock_update>> raw_spin_lock_irq(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|3982| <<kvm_end_pvclock_update>> raw_spin_unlock_irq(&ka->tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|7454| <<kvm_arch_tsc_set_attr>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|7487| <<kvm_arch_tsc_set_attr>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15103| <<kvm_arch_init_vm>> raw_spin_lock_init(&kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15105| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 *   - arch/x86/kvm/x86.c|15118| <<kvm_arch_init_vm>> raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	 *   - arch/x86/kvm/x86.c|15127| <<kvm_arch_init_vm>> raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
+	 */
 	raw_spin_lock_init(&kvm->arch.tsc_write_lock);
 	mutex_init(&kvm->arch.apic_map_lock);
+	/*
+	 * 在以下使用kvm_arch->pvclock_sc:
+	 *   - arch/x86/kvm/x86.c|4010| <<__kvm_start_pvclock_update>> write_seqcount_begin(&kvm->arch.pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4063| <<kvm_end_pvclock_update>> write_seqcount_end(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4293| <<get_kvmclock>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4300| <<get_kvmclock>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|4466| <<kvm_guest_time_update>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4505| <<kvm_guest_time_update>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|4774| <<kvm_get_wall_clock_epoch>> seq = read_seqcount_begin(&ka->pvclock_sc);
+	 *   - arch/x86/kvm/x86.c|4840| <<kvm_get_wall_clock_epoch>> } while (read_seqcount_retry(&ka->pvclock_sc, seq));
+	 *   - arch/x86/kvm/x86.c|15333| <<kvm_arch_init_vm>> seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	 */
 	seqcount_raw_spinlock_init(&kvm->arch.pvclock_sc, &kvm->arch.tsc_write_lock);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_offset:
+	 *   - arch/x86/kvm/pmu.c|668| <<kvm_pmu_rdpmc_vmware>> ctr_val = ktime_get_boottime_ns() + vcpu->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3428| <<__get_kvmclock>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3440| <<__get_kvmclock>> data->clock = get_kvmclock_base_ns() + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3685| <<kvm_guest_time_update>> hv_clock.system_time = kernel_ns + v->kvm->arch.kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|3827| <<kvm_get_wall_clock_epoch>> hv_clock.system_time = ka->master_kernel_ns + ka->kvmclock_offset;
+	 *   - arch/x86/kvm/x86.c|7666| <<kvm_vm_ioctl_set_clock>> ka->kvmclock_offset = data.clock - now_raw_ns;
+	 *   - arch/x86/kvm/x86.c|13658| <<kvm_arch_init_vm>> kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
+	 */
 	kvm->arch.kvmclock_offset = -get_kvmclock_base_ns();
 
 	raw_spin_lock_irqsave(&kvm->arch.tsc_write_lock, flags);
+	/*
+	 * 在以下使用pvclock_update_vm_gtod_copy():
+	 *   - arch/x86/kvm/x86.c|3180| <<kvm_update_masterclock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|7228| <<kvm_vm_ioctl_set_clock>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|9751| <<kvm_hyperv_tsc_notifier>> pvclock_update_vm_gtod_copy(kvm);
+	 *   - arch/x86/kvm/x86.c|13164| <<kvm_arch_init_vm>> pvclock_update_vm_gtod_copy(kvm);
+	 */
 	pvclock_update_vm_gtod_copy(kvm);
 	raw_spin_unlock_irqrestore(&kvm->arch.tsc_write_lock, flags);
 
+	/*
+	 * 在以下使用kvm_arch->default_tsc_khz:
+	 *   - arch/x86/kvm/svm/sev.c|2218| <<snp_launch_start>> if (WARN_ON_ONCE(!kvm->arch.default_tsc_khz))
+	 *   - arch/x86/kvm/svm/sev.c|2221| <<snp_launch_start>> start.desired_tsc_khz = kvm->arch.default_tsc_khz;
+	 *   - arch/x86/kvm/vmx/tdx.c|2436| <<setup_tdparams>> td_params->tsc_frequency = TDX_TSC_KHZ_TO_25MHZ(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|9104| <<kvm_arch_vm_ioctl(KVM_SET_TSC_KHZ)>> WRITE_ONCE(kvm->arch.default_tsc_khz, user_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|9111| <<kvm_arch_vm_ioctl(KVM_GET_TSC_KHZ)>> r = READ_ONCE(kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|14512| <<kvm_arch_vcpu_create>> kvm_set_tsc_khz(vcpu, vcpu->kvm->arch.default_tsc_khz);
+	 *   - arch/x86/kvm/x86.c|15048| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+	 *
+	 * 在以下使用max_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|11786| <<kvm_timer_init>> max_tsc_khz = tsc_khz;
+	 *   - arch/x86/kvm/x86.c|11796| <<kvm_timer_init>> max_tsc_khz = policy->cpuinfo.max_freq;
+	 *   - arch/x86/kvm/x86.c|15440| <<kvm_arch_init_vm>> kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
+	 */
 	kvm->arch.default_tsc_khz = max_tsc_khz ? : tsc_khz;
 	kvm->arch.apic_bus_cycle_ns = APIC_BUS_CYCLE_NS_DEFAULT;
 	kvm->arch.guest_can_read_msr_platform_info = true;
@@ -13160,7 +15532,21 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	kvm->arch.hv_root_tdp = INVALID_PAGE;
 #endif
 
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3893| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3906| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|13791| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3907| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13250| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|13790| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
 
 	kvm_apicv_init(kvm);
@@ -13269,7 +15655,21 @@ void kvm_arch_pre_destroy_vm(struct kvm *kvm)
 	 * is unsafe, i.e. will lead to use-after-free.  The PIT also needs to
 	 * be stopped before IRQ routing is freed.
 	 */
+	/*
+	 * 在以下使用kvm_arch->kvmclock_sync_work:
+	 *   - arch/x86/kvm/x86.c|3907| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13250| <<kvm_arch_vcpu_postcreate>> schedule_delayed_work(&kvm->arch.kvmclock_sync_work, KVMCLOCK_SYNC_PERIOD);
+	 *   - arch/x86/kvm/x86.c|13682| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_sync_work, kvmclock_sync_fn);
+	 *   - arch/x86/kvm/x86.c|13790| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	 */
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_sync_work);
+	/*
+	 * 在以下使用kvm_arch->kvmclock_update_work:
+	 *   - arch/x86/kvm/x86.c|3893| <<kvm_gen_kvmclock_update>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, KVMCLOCK_UPDATE_DELAY);
+	 *   - arch/x86/kvm/x86.c|3906| <<kvmclock_sync_fn>> schedule_delayed_work(&kvm->arch.kvmclock_update_work, 0);
+	 *   - arch/x86/kvm/x86.c|13681| <<kvm_arch_init_vm>> INIT_DELAYED_WORK(&kvm->arch.kvmclock_update_work, kvmclock_update_fn);
+	 *   - arch/x86/kvm/x86.c|13791| <<kvm_arch_pre_destroy_vm>> cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
+	 */
 	cancel_delayed_work_sync(&kvm->arch.kvmclock_update_work);
 
 #ifdef CONFIG_KVM_IOAPIC
@@ -13280,6 +15680,11 @@ void kvm_arch_pre_destroy_vm(struct kvm *kvm)
 	static_call_cond(kvm_x86_vm_pre_destroy)(kvm);
 }
 
+/*
+ * 在以下使用kvm_arch_destroy_vm():
+ *   - virt/kvm/kvm_main.c|1329| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+ *   - virt/kvm/kvm_main.c|1414| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+ */
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	if (current->mm == kvm->mm) {
@@ -13296,6 +15701,16 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		__x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, 0, 0);
 		mutex_unlock(&kvm->slots_lock);
 	}
+	/*
+	 * 在以下调用kvm_destroy_vcpus():
+	 *   - arch/arm64/kvm/arm.c|257| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 *   - arch/loongarch/kvm/vm.c|73| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 *   - arch/mips/kvm/mips.c|171| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 *   - arch/powerpc/kvm/powerpc.c|490| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 *   - arch/riscv/kvm/vm.c|54| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 *   - arch/s390/kvm/kvm-s390.c|3510| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 *   - arch/x86/kvm/x86.c|15451| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+	 */
 	kvm_destroy_vcpus(kvm);
 	kvm_free_msr_filter(srcu_dereference_check(kvm->arch.msr_filter, &kvm->srcu, 1));
 #ifdef CONFIG_KVM_IOAPIC
@@ -13480,6 +15895,18 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 	if (!kvm->arch.cpu_dirty_log_size)
 		return;
 
+	/*
+	 * 在以下使用kvm->nr_memslots_dirty_logging:
+	 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+	 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 */
 	nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
 	if ((enable && nr_slots == 1) || !nr_slots)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
@@ -13701,6 +16128,12 @@ void kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(kvm_set_rflags);
 
+/*
+ * 在以下使用kvm_async_pf_hash_fn():
+ *   - arch/x86/kvm/x86.c|15314| <<kvm_add_async_pf_gfn>> u32 key = kvm_async_pf_hash_fn(gfn);
+ *   - arch/x86/kvm/x86.c|15325| <<kvm_async_pf_gfn_slot>> u32 key = kvm_async_pf_hash_fn(gfn);
+ *   - arch/x86/kvm/x86.c|15355| <<kvm_del_async_pf_gfn>> k = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);
+ */
 static inline u32 kvm_async_pf_hash_fn(gfn_t gfn)
 {
 	BUILD_BUG_ON(!is_power_of_2(ASYNC_PF_PER_VCPU));
@@ -13708,26 +16141,81 @@ static inline u32 kvm_async_pf_hash_fn(gfn_t gfn)
 	return hash_32(gfn & 0xffffffff, order_base_2(ASYNC_PF_PER_VCPU));
 }
 
+/*
+ * 在以下使用kvm_async_pf_next_probe():
+ *   - arch/x86/kvm/x86.c|15317| <<kvm_add_async_pf_gfn>> key = kvm_async_pf_next_probe(key);
+ *   - arch/x86/kvm/x86.c|15330| <<kvm_async_pf_gfn_slot>> key = kvm_async_pf_next_probe(key);
+ *   - arch/x86/kvm/x86.c|15352| <<kvm_del_async_pf_gfn>> j = kvm_async_pf_next_probe(j);
+ */
 static inline u32 kvm_async_pf_next_probe(u32 key)
 {
 	return (key + 1) & (ASYNC_PF_PER_VCPU - 1);
 }
 
+/*
+ * 在以下使用kvm_add_async_pf_gfn():
+ *   - arch/x86/kvm/x86.c|15448| <<kvm_arch_async_page_not_present>> kvm_add_async_pf_gfn(vcpu, work->arch.gfn);
+ */
 static void kvm_add_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
+	/*
+	 * 在以下使用kvm_async_pf_hash_fn():
+	 *   - arch/x86/kvm/x86.c|15314| <<kvm_add_async_pf_gfn>> u32 key = kvm_async_pf_hash_fn(gfn);
+	 *   - arch/x86/kvm/x86.c|15325| <<kvm_async_pf_gfn_slot>> u32 key = kvm_async_pf_hash_fn(gfn);
+	 *   - arch/x86/kvm/x86.c|15355| <<kvm_del_async_pf_gfn>> k = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);
+	 */
 	u32 key = kvm_async_pf_hash_fn(gfn);
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *      1150         struct {
+	 *      1151                 bool halted;
+	 *      1152                 gfn_t gfns[ASYNC_PF_PER_VCPU];
+	 *      1153                 struct gfn_to_hva_cache data;
+	 *      1154                 u64 msr_en_val; // MSR_KVM_ASYNC_PF_EN
+	 *      1155                 u64 msr_int_val; // MSR_KVM_ASYNC_PF_INT
+	 *      1156                 u16 vec;
+	 *      1157                 u32 id;
+	 *      1158                 u32 host_apf_flags;
+	 *      1159                 bool send_always;
+	 *      1160                 bool delivery_as_pf_vmexit;
+	 *      1161                 bool pageready_pending;
+	 *      1162         } apf;
+	 *
+	 * 在以下使用kvm_async_pf_next_probe():
+	 *   - arch/x86/kvm/x86.c|15317| <<kvm_add_async_pf_gfn>> key = kvm_async_pf_next_probe(key);
+	 *   - arch/x86/kvm/x86.c|15330| <<kvm_async_pf_gfn_slot>> key = kvm_async_pf_next_probe(key);
+	 *   - arch/x86/kvm/x86.c|15352| <<kvm_del_async_pf_gfn>> j = kvm_async_pf_next_probe(j);
+	 */
 	while (vcpu->arch.apf.gfns[key] != ~0)
 		key = kvm_async_pf_next_probe(key);
 
 	vcpu->arch.apf.gfns[key] = gfn;
 }
 
+/*
+ * 在以下使用kvm_async_pf_gfn_slot():
+ *   - arch/x86/kvm/x86.c|15337| <<kvm_find_async_pf_gfn>> return vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;
+ *   - arch/x86/kvm/x86.c|15344| <<kvm_del_async_pf_gfn>> i = j = kvm_async_pf_gfn_slot(vcpu, gfn);
+ */
 static u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	int i;
+	/*
+	 * 在以下使用kvm_async_pf_hash_fn():
+	 *   - arch/x86/kvm/x86.c|15314| <<kvm_add_async_pf_gfn>> u32 key = kvm_async_pf_hash_fn(gfn);
+	 *   - arch/x86/kvm/x86.c|15325| <<kvm_async_pf_gfn_slot>> u32 key = kvm_async_pf_hash_fn(gfn);
+	 *   - arch/x86/kvm/x86.c|15355| <<kvm_del_async_pf_gfn>> k = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);
+	 */
 	u32 key = kvm_async_pf_hash_fn(gfn);
 
+	/*
+	 * 在以下使用kvm_async_pf_next_probe():
+	 *   - arch/x86/kvm/x86.c|15317| <<kvm_add_async_pf_gfn>> key = kvm_async_pf_next_probe(key);
+	 *   - arch/x86/kvm/x86.c|15330| <<kvm_async_pf_gfn_slot>> key = kvm_async_pf_next_probe(key);
+	 *   - arch/x86/kvm/x86.c|15352| <<kvm_del_async_pf_gfn>> j = kvm_async_pf_next_probe(j);
+	 */
 	for (i = 0; i < ASYNC_PF_PER_VCPU &&
 		     (vcpu->arch.apf.gfns[key] != gfn &&
 		      vcpu->arch.apf.gfns[key] != ~0); i++)
@@ -13736,15 +16224,33 @@ static u32 kvm_async_pf_gfn_slot(struct kvm_vcpu *vcpu, gfn_t gfn)
 	return key;
 }
 
+/*
+ * 在以下使用kvm_find_async_pf_gfn():
+ *   - arch/x86/kvm/mmu/mmu.c|4633| <<__kvm_mmu_faultin_pfn>> if (kvm_find_async_pf_gfn(vcpu, fault->gfn)) {
+ */
 bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
+	/*
+	 * 在以下使用kvm_async_pf_gfn_slot():
+	 *   - arch/x86/kvm/x86.c|15337| <<kvm_find_async_pf_gfn>> return vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;
+	 *   - arch/x86/kvm/x86.c|15344| <<kvm_del_async_pf_gfn>> i = j = kvm_async_pf_gfn_slot(vcpu, gfn);
+	 */
 	return vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;
 }
 
+/*
+ * 在以下使用kvm_del_async_pf_gfn():
+ *   - arch/x86/kvm/x86.c|15490| <<kvm_arch_async_page_present>> kvm_del_async_pf_gfn(vcpu, work->arch.gfn);
+ */
 static void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
 	u32 i, j, k;
 
+	/*
+	 * 在以下使用kvm_async_pf_gfn_slot():
+	 *   - arch/x86/kvm/x86.c|15337| <<kvm_find_async_pf_gfn>> return vcpu->arch.apf.gfns[kvm_async_pf_gfn_slot(vcpu, gfn)] == gfn;
+	 *   - arch/x86/kvm/x86.c|15344| <<kvm_del_async_pf_gfn>> i = j = kvm_async_pf_gfn_slot(vcpu, gfn);
+	 */
 	i = j = kvm_async_pf_gfn_slot(vcpu, gfn);
 
 	if (WARN_ON_ONCE(vcpu->arch.apf.gfns[i] != gfn))
@@ -13753,9 +16259,21 @@ static void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 	while (true) {
 		vcpu->arch.apf.gfns[i] = ~0;
 		do {
+			/*
+			 * 在以下使用kvm_async_pf_next_probe():
+			 *   - arch/x86/kvm/x86.c|15317| <<kvm_add_async_pf_gfn>> key = kvm_async_pf_next_probe(key);
+			 *   - arch/x86/kvm/x86.c|15330| <<kvm_async_pf_gfn_slot>> key = kvm_async_pf_next_probe(key);
+			 *   - arch/x86/kvm/x86.c|15352| <<kvm_del_async_pf_gfn>> j = kvm_async_pf_next_probe(j);
+			 */
 			j = kvm_async_pf_next_probe(j);
 			if (vcpu->arch.apf.gfns[j] == ~0)
 				return;
+			/*
+			 * 在以下使用kvm_async_pf_hash_fn():
+			 *   - arch/x86/kvm/x86.c|15314| <<kvm_add_async_pf_gfn>> u32 key = kvm_async_pf_hash_fn(gfn);
+			 *   - arch/x86/kvm/x86.c|15325| <<kvm_async_pf_gfn_slot>> u32 key = kvm_async_pf_hash_fn(gfn);
+			 *   - arch/x86/kvm/x86.c|15355| <<kvm_del_async_pf_gfn>> k = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);
+			 */
 			k = kvm_async_pf_hash_fn(vcpu->arch.apf.gfns[j]);
 			/*
 			 * k lies cyclically in ]i,j]
@@ -13768,14 +16286,44 @@ static void kvm_del_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 	}
 }
 
+/*
+ * __kvm_mmu_faultin_pfn()
+ * -> kvm_arch_setup_async_pf()
+ *    -> kvm_setup_async_pf()
+ *       -> kvm_arch_async_page_not_present()
+ *
+ * 在以下使用apf_put_user_notpresent():
+ *   - arch/x86/kvm/x86.c|15451| <<kvm_arch_async_page_not_present>> !apf_put_user_notpresent(vcpu)) {
+ */
 static inline int apf_put_user_notpresent(struct kvm_vcpu *vcpu)
 {
 	u32 reason = KVM_PV_REASON_PAGE_NOT_PRESENT;
 
+	/*
+	 * struct kvm_vcpu *vcpu:
+	 * -> struct kvm_vcpu_arch arch;
+	 *      1150         struct {
+	 *      1151                 bool halted;
+	 *      1152                 gfn_t gfns[ASYNC_PF_PER_VCPU];
+	 *      1153                 struct gfn_to_hva_cache data;
+	 *      1154                 u64 msr_en_val; // MSR_KVM_ASYNC_PF_EN
+	 *      1155                 u64 msr_int_val; // MSR_KVM_ASYNC_PF_INT
+	 *      1156                 u16 vec;
+	 *      1157                 u32 id;
+	 *      1158                 u32 host_apf_flags;
+	 *      1159                 bool send_always;
+	 *      1160                 bool delivery_as_pf_vmexit;
+	 *      1161                 bool pageready_pending;
+	 *      1162         } apf;
+	 */
 	return kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,
 				      sizeof(reason));
 }
 
+/*
+ * 在以下使用apf_put_user_ready():
+ *   - arch/x86/kvm/x86.c|15607| <<kvm_arch_async_page_present>> !apf_put_user_ready(vcpu, work->arch.token)) {
+ */
 static inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)
 {
 	unsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);
@@ -13784,6 +16332,10 @@ static inline int apf_put_user_ready(struct kvm_vcpu *vcpu, u32 token)
 					     &token, offset, sizeof(token));
 }
 
+/*
+ * 在以下使用apf_pageready_slot_free():
+ *   - arch/x86/kvm/x86.c|15636| <<kvm_arch_can_dequeue_async_page_present>> return kvm_lapic_enabled(vcpu) && apf_pageready_slot_free(vcpu);
+ */
 static inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)
 {
 	unsigned int offset = offsetof(struct kvm_vcpu_pv_apf_data, token);
@@ -13796,6 +16348,11 @@ static inline bool apf_pageready_slot_free(struct kvm_vcpu *vcpu)
 	return !val;
 }
 
+/*
+ * 在以下使用kvm_can_deliver_async_pf():
+ *   - arch/x86/kvm/x86.c|15540| <<kvm_can_do_async_pf>> if (kvm_hlt_in_guest(vcpu->kvm) && !kvm_can_deliver_async_pf(vcpu))
+ *   - arch/x86/kvm/x86.c|15562| <<kvm_arch_async_page_not_present>> if (kvm_can_deliver_async_pf(vcpu) &&
+ */
 static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 {
 
@@ -13811,6 +16368,11 @@ static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 		 * L1 needs to opt into the special #PF vmexits that are
 		 * used to deliver async page faults.
 		 */
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.delivery_as_pf_vmexit:
+		 *   - arch/x86/kvm/x86.c|4807| <<kvm_pv_enable_async_pf>> vcpu->arch.apf.delivery_as_pf_vmexit = data & KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;
+		 *   - arch/x86/kvm/x86.c|15522| <<kvm_can_deliver_async_pf>> return vcpu->arch.apf.delivery_as_pf_vmexit;
+		 */
 		return vcpu->arch.apf.delivery_as_pf_vmexit;
 	} else {
 		/*
@@ -13822,6 +16384,10 @@ static bool kvm_can_deliver_async_pf(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * 在以下使用kvm_can_do_async_pf():
+ *   - arch/x86/kvm/mmu/mmu.c|4635| <<__kvm_mmu_faultin_pfn>> if (!fault->prefetch && kvm_can_do_async_pf(vcpu)) {
+ */
 bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 {
 	if (unlikely(!lapic_in_kernel(vcpu) ||
@@ -13839,14 +16405,29 @@ bool kvm_can_do_async_pf(struct kvm_vcpu *vcpu)
 	return kvm_arch_interrupt_allowed(vcpu);
 }
 
+/*
+ * __kvm_mmu_faultin_pfn()
+ * -> kvm_arch_setup_async_pf()
+ *    -> kvm_setup_async_pf()
+ *       -> kvm_arch_async_page_not_present()
+ *
+ * 在以下使用kvm_arch_async_page_not_present():
+ *   - virt/kvm/async_pf.c|280| <<kvm_setup_async_pf>> work->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);
+ */
 bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 				     struct kvm_async_pf *work)
 {
 	struct x86_exception fault;
 
 	trace_kvm_async_pf_not_present(work->arch.token, work->cr2_or_gpa);
+	/*
+	 * 只在这里使用
+	 */
 	kvm_add_async_pf_gfn(vcpu, work->arch.gfn);
 
+	/*
+	 * apf_put_user_notpresent()返回0说明成功
+	 */
 	if (kvm_can_deliver_async_pf(vcpu) &&
 	    !apf_put_user_notpresent(vcpu)) {
 		fault.vector = PF_VECTOR;
@@ -13866,11 +16447,34 @@ bool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,
 		 * another process.  When the instruction that triggered a page
 		 * fault is retried, hopefully the page will be ready in the host.
 		 */
+		/*
+		 * 在以下使用kvm_vcpu_arch->apf.halted:
+		 *   - arch/x86/kvm/x86.c|12577| <<vcpu_enter_guest(KVM_REQ_APF_HALT)>> vcpu->arch.apf.halted = true;
+		 *   - arch/x86/kvm/x86.c|12952| <<kvm_vcpu_running>> return (vcpu->arch.mp_state == KVM_MP_STATE_RUNNABLE &&
+		 *                                                            !vcpu->arch.apf.halted);
+		 *   - arch/x86/kvm/x86.c|13067| <<vcpu_block>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|14383| <<kvm_vcpu_reset>> vcpu->arch.apf.halted = false;
+		 *   - arch/x86/kvm/x86.c|15612| <<kvm_arch_async_page_present>> vcpu->arch.apf.halted = false;
+		 *
+		 *
+		 * 在以下使用KVM_REQ_APF_HALT:
+		 *   - arch/x86/kvm/mmu/mmu.c|4639| <<__kvm_mmu_faultin_pfn>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+		 *   - arch/x86/kvm/x86.c|12575| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_HALT, vcpu)) {
+		 *   - arch/x86/kvm/x86.c|15581| <<kvm_arch_async_page_not_present>> kvm_make_request(KVM_REQ_APF_HALT, vcpu);
+		 *
+		 * 处理的代码:
+		 * vcpu->arch.apf.halted = true;
+		 */
 		kvm_make_request(KVM_REQ_APF_HALT, vcpu);
 		return false;
 	}
 }
 
+/*
+ * 在以下使用kvm_arch_async_page_present():
+ *   - virt/kvm/async_pf.c|78| <<async_pf_execute>> kvm_arch_async_page_present(vcpu, apf);
+ *   - virt/kvm/async_pf.c|167| <<kvm_check_async_pf_completion>> kvm_arch_async_page_present(vcpu, work);
+ */
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work)
 {
@@ -13896,13 +16500,33 @@ void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 	kvm_set_mp_state(vcpu, KVM_MP_STATE_RUNNABLE);
 }
 
+/*
+ * 在以下使用kvm_arch_async_page_present_queued():
+ *   - virt/kvm/async_pf.c|92| <<async_pf_execute>> kvm_arch_async_page_present_queued(vcpu);
+ *   - virt/kvm/async_pf.c|237| <<kvm_async_pf_wakeup_all>> kvm_arch_async_page_present_queued(vcpu);
+ */
 void kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用KVM_REQ_APF_READY:
+	 *   - arch/x86/kvm/lapic.c|478| <<apic_set_spiv>> kvm_make_request(KVM_REQ_APF_READY, apic->vcpu);
+	 *   - arch/x86/kvm/lapic.c|2791| <<__kvm_apic_set_base>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	 *   - arch/x86/kvm/x86.c|12650| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_APF_READY, vcpu))
+	 *   - arch/x86/kvm/x86.c|15626| <<kvm_arch_async_page_present_queued>> kvm_make_request(KVM_REQ_APF_READY, vcpu);
+	 *
+	 * 处理的函数kvm_check_async_pf_completion()
+	 *
+	 * 很重要!!!
+	 */
 	kvm_make_request(KVM_REQ_APF_READY, vcpu);
 	if (!vcpu->arch.apf.pageready_pending)
 		kvm_vcpu_kick(vcpu);
 }
 
+/*
+ * 在以下使用kvm_arch_can_dequeue_async_page_present():
+ *   - virt/kvm/async_pf.c|220| <<kvm_check_async_pf_completion>> kvm_arch_can_dequeue_async_page_present(vcpu)) {
+ */
 bool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu)
 {
 	if (!kvm_pv_async_pf_enabled(vcpu))
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index f3dc77f00..c9fd852dc 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -13,14 +13,55 @@
 #define KVM_MAX_MCE_BANKS 32
 
 struct kvm_caps {
+	/*
+	 * 在以下设置kvm_caps.has_tsc_control:
+	 *   - arch/x86/kvm/svm/svm.c|5452| <<svm_hardware_setup>> kvm_caps.has_tsc_control = true;
+	 *   - arch/x86/kvm/vmx/vmx.c|8688| <<vmx_hardware_setup>> kvm_caps.has_tsc_control = true;
+	 */
 	/* control of guest tsc rate supported? */
 	bool has_tsc_control;
+	/*
+	 * 在以下使用kvm_caps.max_guest_tsc_khz:
+	 *   - arch/x86/kvm/x86.c|8268| <<kvm_arch_vcpu_ioctl>> if (kvm_caps.has_tsc_control &&
+	 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+	 *   - arch/x86/kvm/x86.c|9390| <<kvm_arch_vm_ioctl>> if (kvm_caps.has_tsc_control &&
+	 *                                             user_tsc_khz >= kvm_caps.max_guest_tsc_khz)
+	 *   - arch/x86/kvm/x86.c|11649| <<kvm_hyperv_tsc_notifier>> kvm_caps.max_guest_tsc_khz = tsc_khz;
+	 *   - arch/x86/kvm/x86.c|12117| <<kvm_x86_vendor_init>> kvm_caps.max_guest_tsc_khz = max;
+	 */
 	/* maximum supported tsc_khz for guests */
 	u32  max_guest_tsc_khz;
+	/*
+	 * 在以下设置kvm_caps.tsc_scaling_ratio_frac_bits:
+	 *   - arch/x86/kvm/svm/svm.c|5456| <<svm_hardware_setup>> kvm_caps.tsc_scaling_ratio_frac_bits = 32;
+	 *   - arch/x86/kvm/vmx/vmx.c|8691| <<vmx_hardware_setup>> kvm_caps.tsc_scaling_ratio_frac_bits = 48;
+	 */
 	/* number of bits of the fractional part of the TSC scaling ratio */
 	u8   tsc_scaling_ratio_frac_bits;
+	/*
+	 * 在以下使用kvm_caps.max_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/svm/svm.c|5455| <<svm_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;
+	 *   - arch/x86/kvm/vmx/vmx.c|8690| <<vmx_hardware_setup>> kvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;
+	 *   - arch/x86/kvm/x86.c|2748| <<set_tsc_khz>> if (ratio == 0 || ratio >= kvm_caps.max_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/x86.c|12116| <<kvm_x86_vendor_init>> __scale_tsc(kvm_caps.max_tsc_scaling_ratio, tsc_khz));
+	 */
 	/* maximum allowed value of TSC scaling ratio */
 	u64  max_tsc_scaling_ratio;
+	/*
+	 * 在以下使用kvm_caps.default_tsc_scaling_ratio:
+	 *   - arch/x86/kvm/lapic.c|1891| <<__wait_lapic_expire>> if (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {
+	 *   - arch/x86/kvm/svm/nested.c|828| <<nested_vmcb02_prepare_control>> svm->tsc_ratio_msr != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/svm/svm.c|1186| <<__svm_vcpu_reset>> svm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|1919| <<vmx_get_l2_tsc_multiplier>> return kvm_caps.default_tsc_scaling_ratio;
+	 *   - arch/x86/kvm/vmx/vmx.c|8284| <<vmx_set_hv_timer>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&
+	 *   - arch/x86/kvm/x86.c|2589| <<set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2644| <<kvm_set_tsc_khz>> kvm_vcpu_write_tsc_multiplier(vcpu, kvm_caps.default_tsc_scaling_ratio);
+	 *   - arch/x86/kvm/x86.c|2815| <<kvm_scale_tsc>> if (ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2841| <<kvm_calc_nested_tsc_offset>> if (l2_multiplier == kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|2854| <<kvm_calc_nested_tsc_multiplier>> if (l2_multiplier != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|3264| <<adjust_tsc_offset_host>> if (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio)
+	 *   - arch/x86/kvm/x86.c|11202| <<kvm_x86_vendor_init>> kvm_caps.default_tsc_scaling_ratio = 1ULL << kvm_caps.tsc_scaling_ratio_frac_bits;
+	 */
 	/* 1ull << kvm_caps.tsc_scaling_ratio_frac_bits */
 	u64  default_tsc_scaling_ratio;
 	/* bus lock detection supported? */
@@ -513,8 +554,45 @@ static inline void kvm_pr_unimpl_rdmsr(struct kvm_vcpu *vcpu, u32 msr)
 		vcpu_unimpl(vcpu, "Unhandled RDMSR(0x%x)\n", msr);
 }
 
+/*
+ * 在以下使用nsec_to_cycles():
+ *   - arch/x86/kvm/lapic.c|1893| <<__wait_lapic_expire>> __delay(min(guest_cycles,
+ *              nsec_to_cycles(vcpu, timer_advance_ns)));
+ *   - arch/x86/kvm/lapic.c|2072| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+ *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+ *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+ *   - arch/x86/kvm/lapic.c|2073| <<update_target_expiration>> apic->lapic_timer.tscdeadline +=
+ *              nsec_to_cycles(apic->vcpu, ns_remaining_new) -
+ *              nsec_to_cycles(apic->vcpu, ns_remaining_old);
+ *   - arch/x86/kvm/lapic.c|2121| <<set_target_expiration>> apic->lapic_timer.tscdeadline =
+ *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, deadline);
+ *   - arch/x86/kvm/lapic.c|2145| <<advance_periodic_target_expiration>> apic->lapic_timer.tscdeadline =
+ *              kvm_read_l1_tsc(apic->vcpu, tscl) + nsec_to_cycles(apic->vcpu, delta);
+ *   - arch/x86/kvm/vmx/vmx.c|8255| <<vmx_set_hv_timer>> lapic_timer_advance_cycles =
+ *              nsec_to_cycles(vcpu, ktimer->timer_advance_ns);
+ *   - arch/x86/kvm/x86.c|3156| <<kvm_synchronize_tsc>> u64 tsc_exp =
+ *              kvm->arch.last_tsc_write + nsec_to_cycles(vcpu, elapsed);
+ *   - arch/x86/kvm/x86.c|3206| <<kvm_synchronize_tsc>> u64 delta = nsec_to_cycles(vcpu, elapsed);
+ */
 static inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)
 {
+	/*
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_mult:
+	 *   - arch/x86/kvm/x86.c|2624| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2614| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|518| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *
+	 * 在以下使用kvm_vcpu_arch->virtual_tsc_shift:
+	 *   - arch/x86/kvm/x86.c|2623| <<kvm_set_tsc_khz>> kvm_get_time_scale(user_tsc_khz * 1000LL, NSEC_PER_SEC,
+	 *           &vcpu->arch.virtual_tsc_shift, &vcpu->arch.virtual_tsc_mult);
+	 *   - arch/x86/kvm/x86.c|2664| <<compute_guest_tsc>> u64 tsc = pvclock_scale_delta(kernel_ns-vcpu->arch.this_tsc_nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 *   - arch/x86/kvm/x86.h|528| <<nsec_to_cycles>> return pvclock_scale_delta(nsec,
+	 *           vcpu->arch.virtual_tsc_mult, vcpu->arch.virtual_tsc_shift);
+	 */
 	return pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,
 				   vcpu->arch.virtual_tsc_shift);
 }
diff --git a/arch/x86/kvm/xen.c b/arch/x86/kvm/xen.c
index d6b2a665b..aaa835d71 100644
--- a/arch/x86/kvm/xen.c
+++ b/arch/x86/kvm/xen.c
@@ -60,6 +60,11 @@ static int kvm_xen_shared_info_init(struct kvm *kvm)
 	 * This code mirrors kvm_write_wall_clock() except that it writes
 	 * directly through the pfn cache and doesn't mark the page dirty.
 	 */
+	/*
+	 * 在以下使用kvm_get_wall_clock_epoch():
+	 *   - arch/x86/kvm/x86.c|2423| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+	 */
 	wall_nsec = kvm_get_wall_clock_epoch(kvm);
 
 	/* Paranoia checks on the 32-bit struct layout */
@@ -98,6 +103,18 @@ static int kvm_xen_shared_info_init(struct kvm *kvm)
 	wc->version = wc_version + 1;
 	read_unlock_irq(&gpc->lock);
 
+	/*
+	 * 在以下使用KVM_REQ_MASTERCLOCK_UPDATE():
+	 *   - arch/x86/kvm/hyperv.c|1440| <<kvm_hv_set_msr_pw(HV_X64_MSR_REFERENCE_TSC)>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2448| <<kvm_write_system_time>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|2625| <<kvm_track_tsc_matching>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|9899| <<pvclock_gtod_update_fn>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu))
+	 *   - arch/x86/kvm/x86.c|13085| <<kvm_arch_enable_virtualization_cpu>> kvm_make_request(KVM_REQ_MASTERCLOCK_UPDATE, vcpu);
+	 *   - arch/x86/kvm/xen.c|101| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+	 *
+	 * 处理的函数kvm_update_masterclock()
+	 */
 	kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
 
 out:
diff --git a/drivers/hv/hv_balloon.c b/drivers/hv/hv_balloon.c
index 2b4080e51..5a5a20650 100644
--- a/drivers/hv/hv_balloon.c
+++ b/drivers/hv/hv_balloon.c
@@ -1648,6 +1648,10 @@ static int hv_free_page_report(struct page_reporting_dev_info *pr_dev_info,
 	return 0;
 }
 
+/*
+ * 在以下使用enable_page_reporting():
+ *   - drivers/hv/hv_balloon.c|1990| <<balloon_probe>> enable_page_reporting();
+ */
 static void enable_page_reporting(void)
 {
 	int ret;
@@ -1664,6 +1668,11 @@ static void enable_page_reporting(void)
 	 * in the page_reporting code
 	 */
 	dm_device.pr_dev_info.order = 0;
+	/*
+	 * 在以下使用page_reporting_register():
+	 *   - drivers/hv/hv_balloon.c|1667| <<enable_page_reporting>> ret = page_reporting_register(&dm_device.pr_dev_info);
+	 *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+	 */
 	ret = page_reporting_register(&dm_device.pr_dev_info);
 	if (ret < 0) {
 		dm_device.pr_dev_info.report = NULL;
diff --git a/drivers/iommu/amd/amd_iommu_types.h b/drivers/iommu/amd/amd_iommu_types.h
index a698a2e7c..8dc67f47a 100644
--- a/drivers/iommu/amd/amd_iommu_types.h
+++ b/drivers/iommu/amd/amd_iommu_types.h
@@ -475,6 +475,24 @@
 #define LOOP_TIMEOUT		100000
 #define MMIO_STATUS_TIMEOUT	2000000
 
+/*
+ * 在以下使用DUMP_printk():
+ *   - drivers/iommu/amd/init.c|1737| <<init_iommu_from_acpi>> DUMP_printk(" DEV_ALL\t\t\tsetting: %#02x\n", e->flags);
+ *   - drivers/iommu/amd/init.c|1742| <<init_iommu_from_acpi>> DUMP_printk(" DEV_SELECT\t\t\tdevid: %04x:%02x:%02x.%x flags: %#02x\n",
+ *   - drivers/iommu/amd/init.c|1753| <<init_iommu_from_acpi>> DUMP_printk(" DEV_SELECT_RANGE_START\tdevid: %04x:%02x:%02x.%x flags: %#02x\n",
+ *   - drivers/iommu/amd/init.c|1766| <<init_iommu_from_acpi>> DUMP_printk(" DEV_ALIAS\t\t\tdevid: %04x:%02x:%02x.%x flags: %#02x devid_to: %02x:%02x.%x\n",
+ *   - drivers/iommu/amd/init.c|1783| <<init_iommu_from_acpi>> DUMP_printk(" DEV_ALIAS_RANGE\t\tdevid: %04x:%02x:%02x.%x flags: %#02x devid_to: %04x:%02x:%02x.%x\n",
+ *   - drivers/iommu/amd/init.c|1800| <<init_iommu_from_acpi>> DUMP_printk(" DEV_EXT_SELECT\t\tdevid: %04x:%02x:%02x.%x flags: %#02x ext: %08x\n",
+ *   - drivers/iommu/amd/init.c|1812| <<init_iommu_from_acpi>> DUMP_printk(" DEV_EXT_SELECT_RANGE\tdevid: %04x:%02x:%02x.%x flags: %#02x ext: %08x\n",
+ *   - drivers/iommu/amd/init.c|1825| <<init_iommu_from_acpi>> DUMP_printk(" DEV_RANGE_END\t\tdevid: %04x:%02x:%02x.%x\n",
+ *   - drivers/iommu/amd/init.c|1855| <<init_iommu_from_acpi>> DUMP_printk(" DEV_SPECIAL(%s[%d])\t\tdevid: %04x:%02x:%02x.%x, flags: %#02x\n",
+ *   - drivers/iommu/amd/init.c|1927| <<init_iommu_from_acpi>> DUMP_printk(" DEV_ACPI_HID(%s[%s])\t\tdevid: %04x:%02x:%02x.%x, flags: %#02x\n",
+ *   - drivers/iommu/amd/init.c|1980| <<alloc_pci_segment>> DUMP_printk("PCI segment : 0x%0x, last bdf : 0x%04x\n", id, last_bdf);
+ *   - drivers/iommu/amd/init.c|2312| <<init_iommu_all>> DUMP_printk("device: %04x:%02x:%02x.%01x cap: %04x "
+ *   - drivers/iommu/amd/init.c|2317| <<init_iommu_all>> DUMP_printk(" mmio-addr: %016llx\n",
+ *   - drivers/iommu/amd/init.c|3000| <<init_unity_map_range>> DUMP_printk("%s devid_start: %04x:%02x:%02x.%x devid_end: "
+ *   - drivers/iommu/amd/init.c|3578| <<early_amd_iommu_init>> DUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);
+ */
 extern bool amd_iommu_dump;
 #define DUMP_printk(format, arg...)				\
 	do {							\
@@ -507,15 +525,47 @@ extern bool amdr_ivrs_remap_support;
 						 ((devid) & 0xffff))
 
 /* Make iterating over all pci segment easier */
+/*
+ * 在以下使用amd_iommu_pci_seg_list:
+ *   - drivers/iommu/amd/init.c|208| <<global>> LIST_HEAD(amd_iommu_pci_seg_list);
+ *   - drivers/iommu/amd/amd_iommu_types.h|511| <<for_each_pci_segment>> list_for_each_entry((pci_seg), &amd_iommu_pci_seg_list, list)
+ *   - drivers/iommu/amd/amd_iommu_types.h|513| <<for_each_pci_segment_safe>> list_for_each_entry_safe((pci_seg), (next), &amd_iommu_pci_seg_list, list)
+ *   - drivers/iommu/amd/init.c|1988| <<alloc_pci_segment>> list_add_tail(&pci_seg->list, &amd_iommu_pci_seg_list);
+ */
 #define for_each_pci_segment(pci_seg) \
 	list_for_each_entry((pci_seg), &amd_iommu_pci_seg_list, list)
+/*
+ * 在以下使用amd_iommu_pci_seg_list:
+ *   - drivers/iommu/amd/init.c|208| <<global>> LIST_HEAD(amd_iommu_pci_seg_list);
+ *   - drivers/iommu/amd/amd_iommu_types.h|511| <<for_each_pci_segment>> list_for_each_entry((pci_seg), &amd_iommu_pci_seg_list, list)
+ *   - drivers/iommu/amd/amd_iommu_types.h|513| <<for_each_pci_segment_safe>> list_for_each_entry_safe((pci_seg), (next), &amd_iommu_pci_seg_list, list)
+ *   - drivers/iommu/amd/init.c|1988| <<alloc_pci_segment>> list_add_tail(&pci_seg->list, &amd_iommu_pci_seg_list);
+ */
 #define for_each_pci_segment_safe(pci_seg, next) \
 	list_for_each_entry_safe((pci_seg), (next), &amd_iommu_pci_seg_list, list)
 /*
  * Make iterating over all IOMMUs easier
  */
+/*
+ * 在以下使用amd_iommu_list:
+ *   - drivers/iommu/amd/init.c|209| <<global>> LIST_HEAD(amd_iommu_list);
+ *   - drivers/iommu/amd/amd_iommu_types.h|518| <<for_each_iommu>> list_for_each_entry((iommu), &amd_iommu_list, list)
+ *   - drivers/iommu/amd/amd_iommu_types.h|520| <<for_each_iommu_safe>> list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
+ *   - drivers/iommu/amd/init.c|327| <<get_global_efr>> if (list_is_first(&iommu->list, &amd_iommu_list)) {
+ *   - drivers/iommu/amd/init.c|2137| <<init_iommu_one>> list_add_tail(&iommu->list, &amd_iommu_list);
+ *   - drivers/iommu/amd/init.c|4006| <<amd_iommu_init>> if (ret && list_empty(&amd_iommu_list)) {
+ */
 #define for_each_iommu(iommu) \
 	list_for_each_entry((iommu), &amd_iommu_list, list)
+/*
+ * 在以下使用amd_iommu_list:
+ *   - drivers/iommu/amd/init.c|209| <<global>> LIST_HEAD(amd_iommu_list);
+ *   - drivers/iommu/amd/amd_iommu_types.h|518| <<for_each_iommu>> list_for_each_entry((iommu), &amd_iommu_list, list)
+ *   - drivers/iommu/amd/amd_iommu_types.h|520| <<for_each_iommu_safe>> list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
+ *   - drivers/iommu/amd/init.c|327| <<get_global_efr>> if (list_is_first(&iommu->list, &amd_iommu_list)) {
+ *   - drivers/iommu/amd/init.c|2137| <<init_iommu_one>> list_add_tail(&iommu->list, &amd_iommu_list);
+ *   - drivers/iommu/amd/init.c|4006| <<amd_iommu_init>> if (ret && list_empty(&amd_iommu_list)) {
+ */
 #define for_each_iommu_safe(iommu, next) \
 	list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
 /* Making iterating over protection_domain->dev_data_list easier */
@@ -637,8 +687,29 @@ struct amd_iommu_pci_seg {
 	 * responsible for a specific device. It is indexed by the PCI
 	 * device id.
 	 */
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1, 
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	struct amd_iommu **rlookup_table;
 
+	/*
+	 * 在以下使用amd_iommu_pci_seg->irq_lookup_table:
+	 *   - 
+	 */
 	/*
 	 * This table is used to find the irq remapping table for a given
 	 * device id quickly.
@@ -691,6 +762,11 @@ struct amd_iommu {
 	/* physical end address of MMIO space */
 	u64 mmio_phys_end;
 
+	/*
+	 * 在以下设置amd_iommu->mmio_base:
+	 *   - drivers/iommu/amd/init.c|1994| <<init_iommu_one>> iommu->mmio_base =
+	 *               iommu_map_mmio_space(iommu->mmio_phys, iommu->mmio_phys_end);
+	 */
 	/* virtual address of MMIO space */
 	u8 __iomem *mmio_base;
 
@@ -729,6 +805,19 @@ struct amd_iommu {
 	u32 cmd_buf_head;
 	u32 cmd_buf_tail;
 
+	/*
+	 * 在以下使用amd_iommu->evt_buf:
+	 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+	 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+	 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+	 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+	 */
 	/* event buffer virtual address */
 	u8 *evt_buf;
 
@@ -961,6 +1050,12 @@ static inline int get_ioapic_devid(int id)
 {
 	struct devid_map *entry;
 
+	/*
+	 * 在以下使用ioapic_map:
+	 *   - drivers/iommu/amd/iommu.c|58| <<global>> LIST_HEAD(ioapic_map);
+	 *   - drivers/iommu/amd/amd_iommu_types.h|964| <<get_ioapic_devid>> list_for_each_entry(entry, &ioapic_map, list) {
+	 *   - drivers/iommu/amd/init.c|1393| <<add_special_device>> list = &ioapic_map;
+	 */
 	list_for_each_entry(entry, &ioapic_map, list) {
 		if (entry->id == id)
 			return entry->devid;
diff --git a/drivers/iommu/amd/debugfs.c b/drivers/iommu/amd/debugfs.c
index 10fa217a7..7f2bc26cf 100644
--- a/drivers/iommu/amd/debugfs.c
+++ b/drivers/iommu/amd/debugfs.c
@@ -174,6 +174,25 @@ static ssize_t devid_write(struct file *filp, const char __user *ubuf,
 			kfree(srcid_ptr);
 			return -EINVAL;
 		}
+		/*
+		 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+		 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+		 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+		 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+		 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+		 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+		 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+		 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+		 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+		 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+		 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+		 *
+		 * struct amd_iommu *iommu;
+		 */
 		iommu = pci_seg->rlookup_table[devid];
 		if (!iommu) {
 			kfree(srcid_ptr);
@@ -214,10 +233,30 @@ static void dump_dte(struct seq_file *m, struct amd_iommu_pci_seg *pci_seg, u16
 	struct dev_table_entry *dev_table;
 	struct amd_iommu *iommu;
 
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	iommu = pci_seg->rlookup_table[devid];
 	if (!iommu)
 		return;
 
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	dev_table = get_dev_table(iommu);
 	if (!dev_table) {
 		seq_puts(m, "Device table not found");
@@ -289,6 +328,10 @@ static void dump_32_irte(struct seq_file *m, struct irq_remap_table *table, u16
 	}
 }
 
+/*
+ * 在以下使用dump_irte():
+ *   - drivers/iommu/amd/debugfs.c|356| <<iommu_irqtbl_show>> dump_irte(m, devid, pci_seg);
+ */
 static void dump_irte(struct seq_file *m, u16 devid, struct amd_iommu_pci_seg *pci_seg)
 {
 	struct dev_table_entry *dev_table;
@@ -304,10 +347,30 @@ static void dump_irte(struct seq_file *m, u16 devid, struct amd_iommu_pci_seg *p
 		return;
 	}
 
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	iommu = pci_seg->rlookup_table[devid];
 	if (!iommu)
 		return;
 
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	dev_table = get_dev_table(iommu);
 	if (!dev_table) {
 		seq_puts(m, "Device table not found");
@@ -361,6 +424,10 @@ static int iommu_irqtbl_show(struct seq_file *m, void *unused)
 }
 DEFINE_SHOW_ATTRIBUTE(iommu_irqtbl);
 
+/*
+ * 在以下使用amd_iommu_debugfs_setup():
+ *   - drivers/iommu/amd/init.c|3833| <<amd_iommu_init>> amd_iommu_debugfs_setup();
+ */
 void amd_iommu_debugfs_setup(void)
 {
 	struct amd_iommu *iommu;
diff --git a/drivers/iommu/amd/init.c b/drivers/iommu/amd/init.c
index f2991c118..f03d00e04 100644
--- a/drivers/iommu/amd/init.c
+++ b/drivers/iommu/amd/init.c
@@ -158,12 +158,47 @@ u8 amd_iommu_hpt_level;
 int amd_iommu_gpt_level = PAGE_MODE_4_LEVEL;
 
 int amd_iommu_guest_ir = AMD_IOMMU_GUEST_IR_VAPIC;
+/*
+ * 在以下使用amd_iommu_xt_mode:
+ *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+ *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+ *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+ *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+ *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+ *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+ */
 static int amd_iommu_xt_mode = IRQ_REMAP_XAPIC_MODE;
 
+/*
+ * 在以下使用amd_iommu_detected:
+ *   - drivers/iommu/amd/init.c|163| <<global>> static bool amd_iommu_detected;
+ *   - drivers/iommu/amd/init.c|3210| <<early_amd_iommu_init>> if (!amd_iommu_detected)
+ *   - drivers/iommu/amd/init.c|3660| <<amd_iommu_detect>> amd_iommu_detected = true;
+ */
 static bool amd_iommu_detected;
+/*
+ * 在以下使用amd_iommu_disabled:
+ *   - drivers/iommu/amd/init.c|164| <<global>> static bool amd_iommu_disabled __initdata;
+ *   - drivers/iommu/amd/init.c|3286| <<early_amd_iommu_init>> if (!is_kdump_kernel() || amd_iommu_disabled)
+ *   - drivers/iommu/amd/init.c|3453| <<state_next>> if (amd_iommu_disabled) {
+ *   - drivers/iommu/amd/init.c|3711| <<parse_amd_iommu_options>> amd_iommu_disabled = true;
+ */
 static bool amd_iommu_disabled __initdata;
+/*
+ * 在以下使用amd_iommu_force_enable:
+ *   - drivers/iommu/amd/init.c|165| <<global>> static bool amd_iommu_force_enable __initdata;
+ *   - drivers/iommu/amd/init.c|3354| <<detect_ivrs>> if (amd_iommu_force_enable)
+ *   - drivers/iommu/amd/init.c|3709| <<parse_amd_iommu_options>> amd_iommu_force_enable = true;
+ */
 static bool amd_iommu_force_enable __initdata;
 static bool amd_iommu_irtcachedis;
+/*
+ * 在以下使用amd_iommu_target_ivhd_type:
+ *   - drivers/iommu/amd/init.c|711| <<find_last_devid_acpi>> h->type == amd_iommu_target_ivhd_type) {
+ *   - drivers/iommu/amd/init.c|2310| <<init_iommu_all>> if (*p == amd_iommu_target_ivhd_type) {
+ *   - drivers/iommu/amd/init.c|3577| <<early_amd_iommu_init>> amd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);
+ *   - drivers/iommu/amd/init.c|3578| <<early_amd_iommu_init>> DUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);
+ */
 static int amd_iommu_target_ivhd_type;
 
 /* Global EFR and EFR2 registers */
@@ -177,7 +212,23 @@ bool amd_iommu_hatdis;
 bool amd_iommu_snp_en;
 EXPORT_SYMBOL(amd_iommu_snp_en);
 
+/*
+ * 在以下使用amd_iommu_pci_seg_list:
+ *   - drivers/iommu/amd/init.c|208| <<global>> LIST_HEAD(amd_iommu_pci_seg_list);
+ *   - drivers/iommu/amd/amd_iommu_types.h|511| <<for_each_pci_segment>> list_for_each_entry((pci_seg), &amd_iommu_pci_seg_list, list)
+ *   - drivers/iommu/amd/amd_iommu_types.h|513| <<for_each_pci_segment_safe>> list_for_each_entry_safe((pci_seg), (next), &amd_iommu_pci_seg_list, list)
+ *   - drivers/iommu/amd/init.c|1988| <<alloc_pci_segment>> list_add_tail(&pci_seg->list, &amd_iommu_pci_seg_list);
+ */
 LIST_HEAD(amd_iommu_pci_seg_list);	/* list of all PCI segments */
+/*
+ * 在以下使用amd_iommu_list:
+ *   - drivers/iommu/amd/init.c|209| <<global>> LIST_HEAD(amd_iommu_list);
+ *   - drivers/iommu/amd/amd_iommu_types.h|518| <<for_each_iommu>> list_for_each_entry((iommu), &amd_iommu_list, list)
+ *   - drivers/iommu/amd/amd_iommu_types.h|520| <<for_each_iommu_safe>> list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
+ *   - drivers/iommu/amd/init.c|327| <<get_global_efr>> if (list_is_first(&iommu->list, &amd_iommu_list)) {
+ *   - drivers/iommu/amd/init.c|2137| <<init_iommu_one>> list_add_tail(&iommu->list, &amd_iommu_list);
+ *   - drivers/iommu/amd/init.c|4006| <<amd_iommu_init>> if (ret && list_empty(&amd_iommu_list)) {
+ */
 LIST_HEAD(amd_iommu_list);		/* list of all AMD IOMMUs in the system */
 LIST_HEAD(amd_ivhd_dev_flags_list);	/* list of all IVHD device entry settings */
 
@@ -195,6 +246,32 @@ bool amd_iommu_force_isolation __read_mostly;
 
 unsigned long amd_iommu_pgsize_bitmap __ro_after_init = AMD_IOMMU_PGSIZES;
 
+/*
+ * 在以下设置IOMMU_START_STATE:
+ *   - drivers/iommu/amd/init.c|223| <<global>> static enum iommu_init_state init_state = IOMMU_START_STATE;
+ *
+ * 在以下设置IOMMU_IVRS_DETECTED:
+ *   - drivers/iommu/amd/init.c|3392| <<state_next>> init_state = IOMMU_IVRS_DETECTED;
+ *   - drivers/iommu/amd/init.c|3599| <<amd_iommu_detect>> ret = iommu_go_to_state(IOMMU_IVRS_DETECTED);
+ *
+ * 在以下设置IOMMU_ACPI_FINISHED:
+ *   - drivers/iommu/amd/init.c|3401| <<state_next>> init_state = ret ? IOMMU_INIT_ERROR : IOMMU_ACPI_FINISHED;
+ *   - drivers/iommu/amd/init.c|3497| <<amd_iommu_prepare>> ret = iommu_go_to_state(IOMMU_ACPI_FINISHED);
+ *
+ * 在以下设置IOMMU_ENABLED:
+ *   - drivers/iommu/amd/init.c|3407| <<state_next>> init_state = IOMMU_ENABLED;
+ *   - drivers/iommu/amd/init.c|3510| <<amd_iommu_enable>> ret = iommu_go_to_state(IOMMU_ENABLED);
+ *
+ * 在以下设置IOMMU_PCI_INIT:
+ *   - drivers/iommu/amd/init.c|3413| <<state_next>> init_state = ret ? IOMMU_INIT_ERROR : IOMMU_PCI_INIT;
+ *
+ * 在以下设置IOMMU_INTERRUPTS_EN:
+ *   - drivers/iommu/amd/init.c|3417| <<state_next>> init_state = ret ? IOMMU_INIT_ERROR : IOMMU_INTERRUPTS_EN;
+ *
+ * 在以下设置IOMMU_INITIALIZED:
+ *   - drivers/iommu/amd/init.c|3420| <<state_next>> init_state = IOMMU_INITIALIZED;
+ *   - drivers/iommu/amd/init.c|3546| <<amd_iommu_init>> ret = iommu_go_to_state(IOMMU_INITIALIZED);
+ */
 enum iommu_init_state {
 	IOMMU_START_STATE,
 	IOMMU_IVRS_DETECTED,
@@ -270,6 +347,15 @@ static __init void get_global_efr(void)
 		u64 tmp = iommu->features;
 		u64 tmp2 = iommu->features2;
 
+		/*
+		 * 在以下使用amd_iommu_list:
+		 *   - drivers/iommu/amd/init.c|209| <<global>> LIST_HEAD(amd_iommu_list);
+		 *   - drivers/iommu/amd/amd_iommu_types.h|518| <<for_each_iommu>> list_for_each_entry((iommu), &amd_iommu_list, list)
+		 *   - drivers/iommu/amd/amd_iommu_types.h|520| <<for_each_iommu_safe>> list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
+		 *   - drivers/iommu/amd/init.c|327| <<get_global_efr>> if (list_is_first(&iommu->list, &amd_iommu_list)) {
+		 *   - drivers/iommu/amd/init.c|2137| <<init_iommu_one>> list_add_tail(&iommu->list, &amd_iommu_list);
+		 *   - drivers/iommu/amd/init.c|4006| <<amd_iommu_init>> if (ret && list_empty(&amd_iommu_list)) {
+		 */
 		if (list_is_first(&iommu->list, &amd_iommu_list)) {
 			amd_iommu_efr = tmp;
 			amd_iommu_efr2 = tmp2;
@@ -299,6 +385,10 @@ static __init void get_global_efr(void)
  * Default to IVHD EFR since it is available sooner
  * (i.e. before PCI init).
  */
+/*
+ * 在以下使用early_iommu_features_init():
+ *   - drivers/iommu/amd/init.c|2317| <<init_iommu_one>> early_iommu_features_init(iommu, h);
+ */
 static void __init early_iommu_features_init(struct amd_iommu *iommu,
 					     struct ivhd_header *h)
 {
@@ -397,11 +487,19 @@ static void iommu_set_cwwb_range(struct amd_iommu *iommu)
 		    &entry, sizeof(entry));
 }
 
+/*
+ * 在以下使用iommu_set_device_table():
+ *   - drivers/iommu/amd/init.c|3230| <<early_enable_iommu>> iommu_set_device_table(iommu);
+ *   - drivers/iommu/amd/init.c|3311| <<early_enable_iommus>> iommu_set_device_table(iommu);
+ */
 /* Programs the physical address of the device table into the IOMMU hardware */
 static void iommu_set_device_table(struct amd_iommu *iommu)
 {
 	u64 entry;
 	u32 dev_table_size = iommu->pci_seg->dev_table_size;
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	void *dev_table = (void *)get_dev_table(iommu);
 
 	BUG_ON(iommu->mmio_base == NULL);
@@ -415,10 +513,24 @@ static void iommu_set_device_table(struct amd_iommu *iommu)
 			&entry, sizeof(entry));
 }
 
+/*
+ * 在以下使用iommu_feature_set():
+ *   - drivers/iommu/amd/init.c|458| <<iommu_feature_enable>> iommu_feature_set(iommu, 1ULL, 1ULL, bit);
+ *   - drivers/iommu/amd/init.c|463| <<iommu_feature_disable>> iommu_feature_set(iommu, 0ULL, 1ULL, bit);
+ *   - drivers/iommu/amd/init.c|2799| <<iommu_init_flags>> iommu_feature_set(iommu, CTRL_INV_TO_1S,
+ *                                           CTRL_INV_TO_MASK, CONTROL_INV_TIMEOUT);
+ *   - drivers/iommu/amd/init.c|2897| <<iommu_enable_2k_int>> iommu_feature_set(iommu,
+ *       CONTROL_NUM_INT_REMAP_MODE_2K, CONTROL_NUM_INT_REMAP_MODE_MASK, CONTROL_NUM_INT_REMAP_MODE);
+ */
 static void iommu_feature_set(struct amd_iommu *iommu, u64 val, u64 mask, u8 shift)
 {
 	u64 ctrl;
 
+	/*
+	 * 在以下设置amd_iommu->mmio_base:
+	 *   - drivers/iommu/amd/init.c|1994| <<init_iommu_one>> iommu->mmio_base =
+	 *               iommu_map_mmio_space(iommu->mmio_phys, iommu->mmio_phys_end);
+	 */
 	ctrl = readq(iommu->mmio_base +  MMIO_CONTROL_OFFSET);
 	mask <<= shift;
 	ctrl &= ~mask;
@@ -426,17 +538,68 @@ static void iommu_feature_set(struct amd_iommu *iommu, u64 val, u64 mask, u8 shi
 	writeq(ctrl, iommu->mmio_base +  MMIO_CONTROL_OFFSET);
 }
 
+/*
+ * 在以下使用iommu_feature_enable():
+ *   - drivers/iommu/amd/init.c|535| <<iommu_enable>> iommu_feature_enable(iommu, CONTROL_IOMMU_EN);
+ *   - drivers/iommu/amd/init.c|902| <<amd_iommu_restart_log>> iommu_feature_enable(iommu, cntrl_intr);
+ *   - drivers/iommu/amd/init.c|903| <<amd_iommu_restart_log>> iommu_feature_enable(iommu, cntrl_log);
+ *   - drivers/iommu/amd/init.c|941| <<amd_iommu_reset_cmd_buffer>> iommu_feature_enable(iommu, CONTROL_CMDBUF_EN);
+ *   - drivers/iommu/amd/init.c|1063| <<iommu_enable_event_buffer>> iommu_feature_enable(iommu, CONTROL_EVT_LOG_EN);
+ *   - drivers/iommu/amd/init.c|1120| <<iommu_ga_log_enable>> iommu_feature_enable(iommu, CONTROL_GAINT_EN); 
+ *   - drivers/iommu/amd/init.c|1121| <<iommu_ga_log_enable>> iommu_feature_enable(iommu, CONTROL_GALOG_EN);
+ *   - drivers/iommu/amd/init.c|1328| <<iommu_enable_xt>> iommu_feature_enable(iommu, CONTROL_XT_EN);
+ *   - drivers/iommu/amd/init.c|1337| <<iommu_enable_gt>> iommu_feature_enable(iommu, CONTROL_GT_EN);
+ *   - drivers/iommu/amd/init.c|2923| <<iommu_init_irq>> iommu_feature_enable(iommu, CONTROL_INTCAPXT_EN);
+ *   - drivers/iommu/amd/init.c|2925| <<iommu_init_irq>> iommu_feature_enable(iommu, CONTROL_EVT_INT_EN);
+ *   - drivers/iommu/amd/init.c|3083| <<iommu_init_flags>> iommu_feature_enable(iommu, CONTROL_HT_TUN_EN) :
+ *   - drivers/iommu/amd/init.c|3087| <<iommu_init_flags>> iommu_feature_enable(iommu, CONTROL_PASSPW_EN) :
+ *   - drivers/iommu/amd/init.c|3091| <<iommu_init_flags>> iommu_feature_enable(iommu, CONTROL_RESPASSPW_EN) :
+ *   - drivers/iommu/amd/init.c|3095| <<iommu_init_flags>> iommu_feature_enable(iommu, CONTROL_ISOC_EN) :
+ *   - drivers/iommu/amd/init.c|3101| <<iommu_init_flags>> iommu_feature_enable(iommu, CONTROL_COHERENT_EN);
+ *   - drivers/iommu/amd/init.c|3117| <<iommu_init_flags>> iommu_feature_enable(iommu, CONTROL_EPH_EN);
+ *   - drivers/iommu/amd/init.c|3169| <<iommu_enable_ga>> iommu_feature_enable(iommu, CONTROL_GA_EN);
+ *   - drivers/iommu/amd/init.c|3196| <<iommu_enable_irtcachedis>> iommu_feature_enable(iommu, CONTROL_IRTCACHEDIS);
+ *   - drivers/iommu/amd/init.c|3380| <<enable_iommus_vapic>> iommu_feature_enable(iommu, CONTROL_GAM_EN);
+ *   - drivers/iommu/amd/init.c|3382| <<enable_iommus_vapic>> iommu_feature_enable(iommu, CONTROL_SNPAVIC_EN);
+ *   - drivers/iommu/amd/ppr.c|34| <<amd_iommu_enable_ppr_log>> iommu_feature_enable(iommu, CONTROL_PPR_EN);
+ *   - drivers/iommu/amd/ppr.c|45| <<amd_iommu_enable_ppr_log>> iommu_feature_enable(iommu, CONTROL_PPRINT_EN);
+ *   - drivers/iommu/amd/ppr.c|46| <<amd_iommu_enable_ppr_log>> iommu_feature_enable(iommu, CONTROL_PPRLOG_EN);
+ */
 /* Generic functions to enable/disable certain features of the IOMMU. */
 void iommu_feature_enable(struct amd_iommu *iommu, u8 bit)
 {
+	/*
+	 * 在以下使用iommu_feature_set():
+	 *   - drivers/iommu/amd/init.c|458| <<iommu_feature_enable>> iommu_feature_set(iommu, 1ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|463| <<iommu_feature_disable>> iommu_feature_set(iommu, 0ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|2799| <<iommu_init_flags>> iommu_feature_set(iommu, CTRL_INV_TO_1S,
+	 *                                           CTRL_INV_TO_MASK, CONTROL_INV_TIMEOUT);
+	 *   - drivers/iommu/amd/init.c|2897| <<iommu_enable_2k_int>> iommu_feature_set(iommu,
+	 *       CONTROL_NUM_INT_REMAP_MODE_2K, CONTROL_NUM_INT_REMAP_MODE_MASK, CONTROL_NUM_INT_REMAP_MODE);
+	 */
 	iommu_feature_set(iommu, 1ULL, 1ULL, bit);
 }
 
 static void iommu_feature_disable(struct amd_iommu *iommu, u8 bit)
 {
+	/*
+	 * 在以下使用iommu_feature_set():
+	 *   - drivers/iommu/amd/init.c|458| <<iommu_feature_enable>> iommu_feature_set(iommu, 1ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|463| <<iommu_feature_disable>> iommu_feature_set(iommu, 0ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|2799| <<iommu_init_flags>> iommu_feature_set(iommu, CTRL_INV_TO_1S,
+	 *                                           CTRL_INV_TO_MASK, CONTROL_INV_TIMEOUT);
+	 *   - drivers/iommu/amd/init.c|2897| <<iommu_enable_2k_int>> iommu_feature_set(iommu,
+	 *       CONTROL_NUM_INT_REMAP_MODE_2K, CONTROL_NUM_INT_REMAP_MODE_MASK, CONTROL_NUM_INT_REMAP_MODE);
+	 */
 	iommu_feature_set(iommu, 0ULL, 1ULL, bit);
 }
 
+/*
+ * 在以下使用iommu_enable:
+ *   - drivers/iommu/amd/init.c|2916| <<early_enable_iommu>> iommu_enable(iommu);
+ *   - drivers/iommu/omap-iommu.c|192| <<iommu_enable>> static int iommu_enable(struct omap_iommu *obj)
+ *   - drivers/iommu/omap-iommu.c|861| <<omap_iommu_attach>> err = iommu_enable(obj);
+ */
 /* Function to enable the hardware */
 static void iommu_enable(struct amd_iommu *iommu)
 {
@@ -474,6 +637,10 @@ static void iommu_disable(struct amd_iommu *iommu)
  * mapping and unmapping functions for the IOMMU MMIO space. Each AMD IOMMU in
  * the system has one.
  */
+/*
+ * 在以下使用iommu_map_mmio_space():
+ *   - drivers/iommu/amd/init.c|1994| <<init_iommu_one>> iommu->mmio_base = iommu_map_mmio_space(iommu->mmio_phys,
+ */
 static u8 __iomem * __init iommu_map_mmio_space(u64 address, u64 end)
 {
 	if (!request_mem_region(address, end, "amd_iommu")) {
@@ -521,6 +688,11 @@ static inline u32 get_ivhd_header_size(struct ivhd_header *h)
 /*
  * This function calculates the length of a given IVHD entry
  */
+/*
+ * 在以下使用ivhd_entry_length():
+ *   - drivers/iommu/amd/init.c|738| <<find_last_devid_from_ivhd>> p += ivhd_entry_length(p);
+ *   - drivers/iommu/amd/init.c|2081| <<init_iommu_from_acpi>> p += ivhd_entry_length(p);
+ */
 static inline int ivhd_entry_length(u8 *ivhd)
 {
 	u32 type = ((struct ivhd_entry *)ivhd)->type;
@@ -571,6 +743,11 @@ static int __init find_last_devid_from_ivhd(struct ivhd_header *h)
 		default:
 			break;
 		}
+		/*
+		 * 在以下使用ivhd_entry_length():
+		 *   - drivers/iommu/amd/init.c|738| <<find_last_devid_from_ivhd>> p += ivhd_entry_length(p);
+		 *   - drivers/iommu/amd/init.c|2081| <<init_iommu_from_acpi>> p += ivhd_entry_length(p);
+		 */
 		p += ivhd_entry_length(p);
 	}
 
@@ -611,6 +788,13 @@ static int __init find_last_devid_acpi(struct acpi_table_header *table, u16 pci_
 	end += table->length;
 	while (p < end) {
 		h = (struct ivhd_header *)p;
+		/*
+		 * 在以下使用amd_iommu_target_ivhd_type:
+		 *   - drivers/iommu/amd/init.c|711| <<find_last_devid_acpi>> h->type == amd_iommu_target_ivhd_type) {
+		 *   - drivers/iommu/amd/init.c|2310| <<init_iommu_all>> if (*p == amd_iommu_target_ivhd_type) {
+		 *   - drivers/iommu/amd/init.c|3577| <<early_amd_iommu_init>> amd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);
+		 *   - drivers/iommu/amd/init.c|3578| <<early_amd_iommu_init>> DUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);
+		 */
 		if (h->pci_seg == pci_seg &&
 		    h->type == amd_iommu_target_ivhd_type) {
 			last_devid = find_last_devid_from_ivhd(h);
@@ -659,6 +843,23 @@ static inline void free_dev_table(struct amd_iommu_pci_seg *pci_seg)
 /* Allocate per PCI segment IOMMU rlookup table. */
 static inline int __init alloc_rlookup_table(struct amd_iommu_pci_seg *pci_seg)
 {
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
 					  sizeof(*pci_seg->rlookup_table),
 					  GFP_KERNEL);
@@ -670,6 +871,23 @@ static inline int __init alloc_rlookup_table(struct amd_iommu_pci_seg *pci_seg)
 
 static inline void free_rlookup_table(struct amd_iommu_pci_seg *pci_seg)
 {
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	kvfree(pci_seg->rlookup_table);
 	pci_seg->rlookup_table = NULL;
 }
@@ -815,6 +1033,11 @@ static void amd_iommu_reset_cmd_buffer(struct amd_iommu *iommu)
  * This function writes the command buffer address to the hardware and
  * enables it.
  */
+/*
+ * 在以下使用iommu_enable_command_buffer():
+ *   - drivers/iommu/amd/init.c|3231| <<early_enable_iommu>> iommu_enable_command_buffer(iommu);
+ *   - drivers/iommu/amd/init.c|3300| <<early_enable_iommus>> iommu_enable_command_buffer(iommu);
+ */
 static void iommu_enable_command_buffer(struct amd_iommu *iommu)
 {
 	u64 entry;
@@ -866,19 +1089,56 @@ void *__init iommu_alloc_4k_pages(struct amd_iommu *iommu, gfp_t gfp,
 	return buf;
 }
 
+/*
+ * 在以下使用alloc_event_buffer():
+ *   - drivers/iommu/amd/init.c|1156| <<alloc_iommu_buffers>> ret = alloc_event_buffer(iommu);
+ */
 /* allocates the memory where the IOMMU will log its events to */
 static int __init alloc_event_buffer(struct amd_iommu *iommu)
 {
+	/*
+	 * 在以下使用amd_iommu->evt_buf:
+	 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+	 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+	 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+	 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+	 *
+	 * #define EVT_BUFFER_SIZE         8192 // 512 entries
+	 */
 	iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
 					      EVT_BUFFER_SIZE);
 
 	return iommu->evt_buf ? 0 : -ENOMEM;
 }
 
+/*
+ * 在以下使用iommu_enable_event_buffer():
+ *   - drivers/iommu/amd/init.c|3102| <<early_enable_iommu>> iommu_enable_event_buffer(iommu);
+ *   - drivers/iommu/amd/init.c|3166| <<early_enable_iommus>> iommu_enable_event_buffer(iommu);
+ */
 static void iommu_enable_event_buffer(struct amd_iommu *iommu)
 {
 	u64 entry;
 
+	/*
+	 * 在以下使用amd_iommu->evt_buf:
+	 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+	 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+	 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+	 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+	 */
 	BUG_ON(iommu->evt_buf == NULL);
 
 	if (!is_kdump_kernel()) {
@@ -891,6 +1151,12 @@ static void iommu_enable_event_buffer(struct amd_iommu *iommu)
 			    &entry, sizeof(entry));
 	}
 
+	/*
+	 * 在以下使用MMIO_EVT_HEAD_OFFSET:
+	 *   - drivers/iommu/amd/init.c|1060| <<iommu_enable_event_buffer>> writel(0x00, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
+	 *   - drivers/iommu/amd/iommu.c|1060| <<iommu_poll_events>> head = readl(iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
+	 *   - drivers/iommu/amd/iommu.c|1081| <<iommu_poll_events>> writel(head, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
+	 */
 	/* set head and tail to zero manually */
 	writel(0x00, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
 	writel(0x00, iommu->mmio_base + MMIO_EVT_TAIL_OFFSET);
@@ -908,6 +1174,19 @@ static void iommu_disable_event_buffer(struct amd_iommu *iommu)
 
 static void __init free_event_buffer(struct amd_iommu *iommu)
 {
+	/*
+	 * 在以下使用amd_iommu->evt_buf:
+	 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+	 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+	 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+	 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+	 */
 	iommu_free_pages(iommu->evt_buf);
 }
 
@@ -984,12 +1263,29 @@ static int __init alloc_cwwb_sem(struct amd_iommu *iommu)
 	return 0;
 }
 
+/*
+ * 在以下使用remap_event_buffer():
+ *   - drivers/iommu/amd/init.c|1239| <<alloc_iommu_buffers>> ret = remap_event_buffer(iommu);
+ */
 static int __init remap_event_buffer(struct amd_iommu *iommu)
 {
 	u64 paddr;
 
 	pr_info_once("Re-using event buffer from the previous kernel\n");
 	paddr = readq(iommu->mmio_base + MMIO_EVT_BUF_OFFSET) & PM_ADDR_MASK;
+	/*
+	 * 在以下使用amd_iommu->evt_buf:
+	 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+	 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+	 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+	 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+	 */
 	iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
 
 	return iommu->evt_buf ? 0 : -ENOMEM;
@@ -1087,6 +1383,19 @@ static void __init unmap_command_buffer(struct amd_iommu *iommu)
 
 static void __init unmap_event_buffer(struct amd_iommu *iommu)
 {
+	/*
+	 * 在以下使用amd_iommu->evt_buf:
+	 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+	 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+	 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+	 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+	 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+	 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+	 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+	 */
 	memunmap(iommu->evt_buf);
 }
 
@@ -1106,6 +1415,15 @@ static void __init free_iommu_buffers(struct amd_iommu *iommu)
 static void iommu_enable_xt(struct amd_iommu *iommu)
 {
 #ifdef CONFIG_IRQ_REMAP
+	/*
+	 * 在以下使用amd_iommu_xt_mode:
+	 *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+	 *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+	 */
 	/*
 	 * XT mode (32-bit APIC destination ID) requires
 	 * GA mode (128-bit IRTE support) as a prerequisite.
@@ -1244,6 +1562,12 @@ static bool search_ivhd_dte_flags(u16 segid, u16 first, u16 last)
  * This function takes the device specific flags read from the ACPI
  * table and sets up the device table entry with that information
  */
+/*
+ * 在以下使用set_dev_entry_from_acpi_range():
+ *   - drivers/iommu/amd/init.c|1616| <<set_dev_entry_from_acpi>> set_dev_entry_from_acpi_range(iommu, devid, devid, flags, ext_flags);
+ *   - drivers/iommu/amd/init.c|1834| <<init_iommu_from_acpi>> set_dev_entry_from_acpi_range(iommu, 0, pci_seg->last_bdf, e->flags, 0);
+ *   - drivers/iommu/amd/init.c|1932| <<init_iommu_from_acpi>> set_dev_entry_from_acpi_range(iommu, devid_start, devid, flags, ext_flags);
+ */
 static void __init
 set_dev_entry_from_acpi_range(struct amd_iommu *iommu, u16 first, u16 last,
 			      u32 flags, u32 ext_flags)
@@ -1292,25 +1616,79 @@ set_dev_entry_from_acpi_range(struct amd_iommu *iommu, u16 first, u16 last,
 
 	for (i = first; i <= last; i++)  {
 		if (flags) {
+			/*
+			 * // Structure defining one entry in the device table
+			 * struct dev_table_entry {
+			 *     union {
+			 *         u64 data[4];
+			 *         u128 data128[2];
+			 *     };
+			 * };
+			 *
+			 * 返回amd_iommu->pci_seg->dev_table
+			 */
 			struct dev_table_entry *dev_table = get_dev_table(iommu);
 
 			memcpy(&dev_table[i], &dte, sizeof(dte));
 		}
+		/*
+		 * 在以下使用amd_iommu_set_rlookup_table():
+		 *   - drivers/iommu/amd/init.c|1404| <<set_dev_entry_from_acpi_range>> amd_iommu_set_rlookup_table(iommu, i);
+		 *   - drivers/iommu/amd/iommu.c|423| <<clone_alias>> amd_iommu_set_rlookup_table(iommu, alias);
+		 */
 		amd_iommu_set_rlookup_table(iommu, i);
 	}
 }
 
+/*
+ * 在以下使用set_dev_entry_from_acpi():
+ *   - drivers/iommu/amd/init.c|1845| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags, 0);
+ *   - drivers/iommu/amd/init.c|1873| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid , e->flags, 0);
+ *   - drivers/iommu/amd/init.c|1874| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid_to, e->flags, 0);
+ *   - drivers/iommu/amd/init.c|1903| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags,
+ *   - drivers/iommu/amd/init.c|1930| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid_to, flags, ext_flags);
+ *   - drivers/iommu/amd/init.c|1974| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags, 0);
+ *   - drivers/iommu/amd/init.c|2041| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags, 0);
+ */
 static void __init set_dev_entry_from_acpi(struct amd_iommu *iommu,
 					   u16 devid, u32 flags, u32 ext_flags)
 {
 	set_dev_entry_from_acpi_range(iommu, devid, devid, flags, ext_flags);
 }
 
+/*
+ * [0] add_special_device
+ * [0] init_iommu_from_acpi
+ * [0] state_next
+ * [0] amd_iommu_prepare
+ * [0] irq_remapping_prepare
+ * [0] enable_IR_x2apic
+ * [0] x86_64_probe_apic
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ *
+ * 在以下使用add_special_device():
+ *   - drivers/iommu/amd/init.c|1465| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_IOAPIC,
+ *   - drivers/iommu/amd/init.c|1474| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_HPET,
+ *   - drivers/iommu/amd/init.c|1669| <<init_iommu_from_acpi>> ret = add_special_device(type, handle, &devid, false);
+ *   - drivers/iommu/amd/quirks.c|52| <<ivrs_ioapic_quirk_cb>> add_special_device(IVHD_SPECIAL_IOAPIC, i->id, (u32 *)&i->devid, 0);
+ */
 int __init add_special_device(u8 type, u8 id, u32 *devid, bool cmd_line)
 {
 	struct devid_map *entry;
 	struct list_head *list;
 
+	/*
+	 * 在以下使用ioapic_map:
+	 *   - drivers/iommu/amd/iommu.c|58| <<global>> LIST_HEAD(ioapic_map);
+	 *   - drivers/iommu/amd/amd_iommu_types.h|964| <<get_ioapic_devid>> list_for_each_entry(entry, &ioapic_map, list) {
+	 *   - drivers/iommu/amd/init.c|1393| <<add_special_device>> list = &ioapic_map;
+	 */
 	if (type == IVHD_SPECIAL_IOAPIC)
 		list = &ioapic_map;
 	else if (type == IVHD_SPECIAL_HPET)
@@ -1379,11 +1757,22 @@ static int __init add_acpi_hid_device(u8 *hid, u8 *uid, u32 *devid,
 	return 0;
 }
 
+/*
+ * 在以下使用add_early_maps():
+ *   - drivers/iommu/amd/init.c|1793| <<init_iommu_from_acpi>> ret = add_early_maps();
+ */
 static int __init add_early_maps(void)
 {
 	int i, ret;
 
 	for (i = 0; i < early_ioapic_map_size; ++i) {
+		/*
+		 * 在以下使用add_special_device():
+		 *   - drivers/iommu/amd/init.c|1465| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_IOAPIC,
+		 *   - drivers/iommu/amd/init.c|1474| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_HPET,
+		 *   - drivers/iommu/amd/init.c|1669| <<init_iommu_from_acpi>> ret = add_special_device(type, handle, &devid, false);
+		 *   - drivers/iommu/amd/quirks.c|52| <<ivrs_ioapic_quirk_cb>> add_special_device(IVHD_SPECIAL_IOAPIC, i->id, (u32 *)&i->devid, 0);
+		 */
 		ret = add_special_device(IVHD_SPECIAL_IOAPIC,
 					 early_ioapic_map[i].id,
 					 &early_ioapic_map[i].devid,
@@ -1417,6 +1806,24 @@ static int __init add_early_maps(void)
  * Takes a pointer to an AMD IOMMU entry in the ACPI table and
  * initializes the hardware and our data structures with it.
  */
+/*
+ * [0] add_special_device
+ * [0] init_iommu_from_acpi
+ * [0] state_next
+ * [0] amd_iommu_prepare
+ * [0] irq_remapping_prepare
+ * [0] enable_IR_x2apic
+ * [0] x86_64_probe_apic
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * 在以下使用init_iommu_from_acpi():
+ *   - drivers/iommu/amd/init.c|1999| <<init_iommu_one>> return init_iommu_from_acpi(iommu, h);
+ */
 static int __init init_iommu_from_acpi(struct amd_iommu *iommu,
 					struct ivhd_header *h)
 {
@@ -1456,6 +1863,16 @@ static int __init init_iommu_from_acpi(struct amd_iommu *iommu,
 	end += h->length;
 
 
+	/*
+	 * 在以下使用set_dev_entry_from_acpi():
+	 *   - drivers/iommu/amd/init.c|1845| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags, 0);
+	 *   - drivers/iommu/amd/init.c|1873| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid , e->flags, 0);
+	 *   - drivers/iommu/amd/init.c|1874| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid_to, e->flags, 0);
+	 *   - drivers/iommu/amd/init.c|1903| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags,
+	 *   - drivers/iommu/amd/init.c|1930| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid_to, flags, ext_flags);
+	 *   - drivers/iommu/amd/init.c|1974| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags, 0);
+	 *   - drivers/iommu/amd/init.c|2041| <<init_iommu_from_acpi>> set_dev_entry_from_acpi(iommu, devid, e->flags, 0);
+	 */
 	while (p < end) {
 		e = (struct ivhd_entry *)p;
 		seg_id = pci_seg->id;
@@ -1562,6 +1979,12 @@ static int __init init_iommu_from_acpi(struct amd_iommu *iommu,
 					pci_seg->alias_table[dev_i] = devid_to;
 				set_dev_entry_from_acpi(iommu, devid_to, flags, ext_flags);
 			}
+			/*
+			 * 在以下使用set_dev_entry_from_acpi_range():
+			 *   - drivers/iommu/amd/init.c|1616| <<set_dev_entry_from_acpi>> set_dev_entry_from_acpi_range(iommu, devid, devid, flags, ext_flags);
+			 *   - drivers/iommu/amd/init.c|1834| <<init_iommu_from_acpi>> set_dev_entry_from_acpi_range(iommu, 0, pci_seg->last_bdf, e->flags, 0);
+			 *   - drivers/iommu/amd/init.c|1932| <<init_iommu_from_acpi>> set_dev_entry_from_acpi_range(iommu, devid_start, devid, flags, ext_flags);
+			 */
 			set_dev_entry_from_acpi_range(iommu, devid_start, devid, flags, ext_flags);
 			break;
 		case IVHD_DEV_SPECIAL: {
@@ -1588,6 +2011,13 @@ static int __init init_iommu_from_acpi(struct amd_iommu *iommu,
 				    PCI_FUNC(devid),
 				    e->flags);
 
+			/*
+			 * 在以下使用add_special_device():
+			 *   - drivers/iommu/amd/init.c|1465| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_IOAPIC,
+			 *   - drivers/iommu/amd/init.c|1474| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_HPET,
+			 *   - drivers/iommu/amd/init.c|1669| <<init_iommu_from_acpi>> ret = add_special_device(type, handle, &devid, false);
+			 *   - drivers/iommu/amd/quirks.c|52| <<ivrs_ioapic_quirk_cb>> add_special_device(IVHD_SPECIAL_IOAPIC, i->id, (u32 *)&i->devid, 0);
+			 */
 			ret = add_special_device(type, handle, &devid, false);
 			if (ret)
 				return ret;
@@ -1672,12 +2102,21 @@ static int __init init_iommu_from_acpi(struct amd_iommu *iommu,
 			break;
 		}
 
+		/*
+		 * 在以下使用ivhd_entry_length():
+		 *   - drivers/iommu/amd/init.c|738| <<find_last_devid_from_ivhd>> p += ivhd_entry_length(p);
+		 *   - drivers/iommu/amd/init.c|2081| <<init_iommu_from_acpi>> p += ivhd_entry_length(p);
+		 */
 		p += ivhd_entry_length(p);
 	}
 
 	return 0;
 }
 
+/*
+ * 在以下使用alloc_pci_segment():
+ *   - drivers/iommu/amd/init.c|2010| <<get_pci_segment>> return alloc_pci_segment(id, ivrs_base);
+ */
 /* Allocate PCI segment data structure */
 static struct amd_iommu_pci_seg *__init alloc_pci_segment(u16 id,
 					  struct acpi_table_header *ivrs_base)
@@ -1707,6 +2146,13 @@ static struct amd_iommu_pci_seg *__init alloc_pci_segment(u16 id,
 	pci_seg->id = id;
 	init_llist_head(&pci_seg->dev_data_list);
 	INIT_LIST_HEAD(&pci_seg->unity_map);
+	/*
+	 * 在以下使用amd_iommu_pci_seg_list:
+	 *   - drivers/iommu/amd/init.c|208| <<global>> LIST_HEAD(amd_iommu_pci_seg_list);
+	 *   - drivers/iommu/amd/amd_iommu_types.h|511| <<for_each_pci_segment>> list_for_each_entry((pci_seg), &amd_iommu_pci_seg_list, list)
+	 *   - drivers/iommu/amd/amd_iommu_types.h|513| <<for_each_pci_segment_safe>> list_for_each_entry_safe((pci_seg), (next), &amd_iommu_pci_seg_list, list)
+	 *   - drivers/iommu/amd/init.c|1988| <<alloc_pci_segment>> list_add_tail(&pci_seg->list, &amd_iommu_pci_seg_list);
+	 */
 	list_add_tail(&pci_seg->list, &amd_iommu_pci_seg_list);
 
 	if (alloc_dev_table(pci_seg))
@@ -1719,6 +2165,11 @@ static struct amd_iommu_pci_seg *__init alloc_pci_segment(u16 id,
 	return pci_seg;
 }
 
+/*
+ * 在以下使用get_pci_segment():
+ *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one>> pci_seg = get_pci_segment(h->pci_seg, ivrs_base);
+ *   - drivers/iommu/amd/init.c|2959| <<init_unity_map_range>> pci_seg = get_pci_segment(m->pci_seg, ivrs_base);
+ */
 static struct amd_iommu_pci_seg *__init get_pci_segment(u16 id,
 					struct acpi_table_header *ivrs_base)
 {
@@ -1729,6 +2180,9 @@ static struct amd_iommu_pci_seg *__init get_pci_segment(u16 id,
 			return pci_seg;
 	}
 
+	/*
+	 * 只在这里调用
+	 */
 	return alloc_pci_segment(id, ivrs_base);
 }
 
@@ -1833,16 +2287,98 @@ static void amd_iommu_ats_write_check_workaround(struct amd_iommu *iommu)
 	pci_info(iommu->dev, "Applying ATS write check workaround\n");
 }
 
+/*
+ * [    4.132752] AMD-Vi: AMD-Vi: Using IVHD type 0x11
+ * [    4.138081] AMD-Vi: AMD-Vi: device: 60:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.146420] AMD-Vi: AMD-Vi:        mmio-addr: 00000000cd100000
+ * [    4.153014] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: 60:00.3 flags: 00
+ * [    4.161153] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: 7f:1f.6
+ * [    4.167582] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[241])               devid: 60:00.1
+ * [    4.174943] AMD-Vi: AMD-Vi: device: 40:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.183280] AMD-Vi: AMD-Vi:        mmio-addr: 00000000f4100000
+ * [    4.189877] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: 40:00.3 flags: 00
+ * [    4.198024] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: 5f:1f.6
+ * [    4.204457] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[242])               devid: 40:00.1
+ * [    4.211822] AMD-Vi: AMD-Vi: device: 00:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.220163] AMD-Vi: AMD-Vi:        mmio-addr: 00000000f5100000
+ * [    4.226753] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: 00:00.3 flags: 00
+ * [    4.234895] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: 1f:1f.6
+ * [    4.241318] AMD-Vi: AMD-Vi:   DEV_ALIAS_RANGE                 devid: ff:00.0 flags: 00 devid_to: 00:14.5
+ * [    4.250620] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: ff:1f.7
+ * [    4.257007] AMD-Vi: AMD-Vi:   DEV_SPECIAL(HPET[0])           devid: 00:14.0
+ * [    4.263977] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[240])               devid: 00:14.0
+ * [    4.271341] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[243])               devid: 00:00.1
+ * [    4.278710] AMD-Vi: AMD-Vi: device: 20:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.287047] AMD-Vi: AMD-Vi:        mmio-addr: 00000000cc100000
+ * [    4.293648] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: 20:00.3 flags: 00
+ * [    4.301794] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: 3f:1f.6
+ * [    4.308223] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[244])               devid: 20:00.1
+ * [    4.315585] AMD-Vi: AMD-Vi: device: e0:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.323920] AMD-Vi: AMD-Vi:        mmio-addr: 00000000bf100000
+ * [    4.330506] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: e0:00.3 flags: 00
+ * [    4.338649] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: ff:1f.6
+ * [    4.345076] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[245])               devid: e0:00.1
+ * [    4.352445] AMD-Vi: AMD-Vi: device: c0:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.360778] AMD-Vi: AMD-Vi:        mmio-addr: 00000000be100000
+ * [    4.367374] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: c0:00.3 flags: 00
+ * [    4.375523] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: df:1f.6
+ * [    4.381956] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[246])               devid: c0:00.1
+ * [    4.389321] AMD-Vi: AMD-Vi: device: 80:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.397654] AMD-Vi: AMD-Vi:        mmio-addr: 00000000b5200000
+ * [    4.404243] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: 80:00.3 flags: 00
+ * [    4.412379] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: 9f:1f.6
+ * [    4.418805] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[247])               devid: 80:00.1
+ * [    4.426172] AMD-Vi: AMD-Vi: device: a0:00.2 cap: 0040 seg: 0 flags: b0 info 0000
+ * [    4.434511] AMD-Vi: AMD-Vi:        mmio-addr: 00000000b4100000
+ * [    4.441102] AMD-Vi: AMD-Vi:   DEV_SELECT_RANGE_START  devid: a0:00.3 flags: 00
+ * [    4.449251] AMD-Vi: AMD-Vi:   DEV_RANGE_END           devid: bf:1f.6
+ * [    4.455679] AMD-Vi: AMD-Vi:   DEV_SPECIAL(IOAPIC[248])               devid: a0:00.1
+ * [    4.463034] AMD-Vi: Using global IVHD EFR:0x25bf732fa2295afe, EFR2:0x1d
+ *
+ * [   12.713817] pci 0000:60:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.721609] pci 0000:40:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.735472] pci 0000:00:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.743263] pci 0000:20:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.751049] pci 0000:e0:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.758852] pci 0000:c0:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.766660] pci 0000:80:00.2: AMD-Vi: IOMMU performance counters supported
+ * [   12.774458] pci 0000:a0:00.2: AMD-Vi: IOMMU performance counters supported
+ */
+
 /*
  * This function glues the initialization function for one IOMMU
  * together and also allocates the command buffer and programs the
  * hardware. It does NOT enable the IOMMU. This is done afterwards.
  */
+/*
+ * [0] add_special_device
+ * [0] init_iommu_from_acpi
+ * [0] state_next
+ * [0] amd_iommu_prepare
+ * [0] irq_remapping_prepare
+ * [0] enable_IR_x2apic
+ * [0] x86_64_probe_apic
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ *
+ * 在以下使用init_iommu_one():
+ *   - drivers/iommu/amd/init.c|2038| <<init_iommu_all>> ret = init_iommu_one(iommu, h, table);
+ */
 static int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h,
 				 struct acpi_table_header *ivrs_base)
 {
 	struct amd_iommu_pci_seg *pci_seg;
 
+	/*
+	 * 在以下使用get_pci_segment():
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one>> pci_seg = get_pci_segment(h->pci_seg, ivrs_base);
+	 *   - drivers/iommu/amd/init.c|2959| <<init_unity_map_range>> pci_seg = get_pci_segment(m->pci_seg, ivrs_base);
+	 */
 	pci_seg = get_pci_segment(h->pci_seg, ivrs_base);
 	if (pci_seg == NULL)
 		return -ENOMEM;
@@ -1852,6 +2388,15 @@ static int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h,
 	atomic64_set(&iommu->cmd_sem_val, 0);
 
 	/* Add IOMMU to internal data structures */
+	/*
+	 * 在以下使用amd_iommu_list:
+	 *   - drivers/iommu/amd/init.c|209| <<global>> LIST_HEAD(amd_iommu_list);
+	 *   - drivers/iommu/amd/amd_iommu_types.h|518| <<for_each_iommu>> list_for_each_entry((iommu), &amd_iommu_list, list)
+	 *   - drivers/iommu/amd/amd_iommu_types.h|520| <<for_each_iommu_safe>> list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
+	 *   - drivers/iommu/amd/init.c|327| <<get_global_efr>> if (list_is_first(&iommu->list, &amd_iommu_list)) {
+	 *   - drivers/iommu/amd/init.c|2137| <<init_iommu_one>> list_add_tail(&iommu->list, &amd_iommu_list);
+	 *   - drivers/iommu/amd/init.c|4006| <<amd_iommu_init>> if (ret && list_empty(&amd_iommu_list)) {
+	 */
 	list_add_tail(&iommu->list, &amd_iommu_list);
 	iommu->index = amd_iommus_present++;
 
@@ -1894,6 +2439,15 @@ static int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h,
 			break;
 		}
 
+		/*
+		 * 在以下使用amd_iommu_xt_mode:
+		 *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+		 *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+		 */
 		if (h->efr_reg & BIT(IOMMU_EFR_XTSUP_SHIFT))
 			amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
 
@@ -1909,6 +2463,9 @@ static int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h,
 		return -EINVAL;
 	}
 
+	/*
+	 * 只在这里调用
+	 */
 	iommu->mmio_base = iommu_map_mmio_space(iommu->mmio_phys,
 						iommu->mmio_phys_end);
 	if (!iommu->mmio_base)
@@ -1943,6 +2500,23 @@ static int __init init_iommu_one_late(struct amd_iommu *iommu)
 			return ret;
 	}
 
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	/*
 	 * Make sure IOMMU is not considered to translate itself. The IVRS
 	 * table tells us so, but this is a lie!
@@ -1982,6 +2556,10 @@ static u8 get_highest_supported_ivhd_type(struct acpi_table_header *ivrs)
  * Iterates over all IOMMU entries in the ACPI table, allocates the
  * IOMMU structure and initializes it with init_iommu_one()
  */
+/*
+ * 在以下使用init_iommu_all():
+ *   - drivers/iommu/amd/init.c|3227| <<early_amd_iommu_init>> ret = init_iommu_all(ivrs_base);
+ */
 static int __init init_iommu_all(struct acpi_table_header *table)
 {
 	u8 *p = (u8 *)table, *end = (u8 *)table;
@@ -1995,6 +2573,13 @@ static int __init init_iommu_all(struct acpi_table_header *table)
 	/* Phase 1: Process all IVHD blocks */
 	while (p < end) {
 		h = (struct ivhd_header *)p;
+		/*
+		 * 在以下使用amd_iommu_target_ivhd_type:
+		 *   - drivers/iommu/amd/init.c|711| <<find_last_devid_acpi>> h->type == amd_iommu_target_ivhd_type) {
+		 *   - drivers/iommu/amd/init.c|2310| <<init_iommu_all>> if (*p == amd_iommu_target_ivhd_type) {
+		 *   - drivers/iommu/amd/init.c|3577| <<early_amd_iommu_init>> amd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);
+		 *   - drivers/iommu/amd/init.c|3578| <<early_amd_iommu_init>> DUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);
+		 */
 		if (*p == amd_iommu_target_ivhd_type) {
 
 			DUMP_printk("device: %04x:%02x:%02x.%01x cap: %04x "
@@ -2009,6 +2594,9 @@ static int __init init_iommu_all(struct acpi_table_header *table)
 			if (iommu == NULL)
 				return -ENOMEM;
 
+			/*
+			 * 只在这里嗲欧
+			 */
 			ret = init_iommu_one(iommu, h, table);
 			if (ret)
 				return ret;
@@ -2230,6 +2818,10 @@ static int __init iommu_init_pci(struct amd_iommu *iommu)
 	return pci_enable_device(iommu->dev);
 }
 
+/*
+ * 在以下使用print_iommu_info():
+ *   - drivers/iommu/amd/init.c|2305| <<amd_iommu_init_pci>> print_iommu_info();
+ */
 static void print_iommu_info(void)
 {
 	int i;
@@ -2257,6 +2849,15 @@ static void print_iommu_info(void)
 
 	if (irq_remapping_enabled) {
 		pr_info("Interrupt remapping enabled\n");
+		/*
+		 * 在以下使用amd_iommu_xt_mode:
+		 *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+		 *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+		 *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+		 */
 		if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
 			pr_info("X2APIC enabled\n");
 	}
@@ -2266,6 +2867,10 @@ static void print_iommu_info(void)
 	}
 }
 
+/*
+ * 在以下使用amd_iommu_init_pci():
+ *   - drivers/iommu/amd/init.c|3400| <<state_next>> ret = amd_iommu_init_pci();
+ */
 static int __init amd_iommu_init_pci(void)
 {
 	struct amd_iommu *iommu;
@@ -2521,6 +3126,12 @@ static int iommu_setup_intcapxt(struct amd_iommu *iommu)
 
 	snprintf(iommu->evt_irq_name, sizeof(iommu->evt_irq_name),
 		 "AMD-Vi%d-Evt", iommu->index);
+	/*
+	 * 在以下使用amd_iommu_int_thread_evtlog():
+	 *   - drivers/iommu/amd/init.c|2568| <<iommu_setup_intcapxt>> ret = __iommu_setup_intcapxt(iommu, iommu->evt_irq_name,
+	 *                                  MMIO_INTCAPXT_EVT_OFFSET, amd_iommu_int_thread_evtlog);
+	 *   - drivers/iommu/amd/iommu.c|1142| <<amd_iommu_int_thread>> amd_iommu_int_thread_evtlog(irq, data);
+	 */
 	ret = __iommu_setup_intcapxt(iommu, iommu->evt_irq_name,
 				     MMIO_INTCAPXT_EVT_OFFSET,
 				     amd_iommu_int_thread_evtlog);
@@ -2553,6 +3164,15 @@ static int iommu_init_irq(struct amd_iommu *iommu)
 	if (iommu->int_enabled)
 		goto enable_faults;
 
+	/*
+	 * 在以下使用amd_iommu_xt_mode:
+	 *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+	 *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+	 */
 	if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
 		ret = iommu_setup_intcapxt(iommu);
 	else if (iommu->dev->msi_cap)
@@ -2566,6 +3186,15 @@ static int iommu_init_irq(struct amd_iommu *iommu)
 	iommu->int_enabled = true;
 enable_faults:
 
+	/*
+	 * 在以下使用amd_iommu_xt_mode:
+	 *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+	 *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+	 */
 	if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
 		iommu_feature_enable(iommu, CONTROL_INTCAPXT_EN);
 
@@ -2595,6 +3224,10 @@ static void __init free_unity_maps(void)
 	}
 }
 
+/*
+ * 在以下使用init_unity_map_range():
+ *   - drivers/iommu/amd/init.c|3246| <<init_memory_definitions>> init_unity_map_range(m, table);
+ */
 /* called for unity map ACPI definition */
 static int __init init_unity_map_range(struct ivmd_header *m,
 				       struct acpi_table_header *ivrs_base)
@@ -2603,6 +3236,11 @@ static int __init init_unity_map_range(struct ivmd_header *m,
 	struct amd_iommu_pci_seg *pci_seg;
 	char *s;
 
+	/*
+	 * 在以下使用get_pci_segment():
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one>> pci_seg = get_pci_segment(h->pci_seg, ivrs_base);
+	 *   - drivers/iommu/amd/init.c|2959| <<init_unity_map_range>> pci_seg = get_pci_segment(m->pci_seg, ivrs_base);
+	 */
 	pci_seg = get_pci_segment(m->pci_seg, ivrs_base);
 	if (pci_seg == NULL)
 		return -ENOMEM;
@@ -2658,6 +3296,10 @@ static int __init init_unity_map_range(struct ivmd_header *m,
 	return 0;
 }
 
+/*
+ * 在以下使用init_memory_definitions():
+ *   - drivers/iommu/amd/init.c|3888| <<early_amd_iommu_init>> ret = init_memory_definitions(ivrs_base);
+ */
 /* iterates over all memory definitions we find in the ACPI table */
 static int __init init_memory_definitions(struct acpi_table_header *table)
 {
@@ -2710,6 +3352,10 @@ static void __init uninit_device_table_dma(struct amd_iommu_pci_seg *pci_seg)
 	}
 }
 
+/*
+ * 在以下使用init_device_table():
+ *   - drivers/iommu/amd/init.c|3893| <<early_amd_iommu_init>> init_device_table();
+ */
 static void init_device_table(void)
 {
 	struct amd_iommu_pci_seg *pci_seg;
@@ -2747,6 +3393,15 @@ static void iommu_init_flags(struct amd_iommu *iommu)
 	 */
 	iommu_feature_enable(iommu, CONTROL_COHERENT_EN);
 
+	/*
+	 * 在以下使用iommu_feature_set():
+	 *   - drivers/iommu/amd/init.c|458| <<iommu_feature_enable>> iommu_feature_set(iommu, 1ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|463| <<iommu_feature_disable>> iommu_feature_set(iommu, 0ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|2799| <<iommu_init_flags>> iommu_feature_set(iommu, CTRL_INV_TO_1S,
+	 *                                           CTRL_INV_TO_MASK, CONTROL_INV_TIMEOUT);
+	 *   - drivers/iommu/amd/init.c|2897| <<iommu_enable_2k_int>> iommu_feature_set(iommu,
+	 *       CONTROL_NUM_INT_REMAP_MODE_2K, CONTROL_NUM_INT_REMAP_MODE_MASK, CONTROL_NUM_INT_REMAP_MODE);
+	 */
 	/* Set IOTLB invalidation timeout to 1s */
 	iommu_feature_set(iommu, CTRL_INV_TO_1S, CTRL_INV_TO_MASK, CONTROL_INV_TIMEOUT);
 
@@ -2846,6 +3501,15 @@ static void iommu_enable_2k_int(struct amd_iommu *iommu)
 	if (!FEATURE_NUM_INT_REMAP_SUP_2K(amd_iommu_efr2))
 		return;
 
+	/*
+	 * 在以下使用iommu_feature_set():
+	 *   - drivers/iommu/amd/init.c|458| <<iommu_feature_enable>> iommu_feature_set(iommu, 1ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|463| <<iommu_feature_disable>> iommu_feature_set(iommu, 0ULL, 1ULL, bit);
+	 *   - drivers/iommu/amd/init.c|2799| <<iommu_init_flags>> iommu_feature_set(iommu, CTRL_INV_TO_1S,
+	 *                                           CTRL_INV_TO_MASK, CONTROL_INV_TIMEOUT);
+	 *   - drivers/iommu/amd/init.c|2897| <<iommu_enable_2k_int>> iommu_feature_set(iommu,
+	 *       CONTROL_NUM_INT_REMAP_MODE_2K, CONTROL_NUM_INT_REMAP_MODE_MASK, CONTROL_NUM_INT_REMAP_MODE);
+	 */
 	iommu_feature_set(iommu,
 			  CONTROL_NUM_INT_REMAP_MODE_2K,
 			  CONTROL_NUM_INT_REMAP_MODE_MASK,
@@ -2857,7 +3521,17 @@ static void early_enable_iommu(struct amd_iommu *iommu)
 	iommu_disable(iommu);
 	iommu_init_flags(iommu);
 	iommu_set_device_table(iommu);
+	/*
+	 * 在以下使用iommu_enable_command_buffer():
+	 *   - drivers/iommu/amd/init.c|3231| <<early_enable_iommu>> iommu_enable_command_buffer(iommu);
+	 *   - drivers/iommu/amd/init.c|3300| <<early_enable_iommus>> iommu_enable_command_buffer(iommu);
+	 */
 	iommu_enable_command_buffer(iommu);
+	/*
+	 * 在以下使用iommu_enable_event_buffer():
+	 *   - drivers/iommu/amd/init.c|3102| <<early_enable_iommu>> iommu_enable_event_buffer(iommu);
+	 *   - drivers/iommu/amd/init.c|3166| <<early_enable_iommus>> iommu_enable_event_buffer(iommu);
+	 */
 	iommu_enable_event_buffer(iommu);
 	iommu_set_exclusion_range(iommu);
 	iommu_enable_gt(iommu);
@@ -2877,6 +3551,10 @@ static void early_enable_iommu(struct amd_iommu *iommu)
  * the old content of device table entries. Not this case or reuse failed,
  * just continue as normal kernel does.
  */
+/*
+ * 只在以下使用early_enable_iommus():
+ *   - drivers/iommu/amd/init.c|3839| <<state_next>> early_enable_iommus();
+ */
 static void early_enable_iommus(void)
 {
 	struct amd_iommu *iommu;
@@ -2921,7 +3599,17 @@ static void early_enable_iommus(void)
 			iommu_disable_command_buffer(iommu);
 			iommu_disable_event_buffer(iommu);
 			iommu_disable_irtcachedis(iommu);
+			/*
+			 * 在以下使用iommu_enable_command_buffer():
+			 *   - drivers/iommu/amd/init.c|3231| <<early_enable_iommu>> iommu_enable_command_buffer(iommu);
+			 *   - drivers/iommu/amd/init.c|3300| <<early_enable_iommus>> iommu_enable_command_buffer(iommu);
+			 */
 			iommu_enable_command_buffer(iommu);
+			/*
+			 * 在以下使用iommu_enable_event_buffer():
+			 *   - drivers/iommu/amd/init.c|3102| <<early_enable_iommu>> iommu_enable_event_buffer(iommu);
+			 *   - drivers/iommu/amd/init.c|3166| <<early_enable_iommus>> iommu_enable_event_buffer(iommu);
+			 */
 			iommu_enable_event_buffer(iommu);
 			iommu_enable_ga(iommu);
 			iommu_enable_xt(iommu);
@@ -3148,6 +3836,10 @@ static void __init ivinfo_init(void *ivrs)
  * After everything is set up the IOMMUs are enabled and the necessary
  * hotplug and suspend notifiers are registered.
  */
+/*
+ * 处理IOMMU_IVRS_DETECTED:
+ *   - drivers/iommu/amd/init.c|3426| <<state_next>> ret = early_amd_iommu_init();
+ */
 static int __init early_amd_iommu_init(void)
 {
 	struct acpi_table_header *ivrs_base;
@@ -3155,6 +3847,12 @@ static int __init early_amd_iommu_init(void)
 	acpi_status status;
 	u8 efr_hats;
 
+	/*
+	 * 在以下使用amd_iommu_detected:
+	 *   - drivers/iommu/amd/init.c|163| <<global>> static bool amd_iommu_detected;
+	 *   - drivers/iommu/amd/init.c|3210| <<early_amd_iommu_init>> if (!amd_iommu_detected)
+	 *   - drivers/iommu/amd/init.c|3660| <<amd_iommu_detect>> amd_iommu_detected = true;
+	 */
 	if (!amd_iommu_detected)
 		return -ENODEV;
 
@@ -3183,6 +3881,13 @@ static int __init early_amd_iommu_init(void)
 
 	ivinfo_init(ivrs_base);
 
+	/*
+	 * 在以下使用amd_iommu_target_ivhd_type:
+	 *   - drivers/iommu/amd/init.c|711| <<find_last_devid_acpi>> h->type == amd_iommu_target_ivhd_type) {
+	 *   - drivers/iommu/amd/init.c|2310| <<init_iommu_all>> if (*p == amd_iommu_target_ivhd_type) {
+	 *   - drivers/iommu/amd/init.c|3577| <<early_amd_iommu_init>> amd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);
+	 *   - drivers/iommu/amd/init.c|3578| <<early_amd_iommu_init>> DUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);
+	 */
 	amd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);
 	DUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);
 
@@ -3230,6 +3935,13 @@ static int __init early_amd_iommu_init(void)
 			amd_iommu_pgtable = PD_MODE_NONE;
 	}
 
+	/*
+	 * 在以下使用amd_iommu_disabled:
+	 *   - drivers/iommu/amd/init.c|164| <<global>> static bool amd_iommu_disabled __initdata;
+	 *   - drivers/iommu/amd/init.c|3286| <<early_amd_iommu_init>> if (!is_kdump_kernel() || amd_iommu_disabled)
+	 *   - drivers/iommu/amd/init.c|3453| <<state_next>> if (amd_iommu_disabled) {
+	 *   - drivers/iommu/amd/init.c|3711| <<parse_amd_iommu_options>> amd_iommu_disabled = true;
+	 */
 	/* Disable any previously enabled IOMMUs */
 	if (!is_kdump_kernel() || amd_iommu_disabled)
 		disable_iommus();
@@ -3282,6 +3994,10 @@ static int amd_iommu_enable_interrupts(void)
 	return ret;
 }
 
+/*
+ * 在以下使用detect_ivrs():
+ *   - drivers/iommu/amd/init.c|3445| <<state_next>> if (!detect_ivrs()) {
+ */
 static bool __init detect_ivrs(void)
 {
 	struct acpi_table_header *ivrs_base;
@@ -3299,6 +4015,12 @@ static bool __init detect_ivrs(void)
 
 	acpi_put_table(ivrs_base);
 
+	/*
+	 * 在以下使用amd_iommu_force_enable:
+	 *   - drivers/iommu/amd/init.c|165| <<global>> static bool amd_iommu_force_enable __initdata;
+	 *   - drivers/iommu/amd/init.c|3354| <<detect_ivrs>> if (amd_iommu_force_enable)
+	 *   - drivers/iommu/amd/init.c|3709| <<parse_amd_iommu_options>> amd_iommu_force_enable = true;
+	 */
 	if (amd_iommu_force_enable)
 		goto out;
 
@@ -3367,6 +4089,37 @@ static __init void iommu_snp_enable(void)
  *
  ****************************************************************************/
 
+/*
+ * [0] add_special_device
+ * [0] init_iommu_from_acpi
+ * [0] state_next
+ * [0] amd_iommu_prepare
+ * [0] irq_remapping_prepare
+ * [0] enable_IR_x2apic
+ * [0] x86_64_probe_apic
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * enum iommu_init_state {
+ *     IOMMU_START_STATE,
+ *     IOMMU_IVRS_DETECTED,
+ *     IOMMU_ACPI_FINISHED,
+ *     IOMMU_ENABLED,  
+ *     IOMMU_PCI_INIT, 
+ *     IOMMU_INTERRUPTS_EN,
+ *     IOMMU_INITIALIZED,
+ *     IOMMU_NOT_FOUND,
+ *     IOMMU_INIT_ERROR,
+ *     IOMMU_CMDLINE_DISABLED,
+ * };
+ *
+ * 在以下使用state_next():
+ *   - drivers/iommu/amd/init.c|3452| <<iommu_go_to_state>> ret = state_next();
+ */
 static int __init state_next(void)
 {
 	int ret = 0;
@@ -3381,6 +4134,13 @@ static int __init state_next(void)
 		}
 		break;
 	case IOMMU_IVRS_DETECTED:
+		/*
+		 * 在以下使用amd_iommu_disabled:
+		 *   - drivers/iommu/amd/init.c|164| <<global>> static bool amd_iommu_disabled __initdata;
+		 *   - drivers/iommu/amd/init.c|3286| <<early_amd_iommu_init>> if (!is_kdump_kernel() || amd_iommu_disabled)
+		 *   - drivers/iommu/amd/init.c|3453| <<state_next>> if (amd_iommu_disabled) {
+		 *   - drivers/iommu/amd/init.c|3711| <<parse_amd_iommu_options>> amd_iommu_disabled = true;
+		 */
 		if (amd_iommu_disabled) {
 			init_state = IOMMU_CMDLINE_DISABLED;
 			ret = -EINVAL;
@@ -3440,6 +4200,13 @@ static int __init state_next(void)
 	return ret;
 }
 
+/*
+ * 在以下使用iommu_go_to_state():
+ *   - drivers/iommu/amd/init.c|3478| <<amd_iommu_prepare>> ret = iommu_go_to_state(IOMMU_ACPI_FINISHED);
+ *   - drivers/iommu/amd/init.c|3491| <<amd_iommu_enable>> ret = iommu_go_to_state(IOMMU_ENABLED);
+ *   - drivers/iommu/amd/init.c|3527| <<amd_iommu_init>> ret = iommu_go_to_state(IOMMU_INITIALIZED);
+ *   - drivers/iommu/amd/init.c|3580| <<amd_iommu_detect>> ret = iommu_go_to_state(IOMMU_IVRS_DETECTED);
+ */
 static int __init iommu_go_to_state(enum iommu_init_state state)
 {
 	int ret = -EINVAL;
@@ -3469,6 +4236,21 @@ static int __init iommu_go_to_state(enum iommu_init_state state)
 }
 
 #ifdef CONFIG_IRQ_REMAP
+/*
+ * [0] add_special_device
+ * [0] init_iommu_from_acpi
+ * [0] state_next
+ * [0] amd_iommu_prepare
+ * [0] irq_remapping_prepare
+ * [0] enable_IR_x2apic
+ * [0] x86_64_probe_apic
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 int __init amd_iommu_prepare(void)
 {
 	int ret;
@@ -3493,6 +4275,15 @@ int __init amd_iommu_enable(void)
 		return ret;
 
 	irq_remapping_enabled = 1;
+	/*
+	 * 在以下使用amd_iommu_xt_mode:
+	 *   - drivers/iommu/amd/init.c|1192| <<iommu_enable_xt>> amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|1980| <<init_iommu_one>> amd_iommu_xt_mode = IRQ_REMAP_X2APIC_MODE;
+	 *   - drivers/iommu/amd/init.c|2350| <<print_iommu_info>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2656| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|2669| <<iommu_init_irq>> if (amd_iommu_xt_mode == IRQ_REMAP_X2APIC_MODE)
+	 *   - drivers/iommu/amd/init.c|3672| <<amd_iommu_enable>> return amd_iommu_xt_mode;
+	 */
 	return amd_iommu_xt_mode;
 }
 
@@ -3526,6 +4317,15 @@ static int __init amd_iommu_init(void)
 
 	ret = iommu_go_to_state(IOMMU_INITIALIZED);
 #ifdef CONFIG_GART_IOMMU
+	/*
+	 * 在以下使用amd_iommu_list:
+	 *   - drivers/iommu/amd/init.c|209| <<global>> LIST_HEAD(amd_iommu_list);
+	 *   - drivers/iommu/amd/amd_iommu_types.h|518| <<for_each_iommu>> list_for_each_entry((iommu), &amd_iommu_list, list)
+	 *   - drivers/iommu/amd/amd_iommu_types.h|520| <<for_each_iommu_safe>> list_for_each_entry_safe((iommu), (next), &amd_iommu_list, list)
+	 *   - drivers/iommu/amd/init.c|327| <<get_global_efr>> if (list_is_first(&iommu->list, &amd_iommu_list)) {
+	 *   - drivers/iommu/amd/init.c|2137| <<init_iommu_one>> list_add_tail(&iommu->list, &amd_iommu_list);
+	 *   - drivers/iommu/amd/init.c|4006| <<amd_iommu_init>> if (ret && list_empty(&amd_iommu_list)) {
+	 */
 	if (ret && list_empty(&amd_iommu_list)) {
 		/*
 		 * We failed to initialize the AMD IOMMU - try fallback
@@ -3567,10 +4367,28 @@ static bool amd_iommu_sme_check(void)
  * IOMMUs
  *
  ****************************************************************************/
+/*
+ * 在以下使用amd_iommu_detect():
+ *   - arch/x86/kernel/pci-dma.c|106| <<pci_iommu_alloc>> amd_iommu_detect();
+ */
 void __init amd_iommu_detect(void)
 {
 	int ret;
 
+	/*
+	 * 在以下设置no_iommu:
+	 *   - arch/x86/kernel/pci-dma.c|120| <<iommu_setup>> if (!strncmp(p, "off", 3)) no_iommu = 1;
+	 *   - drivers/iommu/intel/iommu.c|2973| <<platform_optin_force_iommu>> no_iommu = 0;
+	 *   - drivers/iommu/intel/iommu.c|3024| <<tboot_force_iommu>> no_iommu = 0;
+	 *
+	 * 在以下使用iommu_detected:
+	 *   - arch/x86/kernel/pci-dma.c|38| <<global>> int iommu_detected __read_mostly = 0;
+	 *   - arch/x86/kernel/aperture_64.c|429| <<gart_iommu_hole_init>> iommu_detected = 1;
+	 *   - drivers/iommu/amd/init.c|3650| <<amd_iommu_detect>> if (no_iommu || (iommu_detected && !gart_iommu_aperture))
+	 *   - drivers/iommu/amd/init.c|3661| <<amd_iommu_detect>> iommu_detected = 1;
+	 *   - drivers/iommu/intel/dmar.c|931| <<detect_intel_iommu>> if (!ret && !no_iommu && !iommu_detected &&
+	 *   - drivers/iommu/intel/dmar.c|933| <<detect_intel_iommu>> iommu_detected = 1;
+	 */
 	if (no_iommu || (iommu_detected && !gart_iommu_aperture))
 		goto disable_snp;
 
@@ -3581,6 +4399,12 @@ void __init amd_iommu_detect(void)
 	if (ret)
 		goto disable_snp;
 
+	/*
+	 * 在以下使用amd_iommu_detected:
+	 *   - drivers/iommu/amd/init.c|163| <<global>> static bool amd_iommu_detected;
+	 *   - drivers/iommu/amd/init.c|3210| <<early_amd_iommu_init>> if (!amd_iommu_detected)
+	 *   - drivers/iommu/amd/init.c|3660| <<amd_iommu_detect>> amd_iommu_detected = true;
+	 */
 	amd_iommu_detected = true;
 	iommu_detected = 1;
 	x86_init.iommu.iommu_init = amd_iommu_init;
@@ -3630,8 +4454,21 @@ static int __init parse_amd_iommu_options(char *str)
 			pr_warn("amd_iommu=fullflush deprecated; use iommu.strict=1 instead\n");
 			iommu_set_dma_strict();
 		} else if (strncmp(str, "force_enable", 12) == 0) {
+			/*
+			 * 在以下使用amd_iommu_force_enable:
+			 *   - drivers/iommu/amd/init.c|165| <<global>> static bool amd_iommu_force_enable __initdata;
+			 *   - drivers/iommu/amd/init.c|3354| <<detect_ivrs>> if (amd_iommu_force_enable)
+			 *   - drivers/iommu/amd/init.c|3709| <<parse_amd_iommu_options>> amd_iommu_force_enable = true;
+			 */
 			amd_iommu_force_enable = true;
 		} else if (strncmp(str, "off", 3) == 0) {
+			/*
+			 * 在以下使用amd_iommu_disabled:
+			 *   - drivers/iommu/amd/init.c|164| <<global>> static bool amd_iommu_disabled __initdata;
+			 *   - drivers/iommu/amd/init.c|3286| <<early_amd_iommu_init>> if (!is_kdump_kernel() || amd_iommu_disabled)
+			 *   - drivers/iommu/amd/init.c|3453| <<state_next>> if (amd_iommu_disabled) {
+			 *   - drivers/iommu/amd/init.c|3711| <<parse_amd_iommu_options>> amd_iommu_disabled = true;
+			 */
 			amd_iommu_disabled = true;
 		} else if (strncmp(str, "force_isolation", 15) == 0) {
 			amd_iommu_force_isolation = true;
@@ -3837,6 +4674,12 @@ bool amd_iommu_pasid_supported(void)
 	return amd_iommu_gt_ppr_supported() && !amd_iommu_snp_en;
 }
 
+/*
+ * 在以下使用get_amd_iommu():
+ *   - arch/x86/events/amd/iommu.c|432| <<init_one_iommu>> perf_iommu->iommu = get_amd_iommu(idx);
+ *   - drivers/iommu/amd/init.c|4374| <<amd_iommu_pc_get_max_banks>> struct amd_iommu *iommu = get_amd_iommu(idx);
+ *   - drivers/iommu/amd/init.c|4389| <<amd_iommu_pc_get_max_counters>> struct amd_iommu *iommu = get_amd_iommu(idx);
+ */
 struct amd_iommu *get_amd_iommu(unsigned int idx)
 {
 	unsigned int i = 0;
@@ -3935,6 +4778,10 @@ int amd_iommu_pc_set_reg(struct amd_iommu *iommu, u8 bank, u8 cntr, u8 fxn, u64
 }
 
 #ifdef CONFIG_KVM_AMD_SEV
+/*
+ * 在以下使用iommu_page_make_shared();
+ *   - drivers/iommu/amd/init.c|4499| <<iommu_make_shared>> ret = iommu_page_make_shared(page);
+ */
 static int iommu_page_make_shared(void *page)
 {
 	unsigned long paddr, pfn;
@@ -3999,6 +4846,19 @@ int amd_iommu_snp_disable(void)
 		return 0;
 
 	for_each_iommu(iommu) {
+		/*
+		 * 在以下使用amd_iommu->evt_buf:
+		 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+		 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+		 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+		 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+		 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+		 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+		 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+		 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+		 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+		 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+		 */
 		ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
 		if (ret)
 			return ret;
diff --git a/drivers/iommu/amd/iommu.c b/drivers/iommu/amd/iommu.c
index 2e1865daa..25eb18d6a 100644
--- a/drivers/iommu/amd/iommu.c
+++ b/drivers/iommu/amd/iommu.c
@@ -55,6 +55,12 @@
 #define HT_RANGE_START		(0xfd00000000ULL)
 #define HT_RANGE_END		(0xffffffffffULL)
 
+/*
+ * 在以下使用ioapic_map:
+ *   - drivers/iommu/amd/iommu.c|58| <<global>> LIST_HEAD(ioapic_map);
+ *   - drivers/iommu/amd/amd_iommu_types.h|964| <<get_ioapic_devid>> list_for_each_entry(entry, &ioapic_map, list) {
+ *   - drivers/iommu/amd/init.c|1393| <<add_special_device>> list = &ioapic_map;
+ */
 LIST_HEAD(ioapic_map);
 LIST_HEAD(hpet_map);
 LIST_HEAD(acpihid_map);
@@ -135,6 +141,9 @@ static void update_dte256(struct amd_iommu *iommu, struct iommu_dev_data *dev_da
 			  struct dev_table_entry *new)
 {
 	unsigned long flags;
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	struct dev_table_entry *dev_table = get_dev_table(iommu);
 	struct dev_table_entry *ptr = &dev_table[dev_data->devid];
 
@@ -202,6 +211,9 @@ static void get_dte256(struct amd_iommu *iommu, struct iommu_dev_data *dev_data,
 {
 	unsigned long flags;
 	struct dev_table_entry *ptr;
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	struct dev_table_entry *dev_table = get_dev_table(iommu);
 
 	ptr = &dev_table[dev_data->devid];
@@ -287,6 +299,22 @@ static inline int get_device_sbdf_id(struct device *dev)
 	return sbdf;
 }
 
+/*
+ * 在以下使用get_dev_table():
+ *   - drivers/iommu/amd/debugfs.c|257| <<dump_dte>> dev_table = get_dev_table(iommu);
+ *   - drivers/iommu/amd/debugfs.c|368| <<dump_irte>> dev_table = get_dev_table(iommu);
+ *   - drivers/iommu/amd/init.c|500| <<iommu_set_device_table>> void *dev_table = (void *)get_dev_table(iommu);
+ *   - drivers/iommu/amd/init.c|1606| <<set_dev_entry_from_acpi_range>> struct dev_table_entry *dev_table = get_dev_table(iommu);
+ *   - drivers/iommu/amd/iommu.c|144| <<update_dte256>> struct dev_table_entry *dev_table = get_dev_table(iommu);
+ *   - drivers/iommu/amd/iommu.c|211| <<get_dte256>> struct dev_table_entry *dev_table = get_dev_table(iommu);
+ *   - drivers/iommu/amd/iommu.c|767| <<iommu_ignore_device>> struct dev_table_entry *dev_table = get_dev_table(iommu);
+ *   - drivers/iommu/amd/iommu.c|2342| <<set_dte_entry>> struct dev_table_entry *dte = &get_dev_table(iommu)[dev_data->devid];
+ *   - drivers/iommu/amd/iommu.c|2409| <<clear_dte_entry>> struct dev_table_entry *dte = &get_dev_table(iommu)[dev_data->devid];
+ *   - drivers/iommu/amd/iommu.c|3177| <<amd_iommu_set_dirty_tracking>> dte = &get_dev_table(iommu)[dev_data->devid];
+ *   - drivers/iommu/amd/iommu.c|3449| <<set_dte_irq_entry>> struct dev_table_entry *dte = &get_dev_table(iommu)[devid];
+ *
+ * 返回amd_iommu->pci_seg->dev_table
+ */
 struct dev_table_entry *get_dev_table(struct amd_iommu *iommu)
 {
 	struct dev_table_entry *dev_table;
@@ -316,11 +344,33 @@ static inline u16 get_device_segment(struct device *dev)
 	return seg;
 }
 
+/*
+ * 在以下使用amd_iommu_set_rlookup_table():
+ *   - drivers/iommu/amd/init.c|1404| <<set_dev_entry_from_acpi_range>> amd_iommu_set_rlookup_table(iommu, i);
+ *   - drivers/iommu/amd/iommu.c|423| <<clone_alias>> amd_iommu_set_rlookup_table(iommu, alias);
+ */
 /* Writes the specific IOMMU for a device into the PCI segment rlookup table */
 void amd_iommu_set_rlookup_table(struct amd_iommu *iommu, u16 devid)
 {
 	struct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;
 
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	pci_seg->rlookup_table[devid] = iommu;
 }
 
@@ -329,6 +379,23 @@ static struct amd_iommu *__rlookup_amd_iommu(u16 seg, u16 devid)
 	struct amd_iommu_pci_seg *pci_seg;
 
 	for_each_pci_segment(pci_seg) {
+		/*
+		 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+		 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+		 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+		 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+		 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+		 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+		 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+		 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+		 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+		 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+		 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+		 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+		 */
 		if (pci_seg->id == seg)
 			return pci_seg->rlookup_table[devid];
 	}
@@ -414,6 +481,11 @@ static int clone_alias(struct pci_dev *pdev, u16 alias, void *data)
 	}
 	update_dte256(iommu, alias_data, &new);
 
+	/*
+	 * 在以下使用amd_iommu_set_rlookup_table():
+	 *   - drivers/iommu/amd/init.c|1404| <<set_dev_entry_from_acpi_range>> amd_iommu_set_rlookup_table(iommu, i);
+	 *   - drivers/iommu/amd/iommu.c|423| <<clone_alias>> amd_iommu_set_rlookup_table(iommu, alias);
+	 */
 	amd_iommu_set_rlookup_table(iommu, alias);
 out:
 	return ret;
@@ -714,6 +786,9 @@ static int iommu_init_device(struct amd_iommu *iommu, struct device *dev)
 static void iommu_ignore_device(struct amd_iommu *iommu, struct device *dev)
 {
 	struct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	struct dev_table_entry *dev_table = get_dev_table(iommu);
 	int devid, sbdf;
 
@@ -722,6 +797,23 @@ static void iommu_ignore_device(struct amd_iommu *iommu, struct device *dev)
 		return;
 
 	devid = PCI_SBDF_TO_DEVID(sbdf);
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	pci_seg->rlookup_table[devid] = NULL;
 	memset(&dev_table[devid], 0, sizeof(struct dev_table_entry));
 
@@ -878,6 +970,10 @@ static void amd_iommu_report_page_fault(struct amd_iommu *iommu,
 		pci_dev_put(pdev);
 }
 
+/*
+ * 在以下使用iommu_print_event():
+ *   - drivers/iommu/amd/iommu.c|987| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+ */
 static void iommu_print_event(struct amd_iommu *iommu, void *__evt)
 {
 	struct device *dev = iommu->iommu.dev;
@@ -976,14 +1072,39 @@ static void iommu_print_event(struct amd_iommu *iommu, void *__evt)
 		memset(__evt, 0, 4 * sizeof(u32));
 }
 
+/*
+ * 在以下使用iommu_poll_events():
+ *   - drivers/iommu/amd/iommu.c|1115| <<amd_iommu_int_thread_evtlog>>
+ *       amd_iommu_handle_irq(data, "Evt", MMIO_STATUS_EVT_INT_MASK,
+ *           MMIO_STATUS_EVT_OVERFLOW_MASK, iommu_poll_events, amd_iommu_restart_event_logging)
+ */
 static void iommu_poll_events(struct amd_iommu *iommu)
 {
 	u32 head, tail;
 
+	/*
+	 * 在以下使用MMIO_EVT_HEAD_OFFSET:
+	 *   - drivers/iommu/amd/init.c|1060| <<iommu_enable_event_buffer>> writel(0x00, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
+	 *   - drivers/iommu/amd/iommu.c|1060| <<iommu_poll_events>> head = readl(iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
+	 *   - drivers/iommu/amd/iommu.c|1081| <<iommu_poll_events>> writel(head, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
+	 */
 	head = readl(iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);
 	tail = readl(iommu->mmio_base + MMIO_EVT_TAIL_OFFSET);
 
 	while (head != tail) {
+		/*
+		 * 在以下使用amd_iommu->evt_buf:
+		 *   - drivers/iommu/amd/init.c|968| <<alloc_event_buffer>> iommu->evt_buf = iommu_alloc_4k_pages(iommu, GFP_KERNEL,
+		 *   - drivers/iommu/amd/init.c|971| <<alloc_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+		 *   - drivers/iommu/amd/init.c|978| <<iommu_enable_event_buffer>> BUG_ON(iommu->evt_buf == NULL);
+		 *   - drivers/iommu/amd/init.c|985| <<iommu_enable_event_buffer>> entry = iommu_virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;
+		 *   - drivers/iommu/amd/init.c|1007| <<free_event_buffer>> iommu_free_pages(iommu->evt_buf);
+		 *   - drivers/iommu/amd/init.c|1089| <<remap_event_buffer>> iommu->evt_buf = iommu_memremap(paddr, EVT_BUFFER_SIZE);
+		 *   - drivers/iommu/amd/init.c|1091| <<remap_event_buffer>> return iommu->evt_buf ? 0 : -ENOMEM;
+		 *   - drivers/iommu/amd/init.c|1186| <<unmap_event_buffer>> memunmap(iommu->evt_buf);
+		 *   - drivers/iommu/amd/init.c|4376| <<amd_iommu_snp_disable>> ret = iommu_make_shared(iommu->evt_buf, EVT_BUFFER_SIZE);
+		 *   - drivers/iommu/amd/iommu.c|1003| <<iommu_poll_events>> iommu_print_event(iommu, iommu->evt_buf + head);
+		 */
 		iommu_print_event(iommu, iommu->evt_buf + head);
 
 		/* Update head pointer of hardware ring-buffer */
@@ -1108,6 +1229,12 @@ static void amd_iommu_handle_irq(void *data, const char *evt_type,
 	}
 }
 
+/*
+ * 在以下使用amd_iommu_int_thread_evtlog():
+ *   - drivers/iommu/amd/init.c|2568| <<iommu_setup_intcapxt>> ret = __iommu_setup_intcapxt(iommu, iommu->evt_irq_name,
+ *                                  MMIO_INTCAPXT_EVT_OFFSET, amd_iommu_int_thread_evtlog);
+ *   - drivers/iommu/amd/iommu.c|1142| <<amd_iommu_int_thread>> amd_iommu_int_thread_evtlog(irq, data);
+ */
 irqreturn_t amd_iommu_int_thread_evtlog(int irq, void *data)
 {
 	amd_iommu_handle_irq(data, "Evt", MMIO_STATUS_EVT_INT_MASK,
@@ -1139,6 +1266,12 @@ irqreturn_t amd_iommu_int_thread_galog(int irq, void *data)
 
 irqreturn_t amd_iommu_int_thread(int irq, void *data)
 {
+	/*
+	 * 在以下使用amd_iommu_int_thread_evtlog():
+	 *   - drivers/iommu/amd/init.c|2568| <<iommu_setup_intcapxt>> ret = __iommu_setup_intcapxt(iommu, iommu->evt_irq_name,
+	 *                                  MMIO_INTCAPXT_EVT_OFFSET, amd_iommu_int_thread_evtlog);
+	 *   - drivers/iommu/amd/iommu.c|1142| <<amd_iommu_int_thread>> amd_iommu_int_thread_evtlog(irq, data);
+	 */
 	amd_iommu_int_thread_evtlog(irq, data);
 	amd_iommu_int_thread_pprlog(irq, data);
 	amd_iommu_int_thread_galog(irq, data);
@@ -1157,6 +1290,11 @@ irqreturn_t amd_iommu_int_handler(int irq, void *data)
  *
  ****************************************************************************/
 
+/*
+ * 在以下使用wait_on_sem():
+ *   - drivers/iommu/amd/iommu.c|1440| <<iommu_completion_wait>> ret = wait_on_sem(iommu, data);
+ *   - drivers/iommu/amd/iommu.c|3129| <<iommu_flush_irt_and_complete>> wait_on_sem(iommu, data);
+ */
 static int wait_on_sem(struct amd_iommu *iommu, u64 data)
 {
 	int i = 0;
@@ -1192,6 +1330,11 @@ static void copy_cmd_to_buffer(struct amd_iommu *iommu,
 	writel(tail, iommu->mmio_base + MMIO_CMD_TAIL_OFFSET);
 }
 
+/*
+ * 在以下使用build_completion_wait():
+ *   - drivers/iommu/amd/iommu.c|1432| <<iommu_completion_wait>> build_completion_wait(&cmd, iommu, data);
+ *   - drivers/iommu/amd/iommu.c|3120| <<iommu_flush_irt_and_complete>> build_completion_wait(&cmd2, iommu, data);
+ */
 static void build_completion_wait(struct iommu_cmd *cmd,
 				  struct amd_iommu *iommu,
 				  u64 data)
@@ -1330,6 +1473,13 @@ static void build_inv_irt(struct iommu_cmd *cmd, u16 devid)
  * Writes the command to the IOMMUs command buffer and informs the
  * hardware about the new command.
  */
+/*
+ * 在以下使用__iommu_queue_command_sync():
+ *   - drivers/iommu/amd/iommu.c|1406| <<iommu_queue_command_sync>> ret = __iommu_queue_command_sync(iommu, cmd, sync);
+ *   - drivers/iommu/amd/iommu.c|1436| <<iommu_completion_wait>> ret = __iommu_queue_command_sync(iommu, &cmd, false);
+ *   - drivers/iommu/amd/iommu.c|3123| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd, true);
+ *   - drivers/iommu/amd/iommu.c|3126| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd2, false);
+ */
 static int __iommu_queue_command_sync(struct amd_iommu *iommu,
 				      struct iommu_cmd *cmd,
 				      bool sync)
@@ -1375,6 +1525,13 @@ static int iommu_queue_command_sync(struct amd_iommu *iommu,
 	int ret;
 
 	raw_spin_lock_irqsave(&iommu->lock, flags);
+	/*
+	 * 在以下使用__iommu_queue_command_sync():
+	 *   - drivers/iommu/amd/iommu.c|1406| <<iommu_queue_command_sync>> ret = __iommu_queue_command_sync(iommu, cmd, sync);
+	 *   - drivers/iommu/amd/iommu.c|1436| <<iommu_completion_wait>> ret = __iommu_queue_command_sync(iommu, &cmd, false);
+	 *   - drivers/iommu/amd/iommu.c|3123| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd, true);
+	 *   - drivers/iommu/amd/iommu.c|3126| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd2, false);
+	 */
 	ret = __iommu_queue_command_sync(iommu, cmd, sync);
 	raw_spin_unlock_irqrestore(&iommu->lock, flags);
 
@@ -1390,8 +1547,27 @@ static int iommu_queue_command(struct amd_iommu *iommu, struct iommu_cmd *cmd)
  * This function queues a completion wait command into the command
  * buffer of an IOMMU
  */
+/*
+ * 在以下使用iommu_completion_wait():
+ *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+ *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+ *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+ */
 static int iommu_completion_wait(struct amd_iommu *iommu)
 {
+	/*
+	 * struct iommu_cmd {
+	 *     u32 data[4];
+	 * };
+	 */
 	struct iommu_cmd cmd;
 	unsigned long flags;
 	int ret;
@@ -1401,14 +1577,31 @@ static int iommu_completion_wait(struct amd_iommu *iommu)
 		return 0;
 
 	data = atomic64_inc_return(&iommu->cmd_sem_val);
+	/*
+	 * 在以下使用build_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1432| <<iommu_completion_wait>> build_completion_wait(&cmd, iommu, data);
+	 *   - drivers/iommu/amd/iommu.c|3120| <<iommu_flush_irt_and_complete>> build_completion_wait(&cmd2, iommu, data);
+	 */
 	build_completion_wait(&cmd, iommu, data);
 
 	raw_spin_lock_irqsave(&iommu->lock, flags);
 
+	/*
+	 * 在以下使用__iommu_queue_command_sync():
+	 *   - drivers/iommu/amd/iommu.c|1406| <<iommu_queue_command_sync>> ret = __iommu_queue_command_sync(iommu, cmd, sync);
+	 *   - drivers/iommu/amd/iommu.c|1436| <<iommu_completion_wait>> ret = __iommu_queue_command_sync(iommu, &cmd, false);
+	 *   - drivers/iommu/amd/iommu.c|3123| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd, true);
+	 *   - drivers/iommu/amd/iommu.c|3126| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd2, false);
+	 */
 	ret = __iommu_queue_command_sync(iommu, &cmd, false);
 	if (ret)
 		goto out_unlock;
 
+	/*
+	 * 在以下使用wait_on_sem():
+	 *   - drivers/iommu/amd/iommu.c|1440| <<iommu_completion_wait>> ret = wait_on_sem(iommu, data);
+	 *   - drivers/iommu/amd/iommu.c|3129| <<iommu_flush_irt_and_complete>> wait_on_sem(iommu, data);
+	 */
 	ret = wait_on_sem(iommu, data);
 
 out_unlock:
@@ -1424,6 +1617,20 @@ static void domain_flush_complete(struct protection_domain *domain)
 
 	lockdep_assert_held(&domain->lock);
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	/*
 	 * Devices of this domain are behind this IOMMU
 	 * We need to wait for completion of all commands.
@@ -1446,6 +1653,20 @@ static void iommu_flush_dte_sync(struct amd_iommu *iommu, u16 devid)
 	int ret;
 
 	ret = iommu_flush_dte(iommu, devid);
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	if (!ret)
 		iommu_completion_wait(iommu);
 }
@@ -1458,6 +1679,20 @@ static void amd_iommu_flush_dte_all(struct amd_iommu *iommu)
 	for (devid = 0; devid <= last_bdf; ++devid)
 		iommu_flush_dte(iommu, devid);
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -1477,6 +1712,20 @@ static void amd_iommu_flush_tlb_all(struct amd_iommu *iommu)
 		iommu_queue_command(iommu, &cmd);
 	}
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -1488,6 +1737,20 @@ static void amd_iommu_flush_tlb_domid(struct amd_iommu *iommu, u32 dom_id)
 			      dom_id, IOMMU_NO_PASID, false);
 	iommu_queue_command(iommu, &cmd);
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -1498,6 +1761,20 @@ static void amd_iommu_flush_all(struct amd_iommu *iommu)
 	build_inv_all(&cmd);
 
 	iommu_queue_command(iommu, &cmd);
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -1521,6 +1798,20 @@ static void amd_iommu_flush_irt_all(struct amd_iommu *iommu)
 	for (devid = 0; devid <= last_bdf; devid++)
 		iommu_flush_irt(iommu, devid);
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -1746,6 +2037,20 @@ void amd_iommu_dev_flush_pasid_pages(struct iommu_dev_data *dev_data,
 	if (dev_data->ats_enabled)
 		device_flush_iotlb(dev_data, address, size, pasid, true);
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu);
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -2059,6 +2364,9 @@ static void set_dte_entry(struct amd_iommu *iommu,
 	struct dev_table_entry new = {};
 	struct protection_domain *domain = dev_data->domain;
 	struct gcr3_tbl_info *gcr3_info = &dev_data->gcr3_info;
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	struct dev_table_entry *dte = &get_dev_table(iommu)[dev_data->devid];
 
 	if (gcr3_info && gcr3_info->gcr3_tbl)
@@ -2126,6 +2434,9 @@ static void set_dte_entry(struct amd_iommu *iommu,
 static void clear_dte_entry(struct amd_iommu *iommu, struct iommu_dev_data *dev_data)
 {
 	struct dev_table_entry new = {};
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	struct dev_table_entry *dte = &get_dev_table(iommu)[dev_data->devid];
 
 	make_clear_dte(dev_data, dte, &new);
@@ -2144,6 +2455,20 @@ static void dev_update_dte(struct iommu_dev_data *dev_data, bool set)
 
 	clone_aliases(iommu, dev_data->dev);
 	device_flush_dte(dev_data);
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 }
 
@@ -2428,6 +2753,20 @@ static struct iommu_device *amd_iommu_probe_device(struct device *dev)
 
 out_err:
 
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 
 	if (FEATURE_NUM_INT_REMAP_SUP_2K(amd_iommu_efr2))
@@ -2866,6 +3205,9 @@ static int amd_iommu_set_dirty_tracking(struct iommu_domain *domain,
 	list_for_each_entry(dev_data, &pdomain->dev_list, list) {
 		spin_lock(&dev_data->dte_lock);
 		iommu = get_amd_iommu_from_dev_data(dev_data);
+		/*
+		 * 返回amd_iommu->pci_seg->dev_table
+		 */
 		dte = &get_dev_table(iommu)[dev_data->devid];
 		new = dte->data[0];
 		new = (enable ? new | DTE_FLAG_HAD : new & ~DTE_FLAG_HAD);
@@ -3089,15 +3431,39 @@ static void iommu_flush_irt_and_complete(struct amd_iommu *iommu, u16 devid)
 
 	build_inv_irt(&cmd, devid);
 	data = atomic64_inc_return(&iommu->cmd_sem_val);
+	/*
+	 * 在以下使用build_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1432| <<iommu_completion_wait>> build_completion_wait(&cmd, iommu, data);
+	 *   - drivers/iommu/amd/iommu.c|3120| <<iommu_flush_irt_and_complete>> build_completion_wait(&cmd2, iommu, data);
+	 */
 	build_completion_wait(&cmd2, iommu, data);
 
 	raw_spin_lock_irqsave(&iommu->lock, flags);
+	/*
+	 * 在以下使用__iommu_queue_command_sync():
+	 *   - drivers/iommu/amd/iommu.c|1406| <<iommu_queue_command_sync>> ret = __iommu_queue_command_sync(iommu, cmd, sync);
+	 *   - drivers/iommu/amd/iommu.c|1436| <<iommu_completion_wait>> ret = __iommu_queue_command_sync(iommu, &cmd, false);
+	 *   - drivers/iommu/amd/iommu.c|3123| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd, true);
+	 *   - drivers/iommu/amd/iommu.c|3126| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd2, false);
+	 */
 	ret = __iommu_queue_command_sync(iommu, &cmd, true);
 	if (ret)
 		goto out;
+	/*
+	 * 在以下使用__iommu_queue_command_sync():
+	 *   - drivers/iommu/amd/iommu.c|1406| <<iommu_queue_command_sync>> ret = __iommu_queue_command_sync(iommu, cmd, sync);
+	 *   - drivers/iommu/amd/iommu.c|1436| <<iommu_completion_wait>> ret = __iommu_queue_command_sync(iommu, &cmd, false);
+	 *   - drivers/iommu/amd/iommu.c|3123| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd, true);
+	 *   - drivers/iommu/amd/iommu.c|3126| <<iommu_flush_irt_and_complete>> ret = __iommu_queue_command_sync(iommu, &cmd2, false);
+	 */
 	ret = __iommu_queue_command_sync(iommu, &cmd2, false);
 	if (ret)
 		goto out;
+	/*
+	 * 在以下使用wait_on_sem():
+	 *   - drivers/iommu/amd/iommu.c|1440| <<iommu_completion_wait>> ret = wait_on_sem(iommu, data);
+	 *   - drivers/iommu/amd/iommu.c|3129| <<iommu_flush_irt_and_complete>> wait_on_sem(iommu, data);
+	 */
 	wait_on_sem(iommu, data);
 out:
 	raw_spin_unlock_irqrestore(&iommu->lock, flags);
@@ -3114,6 +3480,9 @@ static void set_dte_irq_entry(struct amd_iommu *iommu, u16 devid,
 			      struct irq_remap_table *table)
 {
 	u64 new;
+	/*
+	 * 返回amd_iommu->pci_seg->dev_table
+	 */
 	struct dev_table_entry *dte = &get_dev_table(iommu)[devid];
 	struct iommu_dev_data *dev_data = search_dev_data(iommu, devid);
 
@@ -3137,6 +3506,23 @@ static struct irq_remap_table *get_irq_table(struct amd_iommu *iommu, u16 devid)
 	struct irq_remap_table *table;
 	struct amd_iommu_pci_seg *pci_seg = iommu->pci_seg;
 
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	if (WARN_ONCE(!pci_seg->rlookup_table[devid],
 		      "%s: no iommu for devid %x:%x\n",
 		      __func__, pci_seg->id, devid))
@@ -3192,6 +3578,23 @@ static int set_remap_table_entry_alias(struct pci_dev *pdev, u16 alias,
 	pci_seg = iommu->pci_seg;
 	pci_seg->irq_lookup_table[alias] = table;
 	set_dte_irq_entry(iommu, alias, table);
+	/*
+	 * 在以下使用amd_iommu_pci_seg->rlookup_table:
+	 *   - drivers/iommu/amd/debugfs.c|177| <<devid_write>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|217| <<dump_dte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/debugfs.c|311| <<dump_irte>> iommu = pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/init.c|758| <<alloc_rlookup_table>> pci_seg->rlookup_table = kvcalloc(pci_seg->last_bdf + 1,
+	 *   - drivers/iommu/amd/init.c|759| <<alloc_rlookup_table>> sizeof(*pci_seg->rlookup_table),
+	 *   - drivers/iommu/amd/init.c|761| <<alloc_rlookup_table>> if (pci_seg->rlookup_table == NULL)
+	 *   - drivers/iommu/amd/init.c|769| <<free_rlookup_table>> kvfree(pci_seg->rlookup_table);
+	 *   - drivers/iommu/amd/init.c|770| <<free_rlookup_table>> pci_seg->rlookup_table = NULL;
+	 *   - drivers/iommu/amd/init.c|2128| <<init_iommu_one_late>> iommu->pci_seg->rlookup_table[iommu->devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|330| <<amd_iommu_set_rlookup_table>> pci_seg->rlookup_table[devid] = iommu;
+	 *   - drivers/iommu/amd/iommu.c|339| <<__rlookup_amd_iommu>> return pci_seg->rlookup_table[devid];
+	 *   - drivers/iommu/amd/iommu.c|731| <<iommu_ignore_device>> pci_seg->rlookup_table[devid] = NULL;
+	 *   - drivers/iommu/amd/iommu.c|3168| <<get_irq_table>> if (WARN_ONCE(!pci_seg->rlookup_table[devid],
+	 *   - drivers/iommu/amd/iommu.c|3223| <<set_remap_table_entry_alias>> iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
+	 */
 	iommu_flush_dte(pci_seg->rlookup_table[alias], alias);
 
 	return 0;
@@ -3261,6 +3664,20 @@ static struct irq_remap_table *alloc_irq_table(struct amd_iommu *iommu,
 		set_remap_table_entry(iommu, alias, table);
 
 out_wait:
+	/*
+	 * 在以下使用iommu_completion_wait():
+	 *   - drivers/iommu/amd/iommu.c|1460| <<domain_flush_complete>> iommu_completion_wait(pdom_iommu_info->iommu);
+	 *   - drivers/iommu/amd/iommu.c|1478| <<iommu_flush_dte_sync>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1489| <<amd_iommu_flush_dte_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1508| <<amd_iommu_flush_tlb_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1519| <<amd_iommu_flush_tlb_domid>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1529| <<amd_iommu_flush_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1552| <<amd_iommu_flush_irt_all>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|1777| <<amd_iommu_dev_flush_pasid_pages>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2175| <<dev_update_dte>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|2459| <<amd_iommu_probe_device>> iommu_completion_wait(iommu);
+	 *   - drivers/iommu/amd/iommu.c|3292| <<alloc_irq_table>> iommu_completion_wait(iommu); 
+	 */
 	iommu_completion_wait(iommu);
 
 out_unlock:
@@ -3404,6 +3821,22 @@ static void irte_prepare(void *entry,
 			 u32 delivery_mode, bool dest_mode,
 			 u8 vector, u32 dest_apicid, int devid)
 {
+	/*
+	 * 1000 union irte {
+	 * 1001         u32 val;
+	 * 1002         struct {
+	 * 1003                 u32 valid       : 1,
+	 * 1004                     no_fault    : 1,
+	 * 1005                     int_type    : 3,
+	 * 1006                     rq_eoi      : 1,
+	 * 1007                     dm          : 1,
+	 * 1008                     rsvd_1      : 1,
+	 * 1009                     destination : 8,
+	 * 1010                     vector      : 8,
+	 * 1011                     rsvd_2      : 8;
+	 * 1012         } fields;
+	 * 1013 };
+	 */
 	union irte *irte = (union irte *) entry;
 
 	irte->val                = 0;
@@ -3573,6 +4006,51 @@ static void fill_msi_msg(struct msi_msg *msg, u32 index)
 	msg->address_hi = X86_MSI_BASE_ADDRESS_HIGH;
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] mp_map_pin_to_irq
+ * [0] acpi_register_gsi_ioapic
+ * [0] acpi_pci_irq_enable
+ * [0] do_pci_enable_device
+ * [0] pci_enable_device_flags
+ * [0] e1000_probe
+ * [0] local_pci_probe
+ * [0] pci_device_probe
+ * [0] really_probe
+ * [0] __driver_probe_device
+ * [0] driver_probe_device
+ * [0] __driver_attach
+ * [0] bus_for_each_dev
+ * [0] bus_add_driver
+ * [0] driver_register
+ * [0] do_one_initcall
+ * [0] kernel_init_freeable
+ * [0] kernel_init
+ * [0] ret_from_fork
+ * [0] ret_from_fork_asm
+ *
+ * 在以下使用irq_remapping_prepare_irte():
+ *   - drivers/iommu/amd/iommu.c|3726| <<irq_remapping_alloc>> irq_remapping_prepare_irte(data, cfg, info, devid, index, i);
+ */
 static void irq_remapping_prepare_irte(struct amd_ir_data *data,
 				       struct irq_cfg *irq_cfg,
 				       struct irq_alloc_info *info,
@@ -3586,6 +4064,10 @@ static void irq_remapping_prepare_irte(struct amd_ir_data *data,
 
 	data->irq_2_irte.devid = devid;
 	data->irq_2_irte.index = index + sub_handle;
+	/*
+	 * irte_32_ops
+	 * irte_128_ops
+	 */
 	iommu->irte_ops->prepare(data->entry, APIC_DELIVERY_MODE_FIXED,
 				 apic->dest_mode_logical, irq_cfg->vector,
 				 irq_cfg->dest_apicid, devid);
@@ -3624,6 +4106,30 @@ struct amd_irte_ops irte_128_ops = {
 	.clear_allocated = irte_ga_clear_allocated,
 };
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init       
+ * [0] x86_late_time_init        
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ *
+ * static const struct irq_domain_ops amd_ir_domain_ops = {
+ *     .select = irq_remapping_select,
+ *     .alloc = irq_remapping_alloc,
+ *     .free = irq_remapping_free,
+ *     .activate = irq_remapping_activate,
+ *     .deactivate = irq_remapping_deactivate,
+ * };
+ */
 static int irq_remapping_alloc(struct irq_domain *domain, unsigned int virq,
 			       unsigned int nr_irqs, void *arg)
 {
@@ -4028,6 +4534,21 @@ static int amd_ir_set_affinity(struct irq_data *data,
 	return IRQ_SET_MASK_OK_DONE;
 }
 
+/*
+ * [0] ir_compose_msi_msg
+ * [0] irq_chip_compose_msi_msg
+ * [0] ioapic_configure_entry
+ * [0] mp_irqdomain_activate
+ * [0] __irq_domain_activate_irq
+ * [0] irq_domain_activate_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 static void ir_compose_msi_msg(struct irq_data *irq_data, struct msi_msg *msg)
 {
 	struct amd_ir_data *ir_data = irq_data->chip_data;
diff --git a/drivers/iommu/amd/quirks.c b/drivers/iommu/amd/quirks.c
index 79dbb8f33..d3bf0b46c 100644
--- a/drivers/iommu/amd/quirks.c
+++ b/drivers/iommu/amd/quirks.c
@@ -48,6 +48,13 @@ static int __init ivrs_ioapic_quirk_cb(const struct dmi_system_id *d)
 {
 	const struct ivrs_quirk_entry *i;
 
+	/*
+	 * 在以下使用add_special_device():
+	 *   - drivers/iommu/amd/init.c|1465| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_IOAPIC,
+	 *   - drivers/iommu/amd/init.c|1474| <<add_early_maps>> ret = add_special_device(IVHD_SPECIAL_HPET,
+	 *   - drivers/iommu/amd/init.c|1669| <<init_iommu_from_acpi>> ret = add_special_device(type, handle, &devid, false);
+	 *   - drivers/iommu/amd/quirks.c|52| <<ivrs_ioapic_quirk_cb>> add_special_device(IVHD_SPECIAL_IOAPIC, i->id, (u32 *)&i->devid, 0);
+	 */
 	for (i = d->driver_data; i->id != 0 && i->devid != 0; i++)
 		add_special_device(IVHD_SPECIAL_IOAPIC, i->id, (u32 *)&i->devid, 0);
 
diff --git a/drivers/iommu/intel/dmar.c b/drivers/iommu/intel/dmar.c
index ec975c73c..0c9e79c3a 100644
--- a/drivers/iommu/intel/dmar.c
+++ b/drivers/iommu/intel/dmar.c
@@ -928,6 +928,15 @@ void __init detect_intel_iommu(void)
 	if (!ret)
 		ret = dmar_walk_dmar_table((struct acpi_table_dmar *)dmar_tbl,
 					   &validate_drhd_cb);
+	/*
+	 * 在以下使用iommu_detected:
+	 *   - arch/x86/kernel/pci-dma.c|38| <<global>> int iommu_detected __read_mostly = 0;
+	 *   - arch/x86/kernel/aperture_64.c|429| <<gart_iommu_hole_init>> iommu_detected = 1;
+	 *   - drivers/iommu/amd/init.c|3650| <<amd_iommu_detect>> if (no_iommu || (iommu_detected && !gart_iommu_aperture))
+	 *   - drivers/iommu/amd/init.c|3661| <<amd_iommu_detect>> iommu_detected = 1;
+	 *   - drivers/iommu/intel/dmar.c|931| <<detect_intel_iommu>> if (!ret && !no_iommu && !iommu_detected &&
+	 *   - drivers/iommu/intel/dmar.c|933| <<detect_intel_iommu>> iommu_detected = 1;
+	 */
 	if (!ret && !no_iommu && !iommu_detected &&
 	    (!dmar_disabled || dmar_platform_optin())) {
 		iommu_detected = 1;
diff --git a/drivers/vfio/pci/vfio_pci_intrs.c b/drivers/vfio/pci/vfio_pci_intrs.c
index 30d3e921c..a678449e2 100644
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@ -444,6 +444,12 @@ static int vfio_msi_alloc_irq(struct vfio_pci_core_device *vdev,
 	return map.index < 0 ? map.index : map.virq;
 }
 
+/*
+ * 在以下使用vfio_msi_set_vector_signal():
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|542| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|547| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|563| <<vfio_msi_disable>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+ */
 static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 				      unsigned int vector, int fd, bool msix)
 {
@@ -456,6 +462,11 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 	ctx = vfio_irq_ctx_get(vdev, vector);
 
 	if (ctx) {
+		/*
+		 * 在以下使用irq_bypass_unregister_producer():
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|459| <<vfio_msi_set_vector_signal>> irq_bypass_unregister_producer(&ctx->producer);
+		 *   - drivers/vhost/vdpa.c|226| <<vhost_vdpa_unsetup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+		 */
 		irq_bypass_unregister_producer(&ctx->producer);
 		irq = pci_irq_vector(pdev, vector);
 		cmd = vfio_pci_memory_lock_and_enable(vdev);
@@ -512,6 +523,11 @@ static int vfio_msi_set_vector_signal(struct vfio_pci_core_device *vdev,
 	if (ret)
 		goto out_put_eventfd_ctx;
 
+	/*
+	 * 在以下使用irq_bypass_register_producer():
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|515| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&ctx->producer, trigger, irq);
+	 *   - drivers/vhost/vdpa.c|215| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer, vq->call_ctx.ctx, irq);
+	 */
 	ret = irq_bypass_register_producer(&ctx->producer, trigger, irq);
 	if (unlikely(ret)) {
 		dev_info(&pdev->dev,
@@ -539,10 +555,22 @@ static int vfio_msi_set_block(struct vfio_pci_core_device *vdev, unsigned start,
 
 	for (i = 0, j = start; i < count && !ret; i++, j++) {
 		int fd = fds ? fds[i] : -1;
+		/*
+		 * 在以下使用vfio_msi_set_vector_signal():
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|542| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|547| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|563| <<vfio_msi_disable>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+		 */
 		ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
 	}
 
 	if (ret) {
+		/*
+		 * 在以下使用vfio_msi_set_vector_signal():
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|542| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|547| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|563| <<vfio_msi_disable>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+		 */
 		for (i = start; i < j; i++)
 			vfio_msi_set_vector_signal(vdev, i, -1, msix);
 	}
@@ -560,6 +588,12 @@ static void vfio_msi_disable(struct vfio_pci_core_device *vdev, bool msix)
 	xa_for_each(&vdev->ctx, i, ctx) {
 		vfio_virqfd_disable(&ctx->unmask);
 		vfio_virqfd_disable(&ctx->mask);
+		/*
+		 * 在以下使用vfio_msi_set_vector_signal():
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|542| <<vfio_msi_set_block>> ret = vfio_msi_set_vector_signal(vdev, j, fd, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|547| <<vfio_msi_set_block>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+		 *   - drivers/vfio/pci/vfio_pci_intrs.c|563| <<vfio_msi_disable>> vfio_msi_set_vector_signal(vdev, i, -1, msix);
+		 */
 		vfio_msi_set_vector_signal(vdev, i, -1, msix);
 	}
 
diff --git a/drivers/vfio/vfio_main.c b/drivers/vfio/vfio_main.c
index 38c8e9350..3e375a46c 100644
--- a/drivers/vfio/vfio_main.c
+++ b/drivers/vfio/vfio_main.c
@@ -869,6 +869,11 @@ static int vfio_ioct_mig_return_fd(struct file *filp, void __user *arg,
 		goto out_fput;
 	}
 
+	/*
+	 * 在以下使用vfio_device_feature_mig_state->data_fd:
+	 *   - drivers/vfio/vfio_main.c|872| <<vfio_ioct_mig_return_fd>> mig->data_fd = fd;
+	 *   - drivers/vfio/vfio_main.c|929| <<vfio_ioctl_device_feature_mig_device_state>> mig.data_fd = -1;
+	 */
 	mig->data_fd = fd;
 	if (copy_to_user(arg, mig, sizeof(*mig))) {
 		ret = -EFAULT;
@@ -926,6 +931,11 @@ vfio_ioctl_device_feature_mig_device_state(struct vfio_device *device,
 
 	return vfio_ioct_mig_return_fd(filp, arg, &mig);
 out_copy:
+	/*
+	 * 在以下使用vfio_device_feature_mig_state->data_fd:
+	 *   - drivers/vfio/vfio_main.c|872| <<vfio_ioct_mig_return_fd>> mig->data_fd = fd;
+	 *   - drivers/vfio/vfio_main.c|929| <<vfio_ioctl_device_feature_mig_device_state>> mig.data_fd = -1;
+	 */
 	mig.data_fd = -1;
 	if (copy_to_user(arg, &mig, sizeof(mig)))
 		return -EFAULT;
diff --git a/drivers/vhost/vdpa.c b/drivers/vhost/vdpa.c
index 05a481e4c..97eb90164 100644
--- a/drivers/vhost/vdpa.c
+++ b/drivers/vhost/vdpa.c
@@ -212,6 +212,11 @@ static void vhost_vdpa_setup_vq_irq(struct vhost_vdpa *v, u16 qid)
 	if (!vq->call_ctx.ctx)
 		return;
 
+	/*
+	 * 在以下使用irq_bypass_register_producer():
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|515| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&ctx->producer, trigger, irq);
+	 *   - drivers/vhost/vdpa.c|215| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer, vq->call_ctx.ctx, irq);
+	 */
 	ret = irq_bypass_register_producer(&vq->call_ctx.producer,
 					   vq->call_ctx.ctx, irq);
 	if (unlikely(ret))
@@ -223,6 +228,11 @@ static void vhost_vdpa_unsetup_vq_irq(struct vhost_vdpa *v, u16 qid)
 {
 	struct vhost_virtqueue *vq = &v->vqs[qid];
 
+	/*
+	 * 在以下使用irq_bypass_unregister_producer():
+	 *   - drivers/vfio/pci/vfio_pci_intrs.c|459| <<vfio_msi_set_vector_signal>> irq_bypass_unregister_producer(&ctx->producer);
+	 *   - drivers/vhost/vdpa.c|226| <<vhost_vdpa_unsetup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+	 */
 	irq_bypass_unregister_producer(&vq->call_ctx.producer);
 }
 
diff --git a/drivers/virtio/virtio_balloon.c b/drivers/virtio/virtio_balloon.c
index 1b93d8c64..be1e313f6 100644
--- a/drivers/virtio/virtio_balloon.c
+++ b/drivers/virtio/virtio_balloon.c
@@ -32,6 +32,17 @@
 
 #define VIRTIO_BALLOON_FREE_PAGE_ALLOC_FLAG (__GFP_NORETRY | __GFP_NOWARN | \
 					     __GFP_NOMEMALLOC)
+/*
+ * 在以下使用VIRTIO_BALLOON_HINT_BLOCK_ORDER:
+ *   - drivers/virtio/virtio_balloon.c|39| <<VIRTIO_BALLOON_HINT_BLOCK_BYTES>> (1 << (VIRTIO_BALLOON_HINT_BLOCK_ORDER + PAGE_SHIFT))
+ *   - drivers/virtio/virtio_balloon.c|40| <<VIRTIO_BALLOON_HINT_BLOCK_PAGES>> #define VIRTIO_BALLOON_HINT_BLOCK_PAGES
+ *                                                                   (1 << VIRTIO_BALLOON_HINT_BLOCK_ORDER)
+ *   - drivers/virtio/virtio_balloon.c|491| <<return_free_pages_to_mm>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+ *   - drivers/virtio/virtio_balloon.c|707| <<get_free_page_and_send>> page = alloc_pages(VIRTIO_BALLOON_FREE_PAGE_ALLOC_FLAG,
+ *                                                                   VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+ *   - drivers/virtio/virtio_balloon.c|721| <<get_free_page_and_send>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+ *   - drivers/virtio/virtio_balloon.c|734| <<get_free_page_and_send>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+ */
 /* The order of free page blocks to report to host */
 #define VIRTIO_BALLOON_HINT_BLOCK_ORDER MAX_PAGE_ORDER
 /* The size of a free page block in bytes */
@@ -58,22 +69,53 @@ struct virtio_balloon {
 
 	/* Balloon's own wq for cpu-intensive work items */
 	struct workqueue_struct *balloon_wq;
+	/*
+	 * 在以下使用virtio_balloon->report_free_page_work():
+	 *   - drivers/virtio/virtio_balloon.c|509| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+	 *   - drivers/virtio/virtio_balloon.c|991| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+	 *   - drivers/virtio/virtio_balloon.c|1136| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+	 */
 	/* The free page reporting work item submitted to the balloon wq */
 	struct work_struct report_free_page_work;
 
 	/* The balloon servicing is delegated to a freezable workqueue. */
 	struct work_struct update_balloon_stats_work;
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_size_work:
+	 *   - drivers/virtio/virtio_balloon.c|515| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+	 *   - drivers/virtio/virtio_balloon.c|961| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+	 *   - drivers/virtio/virtio_balloon.c|1132| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+	 */
 	struct work_struct update_balloon_size_work;
 
 	/* Prevent updating balloon when it is being canceled. */
 	spinlock_t stop_update_lock;
 	bool stop_update;
+	/*
+	 * 在以下使用virtio_balloon->config_read_bitmap:
+	 *   - drivers/virtio/virtio_balloon.c|633| <<virtio_balloon_queue_free_page_work>>
+	 *       if (test_and_set_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID, &vb->config_read_bitmap))
+	 *   - drivers/virtio/virtio_balloon.c|835| <<virtio_balloon_cmd_id_received>>
+	 *       if (test_and_clear_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID, &vb->config_read_bitmap)) {
+	 */
 	/* Bitmap to indicate if reading the related config fields are needed */
 	unsigned long config_read_bitmap;
 
 	/* The list of allocated free pages, waiting to be given back to mm */
+	/*
+	 * 在以下使用virtio_balloon->free_page_list:
+	 *   - drivers/virtio/virtio_balloon.c|546| <<return_free_pages_to_mm>> page = balloon_page_pop(&vb->free_page_list);
+	 *   - drivers/virtio/virtio_balloon.c|848| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+	 *   - drivers/virtio/virtio_balloon.c|1152| <<virtballoon_probe>> INIT_LIST_HEAD(&vb->free_page_list);
+	 */
 	struct list_head free_page_list;
 	spinlock_t free_page_list_lock;
+	/*
+	 * 在以下使用virtio_balloon->num_free_page_blocks:
+	 *   - drivers/virtio/virtio_balloon.c|616| <<return_free_pages_to_mm>> vb->num_free_page_blocks -= num_returned;
+	 *   - drivers/virtio/virtio_balloon.c|935| <<get_free_page_and_send>> vb->num_free_page_blocks++;
+	 *   - drivers/virtio/virtio_balloon.c|1152| <<virtio_balloon_shrinker_count>> return vb->num_free_page_blocks * VIRTIO_BALLOON_HINT_BLOCK_PAGES;
+	 */
 	/* The number of free page blocks on the above list */
 	unsigned long num_free_page_blocks;
 	/*
@@ -116,8 +158,21 @@ struct virtio_balloon {
 	/* OOM notifier to deflate on OOM - VIRTIO_BALLOON_F_DEFLATE_ON_OOM */
 	struct notifier_block oom_nb;
 
+	/*
+	 * 在以下使用virtio_balloon->reporting_vq:
+	 *   - drivers/virtio/virtio_balloon.c|227| <<virtballoon_free_page_report>> struct virtqueue *vq = vb->reporting_vq;
+	 *   - drivers/virtio/virtio_balloon.c|718| <<init_vqs>> vb->reporting_vq = vqs[VIRTIO_BALLOON_VQ_REPORTING];
+	 *   - drivers/virtio/virtio_balloon.c|1162| <<virtballoon_probe>> capacity = virtqueue_get_vring_size(vb->reporting_vq);
+	 */
 	/* Free page reporting device */
 	struct virtqueue *reporting_vq;
+	/*
+	 * 在以下使用virtio_balloon->pr_dev_info:
+	 *   - drivers/virtio/virtio_balloon.c|1158| <<virtballoon_probe>> vb->pr_dev_info.report = virtballoon_free_page_report;
+	 *   - drivers/virtio/virtio_balloon.c|1182| <<virtballoon_probe>> vb->pr_dev_info.order = 5;
+	 *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+	 *   - drivers/virtio/virtio_balloon.c|1246| <<virtballoon_remove>> page_reporting_unregister(&vb->pr_dev_info);
+	 */
 	struct page_reporting_dev_info pr_dev_info;
 
 	/* State for keeping the wakeup_source active while adjusting the balloon */
@@ -173,6 +228,12 @@ static void finish_wakeup_event(struct virtio_balloon *vb)
 	spin_unlock_irq(&vb->wakeup_lock);
 }
 
+/*
+ * 在以下使用balloon_ack():
+ *   - drivers/virtio/virtio_balloon.c|690| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_INFLATE].callback = balloon_ack;
+ *   - drivers/virtio/virtio_balloon.c|692| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_DEFLATE].callback = balloon_ack;
+ *   - drivers/virtio/virtio_balloon.c|705| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_REPORTING].callback = balloon_ack;
+ */
 static void balloon_ack(struct virtqueue *vq)
 {
 	struct virtio_balloon *vb = vq->vdev->priv;
@@ -180,6 +241,13 @@ static void balloon_ack(struct virtqueue *vq)
 	wake_up(&vb->acked);
 }
 
+/*
+ * 在以下使用tell_host():
+ *   - drivers/virtio/virtio_balloon.c|348| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+ *   - drivers/virtio/virtio_balloon.c|404| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+ *   - drivers/virtio/virtio_balloon.c|1075| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+ *   - drivers/virtio/virtio_balloon.c|1080| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+ */
 static void tell_host(struct virtio_balloon *vb, struct virtqueue *vq)
 {
 	struct scatterlist sg;
@@ -196,11 +264,21 @@ static void tell_host(struct virtio_balloon *vb, struct virtqueue *vq)
 
 }
 
+/*
+ * 在以下使用virtballoon_free_page_report():
+ *   - drivers/virtio/virtio_balloon.c|1158| <<virtballoon_probe>> vb->pr_dev_info.report = virtballoon_free_page_report;
+ */
 static int virtballoon_free_page_report(struct page_reporting_dev_info *pr_dev_info,
 				   struct scatterlist *sg, unsigned int nents)
 {
 	struct virtio_balloon *vb =
 		container_of(pr_dev_info, struct virtio_balloon, pr_dev_info);
+	/*
+	 * 在以下使用virtio_balloon->reporting_vq:
+	 *   - drivers/virtio/virtio_balloon.c|227| <<virtballoon_free_page_report>> struct virtqueue *vq = vb->reporting_vq;
+	 *   - drivers/virtio/virtio_balloon.c|718| <<init_vqs>> vb->reporting_vq = vqs[VIRTIO_BALLOON_VQ_REPORTING];
+	 *   - drivers/virtio/virtio_balloon.c|1162| <<virtballoon_probe>> capacity = virtqueue_get_vring_size(vb->reporting_vq);
+	 */
 	struct virtqueue *vq = vb->reporting_vq;
 	unsigned int unused, err;
 
@@ -239,6 +317,10 @@ static void set_page_pfns(struct virtio_balloon *vb,
 					  page_to_balloon_pfn(page) + i);
 }
 
+/*
+ * 在以下使用fill_balloon():
+ *   - drivers/virtio/virtio_balloon.c|697| <<update_balloon_size_func>> diff -= fill_balloon(vb, diff);
+ */
 static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 {
 	unsigned int num_allocated_pages;
@@ -281,6 +363,13 @@ static unsigned int fill_balloon(struct virtio_balloon *vb, size_t num)
 	}
 
 	num_allocated_pages = vb->num_pfns;
+	/*
+	 *  在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|348| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|404| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1075| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1080| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	/* Did we get any? */
 	if (vb->num_pfns != 0)
 		tell_host(vb, vb->inflate_vq);
@@ -303,6 +392,12 @@ static void release_pages_balloon(struct virtio_balloon *vb,
 	}
 }
 
+/*
+ * 在以下使用leak_balloon():
+ *   - drivers/virtio/virtio_balloon.c|568| <<update_balloon_size_func>> diff += leak_balloon(vb, -diff);
+ *   - drivers/virtio/virtio_balloon.c|916| <<virtio_balloon_oom_notify>> *freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
+ *   - drivers/virtio/virtio_balloon.c|1106| <<remove_common>> leak_balloon(vb, vb->num_pages);
+ */
 static unsigned int leak_balloon(struct virtio_balloon *vb, size_t num)
 {
 	unsigned int num_freed_pages;
@@ -327,6 +422,13 @@ static unsigned int leak_balloon(struct virtio_balloon *vb, size_t num)
 	}
 
 	num_freed_pages = vb->num_pfns;
+	/*
+	 *  在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|348| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|404| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1075| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1080| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	/*
 	 * Note that if
 	 * virtio_has_feature(vdev, VIRTIO_BALLOON_F_MUST_TELL_HOST);
@@ -398,6 +500,11 @@ static inline unsigned int update_balloon_vm_stats(struct virtio_balloon *vb)
 }
 #endif /* CONFIG_VM_EVENT_COUNTERS */
 
+/*
+ * 在以下使用update_balloon_stats():
+ *   - drivers/virtio/virtio_balloon.c|520| <<stats_handle_request>> num_stats = update_balloon_stats(vb);
+ *   - drivers/virtio/virtio_balloon.c|764| <<init_vqs>> num_stats = update_balloon_stats(vb);
+ */
 static unsigned int update_balloon_stats(struct virtio_balloon *vb)
 {
 	struct sysinfo i;
@@ -459,6 +566,12 @@ static void stats_handle_request(struct virtio_balloon *vb)
 	virtqueue_kick(vq);
 }
 
+/*
+ * 在以下使用towards_target():
+ *   - drivers/virtio/virtio_balloon.c|663| <<update_balloon_size_func>> diff = towards_target(vb);
+ *   - drivers/virtio/virtio_balloon.c|1262| <<virtballoon_probe>> if (towards_target(vb))
+ *   - drivers/virtio/virtio_balloon.c|1368| <<virtballoon_restore>> if (towards_target(vb))
+ */
 static inline s64 towards_target(struct virtio_balloon *vb)
 {
 	s64 target;
@@ -476,6 +589,12 @@ static inline s64 towards_target(struct virtio_balloon *vb)
 	return target - vb->num_pages;
 }
 
+/*
+ * 在以下使用return_free_pages_to_mm():
+ *   - drivers/virtio/virtio_balloon.c|798| <<report_free_page_func>> return_free_pages_to_mm(vb, ULONG_MAX);
+ *   - drivers/virtio/virtio_balloon.c|888| <<shrink_free_pages>> blocks_freed = return_free_pages_to_mm(vb, blocks_to_free);
+ *   - drivers/virtio/virtio_balloon.c|1111| <<remove_common>> return_free_pages_to_mm(vb, ULONG_MAX);
+ */
 /* Gives back @num_to_return blocks of free pages to mm. */
 static unsigned long return_free_pages_to_mm(struct virtio_balloon *vb,
 					     unsigned long num_to_return)
@@ -485,33 +604,83 @@ static unsigned long return_free_pages_to_mm(struct virtio_balloon *vb,
 
 	spin_lock_irq(&vb->free_page_list_lock);
 	for (num_returned = 0; num_returned < num_to_return; num_returned++) {
+		/*
+		 * 在以下使用virtio_balloon->free_page_list:
+		 *   - drivers/virtio/virtio_balloon.c|546| <<return_free_pages_to_mm>> page = balloon_page_pop(&vb->free_page_list);
+		 *   - drivers/virtio/virtio_balloon.c|848| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+		 *   - drivers/virtio/virtio_balloon.c|1152| <<virtballoon_probe>> INIT_LIST_HEAD(&vb->free_page_list);
+		 */
 		page = balloon_page_pop(&vb->free_page_list);
 		if (!page)
 			break;
+		/*
+		 * 在以下使用VIRTIO_BALLOON_HINT_BLOCK_ORDER:
+		 *   - drivers/virtio/virtio_balloon.c|39| <<VIRTIO_BALLOON_HINT_BLOCK_BYTES>> (1 << (VIRTIO_BALLOON_HINT_BLOCK_ORDER + PAGE_SHIFT))
+		 *   - drivers/virtio/virtio_balloon.c|40| <<VIRTIO_BALLOON_HINT_BLOCK_PAGES>> #define VIRTIO_BALLOON_HINT_BLOCK_PAGES
+		 *                                                                   (1 << VIRTIO_BALLOON_HINT_BLOCK_ORDER)
+		 *   - drivers/virtio/virtio_balloon.c|491| <<return_free_pages_to_mm>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+		 *   - drivers/virtio/virtio_balloon.c|707| <<get_free_page_and_send>> page = alloc_pages(VIRTIO_BALLOON_FREE_PAGE_ALLOC_FLAG,
+		 *                                                                   VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+		 *   - drivers/virtio/virtio_balloon.c|721| <<get_free_page_and_send>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+		 *   - drivers/virtio/virtio_balloon.c|734| <<get_free_page_and_send>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+		 */
 		__free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
 	}
+	/*
+	 * 在以下使用virtio_balloon->num_free_page_blocks:
+	 *   - drivers/virtio/virtio_balloon.c|616| <<return_free_pages_to_mm>> vb->num_free_page_blocks -= num_returned;
+	 *   - drivers/virtio/virtio_balloon.c|935| <<get_free_page_and_send>> vb->num_free_page_blocks++;
+	 *   - drivers/virtio/virtio_balloon.c|1152| <<virtio_balloon_shrinker_count>> return vb->num_free_page_blocks * VIRTIO_BALLOON_HINT_BLOCK_PAGES;
+	 */
 	vb->num_free_page_blocks -= num_returned;
 	spin_unlock_irq(&vb->free_page_list_lock);
 
 	return num_returned;
 }
 
+/*
+ * 在以下使用virtio_balloon_queue_free_page_work():
+ *   - drivers/virtio/virtio_balloon.c|526| <<virtballoon_changed>> virtio_balloon_queue_free_page_work(vb);
+ */
 static void virtio_balloon_queue_free_page_work(struct virtio_balloon *vb)
 {
 	if (!virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT))
 		return;
 
+	/*
+	 * 在以下使用virtio_balloon->config_read_bitmap:
+	 *   - drivers/virtio/virtio_balloon.c|633| <<virtio_balloon_queue_free_page_work>>
+	 *       if (test_and_set_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID, &vb->config_read_bitmap))
+	 *   - drivers/virtio/virtio_balloon.c|835| <<virtio_balloon_cmd_id_received>>
+	 *       if (test_and_clear_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID, &vb->config_read_bitmap)) {
+	 */
 	/* No need to queue the work if the bit was already set. */
 	if (test_and_set_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID,
 			     &vb->config_read_bitmap))
 		return;
 
+	/*
+	 * 在以下使用virtio_balloon->report_free_page_work():
+	 *   - drivers/virtio/virtio_balloon.c|509| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+	 *   - drivers/virtio/virtio_balloon.c|991| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+	 *   - drivers/virtio/virtio_balloon.c|1136| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+	 */
 	queue_work(vb->balloon_wq, &vb->report_free_page_work);
 }
 
+/*
+ * 在以下使用start_update_balloon_size():
+ *   - drivers/virtio/virtio_balloon.c|525| <<virtballoon_changed>> start_update_balloon_size(vb);
+ */
 static void start_update_balloon_size(struct virtio_balloon *vb)
 {
 	start_wakeup_event(vb, VIRTIO_BALLOON_WAKEUP_SIGNAL_ADJUST);
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_size_work:
+	 *   - drivers/virtio/virtio_balloon.c|515| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+	 *   - drivers/virtio/virtio_balloon.c|961| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+	 *   - drivers/virtio/virtio_balloon.c|1132| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+	 */
 	queue_work(system_freezable_wq, &vb->update_balloon_size_work);
 }
 
@@ -523,11 +692,21 @@ static void virtballoon_changed(struct virtio_device *vdev)
 	spin_lock_irqsave(&vb->stop_update_lock, flags);
 	if (!vb->stop_update) {
 		start_update_balloon_size(vb);
+		/*
+		 * 只在这里调用
+		 */
 		virtio_balloon_queue_free_page_work(vb);
 	}
 	spin_unlock_irqrestore(&vb->stop_update_lock, flags);
 }
 
+/*
+ * 在以下使用update_balloon_size():
+ *   - drivers/virtio/virtio_balloon.c|709| <<update_balloon_size_func>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1115| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1365| <<remove_common>> update_balloon_size(vb);
+ *   - drivers/virtio/virtio_balloon.c|1453| <<virtballoon_restore>> update_balloon_size(vb);
+ */
 static void update_balloon_size(struct virtio_balloon *vb)
 {
 	u32 actual = vb->num_pages;
@@ -549,6 +728,15 @@ static void update_balloon_stats_func(struct work_struct *work)
 	finish_wakeup_event(vb);
 }
 
+/*
+ * 在以下使用virtio_balloon->update_balloon_size_work:
+ *   - drivers/virtio/virtio_balloon.c|515| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+ *   - drivers/virtio/virtio_balloon.c|961| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+ *   - drivers/virtio/virtio_balloon.c|1132| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+ *
+ * 在以下使用update_balloon_size_func():
+ *   - drivers/virtio/virtio_balloon.c|961| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+ */
 static void update_balloon_size_func(struct work_struct *work)
 {
 	struct virtio_balloon *vb;
@@ -559,6 +747,12 @@ static void update_balloon_size_func(struct work_struct *work)
 
 	process_wakeup_event(vb, VIRTIO_BALLOON_WAKEUP_SIGNAL_ADJUST);
 
+	/*
+	 * 在以下使用towards_target():
+	 *   - drivers/virtio/virtio_balloon.c|663| <<update_balloon_size_func>> diff = towards_target(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1262| <<virtballoon_probe>> if (towards_target(vb))
+	 *   - drivers/virtio/virtio_balloon.c|1368| <<virtballoon_restore>> if (towards_target(vb))
+	 */
 	diff = towards_target(vb);
 
 	if (diff) {
@@ -566,6 +760,13 @@ static void update_balloon_size_func(struct work_struct *work)
 			diff -= fill_balloon(vb, diff);
 		else
 			diff += leak_balloon(vb, -diff);
+		/*
+		 * 在以下使用update_balloon_size():
+		 *   - drivers/virtio/virtio_balloon.c|709| <<update_balloon_size_func>> update_balloon_size(vb);
+		 *   - drivers/virtio/virtio_balloon.c|1115| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+		 *   - drivers/virtio/virtio_balloon.c|1365| <<remove_common>> update_balloon_size(vb);
+		 *   - drivers/virtio/virtio_balloon.c|1453| <<virtballoon_restore>> update_balloon_size(vb);
+		 */
 		update_balloon_size(vb);
 	}
 
@@ -581,6 +782,12 @@ static int init_vqs(struct virtio_balloon *vb)
 	struct virtqueue *vqs[VIRTIO_BALLOON_VQ_MAX];
 	int err;
 
+	/*
+	 * 在以下使用balloon_ack():
+	 *   - drivers/virtio/virtio_balloon.c|690| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_INFLATE].callback = balloon_ack;
+	 *   - drivers/virtio/virtio_balloon.c|692| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_DEFLATE].callback = balloon_ack;
+	 *   - drivers/virtio/virtio_balloon.c|705| <<init_vqs>> vqs_info[VIRTIO_BALLOON_VQ_REPORTING].callback = balloon_ack;
+	 */
 	/*
 	 * Inflateq and deflateq are used unconditionally. The names[]
 	 * will be NULL if the related feature is not enabled, which will
@@ -636,14 +843,33 @@ static int init_vqs(struct virtio_balloon *vb)
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT))
 		vb->free_page_vq = vqs[VIRTIO_BALLOON_VQ_FREE_PAGE];
 
+	/*
+	 * 在以下使用virtio_balloon->reporting_vq:
+	 *   - drivers/virtio/virtio_balloon.c|227| <<virtballoon_free_page_report>> struct virtqueue *vq = vb->reporting_vq;
+	 *   - drivers/virtio/virtio_balloon.c|718| <<init_vqs>> vb->reporting_vq = vqs[VIRTIO_BALLOON_VQ_REPORTING];
+	 *   - drivers/virtio/virtio_balloon.c|1162| <<virtballoon_probe>> capacity = virtqueue_get_vring_size(vb->reporting_vq);
+	 */
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_REPORTING))
 		vb->reporting_vq = vqs[VIRTIO_BALLOON_VQ_REPORTING];
 
 	return 0;
 }
 
+/*
+ * 在以下使用virtio_balloon_cmd_id_received():
+ *   - drivers/virtio/virtio_balloon.c|856| <<send_cmd_id_start>> virtio_balloon_cmd_id_received(vb));
+ *   - drivers/virtio/virtio_balloon.c|964| <<send_free_pages>> virtio_balloon_cmd_id_received(vb)))
+ *   - drivers/virtio/virtio_balloon.c|1023| <<report_free_page_func>> cmd_id_received = virtio_balloon_cmd_id_received(vb);
+ */
 static u32 virtio_balloon_cmd_id_received(struct virtio_balloon *vb)
 {
+	/*
+	 * 在以下使用virtio_balloon->config_read_bitmap:
+	 *   - drivers/virtio/virtio_balloon.c|633| <<virtio_balloon_queue_free_page_work>>
+	 *       if (test_and_set_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID, &vb->config_read_bitmap))
+	 *   - drivers/virtio/virtio_balloon.c|835| <<virtio_balloon_cmd_id_received>>
+	 *       if (test_and_clear_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID, &vb->config_read_bitmap)) {
+	 */
 	if (test_and_clear_bit(VIRTIO_BALLOON_CONFIG_READ_CMD_ID,
 			       &vb->config_read_bitmap)) {
 		/* Legacy balloon config space is LE, unlike all other devices. */
@@ -655,6 +881,10 @@ static u32 virtio_balloon_cmd_id_received(struct virtio_balloon *vb)
 	return vb->cmd_id_received_cache;
 }
 
+/*
+ * 在以下使用send_cmd_id_start():
+ *   - drivers/virtio/virtio_balloon.c|994| <<virtio_balloon_report_free_page>> err = send_cmd_id_start(vb);
+ */
 static int send_cmd_id_start(struct virtio_balloon *vb)
 {
 	struct scatterlist sg;
@@ -691,6 +921,10 @@ static int send_cmd_id_stop(struct virtio_balloon *vb)
 	return err;
 }
 
+/*
+ * 在以下使用get_free_page_and_send():
+ *   - drivers/virtio/virtio_balloon.c|759| <<send_free_pages>> err = get_free_page_and_send(vb);
+ */
 static int get_free_page_and_send(struct virtio_balloon *vb)
 {
 	struct virtqueue *vq = vb->free_page_vq;
@@ -703,6 +937,17 @@ static int get_free_page_and_send(struct virtio_balloon *vb)
 	while (virtqueue_get_buf(vq, &unused))
 		;
 
+	/*
+	 * 在以下使用VIRTIO_BALLOON_HINT_BLOCK_ORDER:
+	 *   - drivers/virtio/virtio_balloon.c|39| <<VIRTIO_BALLOON_HINT_BLOCK_BYTES>> (1 << (VIRTIO_BALLOON_HINT_BLOCK_ORDER + PAGE_SHIFT))
+	 *   - drivers/virtio/virtio_balloon.c|40| <<VIRTIO_BALLOON_HINT_BLOCK_PAGES>> #define VIRTIO_BALLOON_HINT_BLOCK_PAGES
+	 *                                                                   (1 << VIRTIO_BALLOON_HINT_BLOCK_ORDER)
+	 *   - drivers/virtio/virtio_balloon.c|491| <<return_free_pages_to_mm>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+	 *   - drivers/virtio/virtio_balloon.c|707| <<get_free_page_and_send>> page = alloc_pages(VIRTIO_BALLOON_FREE_PAGE_ALLOC_FLAG,
+	 *                                                                   VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+	 *   - drivers/virtio/virtio_balloon.c|721| <<get_free_page_and_send>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+	 *   - drivers/virtio/virtio_balloon.c|734| <<get_free_page_and_send>> __free_pages(page, VIRTIO_BALLOON_HINT_BLOCK_ORDER);
+	 */
 	page = alloc_pages(VIRTIO_BALLOON_FREE_PAGE_ALLOC_FLAG,
 			   VIRTIO_BALLOON_HINT_BLOCK_ORDER);
 	/*
@@ -722,8 +967,20 @@ static int get_free_page_and_send(struct virtio_balloon *vb)
 			return err;
 		}
 		virtqueue_kick(vq);
+		/*
+		 * 在以下使用virtio_balloon->free_page_list:
+		 *   - drivers/virtio/virtio_balloon.c|546| <<return_free_pages_to_mm>> page = balloon_page_pop(&vb->free_page_list);
+		 *   - drivers/virtio/virtio_balloon.c|848| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+		 *   - drivers/virtio/virtio_balloon.c|1152| <<virtballoon_probe>> INIT_LIST_HEAD(&vb->free_page_list);
+		 */
 		spin_lock_irq(&vb->free_page_list_lock);
 		balloon_page_push(&vb->free_page_list, page);
+		/*
+		 * 在以下使用virtio_balloon->num_free_page_blocks:
+		 *   - drivers/virtio/virtio_balloon.c|616| <<return_free_pages_to_mm>> vb->num_free_page_blocks -= num_returned;
+		 *   - drivers/virtio/virtio_balloon.c|935| <<get_free_page_and_send>> vb->num_free_page_blocks++;
+		 *   - drivers/virtio/virtio_balloon.c|1152| <<virtio_balloon_shrinker_count>> return vb->num_free_page_blocks * VIRTIO_BALLOON_HINT_BLOCK_PAGES;
+		 */
 		vb->num_free_page_blocks++;
 		spin_unlock_irq(&vb->free_page_list_lock);
 	} else {
@@ -737,6 +994,10 @@ static int get_free_page_and_send(struct virtio_balloon *vb)
 	return 0;
 }
 
+/*
+ * 在以下使用send_free_pages():
+ *   - drivers/virtio/virtio_balloon.c|779| <<virtio_balloon_report_free_page>> err = send_free_pages(vb);
+ */
 static int send_free_pages(struct virtio_balloon *vb)
 {
 	int err;
@@ -756,6 +1017,9 @@ static int send_free_pages(struct virtio_balloon *vb)
 		 * The free page blocks are allocated and sent to host one by
 		 * one.
 		 */
+		/*
+		 * 只在以下调用
+		 */
 		err = get_free_page_and_send(vb);
 		if (err == -EINTR)
 			break;
@@ -766,6 +1030,10 @@ static int send_free_pages(struct virtio_balloon *vb)
 	return 0;
 }
 
+/*
+ * 在以下使用virtio_balloon_report_free_page():
+ *   - drivers/virtio/virtio_balloon.c|802| <<report_free_page_func>> virtio_balloon_report_free_page(vb);
+ */
 static void virtio_balloon_report_free_page(struct virtio_balloon *vb)
 {
 	int err;
@@ -786,6 +1054,15 @@ static void virtio_balloon_report_free_page(struct virtio_balloon *vb)
 		dev_err(dev, "Failed to send a stop id, err = %d\n", err);
 }
 
+/*
+ * 在以下使用virtio_balloon->report_free_page_work():
+ *   - drivers/virtio/virtio_balloon.c|509| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+ *   - drivers/virtio/virtio_balloon.c|991| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ *   - drivers/virtio/virtio_balloon.c|1136| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+ *
+ * 在以下使用report_free_page_func():
+ *   - drivers/virtio/virtio_balloon.c|991| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+ */
 static void report_free_page_func(struct work_struct *work)
 {
 	struct virtio_balloon *vb = container_of(work, struct virtio_balloon,
@@ -822,6 +1099,10 @@ static void report_free_page_func(struct work_struct *work)
  * This function preforms the balloon page migration task.
  * Called through movable_operations->migrate_page
  */
+/*
+ * 在以下使用virtballoon_migratepage():
+ *   - drivers/virtio/virtio_balloon.c|1174| <<virtballoon_probe>> vb->vb_dev_info.migratepage = virtballoon_migratepage;
+ */
 static int virtballoon_migratepage(struct balloon_dev_info *vb_dev_info,
 		struct page *newpage, struct page *page, enum migrate_mode mode)
 {
@@ -861,11 +1142,25 @@ static int virtballoon_migratepage(struct balloon_dev_info *vb_dev_info,
 	spin_unlock_irqrestore(&vb_dev_info->pages_lock, flags);
 	vb->num_pfns = VIRTIO_BALLOON_PAGES_PER_PAGE;
 	set_page_pfns(vb, vb->pfns, newpage);
+	/*
+	 * 在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|348| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|404| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1075| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1080| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	tell_host(vb, vb->inflate_vq);
 
 	/* balloon's page migration 2nd step -- deflate "page" */
 	vb->num_pfns = VIRTIO_BALLOON_PAGES_PER_PAGE;
 	set_page_pfns(vb, vb->pfns, page);
+	/*
+	 * 在以下使用tell_host():
+	 *   - drivers/virtio/virtio_balloon.c|348| <<fill_balloon>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|404| <<leak_balloon>> tell_host(vb, vb->deflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1075| <<virtballoon_migratepage>> tell_host(vb, vb->inflate_vq);
+	 *   - drivers/virtio/virtio_balloon.c|1080| <<virtballoon_migratepage>> tell_host(vb, vb->deflate_vq);
+	 */
 	tell_host(vb, vb->deflate_vq);
 
 	mutex_unlock(&vb->balloon_lock);
@@ -903,6 +1198,12 @@ static unsigned long virtio_balloon_shrinker_count(struct shrinker *shrinker,
 {
 	struct virtio_balloon *vb = shrinker->private_data;
 
+	/*
+	 * 在以下使用virtio_balloon->num_free_page_blocks:
+	 *   - drivers/virtio/virtio_balloon.c|616| <<return_free_pages_to_mm>> vb->num_free_page_blocks -= num_returned;
+	 *   - drivers/virtio/virtio_balloon.c|935| <<get_free_page_and_send>> vb->num_free_page_blocks++;
+	 *   - drivers/virtio/virtio_balloon.c|1152| <<virtio_balloon_shrinker_count>> return vb->num_free_page_blocks * VIRTIO_BALLOON_HINT_BLOCK_PAGES;
+	 */
 	return vb->num_free_page_blocks * VIRTIO_BALLOON_HINT_BLOCK_PAGES;
 }
 
@@ -915,6 +1216,13 @@ static int virtio_balloon_oom_notify(struct notifier_block *nb,
 
 	*freed += leak_balloon(vb, VIRTIO_BALLOON_OOM_NR_PAGES) /
 		  VIRTIO_BALLOON_PAGES_PER_PAGE;
+	/*
+	 * 在以下使用update_balloon_size():
+	 *   - drivers/virtio/virtio_balloon.c|709| <<update_balloon_size_func>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1115| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1365| <<remove_common>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1453| <<virtballoon_restore>> update_balloon_size(vb);
+	 */
 	update_balloon_size(vb);
 
 	return NOTIFY_OK;
@@ -925,6 +1233,10 @@ static void virtio_balloon_unregister_shrinker(struct virtio_balloon *vb)
 	shrinker_free(vb->shrinker);
 }
 
+/*
+ * 在以下使用virtio_balloon_register_shrinker():
+ *   - drivers/virtio/virtio_balloon.c|1157| <<virtballoon_probe>> err = virtio_balloon_register_shrinker(vb);
+ */
 static int virtio_balloon_register_shrinker(struct virtio_balloon *vb)
 {
 	vb->shrinker = shrinker_alloc(0, "virtio-balloon");
@@ -958,12 +1270,24 @@ static int virtballoon_probe(struct virtio_device *vdev)
 	}
 
 	INIT_WORK(&vb->update_balloon_stats_work, update_balloon_stats_func);
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_size_work:
+	 *   - drivers/virtio/virtio_balloon.c|515| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+	 *   - drivers/virtio/virtio_balloon.c|961| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+	 *   - drivers/virtio/virtio_balloon.c|1132| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+	 */
 	INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
 	spin_lock_init(&vb->stop_update_lock);
 	mutex_init(&vb->balloon_lock);
 	init_waitqueue_head(&vb->acked);
 	vb->vdev = vdev;
 
+	/*
+	 * 在以下使用balloon_devinfo_init():
+	 *   - arch/powerpc/platforms/pseries/cmm.c|553| <<cmm_balloon_compaction_init>> balloon_devinfo_init(&b_dev_info);
+	 *   - drivers/misc/vmw_balloon.c|1878| <<vmballoon_init>> balloon_devinfo_init(&balloon.b_dev_info);
+	 *   - drivers/virtio/virtio_balloon.c|1115| <<virtballoon_probe>> balloon_devinfo_init(&vb->vb_dev_info);
+	 */
 	balloon_devinfo_init(&vb->vb_dev_info);
 
 	err = init_vqs(vb);
@@ -988,6 +1312,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 			err = -ENOMEM;
 			goto out_del_vqs;
 		}
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work():
+		 *   - drivers/virtio/virtio_balloon.c|509| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|991| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1136| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 */
 		INIT_WORK(&vb->report_free_page_work, report_free_page_func);
 		vb->cmd_id_received_cache = VIRTIO_BALLOON_CMD_ID_STOP;
 		vb->cmd_id_active = cpu_to_virtio32(vb->vdev,
@@ -995,6 +1325,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 		vb->cmd_id_stop = cpu_to_virtio32(vb->vdev,
 						  VIRTIO_BALLOON_CMD_ID_STOP);
 		spin_lock_init(&vb->free_page_list_lock);
+		/*
+		 * 在以下使用virtio_balloon->free_page_list:
+		 *   - drivers/virtio/virtio_balloon.c|546| <<return_free_pages_to_mm>> page = balloon_page_pop(&vb->free_page_list);
+		 *   - drivers/virtio/virtio_balloon.c|848| <<get_free_page_and_send>> balloon_page_push(&vb->free_page_list, page);
+		 *   - drivers/virtio/virtio_balloon.c|1152| <<virtballoon_probe>> INIT_LIST_HEAD(&vb->free_page_list);
+		 */
 		INIT_LIST_HEAD(&vb->free_page_list);
 		/*
 		 * We're allowed to reuse any free pages, even if they are
@@ -1033,10 +1369,26 @@ static int virtballoon_probe(struct virtio_device *vdev)
 				 poison_val, &poison_val);
 	}
 
+	/*
+	 * 在以下使用virtio_balloon->pr_dev_info:
+	 *   - drivers/virtio/virtio_balloon.c|1158| <<virtballoon_probe>> vb->pr_dev_info.report = virtballoon_free_page_report;
+	 *   - drivers/virtio/virtio_balloon.c|1182| <<virtballoon_probe>> vb->pr_dev_info.order = 5;
+	 *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+	 *   - drivers/virtio/virtio_balloon.c|1246| <<virtballoon_remove>> page_reporting_unregister(&vb->pr_dev_info);
+	 *
+	 * struct virtio_balloon *vb;
+	 * -> struct page_reporting_dev_info pr_dev_info;
+	 */
 	vb->pr_dev_info.report = virtballoon_free_page_report;
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_REPORTING)) {
 		unsigned int capacity;
 
+		/*
+		 * 在以下使用virtio_balloon->reporting_vq:
+		 *   - drivers/virtio/virtio_balloon.c|227| <<virtballoon_free_page_report>> struct virtqueue *vq = vb->reporting_vq;
+		 *   - drivers/virtio/virtio_balloon.c|718| <<init_vqs>> vb->reporting_vq = vqs[VIRTIO_BALLOON_VQ_REPORTING];
+		 *   - drivers/virtio/virtio_balloon.c|1162| <<virtballoon_probe>> capacity = virtqueue_get_vring_size(vb->reporting_vq);
+		 */
 		capacity = virtqueue_get_vring_size(vb->reporting_vq);
 		if (capacity < PAGE_REPORTING_CAPACITY) {
 			err = -ENOSPC;
@@ -1060,6 +1412,17 @@ static int virtballoon_probe(struct virtio_device *vdev)
 		vb->pr_dev_info.order = 5;
 #endif
 
+		/*
+		 * 在以下使用virtio_balloon->pr_dev_info:
+		 *   - drivers/virtio/virtio_balloon.c|1158| <<virtballoon_probe>> vb->pr_dev_info.report = virtballoon_free_page_report;
+		 *   - drivers/virtio/virtio_balloon.c|1182| <<virtballoon_probe>> vb->pr_dev_info.order = 5;
+		 *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+		 *   - drivers/virtio/virtio_balloon.c|1246| <<virtballoon_remove>> page_reporting_unregister(&vb->pr_dev_info);
+		 *
+		 * 在以下使用page_reporting_register():
+		 *   - drivers/hv/hv_balloon.c|1667| <<enable_page_reporting>> ret = page_reporting_register(&dm_device.pr_dev_info);
+		 *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+		 */
 		err = page_reporting_register(&vb->pr_dev_info);
 		if (err)
 			goto out_unregister_oom;
@@ -1078,6 +1441,12 @@ static int virtballoon_probe(struct virtio_device *vdev)
 
 	virtio_device_ready(vdev);
 
+	/*
+	 * 在以下使用towards_target():
+	 *   - drivers/virtio/virtio_balloon.c|663| <<update_balloon_size_func>> diff = towards_target(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1262| <<virtballoon_probe>> if (towards_target(vb))
+	 *   - drivers/virtio/virtio_balloon.c|1368| <<virtballoon_restore>> if (towards_target(vb))
+	 */
 	if (towards_target(vb))
 		virtballoon_changed(vdev);
 	return 0;
@@ -1104,6 +1473,13 @@ static void remove_common(struct virtio_balloon *vb)
 	/* There might be pages left in the balloon: free them. */
 	while (vb->num_pages)
 		leak_balloon(vb, vb->num_pages);
+	/*
+	 * 在以下使用update_balloon_size():
+	 *   - drivers/virtio/virtio_balloon.c|709| <<update_balloon_size_func>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1115| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1365| <<remove_common>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1453| <<virtballoon_restore>> update_balloon_size(vb);
+	 */
 	update_balloon_size(vb);
 
 	/* There might be free pages that are being reported: release them. */
@@ -1120,6 +1496,13 @@ static void virtballoon_remove(struct virtio_device *vdev)
 {
 	struct virtio_balloon *vb = vdev->priv;
 
+	/*
+	 * 在以下使用virtio_balloon->pr_dev_info:
+	 *   - drivers/virtio/virtio_balloon.c|1158| <<virtballoon_probe>> vb->pr_dev_info.report = virtballoon_free_page_report;
+	 *   - drivers/virtio/virtio_balloon.c|1182| <<virtballoon_probe>> vb->pr_dev_info.order = 5;
+	 *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+	 *   - drivers/virtio/virtio_balloon.c|1246| <<virtballoon_remove>> page_reporting_unregister(&vb->pr_dev_info);
+	 */
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_REPORTING))
 		page_reporting_unregister(&vb->pr_dev_info);
 	if (virtio_has_feature(vb->vdev, VIRTIO_BALLOON_F_DEFLATE_ON_OOM))
@@ -1129,10 +1512,22 @@ static void virtballoon_remove(struct virtio_device *vdev)
 	spin_lock_irq(&vb->stop_update_lock);
 	vb->stop_update = true;
 	spin_unlock_irq(&vb->stop_update_lock);
+	/*
+	 * 在以下使用virtio_balloon->update_balloon_size_work:
+	 *   - drivers/virtio/virtio_balloon.c|515| <<start_update_balloon_size>> queue_work(system_freezable_wq, &vb->update_balloon_size_work);
+	 *   - drivers/virtio/virtio_balloon.c|961| <<virtballoon_probe>> INIT_WORK(&vb->update_balloon_size_work, update_balloon_size_func);
+	 *   - drivers/virtio/virtio_balloon.c|1132| <<virtballoon_remove>> cancel_work_sync(&vb->update_balloon_size_work);
+	 */
 	cancel_work_sync(&vb->update_balloon_size_work);
 	cancel_work_sync(&vb->update_balloon_stats_work);
 
 	if (virtio_has_feature(vdev, VIRTIO_BALLOON_F_FREE_PAGE_HINT)) {
+		/*
+		 * 在以下使用virtio_balloon->report_free_page_work():
+		 *   - drivers/virtio/virtio_balloon.c|509| <<virtio_balloon_queue_free_page_work>> queue_work(vb->balloon_wq, &vb->report_free_page_work);
+		 *   - drivers/virtio/virtio_balloon.c|991| <<virtballoon_probe>> INIT_WORK(&vb->report_free_page_work, report_free_page_func);
+		 *   - drivers/virtio/virtio_balloon.c|1136| <<virtballoon_remove>> cancel_work_sync(&vb->report_free_page_work);
+		 */
 		cancel_work_sync(&vb->report_free_page_work);
 		destroy_workqueue(vb->balloon_wq);
 	}
@@ -1165,8 +1560,21 @@ static int virtballoon_restore(struct virtio_device *vdev)
 
 	virtio_device_ready(vdev);
 
+	/*
+	 * 在以下使用towards_target():
+	 *   - drivers/virtio/virtio_balloon.c|663| <<update_balloon_size_func>> diff = towards_target(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1262| <<virtballoon_probe>> if (towards_target(vb))
+	 *   - drivers/virtio/virtio_balloon.c|1368| <<virtballoon_restore>> if (towards_target(vb))
+	 */
 	if (towards_target(vb))
 		virtballoon_changed(vdev);
+	/*
+	 * 在以下使用update_balloon_size():
+	 *   - drivers/virtio/virtio_balloon.c|709| <<update_balloon_size_func>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1115| <<virtio_balloon_oom_notify>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1365| <<remove_common>> update_balloon_size(vb);
+	 *   - drivers/virtio/virtio_balloon.c|1453| <<virtballoon_restore>> update_balloon_size(vb);
+	 */
 	update_balloon_size(vb);
 	return 0;
 }
diff --git a/include/linux/balloon_compaction.h b/include/linux/balloon_compaction.h
index 7cfe48769..e6c5a0c4e 100644
--- a/include/linux/balloon_compaction.h
+++ b/include/linux/balloon_compaction.h
@@ -67,6 +67,12 @@ extern size_t balloon_page_list_enqueue(struct balloon_dev_info *b_dev_info,
 extern size_t balloon_page_list_dequeue(struct balloon_dev_info *b_dev_info,
 				     struct list_head *pages, size_t n_req_pages);
 
+/*
+ * 在以下使用balloon_devinfo_init():
+ *   - arch/powerpc/platforms/pseries/cmm.c|553| <<cmm_balloon_compaction_init>> balloon_devinfo_init(&b_dev_info);
+ *   - drivers/misc/vmw_balloon.c|1878| <<vmballoon_init>> balloon_devinfo_init(&balloon.b_dev_info);
+ *   - drivers/virtio/virtio_balloon.c|1115| <<virtballoon_probe>> balloon_devinfo_init(&vb->vb_dev_info);
+ */
 static inline void balloon_devinfo_init(struct balloon_dev_info *balloon)
 {
 	balloon->isolated_pages = 0;
diff --git a/include/linux/irqbypass.h b/include/linux/irqbypass.h
index ede1fa938..7a06c2786 100644
--- a/include/linux/irqbypass.h
+++ b/include/linux/irqbypass.h
@@ -45,6 +45,29 @@ struct irq_bypass_consumer;
  * participation in possible host bypass, for instance an interrupt vector
  * for a physical device assigned to a VM.
  */
+/*
+ * 在以下使用irq_bypass_producer->add_consumer:
+ *   - virt/lib/irqbypass.c|40| <<__connect>> if (prod->add_consumer)
+ *   - virt/lib/irqbypass.c|41| <<__connect>> ret = prod->add_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->del_consumer:
+ *   - virt/lib/irqbypass.c|45| <<__connect>> if (ret && prod->del_consumer)
+ *   - virt/lib/irqbypass.c|46| <<__connect>> prod->del_consumer(prod, cons);
+ *   - virt/lib/irqbypass.c|72| <<__disconnect>> if (prod->del_consumer)
+ *   - virt/lib/irqbypass.c|73| <<__disconnect>> prod->del_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->stop:
+ *   - virt/lib/irqbypass.c|35| <<__connect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|36| <<__connect>> prod->stop(prod);
+ *   - virt/lib/irqbypass.c|65| <<__disconnect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|66| <<__disconnect>> prod->stop(prod); 
+ *
+ * 在以下使用irq_bypass_producer->start:
+ *   - virt/lib/irqbypass.c|51| <<__connect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|52| <<__connect>> prod->start(prod);
+ *   - virt/lib/irqbypass.c|77| <<__disconnect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|78| <<__disconnect>> prod->start(prod);
+ */
 struct irq_bypass_producer {
 	struct eventfd_ctx *eventfd;
 	struct irq_bypass_consumer *consumer;
@@ -71,6 +94,29 @@ struct irq_bypass_producer {
  * support offloads to allow bypassing the host entirely or offload
  * portions of the interrupt handling to the VM.
  */
+/*
+ * 在以下使用irq_bypass_consumer->add_producer:
+ *   - virt/kvm/eventfd.c|478| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ *   - virt/lib/irqbypass.c|44| <<__connect>> ret = cons->add_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->del_producer:
+ *   - virt/kvm/eventfd.c|479| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ *   - virt/lib/irqbypass.c|70| <<__disconnect>> cons->del_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->stop:
+ *   - virt/lib/irqbypass.c|37| <<__connect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|38| <<__connect>> cons->stop(cons);
+ *   - virt/lib/irqbypass.c|67| <<__disconnect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|68| <<__disconnect>> cons->stop(cons);
+ *
+ * 在以下使用irq_bypass_consumer->start:
+ *   - virt/lib/irqbypass.c|49| <<__connect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|50| <<__connect>> cons->start(cons);
+ *   - virt/lib/irqbypass.c|75| <<__disconnect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|76| <<__disconnect>> cons->start(cons);
+ */
 struct irq_bypass_consumer {
 	struct eventfd_ctx *eventfd;
 	struct irq_bypass_producer *producer;
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 5bd76cf39..4a4183761 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -789,11 +789,34 @@ struct kvm {
 	struct kvm_memslots __memslots[KVM_MAX_NR_ADDRESS_SPACES][2];
 	/* The current active memslot set for each address space */
 	struct kvm_memslots __rcu *memslots[KVM_MAX_NR_ADDRESS_SPACES];
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	struct xarray vcpu_array;
 	/*
 	 * Protected by slots_lock, but can be read outside if an
 	 * incorrect answer is acceptable.
 	 */
+	/*
+	 * 在以下使用kvm->nr_memslots_dirty_logging:
+	 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+	 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+	 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+	 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+	 */
 	atomic_t nr_memslots_dirty_logging;
 
 	/* Used to wait for completion of MMU notifiers.  */
@@ -830,6 +853,20 @@ struct kvm {
 	struct list_head ioeventfds;
 	struct kvm_vm_stat stat;
 	struct kvm_arch arch;
+	/*
+	 * 在以下使用kvm->users_count:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1144| <<kvm_tdp_mmu_invalidate_roots>> refcount_read(&kvm->users_count) && kvm->created_vcpus)
+	 *   - arch/x86/kvm/vmx/nested.h|61| <<get_vmcs12>> !refcount_read(&vcpu->kvm->users_count));
+	 *   - arch/x86/kvm/vmx/nested.h|69| <<get_shadow_vmcs12>> !refcount_read(&vcpu->kvm->users_count));
+	 *   - include/linux/kvm_host.h|1122| <<__kvm_memslots>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1190| <<kvm_get_bus_for_destruction>> !refcount_read(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1259| <<kvm_create_vm>> refcount_set(&kvm->users_count, 1);
+	 *   - virt/kvm/kvm_main.c|1331| <<kvm_create_vm>> WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
+	 *   - virt/kvm/kvm_main.c|1452| <<kvm_get_kvm>> refcount_inc(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1462| <<kvm_get_kvm_safe>> return refcount_inc_not_zero(&kvm->users_count);
+	 *   - virt/kvm/kvm_main.c|1468| <<kvm_put_kvm>> if (refcount_dec_and_test(&kvm->users_count))
+	 *   - virt/kvm/kvm_main.c|1482| <<kvm_put_kvm_no_destroy>> WARN_ON(refcount_dec_and_test(&kvm->users_count));
+	 */
 	refcount_t users_count;
 #ifdef CONFIG_KVM_MMIO
 	struct kvm_coalesced_mmio_ring *coalesced_mmio_ring;
@@ -1003,11 +1040,33 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 
 	i = array_index_nospec(i, num_vcpus);
 
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	/* Pairs with smp_wmb() in kvm_vm_ioctl_create_vcpu.  */
 	smp_rmb();
 	return xa_load(&kvm->vcpu_array, i);
 }
 
+/*
+ * 在以下使用kvm->vcpu_array:
+ *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+ *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+ *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+ *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+ *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+ *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+ *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+ *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+ */
 #define kvm_for_each_vcpu(idx, vcpup, kvm)				\
 	if (atomic_read(&kvm->online_vcpus))				\
 		xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0,	\
@@ -1386,15 +1445,40 @@ int __kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map,
 		   bool writable);
 void kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map);
 
+/*
+ * 在以下使用kvm_vcpu_map():
+ *   - arch/powerpc/kvm/book3s_pr.c|647| <<kvmppc_patch_dcbz>> r = kvm_vcpu_map(vcpu, pte->raddr >> PAGE_SHIFT, &map);
+ *   - arch/x86/kvm/svm/nested.c|965| <<nested_svm_vmrun>> ret = kvm_vcpu_map(vcpu, gpa_to_gfn(vmcb12_gpa), &map);
+ *   - arch/x86/kvm/svm/nested.c|1083| <<nested_svm_vmexit>> rc = kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.vmcb12_gpa), &map);
+ *   - arch/x86/kvm/svm/sev.c|4361| <<sev_handle_vmgexit>> if (kvm_vcpu_map(vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->sev_es.ghcb_map)) {
+ *   - arch/x86/kvm/svm/svm.c|2083| <<vmload_vmsave_interception>> ret = kvm_vcpu_map(vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);
+ *   - arch/x86/kvm/svm/svm.c|4703| <<svm_enter_smm>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.hsave_msr), &map_save))
+ *   - arch/x86/kvm/svm/svm.c|4737| <<svm_leave_smm>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(smram64->svm_guest_vmcb_gpa), &map))
+ *   - arch/x86/kvm/svm/svm.c|4741| <<svm_leave_smm>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.hsave_msr), &map_save))
+ *   - arch/x86/kvm/vmx/nested.c|2141| <<nested_vmx_handle_enlightened_vmptrld>> if (kvm_vcpu_map(vcpu, gpa_to_gfn(evmcs_gpa),
+ *   - arch/x86/kvm/vmx/nested.c|3465| <<nested_get_vmcs12_pages>> if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->apic_access_addr), map)) {
+ *   - arch/x86/kvm/vmx/nested.c|3481| <<nested_get_vmcs12_pages>> if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->virtual_apic_page_addr), map)) {
+ *   - arch/x86/kvm/vmx/nested.c|3507| <<nested_get_vmcs12_pages>> if (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->posted_intr_desc_addr), map)) {
+ */
 static inline int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa,
 			       struct kvm_host_map *map)
 {
+	/*
+	 * 在以下使用__kvm_vcpu_map():
+	 *   - include/linux/kvm_host.h|1419| <<kvm_vcpu_map>> return __kvm_vcpu_map(vcpu, gpa, map, true);
+	 *   - include/linux/kvm_host.h|1425| <<kvm_vcpu_map_readonly>> return __kvm_vcpu_map(vcpu, gpa, map, false);
+	 */
 	return __kvm_vcpu_map(vcpu, gpa, map, true);
 }
 
 static inline int kvm_vcpu_map_readonly(struct kvm_vcpu *vcpu, gpa_t gpa,
 					struct kvm_host_map *map)
 {
+	/*
+	 * 在以下使用__kvm_vcpu_map():
+	 *   - include/linux/kvm_host.h|1419| <<kvm_vcpu_map>> return __kvm_vcpu_map(vcpu, gpa, map, true);
+	 *   - include/linux/kvm_host.h|1425| <<kvm_vcpu_map_readonly>> return __kvm_vcpu_map(vcpu, gpa, map, false);
+	 */
 	return __kvm_vcpu_map(vcpu, gpa, map, false);
 }
 
@@ -2236,6 +2320,12 @@ static inline bool kvm_notify_irqfd_resampler(struct kvm *kvm,
 
 void kvm_arch_irq_routing_update(struct kvm *kvm);
 
+/*
+ * 在以下使用__kvm_make_request():
+ *   - arch/s390/kvm/kvm-s390.c|4106| <<kvm_s390_sync_request>> __kvm_make_request(req, vcpu);
+ *   - include/linux/kvm_host.h|2286| <<kvm_make_request>> __kvm_make_request(req, vcpu);
+ *   - virt/kvm/kvm_main.c|227| <<kvm_make_vcpu_request>> __kvm_make_request(req, vcpu);
+ */
 static inline void __kvm_make_request(int req, struct kvm_vcpu *vcpu)
 {
 	/*
@@ -2256,6 +2346,12 @@ static __always_inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)
 	BUILD_BUG_ON(!__builtin_constant_p(req) ||
 		     (req & KVM_REQUEST_NO_ACTION));
 
+	/*
+	 * 在以下使用__kvm_make_request():
+	 *   - arch/s390/kvm/kvm-s390.c|4106| <<kvm_s390_sync_request>> __kvm_make_request(req, vcpu);
+	 *   - include/linux/kvm_host.h|2286| <<kvm_make_request>> __kvm_make_request(req, vcpu);
+	 *   - virt/kvm/kvm_main.c|227| <<kvm_make_vcpu_request>> __kvm_make_request(req, vcpu);
+	 */
 	__kvm_make_request(req, vcpu);
 }
 
diff --git a/include/uapi/linux/vfio.h b/include/uapi/linux/vfio.h
index 75100bf00..610ac4488 100644
--- a/include/uapi/linux/vfio.h
+++ b/include/uapi/linux/vfio.h
@@ -1064,6 +1064,11 @@ struct vfio_device_feature_migration {
  * Upon VFIO_DEVICE_FEATURE_GET, get the current migration state of the VFIO
  * device, data_fd will be -1.
  */
+/*
+ * 在以下使用vfio_device_feature_mig_state->data_fd:
+ *   - drivers/vfio/vfio_main.c|872| <<vfio_ioct_mig_return_fd>> mig->data_fd = fd;
+ *   - drivers/vfio/vfio_main.c|929| <<vfio_ioctl_device_feature_mig_device_state>> mig.data_fd = -1;
+ */
 struct vfio_device_feature_mig_state {
 	__u32 device_state; /* From enum vfio_device_mig_state */
 	__s32 data_fd;
diff --git a/kernel/irq/irqdomain.c b/kernel/irq/irqdomain.c
index dc473faad..f15fbd45b 100644
--- a/kernel/irq/irqdomain.c
+++ b/kernel/irq/irqdomain.c
@@ -1590,6 +1590,22 @@ static int irq_domain_alloc_irqs_hierarchy(struct irq_domain *domain, unsigned i
 	return domain->ops->alloc(domain, irq_base, nr_irqs, arg);
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 static int irq_domain_alloc_irqs_locked(struct irq_domain *domain, int irq_base,
 					unsigned int nr_irqs, int node, void *arg,
 					bool realloc, const struct irq_affinity_desc *affinity)
@@ -1636,6 +1652,22 @@ static int irq_domain_alloc_irqs_locked(struct irq_domain *domain, int irq_base,
 	return ret;
 }
 
+/*
+ * [0] irq_remapping_prepare_irte
+ * [0] irq_remapping_alloc
+ * [0] mp_irqdomain_alloc
+ * [0] irq_domain_alloc_irqs_locked
+ * [0] __irq_domain_alloc_irqs
+ * [0] alloc_isa_irq_from_domain
+ * [0] mp_map_pin_to_irq
+ * [0] setup_IO_APIC
+ * [0] apic_intr_mode_init
+ * [0] x86_late_time_init
+ * [0] start_kernel
+ * [0] x86_64_start_reservations
+ * [0] x86_64_start_kernel
+ * [0] common_startup_64
+ */
 /**
  * __irq_domain_alloc_irqs - Allocate IRQs from domain
  * @domain:	domain to allocate from
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f754a60de..4b558f975 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -6781,6 +6781,15 @@ static inline void proxy_tag_curr(struct rq *rq, struct task_struct *owner)
  *
  * WARNING: must be called with preemption disabled!
  */
+/*
+ * 在以下使用__schedule():
+ *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+ *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+ *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+ *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+ *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+ *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+ */
 static void __sched notrace __schedule(int sched_mode)
 {
 	struct task_struct *prev, *next;
@@ -6947,6 +6956,15 @@ void __noreturn do_task_dead(void)
 	/* Tell freezer to ignore us: */
 	current->flags |= PF_NOFREEZE;
 
+	/*
+	 * 在以下使用__schedule():
+	 *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+	 *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+	 *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+	 *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+	 *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+	 *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+	 */
 	__schedule(SM_NONE);
 	BUG();
 
@@ -7008,6 +7026,15 @@ static __always_inline void __schedule_loop(int sched_mode)
 {
 	do {
 		preempt_disable();
+		/*
+		 * 在以下使用__schedule():
+		 *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+		 *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+		 *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+		 *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+		 */
 		__schedule(sched_mode);
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
@@ -7049,6 +7076,15 @@ void __sched schedule_idle(void)
 	 */
 	WARN_ON_ONCE(current->__state);
 	do {
+		/*
+		 * 在以下使用__schedule():
+		 *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+		 *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+		 *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+		 *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+		 */
 		__schedule(SM_IDLE);
 	} while (need_resched());
 }
@@ -7110,6 +7146,15 @@ static void __sched notrace preempt_schedule_common(void)
 		 */
 		preempt_disable_notrace();
 		preempt_latency_start(1);
+		/*
+		 * 在以下使用__schedule():
+		 *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+		 *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+		 *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+		 *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+		 */
 		__schedule(SM_PREEMPT);
 		preempt_latency_stop(1);
 		preempt_enable_no_resched_notrace();
@@ -7203,6 +7248,15 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 		 * an infinite recursion.
 		 */
 		prev_ctx = exception_enter();
+		/*
+		 * 在以下使用__schedule():
+		 *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+		 *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+		 *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+		 *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+		 */
 		__schedule(SM_PREEMPT);
 		exception_exit(prev_ctx);
 
@@ -7253,6 +7307,15 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 	do {
 		preempt_disable();
 		local_irq_enable();
+		/*
+		 * 在以下使用__schedule():
+		 *   - kernel/sched/core.c|6950| <<do_task_dead>> __schedule(SM_NONE);
+		 *   - kernel/sched/core.c|7011| <<__schedule_loop>> __schedule(sched_mode);
+		 *   - kernel/sched/core.c|7052| <<schedule_idle>> __schedule(SM_IDLE);
+		 *   - kernel/sched/core.c|7113| <<preempt_schedule_common>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7206| <<preempt_schedule_notrace>> __schedule(SM_PREEMPT);
+		 *   - kernel/sched/core.c|7256| <<preempt_schedule_irq>> __schedule(SM_PREEMPT);
+		 */
 		__schedule(SM_PREEMPT);
 		local_irq_disable();
 		sched_preempt_enable_no_resched();
diff --git a/mm/balloon_compaction.c b/mm/balloon_compaction.c
index 03c5dbabb..11fd52393 100644
--- a/mm/balloon_compaction.c
+++ b/mm/balloon_compaction.c
@@ -118,6 +118,12 @@ EXPORT_SYMBOL_GPL(balloon_page_list_dequeue);
  *
  * Return: struct page for the allocated page or NULL on allocation failure.
  */
+/*
+ * 在以下使用balloon_page_alloc():
+ *   - arch/powerpc/platforms/pseries/cmm.c|156| <<cmm_alloc_pages>> page = balloon_page_alloc();
+ *   - drivers/misc/vmw_balloon.c|672| <<vmballoon_alloc_page_list>> page = balloon_page_alloc();
+ *   - drivers/virtio/virtio_balloon.c|316| <<fill_balloon>> page = balloon_page_alloc();
+ */
 struct page *balloon_page_alloc(void)
 {
 	struct page *page = alloc_page(balloon_mapping_gfp_mask() |
diff --git a/mm/page_reporting.c b/mm/page_reporting.c
index e4c428e61..ef5066f81 100644
--- a/mm/page_reporting.c
+++ b/mm/page_reporting.c
@@ -349,6 +349,11 @@ static void page_reporting_process(struct work_struct *work)
 static DEFINE_MUTEX(page_reporting_mutex);
 DEFINE_STATIC_KEY_FALSE(page_reporting_enabled);
 
+/*
+ * 在以下使用page_reporting_register():
+ *   - drivers/hv/hv_balloon.c|1667| <<enable_page_reporting>> ret = page_reporting_register(&dm_device.pr_dev_info);
+ *   - drivers/virtio/virtio_balloon.c|1185| <<virtballoon_probe>> err = page_reporting_register(&vb->pr_dev_info);
+ */
 int page_reporting_register(struct page_reporting_dev_info *prdev)
 {
 	int err = 0;
diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c
index b8aaa96b7..f0ab76b9c 100644
--- a/virt/kvm/async_pf.c
+++ b/virt/kvm/async_pf.c
@@ -17,8 +17,23 @@
 #include "async_pf.h"
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用async_pf_cache:
+ *   - virt/kvm/async_pf.c|24| <<kvm_async_pf_init>> async_pf_cache = KMEM_CACHE(kvm_async_pf, 0);
+ *   - virt/kvm/async_pf.c|26| <<kvm_async_pf_init>> if (!async_pf_cache)
+ *   - virt/kvm/async_pf.c|34| <<kvm_async_pf_deinit>> kmem_cache_destroy(async_pf_cache);
+ *   - virt/kvm/async_pf.c|35| <<kvm_async_pf_deinit>> async_pf_cache = NULL;
+ *   - virt/kvm/async_pf.c|117| <<kvm_flush_and_free_async_pf_work>> kmem_cache_free(async_pf_cache, work);
+ *   - virt/kvm/async_pf.c|133| <<kvm_clear_async_pf_completion_queue>> kmem_cache_free(async_pf_cache, work);
+ *   - virt/kvm/async_pf.c|195| <<kvm_setup_async_pf>> work = kmem_cache_zalloc(async_pf_cache, GFP_NOWAIT);
+ *   - virt/kvm/async_pf.c|224| <<kvm_async_pf_wakeup_all>> work = kmem_cache_zalloc(async_pf_cache, GFP_ATOMIC);
+ */
 static struct kmem_cache *async_pf_cache;
 
+/*
+ * 在以下使用kvm_async_pf_init():
+ *   - virt/kvm/kvm_main.c|6582| <<kvm_init>> r = kvm_async_pf_init();
+ */
 int kvm_async_pf_init(void)
 {
 	async_pf_cache = KMEM_CACHE(kvm_async_pf, 0);
@@ -35,6 +50,10 @@ void kvm_async_pf_deinit(void)
 	async_pf_cache = NULL;
 }
 
+/*
+ * 在以下使用kvm_async_pf_vcpu_init():
+ *   - virt/kvm/kvm_main.c|510| <<kvm_vcpu_init>> kvm_async_pf_vcpu_init(vcpu);
+ */
 void kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)
 {
 	INIT_LIST_HEAD(&vcpu->async_pf.done);
@@ -42,6 +61,10 @@ void kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)
 	spin_lock_init(&vcpu->async_pf.lock);
 }
 
+/*
+ * 在以下使用async_pf_execute():
+ *   - virt/kvm/async_pf.c|205| <<kvm_setup_async_pf>> INIT_WORK(&work->work, async_pf_execute);
+ */
 static void async_pf_execute(struct work_struct *work)
 {
 	struct kvm_async_pf *apf =
@@ -74,6 +97,11 @@ static void async_pf_execute(struct work_struct *work)
 	 * Notify and kick the vCPU even if faulting in the page failed, e.g.
 	 * so that the vCPU can retry the fault synchronously.
 	 */
+	/*
+	 * 在以下使用kvm_arch_async_page_present():
+	 *   - virt/kvm/async_pf.c|78| <<async_pf_execute>> kvm_arch_async_page_present(vcpu, apf);
+	 *   - virt/kvm/async_pf.c|167| <<kvm_check_async_pf_completion>> kvm_arch_async_page_present(vcpu, work);
+	 */
 	if (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
 		kvm_arch_async_page_present(vcpu, apf);
 
@@ -88,6 +116,11 @@ static void async_pf_execute(struct work_struct *work)
 	 */
 	apf = NULL;
 
+	/*
+	 * 在以下使用kvm_arch_async_page_present_queued():
+	 *   - virt/kvm/async_pf.c|92| <<async_pf_execute>> kvm_arch_async_page_present_queued(vcpu);
+	 *   - virt/kvm/async_pf.c|237| <<kvm_async_pf_wakeup_all>> kvm_arch_async_page_present_queued(vcpu);
+	 */
 	if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
 		kvm_arch_async_page_present_queued(vcpu);
 
@@ -96,6 +129,11 @@ static void async_pf_execute(struct work_struct *work)
 	__kvm_vcpu_wake_up(vcpu);
 }
 
+/*
+ * 在以下使用kvm_flush_and_free_async_pf_work():
+ *   - virt/kvm/async_pf.c|145| <<kvm_clear_async_pf_completion_queue>> kvm_flush_and_free_async_pf_work(work);
+ *   - virt/kvm/async_pf.c|171| <<kvm_check_async_pf_completion>> kvm_flush_and_free_async_pf_work(work);
+ */
 static void kvm_flush_and_free_async_pf_work(struct kvm_async_pf *work)
 {
 	/*
@@ -117,6 +155,19 @@ static void kvm_flush_and_free_async_pf_work(struct kvm_async_pf *work)
 	kmem_cache_free(async_pf_cache, work);
 }
 
+/*
+ * 在以下使用kvm_clear_async_pf_completion_queue():
+ *   - arch/s390/kvm/interrupt.c|2695| <<flic_set_attr>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|3490| <<kvm_arch_vcpu_destroy>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4001| <<kvm_arch_vcpu_create>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4231| <<kvm_arch_vcpu_ioctl_set_one_reg>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|4262| <<kvm_arch_vcpu_ioctl_normal_reset>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/s390/kvm/kvm-s390.c|5175| <<sync_regs_fmt2>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|1167| <<kvm_post_set_cr0>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|4797| <<kvm_pv_enable_async_pf>> kvm_clear_async_pf_completion_queue(vcpu);
+ *   - arch/x86/kvm/x86.c|14242| <<kvm_arch_vcpu_destroy>> kvm_clear_async_pf_completion_queue(vcpu)
+ *   - arch/x86/kvm/x86.c|14369| <<kvm_vcpu_reset>> kvm_clear_async_pf_completion_queue(vcpu);
+ */
 void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 {
 	/* cancel outstanding work queue item */
@@ -142,6 +193,11 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 		list_del(&work->link);
 
 		spin_unlock(&vcpu->async_pf.lock);
+		/*
+		 * 在以下使用kvm_flush_and_free_async_pf_work():
+		 *   - virt/kvm/async_pf.c|145| <<kvm_clear_async_pf_completion_queue>> kvm_flush_and_free_async_pf_work(work);
+		 *   - virt/kvm/async_pf.c|171| <<kvm_check_async_pf_completion>> kvm_flush_and_free_async_pf_work(work);
+		 */
 		kvm_flush_and_free_async_pf_work(work);
 		spin_lock(&vcpu->async_pf.lock);
 	}
@@ -150,6 +206,12 @@ void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)
 	vcpu->async_pf.queued = 0;
 }
 
+/*
+ * 在以下使用kvm_check_async_pf_completion():
+ *   - arch/s390/kvm/kvm-s390.c|4785| <<vcpu_pre_run>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|5320| <<kvm_set_msr_common>> kvm_check_async_pf_completion(vcpu);
+ *   - arch/x86/kvm/x86.c|12639| <<vcpu_enter_guest>> kvm_check_async_pf_completion(vcpu);
+ */
 void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
@@ -162,12 +224,25 @@ void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
 		list_del(&work->link);
 		spin_unlock(&vcpu->async_pf.lock);
 
+		/*
+		 * 在x86.c实现, 只在这里调用
+		 */
 		kvm_arch_async_page_ready(vcpu, work);
+		/*
+		 * 在以下使用kvm_arch_async_page_present():
+		 *   - virt/kvm/async_pf.c|78| <<async_pf_execute>> kvm_arch_async_page_present(vcpu, apf);
+		 *   - virt/kvm/async_pf.c|167| <<kvm_check_async_pf_completion>> kvm_arch_async_page_present(vcpu, work);
+		 */
 		if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))
 			kvm_arch_async_page_present(vcpu, work);
 
 		list_del(&work->queue);
 		vcpu->async_pf.queued--;
+		/*
+		 * 在以下使用kvm_flush_and_free_async_pf_work():
+		 *   - virt/kvm/async_pf.c|145| <<kvm_clear_async_pf_completion_queue>> kvm_flush_and_free_async_pf_work(work);
+		 *   - virt/kvm/async_pf.c|171| <<kvm_check_async_pf_completion>> kvm_flush_and_free_async_pf_work(work);
+		 */
 		kvm_flush_and_free_async_pf_work(work);
 	}
 }
@@ -176,6 +251,11 @@ void kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)
  * Try to schedule a job to handle page fault asynchronously. Returns 'true' on
  * success, 'false' on failure (page fault has to be handled synchronously).
  */
+/*
+ * 在以下使用kvm_setup_async_pf():
+ *  - arch/s390/kvm/kvm-s390.c|4773| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, current->thread.gmap_teid.addr * PAGE_SIZE, hva, &arch);
+ *  - arch/x86/kvm/mmu/mmu.c|4526| <<kvm_arch_setup_async_pf>> return kvm_setup_async_pf(vcpu, fault->addr,
+ */
 bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			unsigned long hva, struct kvm_arch_async_pf *arch)
 {
@@ -213,6 +293,10 @@ bool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return true;
 }
 
+/*
+ * 在以下使用kvm_async_pf_wakeup_all():
+ *   - arch/x86/kvm/x86.c|4809| <<kvm_pv_enable_async_pf>> kvm_async_pf_wakeup_all(vcpu);
+ */
 int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 {
 	struct kvm_async_pf *work;
@@ -233,6 +317,11 @@ int kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)
 	list_add_tail(&work->link, &vcpu->async_pf.done);
 	spin_unlock(&vcpu->async_pf.lock);
 
+	/*
+	 * 在以下使用kvm_arch_async_page_present_queued():
+	 *   - virt/kvm/async_pf.c|92| <<async_pf_execute>> kvm_arch_async_page_present_queued(vcpu);
+	 *   - virt/kvm/async_pf.c|237| <<kvm_async_pf_wakeup_all>> kvm_arch_async_page_present_queued(vcpu);
+	 */
 	if (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)
 		kvm_arch_async_page_present_queued(vcpu);
 
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index a7794ffdb..cffe3a704 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -30,6 +30,53 @@
 
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 
+/*
+ * 在以下使用irq_bypass_producer->add_consumer:
+ *   - virt/lib/irqbypass.c|40| <<__connect>> if (prod->add_consumer)
+ *   - virt/lib/irqbypass.c|41| <<__connect>> ret = prod->add_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->del_consumer:
+ *   - virt/lib/irqbypass.c|45| <<__connect>> if (ret && prod->del_consumer)
+ *   - virt/lib/irqbypass.c|46| <<__connect>> prod->del_consumer(prod, cons);
+ *   - virt/lib/irqbypass.c|72| <<__disconnect>> if (prod->del_consumer)
+ *   - virt/lib/irqbypass.c|73| <<__disconnect>> prod->del_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->stop:
+ *   - virt/lib/irqbypass.c|35| <<__connect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|36| <<__connect>> prod->stop(prod);
+ *   - virt/lib/irqbypass.c|65| <<__disconnect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|66| <<__disconnect>> prod->stop(prod);
+ *
+ * 在以下使用irq_bypass_producer->start:
+ *   - virt/lib/irqbypass.c|51| <<__connect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|52| <<__connect>> prod->start(prod);
+ *   - virt/lib/irqbypass.c|77| <<__disconnect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|78| <<__disconnect>> prod->start(prod);
+ *
+ *
+ * 在以下使用irq_bypass_consumer->add_producer:
+ *   - virt/kvm/eventfd.c|478| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ *   - virt/lib/irqbypass.c|44| <<__connect>> ret = cons->add_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->del_producer:
+ *   - virt/kvm/eventfd.c|479| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ *   - virt/lib/irqbypass.c|70| <<__disconnect>> cons->del_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->stop:
+ *   - virt/lib/irqbypass.c|37| <<__connect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|38| <<__connect>> cons->stop(cons);
+ *   - virt/lib/irqbypass.c|67| <<__disconnect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|68| <<__disconnect>> cons->stop(cons);
+ *
+ * 在以下使用irq_bypass_consumer->start:
+ *   - virt/lib/irqbypass.c|49| <<__connect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|50| <<__connect>> cons->start(cons);
+ *   - virt/lib/irqbypass.c|75| <<__disconnect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|76| <<__disconnect>> cons->start(cons);
+ */
+
 static struct workqueue_struct *irqfd_cleanup_wq;
 
 bool __attribute__((weak))
@@ -263,6 +310,11 @@ static void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)
 
 	lockdep_assert_held(&kvm->irqfds.lock);
 
+	/*
+	 * 在以下使用kvm_irq_map_gsi():
+	 *   - virt/kvm/eventfd.c|313| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+	 *   - virt/kvm/irqchip.c|83| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+	 */
 	n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
 
 	write_seqcount_begin(&irqfd->irq_entry_sc);
@@ -352,6 +404,15 @@ void __weak kvm_arch_update_irqfd_routing(struct kvm_kernel_irqfd *irqfd,
 }
 #endif
 
+/*
+ * struct kvm_irqfd {
+ *     __u32 fd;
+ *     __u32 gsi;
+ *     __u32 flags;
+ *     __u32 resamplefd;
+ *     __u8  pad[16];
+ * };
+ */
 static int
 kvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -512,6 +573,13 @@ bool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	int gsi, idx;
 
 	idx = srcu_read_lock(&kvm->irq_srcu);
+	/*
+	 * 在以下使用kvm_irq_map_chip_pin():
+	 *   - arch/x86/kvm/ioapic.c|328| <<kvm_fire_mask_notifiers>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|571| <<kvm_irq_has_notifier>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|603| <<kvm_notify_acked_irq>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|746| <<kvm_notify_irqfd_resampler>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 */
 	gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
 	if (gsi != -1)
 		hlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,
@@ -544,6 +612,13 @@ void kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	trace_kvm_ack_irq(irqchip, pin);
 
 	idx = srcu_read_lock(&kvm->irq_srcu);
+	/*
+	 * 在以下使用kvm_irq_map_chip_pin():
+	 *   - arch/x86/kvm/ioapic.c|328| <<kvm_fire_mask_notifiers>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|571| <<kvm_irq_has_notifier>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|603| <<kvm_notify_acked_irq>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|746| <<kvm_notify_irqfd_resampler>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 */
 	gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
 	if (gsi != -1)
 		kvm_notify_acked_gsi(kvm, gsi);
@@ -652,6 +727,10 @@ kvm_irqfd_release(struct kvm *kvm)
  * Take note of a change in irq routing.
  * Caller must invoke synchronize_srcu_expedited(&kvm->irq_srcu) afterwards.
  */
+/*
+ * 在以下使用kvm_irq_routing_update():
+ *   - virt/kvm/irqchip.c|221| <<kvm_set_irq_routing>> kvm_irq_routing_update(kvm);
+ */
 void kvm_irq_routing_update(struct kvm *kvm)
 {
 	struct kvm_kernel_irqfd *irqfd;
@@ -664,6 +743,9 @@ void kvm_irq_routing_update(struct kvm *kvm)
 		struct kvm_kernel_irq_routing_entry old = irqfd->irq_entry;
 #endif
 
+		/*
+		 * 只在这里调用
+		 */
 		irqfd_update(kvm, irqfd);
 
 #if IS_ENABLED(CONFIG_HAVE_KVM_IRQ_BYPASS)
@@ -683,6 +765,13 @@ bool kvm_notify_irqfd_resampler(struct kvm *kvm,
 	int gsi, idx;
 
 	idx = srcu_read_lock(&kvm->irq_srcu);
+	/*
+	 * 在以下使用kvm_irq_map_chip_pin():
+	 *   - arch/x86/kvm/ioapic.c|328| <<kvm_fire_mask_notifiers>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|571| <<kvm_irq_has_notifier>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|603| <<kvm_notify_acked_irq>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 *   - virt/kvm/eventfd.c|746| <<kvm_notify_irqfd_resampler>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+	 */
 	gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
 	if (gsi != -1) {
 		list_for_each_entry_srcu(resampler,
diff --git a/virt/kvm/irqchip.c b/virt/kvm/irqchip.c
index 6ccabfd32..3fe622eb6 100644
--- a/virt/kvm/irqchip.c
+++ b/virt/kvm/irqchip.c
@@ -18,6 +18,11 @@
 #include <linux/export.h>
 #include <trace/events/kvm.h>
 
+/*
+ * 在以下使用kvm_irq_map_gsi():
+ *   - virt/kvm/eventfd.c|313| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+ *   - virt/kvm/irqchip.c|83| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+ */
 int kvm_irq_map_gsi(struct kvm *kvm,
 		    struct kvm_kernel_irq_routing_entry *entries, int gsi)
 {
@@ -37,6 +42,13 @@ int kvm_irq_map_gsi(struct kvm *kvm,
 	return n;
 }
 
+/*
+ * 在以下使用kvm_irq_map_chip_pin():
+ *   - arch/x86/kvm/ioapic.c|328| <<kvm_fire_mask_notifiers>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ *   - virt/kvm/eventfd.c|571| <<kvm_irq_has_notifier>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ *   - virt/kvm/eventfd.c|603| <<kvm_notify_acked_irq>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ *   - virt/kvm/eventfd.c|746| <<kvm_notify_irqfd_resampler>> gsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);
+ */
 int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 {
 	struct kvm_irq_routing_table *irq_rt;
@@ -45,6 +57,10 @@ int kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin)
 	return irq_rt->chip[irqchip][pin];
 }
 
+/*
+ * 在以下使用kvm_send_userspace_msi():
+ *   - virt/kvm/kvm_main.c|5344| <<kvm_vm_ioctl(KVM_SIGNAL_MSI)>> r = kvm_send_userspace_msi(kvm, &msi);
+ */
 int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 {
 	struct kvm_kernel_irq_routing_entry route;
@@ -61,6 +77,22 @@ int kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi)
 	return kvm_set_msi(&route, kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false);
 }
 
+/*
+ * 在以下使用kvm_set_irq():
+ *   - arch/loongarch/kvm/vm.c|192| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+ *   - arch/powerpc/kvm/book3s.c|1022| <<kvm_set_irq>> int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
+ *   - arch/powerpc/kvm/book3s.c|1037| <<kvm_arch_set_irq_inatomic>> return kvm_set_irq(kvm, irq_source_id, irq_entry->gsi,
+ *   - arch/powerpc/kvm/book3s.c|1044| <<kvmppc_book3s_set_irq>> return kvm_set_irq(kvm, irq_source_id, e->gsi, level, line_status);
+ *   - arch/powerpc/kvm/powerpc.c|2161| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+ *   - arch/x86/kvm/i8254.c|251| <<pit_do_work>> kvm_set_irq(kvm, KVM_PIT_IRQ_SOURCE_ID, 0, 1, false);
+ *   - arch/x86/kvm/i8254.c|252| <<pit_do_work>> kvm_set_irq(kvm, KVM_PIT_IRQ_SOURCE_ID, 0, 0, false);
+ *   - arch/x86/kvm/irq.c|285| <<kvm_vm_ioctl_irq_line>> irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|96| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,
+ *   - virt/kvm/eventfd.c|98| <<irqfd_inject>> kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,
+ *   - virt/kvm/eventfd.c|101| <<irqfd_inject>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|130| <<irqfd_resampler_ack>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ *   - virt/kvm/eventfd.c|155| <<irqfd_resampler_shutdown>> kvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,
+ */
 /*
  * Return value:
  *  < 0   Interrupt was ignored (masked or not delivered for other reasons)
@@ -80,6 +112,11 @@ int kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,
 	 * writes to the unused one.
 	 */
 	idx = srcu_read_lock(&kvm->irq_srcu);
+	/*
+	 * 在以下使用kvm_irq_map_gsi():
+	 *   - virt/kvm/eventfd.c|313| <<irqfd_update>> n_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);
+	 *   - virt/kvm/irqchip.c|83| <<kvm_set_irq>> i = kvm_irq_map_gsi(kvm, irq_set, irq);
+	 */
 	i = kvm_irq_map_gsi(kvm, irq_set, irq);
 	srcu_read_unlock(&kvm->irq_srcu, idx);
 
@@ -124,6 +161,10 @@ void kvm_free_irq_routing(struct kvm *kvm)
 	free_irq_routing_table(rt);
 }
 
+/*
+ * 在以下使用setup_routing_entry():
+ *   - virt/kvm/irqchip.c|221| <<kvm_set_irq_routing>> r = setup_routing_entry(kvm, new, e, ue);
+ */
 static int setup_routing_entry(struct kvm *kvm,
 			       struct kvm_irq_routing_table *rt,
 			       struct kvm_kernel_irq_routing_entry *e,
@@ -145,6 +186,9 @@ static int setup_routing_entry(struct kvm *kvm,
 
 	e->gsi = gsi;
 	e->type = ue->type;
+	/*
+	 * 只在这里使用
+	 */
 	r = kvm_set_routing_entry(kvm, e, ue);
 	if (r)
 		return r;
@@ -165,6 +209,31 @@ bool __weak kvm_arch_can_set_irq_routing(struct kvm *kvm)
 	return true;
 }
 
+/*
+ * x86初始化的数据!!!
+ * 593 static const struct kvm_irq_routing_entry default_routing[] = {
+ * 594         ROUTING_ENTRY2(0), ROUTING_ENTRY2(1),
+ * 595         ROUTING_ENTRY2(2), ROUTING_ENTRY2(3),
+ * 596         ROUTING_ENTRY2(4), ROUTING_ENTRY2(5),
+ * 597         ROUTING_ENTRY2(6), ROUTING_ENTRY2(7),
+ * 598         ROUTING_ENTRY2(8), ROUTING_ENTRY2(9),
+ * 599         ROUTING_ENTRY2(10), ROUTING_ENTRY2(11),
+ * 600         ROUTING_ENTRY2(12), ROUTING_ENTRY2(13), 
+ * 601         ROUTING_ENTRY2(14), ROUTING_ENTRY2(15),
+ * 602         ROUTING_ENTRY1(16), ROUTING_ENTRY1(17),
+ * 603         ROUTING_ENTRY1(18), ROUTING_ENTRY1(19),
+ * 604         ROUTING_ENTRY1(20), ROUTING_ENTRY1(21),
+ * 605         ROUTING_ENTRY1(22), ROUTING_ENTRY1(23),
+ *
+ *
+ * 在以下使用kvm_set_irq_routing():
+ *   - arch/arm64/kvm/vgic/vgic-irqfd.c|153| <<kvm_vgic_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - arch/loongarch/kvm/intc/pch_pic.c|418| <<kvm_setup_default_irq_routing>> ret = kvm_set_irq_routing(kvm, entries, nr, 0);
+ *   - arch/powerpc/kvm/mpic.c|1649| <<mpic_set_default_irq_routing>> kvm_set_irq_routing(opp->kvm, routing, 0, 0);
+ *   - arch/riscv/kvm/vm.c|108| <<kvm_riscv_setup_default_irq_routing>> rc = kvm_set_irq_routing(kvm, ents, lines, 0);
+ *   - arch/x86/kvm/irq.c|603| <<kvm_setup_default_ioapic_and_pic_routing>> return kvm_set_irq_routing(kvm, default_routing,
+ *   - virt/kvm/kvm_main.c|5322| <<kvm_vm_ioctl(KVM_SET_GSI_ROUTING)>> r = kvm_set_irq_routing(kvm, entries, routing.nr,
+ */
 int kvm_set_irq_routing(struct kvm *kvm,
 			const struct kvm_irq_routing_entry *ue,
 			unsigned nr,
@@ -241,6 +310,10 @@ int kvm_set_irq_routing(struct kvm *kvm,
  * when userspace-driven IRQ routing is activated, and so that kvm->irq_routing
  * is guaranteed to be non-NULL.
  */
+/*
+ * 在以下使用kvm_init_irq_routing():
+ *   - virt/kvm/kvm_main.c|1219| <<kvm_create_vm>> r = kvm_init_irq_routing(kvm);
+ */
 int kvm_init_irq_routing(struct kvm *kvm)
 {
 	struct kvm_irq_routing_table *new;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index b7a0ae2a7..a3111257a 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -167,6 +167,11 @@ void vcpu_load(struct kvm_vcpu *vcpu)
 
 	__this_cpu_write(kvm_running_vcpu, vcpu);
 	preempt_notifier_register(&vcpu->preempt_notifier);
+	/*
+	 * 在以下使用kvm_arch_vcpu_load():
+	 *   - virt/kvm/kvm_main.c|170| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *   - virt/kvm/kvm_main.c|6379| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+	 */
 	kvm_arch_vcpu_load(vcpu, cpu);
 	put_cpu();
 }
@@ -213,11 +218,22 @@ static inline bool kvm_kick_many_cpus(struct cpumask *cpus, bool wait)
 	return true;
 }
 
+/*
+ * 在以下使用kvm_make_vcpu_request():
+ *   - virt/kvm/kvm_main.c|266| <<kvm_make_vcpus_request_mask>> kvm_make_vcpu_request(vcpu, req, cpus, me);
+ *   - virt/kvm/kvm_main.c|289| <<kvm_make_all_cpus_request>> kvm_make_vcpu_request(vcpu, req, cpus, me);
+ */
 static void kvm_make_vcpu_request(struct kvm_vcpu *vcpu, unsigned int req,
 				  struct cpumask *tmp, int current_cpu)
 {
 	int cpu;
 
+	/*
+	 * 在以下使用__kvm_make_request():
+	 *   - arch/s390/kvm/kvm-s390.c|4106| <<kvm_s390_sync_request>> __kvm_make_request(req, vcpu);
+	 *   - include/linux/kvm_host.h|2286| <<kvm_make_request>> __kvm_make_request(req, vcpu);
+	 *   - virt/kvm/kvm_main.c|227| <<kvm_make_vcpu_request>> __kvm_make_request(req, vcpu);
+	 */
 	if (likely(!(req & KVM_REQUEST_NO_ACTION)))
 		__kvm_make_request(req, vcpu);
 
@@ -241,6 +257,13 @@ static void kvm_make_vcpu_request(struct kvm_vcpu *vcpu, unsigned int req,
 	}
 }
 
+/*
+ * 在以下使用kvm_make_vcpus_request_mask():
+ *   - arch/riscv/kvm/tlb.c|331| <<make_xfence_request>> kvm_make_vcpus_request_mask(kvm, actual_req, vcpu_mask);
+ *   - arch/x86/kvm/hyperv.c|2176| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+ *   - arch/x86/kvm/hyperv.c|2209| <<kvm_hv_flush_tlb>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_HV_TLB_FLUSH, vcpu_mask);
+ *   - arch/x86/kvm/x86.c|11028| <<kvm_make_scan_ioapic_request_mask>> kvm_make_vcpus_request_mask(kvm, KVM_REQ_SCAN_IOAPIC, vcpu_bitmap);
+ */
 bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 				 unsigned long *vcpu_bitmap)
 {
@@ -258,6 +281,11 @@ bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 		vcpu = kvm_get_vcpu(kvm, i);
 		if (!vcpu)
 			continue;
+		/*
+		 * 在以下使用kvm_make_vcpu_request():
+		 *   - virt/kvm/kvm_main.c|266| <<kvm_make_vcpus_request_mask>> kvm_make_vcpu_request(vcpu, req, cpus, me);
+		 *   - virt/kvm/kvm_main.c|289| <<kvm_make_all_cpus_request>> kvm_make_vcpu_request(vcpu, req, cpus, me);
+		 */
 		kvm_make_vcpu_request(vcpu, req, cpus, me);
 	}
 
@@ -267,6 +295,31 @@ bool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,
 	return called;
 }
 
+/*
+ * 在以下使用kvm_make_all_cpus_request():
+ *   - arch/arm64/kvm/arm.c|920| <<kvm_arm_halt_guest>> kvm_make_all_cpus_request(kvm, KVM_REQ_SLEEP);
+ *   - arch/arm64/kvm/psci.c|183| <<kvm_prepare_system_event>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
+ *   - arch/arm64/kvm/vgic/vgic-mmio-v3.c|155| <<vgic_mmio_write_v3_misc>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_RELOAD_GICv4);
+ *   - arch/riscv/kvm/vcpu_sbi.c|154| <<kvm_riscv_vcpu_sbi_system_reset>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_SLEEP);
+ *   - arch/x86/kvm/hyperv.c|2163| <<kvm_hv_flush_tlb>> kvm_make_all_cpus_request(kvm, KVM_REQ_HV_TLB_FLUSH);
+ *   - arch/x86/kvm/mmu/mmu.c|1672| <<kvm_unmap_gfn_range>> kvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);
+ *   - arch/x86/kvm/mmu/mmu.c|2741| <<__kvm_mmu_prepare_zap_page>> kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);
+ *   - arch/x86/kvm/mmu/mmu.c|6744| <<kvm_mmu_zap_all_fast>> kvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);
+ *   - arch/x86/kvm/mmu/spte.c|154| <<kvm_track_host_mmio_mapping>> kvm_make_all_cpus_request(vcpu->kvm, KVM_REQ_OUTSIDE_GUEST_MODE);
+ *   - arch/x86/kvm/pmu.c|1144| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/vmx/posted_intr.c|349| <<vmx_pi_start_bypass>> kvm_make_all_cpus_request(kvm, KVM_REQ_UNBLOCK);
+ *   - arch/x86/kvm/vmx/tdx.c|292| <<tdx_no_vcpus_enter_start>> kvm_make_all_cpus_request(kvm, KVM_REQ_OUTSIDE_GUEST_MODE);
+ *   - arch/x86/kvm/vmx/tdx.c|1852| <<tdx_track>> kvm_make_all_cpus_request(kvm, KVM_REQ_OUTSIDE_GUEST_MODE);
+ *   - arch/x86/kvm/x86.c|3184| <<kvm_make_mclock_inprogress_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_MCLOCK_INPROGRESS);
+ *   - arch/x86/kvm/x86.c|7221| <<kvm_vm_ioctl_set_msr_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_RECALC_INTERCEPTS);
+ *   - arch/x86/kvm/x86.c|11033| <<kvm_make_scan_ioapic_request>> kvm_make_all_cpus_request(kvm, KVM_REQ_SCAN_IOAPIC);
+ *   - arch/x86/kvm/x86.c|11123| <<__kvm_set_or_clear_apicv_inhibit>> kvm_make_all_cpus_request(kvm, KVM_REQ_APICV_UPDATE);
+ *   - arch/x86/kvm/x86.c|13711| <<kvm_mmu_update_cpu_dirty_logging>> kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
+ *   - arch/x86/kvm/xen.c|113| <<kvm_xen_shared_info_init>> kvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);
+ *   - arch/x86/kvm/xen.c|1416| <<kvm_xen_hvm_config>> kvm_make_all_cpus_request(kvm, KVM_REQ_CLOCK_UPDATE);
+ *   - include/linux/kvm_host.h|922| <<kvm_vm_dead>> kvm_make_all_cpus_request(kvm, KVM_REQ_VM_DEAD);
+ *   - virt/kvm/kvm_main.c|314| <<kvm_flush_remote_tlbs>> || kvm_make_all_cpus_request(kvm, KVM_REQ_TLB_FLUSH))
+ */
 bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 {
 	struct kvm_vcpu *vcpu;
@@ -280,6 +333,11 @@ bool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req)
 	cpus = this_cpu_cpumask_var_ptr(cpu_kick_mask);
 	cpumask_clear(cpus);
 
+	/*
+	 * 在以下使用kvm_make_vcpu_request():
+	 *   - virt/kvm/kvm_main.c|266| <<kvm_make_vcpus_request_mask>> kvm_make_vcpu_request(vcpu, req, cpus, me);
+	 *   - virt/kvm/kvm_main.c|289| <<kvm_make_all_cpus_request>> kvm_make_vcpu_request(vcpu, req, cpus, me);
+	 */
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_make_vcpu_request(vcpu, req, cpus, me);
 
@@ -463,6 +521,10 @@ static void kvm_vcpu_init(struct kvm_vcpu *vcpu, struct kvm *kvm, unsigned id)
 		 task_pid_nr(current), id);
 }
 
+/*
+ * 在以下使用kvm_vcpu_destroy():
+ *   - virt/kvm/kvm_main.c|556| <<kvm_destroy_vcpus>> kvm_vcpu_destroy(vcpu);
+ */
 static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kvm_arch_vcpu_destroy(vcpu);
@@ -479,6 +541,16 @@ static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
+/*
+ * 在以下调用kvm_destroy_vcpus():
+ *   - arch/arm64/kvm/arm.c|257| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/loongarch/kvm/vm.c|73| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/mips/kvm/mips.c|171| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/powerpc/kvm/powerpc.c|490| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/riscv/kvm/vm.c|54| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/s390/kvm/kvm-s390.c|3510| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ *   - arch/x86/kvm/x86.c|15451| <<kvm_arch_destroy_vm>> kvm_destroy_vcpus(kvm);
+ */
 void kvm_destroy_vcpus(struct kvm *kvm)
 {
 	unsigned long i;
@@ -486,6 +558,17 @@ void kvm_destroy_vcpus(struct kvm *kvm)
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
 		kvm_vcpu_destroy(vcpu);
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		xa_erase(&kvm->vcpu_array, i);
 
 		/*
@@ -1111,6 +1194,10 @@ static inline struct kvm_io_bus *kvm_get_bus_for_destruction(struct kvm *kvm,
 					 !refcount_read(&kvm->users_count));
 }
 
+/*
+ * 在以下使用kvm_create_vm():
+ *   - virt/kvm/kvm_main.c|5567| <<kvm_dev_ioctl_create_vm>> kvm = kvm_create_vm(type, fdname);
+ */
 static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 {
 	struct kvm *kvm = kvm_arch_alloc_vm();
@@ -1130,6 +1217,17 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 	mutex_init(&kvm->slots_arch_lock);
 	spin_lock_init(&kvm->mn_invalidate_lock);
 	rcuwait_init(&kvm->mn_memslots_update_rcuwait);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_init(&kvm->vcpu_array);
 #ifdef CONFIG_KVM_GENERIC_MEMORY_ATTRIBUTES
 	xa_init(&kvm->mem_attr_array);
@@ -1232,6 +1330,11 @@ static struct kvm *kvm_create_vm(unsigned long type, const char *fdname)
 out_err_no_mmu_notifier:
 	kvm_disable_virtualization();
 out_err_no_disable:
+	/*
+	 * 在以下使用kvm_arch_destroy_vm():
+	 *   - virt/kvm/kvm_main.c|1329| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+	 *   - virt/kvm/kvm_main.c|1414| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+	 */
 	kvm_arch_destroy_vm(kvm);
 out_err_no_arch_destroy_vm:
 	WARN_ON_ONCE(!refcount_dec_and_test(&kvm->users_count));
@@ -1269,6 +1372,10 @@ static void kvm_destroy_devices(struct kvm *kvm)
 	}
 }
 
+/*
+ * 在以下使用kvm_destroy_vm():
+ *   - virt/kvm/kvm_main.c|1415| <<kvm_put_kvm>> kvm_destroy_vm(kvm);
+ */
 static void kvm_destroy_vm(struct kvm *kvm)
 {
 	int i;
@@ -1313,6 +1420,11 @@ static void kvm_destroy_vm(struct kvm *kvm)
 #else
 	kvm_flush_shadow_all(kvm);
 #endif
+	/*
+	 * 在以下使用kvm_arch_destroy_vm():
+	 *   - virt/kvm/kvm_main.c|1329| <<kvm_create_vm>> kvm_arch_destroy_vm(kvm);
+	 *   - virt/kvm/kvm_main.c|1414| <<kvm_destroy_vm>> kvm_arch_destroy_vm(kvm);
+	 */
 	kvm_arch_destroy_vm(kvm);
 	kvm_destroy_devices(kvm);
 	for (i = 0; i < kvm_arch_nr_memslot_as_ids(kvm); i++) {
@@ -1331,6 +1443,24 @@ static void kvm_destroy_vm(struct kvm *kvm)
 	mmdrop(mm);
 }
 
+/*
+ * 在以下使用kvm_get_kvm():
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1949| <<kvm_vm_ioctl_get_htab_fd>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1992| <<debugfs_htab_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_mmu_radix.c|1263| <<debugfs_radix_open>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|331| <<kvm_vm_ioctl_create_spapr_tce>> kvm_get_kvm(kvm);
+ *   - arch/powerpc/kvm/book3s_hv.c|2849| <<debugfs_timings_open>> kvm_get_kvm(vcpu->kvm);
+ *   - arch/s390/kvm/pci.c|448| <<kvm_s390_pci_register_kvm>> kvm_get_kvm(kvm);
+ *   - arch/x86/kvm/mmu/page_track.c|235| <<kvm_page_track_register_notifier>> kvm_get_kvm(kvm);
+ *   - arch/x86/kvm/svm/sev.c|1986| <<sev_migrate_from>> kvm_get_kvm(dst_kvm);
+ *   - arch/x86/kvm/svm/sev.c|2833| <<sev_vm_copy_enc_context_from>> kvm_get_kvm(source_kvm);
+ *   - drivers/s390/crypto/vfio_ap_ops.c|1840| <<vfio_ap_mdev_set_kvm>> kvm_get_kvm(kvm);
+ *   - virt/kvm/guest_memfd.c|533| <<__kvm_gmem_create>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4333| <<kvm_vm_ioctl_create_vcpu>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|4426| <<kvm_vcpu_ioctl_get_stats_fd>> kvm_get_kvm(vcpu->kvm);
+ *   - virt/kvm/kvm_main.c|4938| <<kvm_ioctl_create_device>> kvm_get_kvm(kvm);
+ *   - virt/kvm/kvm_main.c|5226| <<kvm_vm_ioctl_get_stats_fd>> kvm_get_kvm(kvm);
+ */
 void kvm_get_kvm(struct kvm *kvm)
 {
 	refcount_inc(&kvm->users_count);
@@ -1361,6 +1491,13 @@ EXPORT_SYMBOL_GPL(kvm_put_kvm);
  * its final owner.  In such cases, the caller is still actively using @kvm and
  * will fail miserably if the refcount unexpectedly hits zero.
  */
+/*
+ * 在以下使用kvm_put_kvm_no_destroy():
+ *   - arch/powerpc/kvm/book3s_64_mmu_hv.c|1959| <<kvm_vm_ioctl_get_htab_fd>> kvm_put_kvm_no_destroy(kvm);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|339| <<kvm_vm_ioctl_create_spapr_tce>> kvm_put_kvm_no_destroy(kvm);
+ *   - virt/kvm/kvm_main.c|4415| <<kvm_vm_ioctl_create_vcpu>> kvm_put_kvm_no_destroy(kvm);
+ *   - virt/kvm/kvm_main.c|5014| <<kvm_ioctl_create_device>> kvm_put_kvm_no_destroy(kvm);
+ */
 void kvm_put_kvm_no_destroy(struct kvm *kvm)
 {
 	WARN_ON(refcount_dec_and_test(&kvm->users_count));
@@ -1733,6 +1870,18 @@ static void kvm_commit_memory_region(struct kvm *kvm,
 
 	if ((old_flags ^ new_flags) & KVM_MEM_LOG_DIRTY_PAGES) {
 		int change = (new_flags & KVM_MEM_LOG_DIRTY_PAGES) ? 1 : -1;
+		/*
+		 * 在以下使用kvm->nr_memslots_dirty_logging:
+		 *   - arch/arm64/kvm/guest.c|1005| <<kvm_vm_ioctl_mte_copy_tags>> if (write && atomic_read(&kvm->nr_memslots_dirty_logging)) {
+		 *   - arch/x86/kvm/mmu/mmu.c|7634| <<kvm_mmu_sp_dirty_logging_enabled>> if (!atomic_read(&kvm->nr_memslots_dirty_logging))
+		 *   - arch/x86/kvm/vmx/vmx.c|4605| <<vmx_secondary_exec_control>> if (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+		 *   - arch/x86/kvm/vmx/vmx.c|8250| <<vmx_update_cpu_dirty_logging>> if (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))
+		 *   - arch/x86/kvm/x86.c|13483| <<kvm_mmu_update_cpu_dirty_logging>> nr_slots = atomic_read(&kvm->nr_memslots_dirty_logging);
+		 *   - virt/kvm/kvm_main.c|1736| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+		 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+		 *   - virt/kvm/kvm_main.c|1737| <<kvm_commit_memory_region>> atomic_set(&kvm->nr_memslots_dirty_logging,
+		 *                                         atomic_read(&kvm->nr_memslots_dirty_logging) + change);
+		 */
 		atomic_set(&kvm->nr_memslots_dirty_logging,
 			   atomic_read(&kvm->nr_memslots_dirty_logging) + change);
 	}
@@ -3114,6 +3263,11 @@ struct page *__gfn_to_page(struct kvm *kvm, gfn_t gfn, bool write)
 }
 EXPORT_SYMBOL_FOR_KVM_INTERNAL(__gfn_to_page);
 
+/*
+ * 在以下使用__kvm_vcpu_map():
+ *   - include/linux/kvm_host.h|1419| <<kvm_vcpu_map>> return __kvm_vcpu_map(vcpu, gpa, map, true);
+ *   - include/linux/kvm_host.h|1425| <<kvm_vcpu_map_readonly>> return __kvm_vcpu_map(vcpu, gpa, map, false);
+ */
 int __kvm_vcpu_map(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
 		   bool writable)
 {
@@ -4004,6 +4158,17 @@ void kvm_vcpu_on_spin(struct kvm_vcpu *me, bool yield_to_kernel_mode)
 		if (idx == me->vcpu_idx)
 			continue;
 
+		/*
+		 * 在以下使用kvm->vcpu_array:
+		 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+		 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+		 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+		 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+		 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+		 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+		 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+		 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+		 */
 		vcpu = xa_load(&kvm->vcpu_array, idx);
 		if (!READ_ONCE(vcpu->ready))
 			continue;
@@ -4223,6 +4388,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 	}
 
 	vcpu->vcpu_idx = atomic_read(&kvm->online_vcpus);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
 	WARN_ON_ONCE(r == -EBUSY);
 	if (r)
@@ -4258,6 +4434,17 @@ static int kvm_vm_ioctl_create_vcpu(struct kvm *kvm, unsigned long id)
 kvm_put_xa_erase:
 	mutex_unlock(&vcpu->mutex);
 	kvm_put_kvm_no_destroy(kvm);
+	/*
+	 * 在以下使用kvm->vcpu_array:
+	 *   - include/linux/kvm_host.h|1020| <<kvm_get_vcpu>> return xa_load(&kvm->vcpu_array, i);
+	 *   - include/linux/kvm_host.h|1025| <<kvm_for_each_vcpu>> xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
+	 *   - virt/kvm/kvm_main.c|557| <<kvm_destroy_vcpus>> xa_erase(&kvm->vcpu_array, i);
+	 *   - virt/kvm/kvm_main.c|565| <<kvm_destroy_vcpus>> WARN_ON_ONCE(xa_load(&kvm->vcpu_array, i) || kvm_get_vcpu(kvm, i));
+	 *   - virt/kvm/kvm_main.c|1205| <<kvm_create_vm>> xa_init(&kvm->vcpu_array);
+	 *   - virt/kvm/kvm_main.c|4100| <<kvm_vcpu_on_spin>> vcpu = xa_load(&kvm->vcpu_array, idx);
+	 *   - virt/kvm/kvm_main.c|4319| <<kvm_vm_ioctl_create_vcpu>> r = xa_insert(&kvm->vcpu_array, vcpu->vcpu_idx, vcpu, GFP_KERNEL_ACCOUNT);
+	 *   - virt/kvm/kvm_main.c|4354| <<kvm_vm_ioctl_create_vcpu>> xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
+	 */
 	xa_erase(&kvm->vcpu_array, vcpu->vcpu_idx);
 unlock_vcpu_destroy:
 	mutex_unlock(&kvm->lock);
@@ -6376,6 +6563,11 @@ static void kvm_sched_in(struct preempt_notifier *pn, int cpu)
 	WRITE_ONCE(vcpu->ready, false);
 
 	__this_cpu_write(kvm_running_vcpu, vcpu);
+	/*
+	 * 在以下使用kvm_arch_vcpu_load():
+	 *   - virt/kvm/kvm_main.c|170| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+	 *   - virt/kvm/kvm_main.c|6379| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+	 */
 	kvm_arch_vcpu_load(vcpu, cpu);
 
 	WRITE_ONCE(vcpu->scheduled_out, false);
diff --git a/virt/lib/irqbypass.c b/virt/lib/irqbypass.c
index 62c160200..18da9af87 100644
--- a/virt/lib/irqbypass.c
+++ b/virt/lib/irqbypass.c
@@ -22,6 +22,53 @@
 MODULE_LICENSE("GPL v2");
 MODULE_DESCRIPTION("IRQ bypass manager utility module");
 
+/*
+ * 在以下使用irq_bypass_producer->add_consumer:
+ *   - virt/lib/irqbypass.c|40| <<__connect>> if (prod->add_consumer)
+ *   - virt/lib/irqbypass.c|41| <<__connect>> ret = prod->add_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->del_consumer:
+ *   - virt/lib/irqbypass.c|45| <<__connect>> if (ret && prod->del_consumer)
+ *   - virt/lib/irqbypass.c|46| <<__connect>> prod->del_consumer(prod, cons);
+ *   - virt/lib/irqbypass.c|72| <<__disconnect>> if (prod->del_consumer)
+ *   - virt/lib/irqbypass.c|73| <<__disconnect>> prod->del_consumer(prod, cons);
+ *
+ * 在以下使用irq_bypass_producer->stop:
+ *   - virt/lib/irqbypass.c|35| <<__connect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|36| <<__connect>> prod->stop(prod);
+ *   - virt/lib/irqbypass.c|65| <<__disconnect>> if (prod->stop)
+ *   - virt/lib/irqbypass.c|66| <<__disconnect>> prod->stop(prod);
+ *
+ * 在以下使用irq_bypass_producer->start:
+ *   - virt/lib/irqbypass.c|51| <<__connect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|52| <<__connect>> prod->start(prod);
+ *   - virt/lib/irqbypass.c|77| <<__disconnect>> if (prod->start)
+ *   - virt/lib/irqbypass.c|78| <<__disconnect>> prod->start(prod);
+ *
+ *
+ * 在以下使用irq_bypass_consumer->add_producer:
+ *   - virt/kvm/eventfd.c|478| <<kvm_irqfd_assign>> irqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;
+ *   - virt/lib/irqbypass.c|44| <<__connect>> ret = cons->add_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->del_producer:
+ *   - virt/kvm/eventfd.c|479| <<kvm_irqfd_assign>> irqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;
+ *   - virt/lib/irqbypass.c|70| <<__disconnect>> cons->del_producer(cons, prod);
+ *   - virt/lib/irqbypass.c|168| <<irq_bypass_register_consumer>> if (!consumer->add_producer || !consumer->del_producer)
+ *
+ * 在以下使用irq_bypass_consumer->stop:
+ *   - virt/lib/irqbypass.c|37| <<__connect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|38| <<__connect>> cons->stop(cons);
+ *   - virt/lib/irqbypass.c|67| <<__disconnect>> if (cons->stop)
+ *   - virt/lib/irqbypass.c|68| <<__disconnect>> cons->stop(cons);
+ *
+ * 在以下使用irq_bypass_consumer->start:
+ *   - virt/lib/irqbypass.c|49| <<__connect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|50| <<__connect>> cons->start(cons);
+ *   - virt/lib/irqbypass.c|75| <<__disconnect>> if (cons->start)
+ *   - virt/lib/irqbypass.c|76| <<__disconnect>> cons->start(cons);
+ */
+
 static DEFINE_XARRAY(producers);
 static DEFINE_XARRAY(consumers);
 static DEFINE_MUTEX(lock);
@@ -90,6 +137,11 @@ static void __disconnect(struct irq_bypass_producer *prod,
  * Add the provided IRQ producer to the set of producers and connect with the
  * consumer with a matching eventfd, if one exists.
  */
+/*
+ * 在以下使用irq_bypass_register_producer():
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|515| <<vfio_msi_set_vector_signal>> ret = irq_bypass_register_producer(&ctx->producer, trigger, irq);
+ *   - drivers/vhost/vdpa.c|215| <<vhost_vdpa_setup_vq_irq>> ret = irq_bypass_register_producer(&vq->call_ctx.producer, vq->call_ctx.ctx, irq);
+ */
 int irq_bypass_register_producer(struct irq_bypass_producer *producer,
 				 struct eventfd_ctx *eventfd, int irq)
 {
@@ -130,6 +182,11 @@ EXPORT_SYMBOL_GPL(irq_bypass_register_producer);
  * even if registration was unsuccessful).  Disconnect from the associated
  * consumer, if one exists.
  */
+/*
+ * 在以下使用irq_bypass_unregister_producer():
+ *   - drivers/vfio/pci/vfio_pci_intrs.c|459| <<vfio_msi_set_vector_signal>> irq_bypass_unregister_producer(&ctx->producer);
+ *   - drivers/vhost/vdpa.c|226| <<vhost_vdpa_unsetup_vq_irq>> irq_bypass_unregister_producer(&vq->call_ctx.producer);
+ */
 void irq_bypass_unregister_producer(struct irq_bypass_producer *producer)
 {
 	unsigned long index = (unsigned long)producer->eventfd;
@@ -155,6 +212,10 @@ EXPORT_SYMBOL_GPL(irq_bypass_unregister_producer);
  * Add the provided IRQ consumer to the set of consumers and connect with the
  * producer with a matching eventfd, if one exists.
  */
+/*
+ * 在以下使用irq_bypass_register_consumer():
+ *   - virt/kvm/eventfd.c|482| <<kvm_irqfd_assign>> ret = irq_bypass_register_consumer(&irqfd->consumer, irqfd->eventfd);
+ */
 int irq_bypass_register_consumer(struct irq_bypass_consumer *consumer,
 				 struct eventfd_ctx *eventfd)
 {
-- 
2.50.1 (Apple Git-155)

