CVE-2022-39189

An issue was discovered the x86 KVM subsystem in the Linux kernel before
5.18.17. Unprivileged guest users can compromise the guest kernel because TLB
flush operations are mishandled in certain KVM_VCPU_PREEMPTED situations.

This issue is because of feature 'KVM_VCPU_FLUSH_TLB'.

Without the 'KVM_VCPU_FLUSH_TLB', the guest mode vcpu sends IPI to all involved
vcpus to flush the tlb. The native_flush_tlb_multi() hangs/waits until all
involved vcpus have tlb flushed (this is a requirement of synchronous ipi). The
guest mode vcpus concludes that memory mappings are up-to-date only after the
ipi call finishes. All vcpus should have the same view of guest gva to gpa
mapping after that barrier.

The bad thing is: the vcpu waits for the vcpus that are even preempted and not
in guest mode. The vcpu waits for extra time that is not expected.

With the 'KVM_VCPU_FLUSH_TLB', the guest mode vcpu sends IPI to vcpus that are
not preempted (e.g., line 673-677). The vcpu does not wait for preempted vcpus
because they are running in hypervisor mode.

652 static void kvm_flush_tlb_multi(const struct cpumask *cpumask,
653                         const struct flush_tlb_info *info)
654 {
655         u8 state;
656         int cpu;
657         struct kvm_steal_time *src;
658         struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
659
660         cpumask_copy(flushmask, cpumask);
661         /*
662          * We have to call flush only on online vCPUs. And
663          * queue flush_on_enter for pre-empted vCPUs
664          */
665         for_each_cpu(cpu, flushmask) {
666                 /*
667                  * The local vCPU is never preempted, so we do not explicitly
668                  * skip check for local vCPU - it will never be cleared from
669                  * flushmask.
670                  */
671                 src = &per_cpu(steal_time, cpu);
672                 state = READ_ONCE(src->preempted);
673                 if ((state & KVM_VCPU_PREEMPTED)) {
674                         if (try_cmpxchg(&src->preempted, &state,
675                                         state | KVM_VCPU_FLUSH_TLB))
676                                 __cpumask_clear_cpu(cpu, flushmask);
677                 }
678         }
679
680         native_flush_tlb_multi(flushmask, info);
681 }

The preempted vcpus may pro-actively flush the tlbs before entering into the
guest mode. Therefore, it is not required for the guest mode initiator vcpu to
wait for the preempted ones.

3049 static void vmx_flush_tlb_guest(struct kvm_vcpu *vcpu)
3050 {
3051         /*
3052          * vpid_sync_context() is a nop if vpid==0, e.g. if enable_vpid==0 or a
3053          * vpid couldn't be allocated for this vCPU.  VM-Enter and VM-Exit are
3054          * required to flush GVA->{G,H}PA mappings from the TLB if vpid is
3055          * disabled (VM-Enter with vpid enabled and vpid==0 is disallowed),
3056          * i.e. no explicit INVVPID is necessary.
3057          */
3058         vpid_sync_context(vmx_get_current_vpid(vcpu));
3059 }


Unfortunately, there is a corner case in instruction emulation, e.g., the below
callstack captured on 4.14 KVM.

1. The VCPU B traps to KVM to emulate one instruction. In order to do the
emulation, KVM may translate the gva (arguments to the instruction to be
emulated) to gpa.

2. The gva is translated to gpa (e.g., via paging64_gva_to_gpa() as in
callstack).

3. Now the VCPU A has updated the guest memory mapping (e.g., page table), and
would like all VM VCPUs to flush the tlb (via ipi). Since VCPU B is preempted,
VCPU will not wait for B.

4. Now VCPU B uses the old gpa (obtained from old gva->gpa mapping) to emulate
the instruction. This is a bug!

5. As a result, the VCPU B is still using old mapping (tlb), although VPCU A
thinks all other VCPUs have flushed the tlb and are with new mappings.


paging64_gva_to_gpa
emulator_read_write
emulator_read_emulated
segmented_read.isra.54
x86_emulate_insn
x86_emulate_instruction
kvm_emulate_instruction
handle_io
vmx_handle_exit
__dta_vcpu_enter_guest_1387
kvm_arch_vcpu_ioctl_run
kvm_vcpu_ioctl
do_vfs_ioctl
sys_ioctl
do_syscall_64
entry_SYSCALL_64_after_hwframe



    VCPU A                       VCPU B

                            traps to KVM to emulate an instruction,
                            gva is translated to gpa via old mapping

                            the vcpu B is preempted

 the gva->gpa mapping is updated
 tlbflush ipi to all vcpus

 the vcpu B is skipped because it is preempted
 tlbflush done!

                            the preempted vcpu B may still uses gpa derived from old mapping

---------------------------------------------

The fix is in below patch.

KVM: x86: do not report a vCPU as preempted outside instruction boundaries
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=6cd88243c7e03845a450795e134b488fc2afb736


The core idea is to do not set vcpu as preempted, unless the vcpu exits due to
external interrupt.

diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index f5aeade623d61..14e01178a753c 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -6547,6 +6547,7 @@ static void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu)
 		return;

 	handle_interrupt_nmi_irqoff(vcpu, gate_offset(desc));
+	vcpu->arch.at_instruction_boundary = true;
 }


E.g., when the exit reason is EXIT_REASON_EXTERNAL_INTERRUPT.

6647 static void vmx_handle_exit_irqoff(struct kvm_vcpu *vcpu)
6648 {
6649         struct vcpu_vmx *vmx = to_vmx(vcpu);
6650 
6651         if (vmx->emulation_required)
6652                 return;
6653 
6654         if (vmx->exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)
6655                 handle_external_interrupt_irqoff(vcpu);
6656         else if (vmx->exit_reason.basic == EXIT_REASON_EXCEPTION_NMI)
6657                 handle_exception_nmi_irqoff(vmx);
6658 }

---------------------------------------------

While I did not evaluate the performance overhead of fix, the method mentioned
in below patchset should help.

https://lore.kernel.org/all/20220419153155.11504-1-guang.zeng@intel.com/


References:

https://bugs.chromium.org/p/project-zero/issues/detail?id=2309
