From 1767924a76c05169f9d477374f8050b3578fd986 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Fri, 28 Sep 2018 10:58:15 +0800
Subject: [PATCH 1/1] block: comment for block and drivers for 4.17.14

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 block/bio.c               |   1 +
 block/blk-cgroup.c        |  23 ++
 block/blk-core.c          | 184 ++++++++-
 block/blk-flush.c         |  21 +
 block/blk-integrity.c     |  15 +
 block/blk-merge.c         |  58 +++
 block/blk-mq-cpumap.c     |  21 +
 block/blk-mq-pci.c        |   9 +
 block/blk-mq-sched.c      | 268 ++++++++++++-
 block/blk-mq-sched.h      |  76 ++++
 block/blk-mq-tag.c        | 172 +++++++++
 block/blk-mq-tag.h        |  62 ++-
 block/blk-mq-virtio.c     |   6 +
 block/blk-mq.c            | 959 +++++++++++++++++++++++++++++++++++++++++++++-
 block/blk-mq.h            |  61 ++-
 block/blk-settings.c      |  12 +
 block/blk-softirq.c       |  73 +++-
 block/blk-stat.c          |  77 +++-
 block/blk-stat.h          |  22 +-
 block/blk-throttle.c      |   3 +
 block/blk-timeout.c       |  43 ++-
 block/blk.h               |   4 +
 block/elevator.c          | 402 ++++++++++++++++++-
 block/kyber-iosched.c     | 313 ++++++++++++++-
 block/mq-deadline.c       | 331 +++++++++++++++-
 block/noop-iosched.c      |  27 ++
 block/partition-generic.c |  10 +
 include/linux/blk-mq.h    | 136 ++++++-
 include/linux/blk_types.h |   1 +
 include/linux/blkdev.h    | 226 ++++++++++-
 include/linux/elevator.h  |   5 +
 include/linux/sbitmap.h   | 147 ++++++-
 lib/sbitmap.c             | 208 +++++++++-
 33 files changed, 3892 insertions(+), 84 deletions(-)

diff --git a/block/bio.c b/block/bio.c
index 9f7fa24..d3e6e3c 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -838,6 +838,7 @@ int bio_add_page(struct bio *bio, struct page *page,
 	/*
 	 * cloned bio must not modify vec list
 	 */
+	/* BIO_CLONED: doesn't own data */
 	if (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))
 		return 0;
 
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index eb85cb8..6544a98 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -89,9 +89,16 @@ static void blkg_free(struct blkcg_gq *blkg)
  *
  * Allocate a new blkg assocating @blkcg and @q.
  */
+/*
+ * called by:
+ *   - blkg_create()
+ *   - __acquires()
+ *   - blkcg_init_queue()
+ */
 static struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,
 				   gfp_t gfp_mask)
 {
+	/* association between a blk cgroup and a request queue */
 	struct blkcg_gq *blkg;
 	int i;
 
@@ -168,6 +175,12 @@ EXPORT_SYMBOL_GPL(blkg_lookup_slowpath);
  * If @new_blkg is %NULL, this function tries to allocate a new one as
  * necessary using %GFP_NOWAIT.  @new_blkg is always consumed on return.
  */
+/*
+ * called by:
+ *   - blkg_lookup_create()
+ *   - __acquires()
+ *   - blkcg_init_queue()
+ */
 static struct blkcg_gq *blkg_create(struct blkcg *blkcg,
 				    struct request_queue *q,
 				    struct blkcg_gq *new_blkg)
@@ -1165,8 +1178,12 @@ blkcg_css_alloc(struct cgroup_subsys_state *parent_css)
  * RETURNS:
  * 0 on success, -errno on failure.
  */
+/*
+ * called only by blk_alloc_queue_node()
+ */
 int blkcg_init_queue(struct request_queue *q)
 {
+	/* association between a blk cgroup and a request queue */
 	struct blkcg_gq *new_blkg, *blkg;
 	bool preloaded;
 	int ret;
@@ -1438,6 +1455,12 @@ EXPORT_SYMBOL_GPL(blkcg_deactivate_policy);
  * Register @pol with blkcg core.  Might sleep and @pol may be modified on
  * successful registration.  Returns 0 on success and -errno on failure.
  */
+/*
+ * called by:
+ *   - bfq_init()    -- blkcg_policy_register(&blkcg_policy_bfq);
+ *   - throtl_init() -- blkcg_policy_register(&blkcg_policy_throtl);
+ *   - cfq_init()    -- blkcg_policy_register(&blkcg_policy_cfq);
+ */
 int blkcg_policy_register(struct blkcg_policy *pol)
 {
 	struct blkcg *blkcg;
diff --git a/block/blk-core.c b/block/blk-core.c
index 77938b5..bcfa486 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -410,6 +410,11 @@ EXPORT_SYMBOL(blk_stop_queue);
  *     and blkcg_exit_queue() to be called with queue lock initialized.
  *
  */
+/*
+ * called only by:
+ *   - block/blk-core.c|783| <<blk_cleanup_queue>> blk_sync_queue(q);
+ *   - drivers/md/md.c|5865| <<mddev_detach>> blk_sync_queue(mddev->queue);
+ */
 void blk_sync_queue(struct request_queue *q)
 {
 	del_timer_sync(&q->timeout);
@@ -972,10 +977,15 @@ static void blk_queue_usage_counter_release(struct percpu_ref *ref)
 	wake_up_all(&q->mq_freeze_wq);
 }
 
+/*
+ * 只在一处被设置为request_queue q->timeout
+ *   - block/blk-core.c|1036| <<blk_alloc_queue_node>> timer_setup(&q->timeout, blk_rq_timed_out_timer, 0);
+ */
 static void blk_rq_timed_out_timer(struct timer_list *t)
 {
 	struct request_queue *q = from_timer(q, t, timeout);
 
+	/* 根据种类, blk_timeout_work()或者blk_mq_timeout_work() */
 	kblockd_schedule_work(&q->timeout_work);
 }
 
@@ -992,6 +1002,11 @@ static void blk_rq_timed_out_timer(struct timer_list *t)
  * crash in the blkcg code. This function namely calls blkcg_init_queue() and
  * the queue lock pointer must be set before blkcg_init_queue() is called.
  */
+/*
+ * 分配request_queue
+ *
+ * mq可以忽略lock
+ */
 struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
 					   spinlock_t *lock)
 {
@@ -1014,6 +1029,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
 	if (!q->backing_dev_info)
 		goto fail_split;
 
+	/* 分配struct blk_queue_stats */
 	q->stats = blk_alloc_queue_stats();
 	if (!q->stats)
 		goto fail_stats;
@@ -1062,11 +1078,15 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
 	 * Init percpu_ref in atomic mode so that it's faster to shutdown.
 	 * See blk_register_queue() for details.
 	 */
+	/*
+	 * 当refcount是0的时候调用release=blk_queue_usage_counter_release()
+	 */
 	if (percpu_ref_init(&q->q_usage_counter,
 				blk_queue_usage_counter_release,
 				PERCPU_REF_INIT_ATOMIC, GFP_KERNEL))
 		goto fail_bdi;
 
+	/* initialize blkcg part of request queue */
 	if (blkcg_init_queue(q))
 		goto fail_ref;
 
@@ -1148,7 +1168,14 @@ EXPORT_SYMBOL(blk_init_queue_node);
 
 static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio);
 
-
+/*
+ * 对于mq的设备:
+ *     如果queue的数量大于1, 默认用none (不是noop!)
+ *     如果queue的数量等于1, 默认用mq-deadline
+ *
+ * 对于sq的设备:
+ *    默认用CONFIG_DEFAULT_IOSCHED
+ */
 int blk_init_allocated_queue(struct request_queue *q)
 {
 	WARN_ON_ONCE(q->mq_ops);
@@ -1804,6 +1831,13 @@ void blk_put_request(struct request *req)
 }
 EXPORT_SYMBOL(blk_put_request);
 
+/*
+ * called by:
+ *   - blk_attempt_plug_merge()
+ *   - blk_queue_bio()
+ *   - blk_mq_sched_try_merge()
+ *   - blk_mq_attempt_merge()
+ */
 bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 			    struct bio *bio)
 {
@@ -1826,6 +1860,13 @@ bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 	return true;
 }
 
+/*
+ * called by:
+ *   - blk_attempt_plug_merge()
+ *   - blk_queue_bio()
+ *   - blk_mq_sched_try_merge()
+ *   - blk_mq_attempt_merge()
+ */
 bool bio_attempt_front_merge(struct request_queue *q, struct request *req,
 			     struct bio *bio)
 {
@@ -1850,6 +1891,12 @@ bool bio_attempt_front_merge(struct request_queue *q, struct request *req,
 	return true;
 }
 
+/*
+ * called by:
+ *   - blk_attempt_plug_merge()
+ *   - blk_mq_sched_try_merge()
+ *   - blk_mq_attempt_merge()
+ */
 bool bio_attempt_discard_merge(struct request_queue *q, struct request *req,
 		struct bio *bio)
 {
@@ -1896,10 +1943,30 @@ bool bio_attempt_discard_merge(struct request_queue *q, struct request *req,
  *
  * Caller must ensure !blk_queue_nomerges(q) beforehand.
  */
+/*
+ * called by:
+ *   - blk_queue_bio()
+ *   - blk_mq_make_request()
+ *
+ * 试一下能不能和current->plug里的已经存在的request们merge
+ * 如果q->mq_ops存在, 使用mq_list, 否则使用list
+ */
 bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
 			    unsigned int *request_count,
 			    struct request **same_queue_rq)
 {
+	/*
+	 * blk_plug permits building a queue of related requests by holding the I/O
+	 * fragments for a short period. This allows merging of sequential requests
+	 * into single larger request. As the requests are moved from a per-task list to
+	 * the device's request_queue in a batch, this results in improved scalability
+	 * as the lock contention for request_queue lock is reduced.
+	 *
+	 * It is ok not to disable preemption when adding the request to the plug list
+	 * or when attempting a merge, because blk_schedule_flush_list() will only flush
+	 * the plug list when the task sleeps by itself. For details, please see
+	 * schedule() where blk_schedule_flush_plug() is called.
+	 */
 	struct blk_plug *plug;
 	struct request *rq;
 	struct list_head *plug_list;
@@ -2026,6 +2093,10 @@ static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)
 	 * any locks.
 	 */
 	if (!blk_queue_nomerges(q)) {
+		/*
+		 * 试一下能不能和current->plug里的已经存在的request们merge
+		 * 如果q->mq_ops存在, 使用mq_list, 否则使用list
+		 */
 		if (blk_attempt_plug_merge(q, bio, &request_count, NULL))
 			return BLK_QC_T_NONE;
 	} else
@@ -2572,6 +2643,15 @@ blk_qc_t submit_bio(struct bio *bio)
 }
 EXPORT_SYMBOL(submit_bio);
 
+/*
+ * called by:
+ *   - drivers/nvme/target/io-cmd.c|89| <<nvmet_execute_rw>> blk_poll(bdev_get_queue(req->ns->bdev), cookie);
+ *   - fs/block_dev.c|240| <<__blkdev_direct_IO_simple>> !blk_poll(bdev_get_queue(bdev), qc))
+ *   - fs/block_dev.c|406| <<__blkdev_direct_IO>> !blk_poll(bdev_get_queue(bdev), qc))
+ *   - fs/direct-io.c|521| <<dio_await_one>> !blk_poll(dio->bio_disk->queue, dio->bio_cookie))
+ *   - fs/iomap.c|1076| <<iomap_dio_rw>> !blk_poll(dio->submit.last_queue,
+ *   - mm/page_io.c|411| <<swap_readpage>> if (!blk_poll(disk->queue, qc))
+ */
 bool blk_poll(struct request_queue *q, blk_qc_t cookie)
 {
 	if (!q->poll_fn || !blk_qc_t_valid(cookie))
@@ -2628,6 +2708,10 @@ static int blk_cloned_rq_check_limits(struct request_queue *q,
  * @q:  the queue to submit the request
  * @rq: the request being queued
  */
+/*
+ * called only by:
+ *   - drivers/md/dm-rq.c|410| <<dm_dispatch_clone_request>> r = blk_insert_cloned_request(clone->q, clone);
+ */
 blk_status_t blk_insert_cloned_request(struct request_queue *q, struct request *rq)
 {
 	unsigned long flags;
@@ -2816,6 +2900,10 @@ void blk_account_io_start(struct request *rq, bool new_io)
 	part_stat_unlock();
 }
 
+/*
+ * called only by:
+ *   - blk_peek_request()
+ */
 static struct request *elv_next_request(struct request_queue *q)
 {
 	struct request *rq;
@@ -2959,6 +3047,11 @@ struct request *blk_peek_request(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_peek_request);
 
+/*
+ * called only by blk_start_request()
+ *
+ * 把request从rq->queuelist删除
+ */
 static void blk_dequeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -2966,6 +3059,7 @@ static void blk_dequeue_request(struct request *rq)
 	BUG_ON(list_empty(&rq->queuelist));
 	BUG_ON(ELV_ON_HASH(rq));
 
+	/* 把request从rq->queuelist删除 */
 	list_del_init(&rq->queuelist);
 
 	/*
@@ -3556,12 +3650,14 @@ int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
 }
 EXPORT_SYMBOL_GPL(blk_rq_prep_clone);
 
+/* 把一个work放到kblockd_workqueue */
 int kblockd_schedule_work(struct work_struct *work)
 {
 	return queue_work(kblockd_workqueue, work);
 }
 EXPORT_SYMBOL(kblockd_schedule_work);
 
+/* 把一个work放到某cpu的kblockd_workqueue */
 int kblockd_schedule_work_on(int cpu, struct work_struct *work)
 {
 	return queue_work_on(cpu, kblockd_workqueue, work);
@@ -3571,6 +3667,7 @@ EXPORT_SYMBOL(kblockd_schedule_work_on);
 int kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork,
 				unsigned long delay)
 {
+	/* delay: number of jiffies to wait before queueing */
 	return mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);
 }
 EXPORT_SYMBOL(kblockd_mod_delayed_work_on);
@@ -3589,6 +3686,12 @@ EXPORT_SYMBOL(kblockd_mod_delayed_work_on);
  *   plug. By flushing the pending I/O when the process goes to sleep, we avoid
  *   this kind of deadlock.
  */
+/*
+ * plug的故事似乎是这样:
+ * 1. 先blk_start_plug(), 好多好多地方都调用了blk_start_plug(), 参数plug一般是caller的本地变量
+ * 2. 做好多操作
+ * 3. 最后blk_finish_plug()
+ */
 void blk_start_plug(struct blk_plug *plug)
 {
 	struct task_struct *tsk = current;
@@ -3610,11 +3713,15 @@ void blk_start_plug(struct blk_plug *plug)
 }
 EXPORT_SYMBOL(blk_start_plug);
 
+/*
+ * called used by blk_flush_plug_list() as sorting comparasion
+ */
 static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
 {
 	struct request *rqa = container_of(a, struct request, queuelist);
 	struct request *rqb = container_of(b, struct request, queuelist);
 
+	/* blk_rq_pos(): the current sector */
 	return !(rqa->q < rqb->q ||
 		(rqa->q == rqb->q && blk_rq_pos(rqa) < blk_rq_pos(rqb)));
 }
@@ -3625,6 +3732,11 @@ static int plug_rq_cmp(void *priv, struct list_head *a, struct list_head *b)
  * additional stack usage in driver dispatch, in places where the originally
  * plugger did not intend it.
  */
+/*
+ * called only by blk_flush_plug_list() 两处
+ *
+ * 根据from_schedule决定是用blk_run_queue_async()还是__blk_run_queue()
+ */
 static void queue_unplugged(struct request_queue *q, unsigned int depth,
 			    bool from_schedule)
 	__releases(q->queue_lock)
@@ -3640,11 +3752,17 @@ static void queue_unplugged(struct request_queue *q, unsigned int depth,
 	spin_unlock(q->queue_lock);
 }
 
+/*
+ * called only by blk_flush_plug_list()
+ *
+ * 把plug->cb_list中的entry删掉, 遍历每一个entry (struct blk_plug_cb) 调用callback
+ */
 static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
 {
 	LIST_HEAD(callbacks);
 
 	while (!list_empty(&plug->cb_list)) {
+		/* 从plug->cb_list加到callbacks */
 		list_splice_init(&plug->cb_list, &callbacks);
 
 		while (!list_empty(&callbacks)) {
@@ -3657,6 +3775,18 @@ static void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/block/drbd/drbd_req.c|1313| <<drbd_check_plugged>>
+ *   - drivers/block/umem.c|522| <<mm_check_plugged>>
+ *   - drivers/md/raid1.c|1513| <<raid1_write_request>>
+ *   - drivers/md/raid10.c|1290| <<raid10_write_one_disk>>
+ *   - drivers/md/raid5.c|5442| <<release_stripe_plug>>
+ *   - fs/btrfs/raid56.c|1786| <<raid56_parity_write>>
+ *
+ * 遍历current的plug->cb_list, 根据参数unplug和data寻找已有的struct blk_plug*
+ * 有的话就返回 没有创建新的加入到plug->cb_list
+ */
 struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 				      int size)
 {
@@ -3682,6 +3812,18 @@ struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,
 }
 EXPORT_SYMBOL(blk_check_plugged);
 
+/*
+ * called by:
+ *   - blk_queue_bio()
+ *   - blk_poll()
+ *   - blk_finish_plug()
+ *   - blk_mq_make_request() -- 只有一个队列的情况
+ *   - blk_flush_plug()
+ *   - blk_schedule_flush_plug()
+ *
+ * 核心思想是遍历plug->list上的request, 然后把request应__elv_add_request()加入到queue
+ * 清空plug->list
+ */
 void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct request_queue *q;
@@ -3690,6 +3832,7 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	LIST_HEAD(list);
 	unsigned int depth;
 
+	/* 把plug->cb_list中的entry删掉, 遍历每一个entry (struct blk_plug_cb) 调用callback */
 	flush_plug_callbacks(plug, from_schedule);
 
 	if (!list_empty(&plug->mq_list))
@@ -3698,6 +3841,11 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	if (list_empty(&plug->list))
 		return;
 
+	/*
+	 * blk_plug有list, mq_list和cb_list 3个list_head
+	 *
+	 * list里是request
+	 */
 	list_splice_init(&plug->list, &list);
 
 	list_sort(NULL, &list, plug_rq_cmp);
@@ -3709,6 +3857,12 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	 * Save and disable interrupts here, to avoid doing it for every
 	 * queue lock we have to take.
 	 */
+	/*
+	 * 上面有list_splice_init(&plug->list, &list);
+	 *
+	 * 这里等价遍历plug->list上的request, 然后把request应__elv_add_request()加入到queue
+	 * 清空plug->list
+	 */
 	local_irq_save(flags);
 	while (!list_empty(&list)) {
 		rq = list_entry_rq(list.next);
@@ -3718,6 +3872,7 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 			/*
 			 * This drops the queue lock
 			 */
+			/* 根据from_schedule决定是用blk_run_queue_async()还是__blk_run_queue() */
 			if (q)
 				queue_unplugged(q, depth, from_schedule);
 			q = rq->q;
@@ -3736,6 +3891,11 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 		/*
 		 * rq is already accounted, so use raw insert
 		 */
+		/*
+		 * 这里是核心代码
+		 *
+		 * 把request加入到rq->queuelist
+		 */
 		if (op_is_flush(rq->cmd_flags))
 			__elv_add_request(q, rq, ELEVATOR_INSERT_FLUSH);
 		else
@@ -3747,6 +3907,7 @@ void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	/*
 	 * This drops the queue lock
 	 */
+	/* 根据from_schedule决定是用blk_run_queue_async()还是__blk_run_queue() */
 	if (q)
 		queue_unplugged(q, depth, from_schedule);
 
@@ -3785,6 +3946,11 @@ EXPORT_SYMBOL(blk_finish_plug);
  *    The block layer runtime PM is request based, so only works for drivers
  *    that use request as their IO unit instead of those directly use bio's.
  */
+/*
+ * called by:
+ *   - sd_probe_async() drivers/scsi/sd.c
+ *   - sr_probe() drivers/scsi/sr.c
+ */
 void blk_pm_runtime_init(struct request_queue *q, struct device *dev)
 {
 	/* not support for RQF_PM and ->rpm_status in blk-mq yet */
@@ -3819,6 +3985,9 @@ EXPORT_SYMBOL(blk_pm_runtime_init);
  *    0		- OK to runtime suspend the device
  *    -EBUSY	- Device should not be runtime suspended
  */
+/*
+ * called only by sdev_runtime_suspend() drivers/scsi/scsi_pm.c
+ */
 int blk_pre_runtime_suspend(struct request_queue *q)
 {
 	int ret = 0;
@@ -3851,6 +4020,9 @@ EXPORT_SYMBOL(blk_pre_runtime_suspend);
  *    This function should be called near the end of the device's
  *    runtime_suspend callback.
  */
+/*
+ * called only by sdev_runtime_suspend() drivers/scsi/scsi_pm.c
+ */
 void blk_post_runtime_suspend(struct request_queue *q, int err)
 {
 	if (!q->dev)
@@ -3878,6 +4050,9 @@ EXPORT_SYMBOL(blk_post_runtime_suspend);
  *    This function should be called near the start of the device's
  *    runtime_resume callback.
  */
+/*
+ * called obly by sdev_runtime_resume() drivers/scsi/scsi_pm.c
+ */
 void blk_pre_runtime_resume(struct request_queue *q)
 {
 	if (!q->dev)
@@ -3903,6 +4078,9 @@ EXPORT_SYMBOL(blk_pre_runtime_resume);
  *    This function should be called near the end of the device's
  *    runtime_resume callback.
  */
+/*
+ * called only by sdev_runtime_resume() drivers/scsi/scsi_pm.c
+ */
 void blk_post_runtime_resume(struct request_queue *q, int err)
 {
 	if (!q->dev)
@@ -3935,6 +4113,9 @@ EXPORT_SYMBOL(blk_post_runtime_resume);
  * runtime PM status and re-enable peeking requests from the queue. It
  * should be called before first request is added to the queue.
  */
+/*
+ * called only by scsi_bus_resume_common()
+ */
 void blk_set_runtime_active(struct request_queue *q)
 {
 	spin_lock_irq(q->queue_lock);
@@ -3946,6 +4127,7 @@ void blk_set_runtime_active(struct request_queue *q)
 EXPORT_SYMBOL(blk_set_runtime_active);
 #endif
 
+/* called only by genhd_device_init() */
 int __init blk_dev_init(void)
 {
 	BUILD_BUG_ON(REQ_OP_LAST >= (1 << REQ_OP_BITS));
diff --git a/block/blk-flush.c b/block/blk-flush.c
index f171706..124c202 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -132,6 +132,11 @@ static void blk_flush_restore_request(struct request *rq)
 	rq->end_io = rq->flush.saved_end_io;
 }
 
+/*
+ * called by:
+ *   - blk_flush_complete_seq()
+ *   - blk_kick_flush()
+ */
 static bool blk_flush_queue_rq(struct request *rq, bool add_front)
 {
 	if (rq->q->mq_ops) {
@@ -431,6 +436,12 @@ static void mq_flush_data_end_io(struct request *rq, blk_status_t error)
  * @rq is being submitted.  Analyze what needs to be done and put it on the
  * right queue.
  */
+/*
+ * called by:
+ *   - blk_mq_sched_insert_request()
+ *   - blk_mq_make_request()
+ *   - __elv_add_request()
+ */
 void blk_insert_flush(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -518,6 +529,9 @@ void blk_insert_flush(struct request *rq)
  *    room for storing the error offset in case of a flush error, if they
  *    wish to.
  */
+/*
+ * 被文件系统调用的很多
+ */
 int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 		sector_t *error_sector)
 {
@@ -560,6 +574,11 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 }
 EXPORT_SYMBOL(blkdev_issue_flush);
 
+/*
+ * called by:
+ *   - blk_init_allocated_queue()
+ *   - blk_mq_init_hctx()
+ */
 struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		int node, int cmd_size)
 {
@@ -574,6 +593,7 @@ struct blk_flush_queue *blk_alloc_flush_queue(struct request_queue *q,
 		spin_lock_init(&fq->mq_flush_lock);
 
 	rq_sz = round_up(rq_sz + cmd_size, cache_line_size());
+	/* 就分配了一个request */
 	fq->flush_rq = kzalloc_node(rq_sz, GFP_KERNEL, node);
 	if (!fq->flush_rq)
 		goto fail_rq;
@@ -596,6 +616,7 @@ void blk_free_flush_queue(struct blk_flush_queue *fq)
 	if (!fq)
 		return;
 
+	/* flush_rq就是一个元素 */
 	kfree(fq->flush_rq);
 	kfree(fq);
 }
diff --git a/block/blk-integrity.c b/block/blk-integrity.c
index feb3057..d23db2f 100644
--- a/block/blk-integrity.c
+++ b/block/blk-integrity.c
@@ -406,6 +406,15 @@ static const struct blk_integrity_profile nop_profile = {
  * struct with values appropriate for the underlying hardware. See
  * Documentation/block/data-integrity.txt.
  */
+/*
+ * called by:
+ *   - dm_integrity_set()            -- drivers/md/dm-integrity.c
+ *   - dm_table_register_integrity() -- drivers/md/dm-table.c
+ *   - md_integrity_register()       -- drivers/md/md.c
+ *   - nd_integrity_init()           -- drivers/nvdimm/core.c
+ *   - nvme_init_integrity()         -- drivers/nvme/host/core.c
+ *   - sd_dif_config_host()          -- drivers/scsi/sd_dif.c 
+ */
 void blk_integrity_register(struct gendisk *disk, struct blk_integrity *template)
 {
 	struct blk_integrity *bi = &disk->queue->integrity;
@@ -436,6 +445,9 @@ void blk_integrity_unregister(struct gendisk *disk)
 }
 EXPORT_SYMBOL(blk_integrity_unregister);
 
+/*
+ * called only by __device_add_disk()
+ */
 void blk_integrity_add(struct gendisk *disk)
 {
 	if (kobject_init_and_add(&disk->integrity_kobj, &integrity_ktype,
@@ -445,6 +457,9 @@ void blk_integrity_add(struct gendisk *disk)
 	kobject_uevent(&disk->integrity_kobj, KOBJ_ADD);
 }
 
+/*
+ * called only by del_gendisk()
+ */
 void blk_integrity_del(struct gendisk *disk)
 {
 	kobject_uevent(&disk->integrity_kobj, KOBJ_REMOVE);
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 481dc02..2eba4d9 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -673,24 +673,48 @@ static void blk_account_io_merge(struct request *req)
  * For non-mq, this has to be called with the request spinlock acquired.
  * For mq with scheduling, the appropriate queue wide lock should be held.
  */
+/*
+ * called by:
+ *   - attempt_back_merge()
+ *   - attempt_front_merge()
+ *   - blk_attempt_req_merge()
+ *
+ * 把req和next合并 返回next
+ */
 static struct request *attempt_merge(struct request_queue *q,
 				     struct request *req, struct request *next)
 {
 	if (!q->mq_ops)
 		lockdep_assert_held(q->queue_lock);
 
+	/*
+	 * 判断这个request是否允许merge,
+	 * 比如scsi passthrough或者Driver private requests不能被merge
+	 *
+	 * 两个(req和next)必须都允许merge
+	 */
 	if (!rq_mergeable(req) || !rq_mergeable(next))
 		return NULL;
 
+	/*  request必须有一样的op */
 	if (req_op(req) != req_op(next))
 		return NULL;
 
 	/*
 	 * not contiguous
 	 */
+	/*
+	 * blk_rq_pos(req)      ---->  rq->__sector: the current sector
+	 * blk_rq_sectors(req)  ---->  rq->__data_len >> SECTOR_SHIFT
+	 *
+	 * 如果两个request前后不连续就不能merge
+	 */
 	if (blk_rq_pos(req) + blk_rq_sectors(req) != blk_rq_pos(next))
 		return NULL;
 
+	/*
+	 * rq_disk是struct gendisk
+	 */
 	if (rq_data_dir(req) != rq_data_dir(next)
 	    || req->rq_disk != next->rq_disk
 	    || req_no_special_merge(next))
@@ -767,26 +791,51 @@ static struct request *attempt_merge(struct request_queue *q,
 	return next;
 }
 
+/*
+ * called by:
+ *   - blk_queue_bio()
+ *   - blk_mq_sched_try_merge()
+ *
+ * 取出rq在request_queue中的下一个request(next)
+ * 把rq和next合并 返回next
+ */
 struct request *attempt_back_merge(struct request_queue *q, struct request *rq)
 {
+	/* 取出rq在request_queue中的下一个request */
 	struct request *next = elv_latter_request(q, rq);
 
+	/* 把rq和next合并 返回next */
 	if (next)
 		return attempt_merge(q, rq, next);
 
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - blk_queue_bio()
+ *   - blk_mq_sched_try_merge()
+ *
+ * 取出rq在request_queue中的上一个request(prev)
+ * 把rq和prev合并 返回prev
+ */
 struct request *attempt_front_merge(struct request_queue *q, struct request *rq)
 {
+	/* 取出rq在request_queue中的上一个request */
 	struct request *prev = elv_former_request(q, rq);
 
+	/* 把rq和prev合并 返回prev */
 	if (prev)
 		return attempt_merge(q, prev, rq);
 
 	return NULL;
 }
 
+/*
+ * 被elv_attempt_insert_merge()调用两处
+ *
+ * 把req和next合并 释放next
+ */
 int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 			  struct request *next)
 {
@@ -797,6 +846,7 @@ int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 		if (!e->type->ops.sq.elevator_allow_rq_merge_fn(q, rq, next))
 			return 0;
 
+	/* 把rq和next合并 返回next */
 	free = attempt_merge(q, rq, next);
 	if (free) {
 		__blk_put_request(q, free);
@@ -806,6 +856,14 @@ int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - blk_attempt_plug_merge()
+ *   - blk_mq_attempt_merge()
+ *   - elv_bio_merge_ok()
+ *
+ * 通过request和bio的属性确认他们能否merge
+ */
 bool blk_rq_merge_ok(struct request *rq, struct bio *bio)
 {
 	if (!rq_mergeable(rq) || !bio_mergeable(bio))
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 3eb169f..dda55f2 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -14,11 +14,13 @@
 #include "blk.h"
 #include "blk-mq.h"
 
+/* 两次被blk_mq_map_queues()调用 */
 static int cpu_to_queue_index(unsigned int nr_queues, const int cpu)
 {
 	return cpu % nr_queues;
 }
 
+/* called only by blk_mq_map_queues() */
 static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
@@ -30,8 +32,19 @@ static int get_first_sibling(unsigned int cpu)
 	return cpu;
 }
 
+/*
+ * called by:
+ *   - blk_mq_rdma_map_queues()   -- block/blk-mq-rdma.c
+ *   - blk_mq_virtio_map_queues() -- block/blk-mq-virtio.c
+ *   - blk_mq_update_queue_map()  -- block/blk-mq.c
+ *   - qla2xxx_map_queues()       -- drivers/scsi/qla2xxx/qla_os.c
+ *   - scsi_map_queues() 两次     -- drivers/scsi/scsi_lib.c
+ *
+ * 设置set->mq_map, 初始化里面每一个元素
+ */
 int blk_mq_map_queues(struct blk_mq_tag_set *set)
 {
+	/* mq_map是cpu个元素的数组 */
 	unsigned int *map = set->mq_map;
 	unsigned int nr_queues = set->nr_hw_queues;
 	unsigned int cpu, first_sibling;
@@ -62,6 +75,14 @@ EXPORT_SYMBOL_GPL(blk_mq_map_queues);
  * We have no quick way of doing reverse lookups. This is only used at
  * queue init time, so runtime isn't important.
  */
+/*
+ * called by:
+ *   - blk_mq_alloc_rq_map()
+ *   - blk_mq_alloc_rqs()
+ *   - blk_mq_realloc_hw_ctxs()
+ *
+ * index是hw queue的index 返回hw queue index对应的numa node
+ */
 int blk_mq_hw_queue_to_node(unsigned int *mq_map, unsigned int index)
 {
 	int i;
diff --git a/block/blk-mq-pci.c b/block/blk-mq-pci.c
index e233996..1f1b3b0 100644
--- a/block/blk-mq-pci.c
+++ b/block/blk-mq-pci.c
@@ -29,17 +29,26 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - nvme_pci_map_queues()  -- drivers/nvme/host/pci.c
+ *   - qla2xxx_map_queues()   -- drivers/scsi/qla2xxx/qla_os.c
+ *   - pqi_map_queues()       -- drivers/scsi/smartpqi/smartpqi_init.c
+ */
 int blk_mq_pci_map_queues(struct blk_mq_tag_set *set, struct pci_dev *pdev,
 			    int offset)
 {
+	/* blk_mq_tag_set中包含struct blk_mq_tags指针数组 */
 	const struct cpumask *mask;
 	unsigned int queue, cpu;
 
 	for (queue = 0; queue < set->nr_hw_queues; queue++) {
+		/* 返回vector (queue+offset)的affinity */
 		mask = pci_irq_get_affinity(pdev, queue + offset);
 		if (!mask)
 			goto fallback;
 
+		/* 如果一个interrupt对多个cpu怎么办?? */
 		for_each_cpu(cpu, mask)
 			set->mq_map[cpu] = queue;
 	}
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index 25c14c5..f33da05 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -16,12 +16,16 @@
 #include "blk-mq-tag.h"
 #include "blk-wbt.h"
 
+/*
+ * 目前没人调用
+ */
 void blk_mq_sched_free_hctx_data(struct request_queue *q,
 				 void (*exit)(struct blk_mq_hw_ctx *))
 {
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+	/* (q)->queue_hw_ctx[i]; */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (exit && hctx->sched_data)
 			exit(hctx);
@@ -31,6 +35,9 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_free_hctx_data);
 
+/*
+ * called only by blk_mq_get_request()
+ */
 void blk_mq_sched_assign_ioc(struct request *rq, struct bio *bio)
 {
 	struct request_queue *q = rq->q;
@@ -54,6 +61,9 @@ void blk_mq_sched_assign_ioc(struct request *rq, struct bio *bio)
  * Mark a hardware queue as needing a restart. For shared queues, maintain
  * a count of how many hardware queues are marked for restart.
  */
+/*
+ * called only by blk_mq_sched_dispatch_requests()
+ */
 static void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
@@ -68,6 +78,9 @@ static void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 		set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 }
 
+/*
+ * called only by blk_mq_sched_dispatch_requests()
+ */
 static bool blk_mq_sched_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
@@ -89,6 +102,11 @@ static bool blk_mq_sched_restart_hctx(struct blk_mq_hw_ctx *hctx)
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * 在两处被blk_mq_sched_dispatch_requests()调用
+ *
+ * 在循环中用scheduler的.dispatch_request()获取request然后用blk_mq_dispatch_rq_list()-->.queue_rq()发出去!
+ */
 static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -98,6 +116,11 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 	do {
 		struct request *rq;
 
+		/*
+		 * bfq_has_work()   -- block/bfq-iosched.c
+		 * kyber_has_work() -- block/kyber-iosched.c
+		 * dd_has_work()    -- block/mq-deadline.c 
+		 */
 		if (e->type->ops.mq.has_work &&
 				!e->type->ops.mq.has_work(hctx))
 			break;
@@ -105,6 +128,16 @@ static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/*
+		 * bfq_dispatch_request()   -- block/bfq-iosched.c
+		 * kyber_dispatch_request() -- block/kyber-iosched.c
+		 * dd_dispatch_request()    -- block/mq-deadline.c
+		 *
+		 * 不同的scheduler会根据自己的情况返回一个request
+		 * 下面会把返回的request放在临时链表rq_list
+		 *
+		 * 下面的blk_mq_dispatch_rq_list()会把rq_list中的request用每个driver的queue_rq发出去
+		 */
 		rq = e->type->ops.mq.dispatch_request(hctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -136,6 +169,12 @@ static struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,
  * its queue by itself in its completion handler, so we don't need to
  * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE.
  */
+/*
+ * 只被blk_mq_sched_dispatch_requests()调用
+ *
+ * 在循环中round robin参数hctx中的ctx, 每次从一个ctx取出一个request
+ * 然后用blk_mq_dispatch_rq_list()-->.queue_rq()下发request!
+ */
 static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -151,6 +190,7 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		if (!blk_mq_get_dispatch_budget(hctx))
 			break;
 
+		/* 以ctx为开始, 遍历hctx的sbitmap, 如果找到一个pending的request就返回 */
 		rq = blk_mq_dequeue_from_ctx(hctx, ctx);
 		if (!rq) {
 			blk_mq_put_dispatch_budget(hctx);
@@ -162,20 +202,34 @@ static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)
 		 * if this rq won't be queued to driver via .queue_rq()
 		 * in blk_mq_dispatch_rq_list().
 		 */
+		/* 把上面找到的request添加到rq_list */
 		list_add(&rq->queuelist, &rq_list);
 
 		/* round robin for fair dispatch */
+		/*
+		 * 换一个ctx, 下一次调用上面的blk_mq_dequeue_from_ctx()就会以其他的ctx为开始了
+		 */
 		ctx = blk_mq_next_ctx(hctx, rq->mq_ctx);
 
-	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));
+	} while (blk_mq_dispatch_rq_list(q, &rq_list, true));  // 用.queue_rq()下发list中的request
 
 	WRITE_ONCE(hctx->dispatch_from, ctx);
 }
 
+/*
+ * called only by __blk_mq_run_hw_queue()
+ */
 void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
 	struct elevator_queue *e = q->elevator;
+	/*
+	 * bfq_dispatch_request()    -- bfq-iosched.c
+	 * kyber_dispatch_request()  -- kyber-iosched.c
+	 * dd_dispatch_request()     -- mq-deadline.c
+	 *
+	 * 如果有scheduler的话就有has_sched_dispatch为true
+	 */
 	const bool has_sched_dispatch = e && e->type->ops.mq.dispatch_request;
 	LIST_HEAD(rq_list);
 
@@ -183,14 +237,17 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	if (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))
 		return;
 
+	/* 只在当前函数增加 会在debugfs被打印 */
 	hctx->run++;
 
 	/*
 	 * If we have previous entries on our dispatch list, grab them first for
 	 * more fair dispatch.
 	 */
+	/* tests whether a list is empty and not being modified */
 	if (!list_empty_careful(&hctx->dispatch)) {
 		spin_lock(&hctx->lock);
+		/* 把hctx->dispatch放入rq_list */
 		if (!list_empty(&hctx->dispatch))
 			list_splice_init(&hctx->dispatch, &rq_list);
 		spin_unlock(&hctx->lock);
@@ -211,13 +268,31 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 	 */
 	if (!list_empty(&rq_list)) {
 		blk_mq_sched_mark_restart_hctx(hctx);
+		/* 用.queue_rq()下发list中的request */
 		if (blk_mq_dispatch_rq_list(q, &rq_list, false)) {
+			/*
+			 * blk_mq_do_dispatch_sched()注释:
+			 *     在循环中用scheduler的.dispatch_request()获取request然后用blk_mq_dispatch_rq_list()-->.queue_rq()发出去!
+			 *
+			 * blk_mq_do_dispatch_ctx()注释:
+			 *     在循环中round robin参数hctx中的ctx, 每次从一个ctx取出一个request
+			 *     然后用blk_mq_dispatch_rq_list()-->.queue_rq()下发request!
+			 *
+			 * 总结两者的不同:
+			 *     has_sched_dispatch的时候用scheduler的dispatch_request()获取request
+			 *     否则直接在ctx们获取request
+			 */
 			if (has_sched_dispatch)
 				blk_mq_do_dispatch_sched(hctx);
 			else
 				blk_mq_do_dispatch_ctx(hctx);
 		}
 	} else if (has_sched_dispatch) {
+		/*
+		 * 在两处被blk_mq_sched_dispatch_requests()调用
+		 *
+		 * 在循环中用scheduler的.dispatch_request()获取request然后用blk_mq_dispatch_rq_list()-->.queue_rq()发出去!
+		 */
 		blk_mq_do_dispatch_sched(hctx);
 	} else if (q->mq_ops->get_budget) {
 		/*
@@ -228,34 +303,75 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 		 * TODO: get more budgets, and dequeue more requests in
 		 * one time.
 		 */
+		/*
+		 * 在循环中round robin参数hctx中的ctx, 每次从一个ctx取出一个request
+		 * 然后用blk_mq_dispatch_rq_list()-->.queue_rq()下发request!
+		 */
 		blk_mq_do_dispatch_ctx(hctx);
 	} else {
+		/*
+		 * 参数list是输出
+		 * 对一sbitmap中每一个不为1的bit, 调用flush_busy_ctx(), 从0开始
+		 * 调用flush_busy_ctx会:
+		 *     把ctx->rq_list中的request们放入flush_data->list (参数list)
+		 *     在sbitmap中清空对应的bit
+		 */
 		blk_mq_flush_busy_ctxs(hctx, &rq_list);
+		/* 用.queue_rq()下发list中的request */
 		blk_mq_dispatch_rq_list(q, &rq_list, false);
 	}
 }
 
+/*
+ * called by:
+ *   - dd_bio_merge()  -- block/mq-deadline.c
+ *   - bfq_bio_merge() -- block/bfq-iosched.c
+ *
+ * 把bio给merge到request_queue中
+ * 合并到某个request后, 如果该request可以和前面或后面继续合并就再合并一次
+ *      把再合并一次的request返回到参数merged_request
+ * 否则因为request的起始sector变了, 把request从红黑树中删除, 重新插入
+ */
 bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 			    struct request **merged_request)
 {
 	struct request *rq;
 
+	/*
+	 * 这个函数通过elv自己的和相关scheduler的函数查询是否可以front或者back merge
+	 * 把可以merge的request返回到参数req
+	 * 返回值为ELEVATOR_BACK_MERGE, ELEVATOR_FRONT_MERGE, ELEVATOR_DISCARD_MERGE等
+	 */
 	switch (elv_merge(q, &rq, bio)) {
 	case ELEVATOR_BACK_MERGE:
+		/* 通过scheduler实现的.allow_merge()确认是否可以merge */
 		if (!blk_mq_sched_allow_merge(q, rq, bio))
 			return false;
+		/* 把bio合并到rq */
 		if (!bio_attempt_back_merge(q, rq, bio))
 			return false;
+		/*
+		 * 取出rq在request_queue中的下一个request(next)
+		 * 把rq和next合并 返回next
+		 */
 		*merged_request = attempt_back_merge(q, rq);
+		/* 对于mq-deadline, 合并bio到请求后的处理, 向前合并后, 将请求从红黑树中删除, 重新插入 */
 		if (!*merged_request)
 			elv_merged_request(q, rq, ELEVATOR_BACK_MERGE);
 		return true;
 	case ELEVATOR_FRONT_MERGE:
+		/* 通过scheduler实现的.allow_merge()确认是否可以merge */
 		if (!blk_mq_sched_allow_merge(q, rq, bio))
 			return false;
+		/* 把bio合并到rq */
 		if (!bio_attempt_front_merge(q, rq, bio))
 			return false;
+		/*
+		 * 取出rq在request_queue中的上一个request(prev)
+		 * 把rq和prev合并 返回prev
+		 */
 		*merged_request = attempt_front_merge(q, rq);
+		/* 对于mq-deadline, 合并bio到请求后的处理, 向前合并后, 将请求从红黑树中删除, 重新插入 */
 		if (!*merged_request)
 			elv_merged_request(q, rq, ELEVATOR_FRONT_MERGE);
 		return true;
@@ -272,6 +388,12 @@ EXPORT_SYMBOL_GPL(blk_mq_sched_try_merge);
  * merge with. Currently includes a hand-wavy stop count of 8, to not spend
  * too much time checking for merges.
  */
+/*
+ * called only by __blk_mq_sched_bio_merge()
+ *
+ * 遍历blk_mq_ctx.rq_list尝试bio是否可以和某一个request来merge
+ * blk_mq_ctx.rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+ */
 static bool blk_mq_attempt_merge(struct request_queue *q,
 				 struct blk_mq_ctx *ctx, struct bio *bio)
 {
@@ -280,6 +402,7 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/* rq_list上放的是用__blk_mq_insert_request()插入的待处理的request */
 	list_for_each_entry_reverse(rq, &ctx->rq_list, queuelist) {
 		bool merged = false;
 
@@ -313,18 +436,48 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	return false;
 }
 
+/*
+ * called only by blk_mq_sched_bio_merge()
+ *
+ * 如果支持e->type->ops.mq.bio_merge就用相关scheduler的函数进行merge
+ *     mq-deadline是通过struct elevator_queue的hash来查看
+ * 否则根据情况用blk_mq_attempt_merge()
+ *     遍历blk_mq_ctx.rq_list尝试bio是否可以和某一个request来merge
+ *     blk_mq_ctx.rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+ */
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
+	/* 通过get_cpu()获得 */
 	struct blk_mq_ctx *ctx = blk_mq_get_ctx(q);
 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
 	bool ret = false;
 
+	/*
+	 * 实现了以下:
+	 *     block/mq-deadline.c --> dd_bio_merge()
+	 *     block/bfq-iosched.c --> bfq_bio_merge()
+	 *
+	 * 对于dd_bio_merge:
+	 *      把bio给merge到request_queue中
+	 *      合并到某个request后, 如果该request可以和前面或后面继续合并就再合并一次
+	 *      否则因为request的起始sector变了, 把request从红黑树中删除, 重新插入
+	 *
+	 *      mq-deadline是通过struct elevator_queue的hash来查看
+	 */
 	if (e && e->type->ops.mq.bio_merge) {
 		blk_mq_put_ctx(ctx);
 		return e->type->ops.mq.bio_merge(hctx, bio);
 	}
 
+	/*
+	 * 下面是如果不支持e->type->ops.mq.bio_merge的代码
+	 */
+
+	/*
+	 * 遍历blk_mq_ctx.rq_list尝试bio是否可以和某一个request来merge
+	 * blk_mq_ctx.rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+	 */
 	if (hctx->flags & BLK_MQ_F_SHOULD_MERGE) {
 		/* default per sw-queue merge */
 		spin_lock(&ctx->lock);
@@ -336,18 +489,54 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - bfq_insert_request()
+ *   - dd_insert_request()
+ *
+ * 先判断这个request是否允许merge,
+ * 比如scsi passthrough或者Driver private requests不能被merge
+ *
+ * 然后:
+ * Attempt to do an insertion back merge. Only check for the case where
+ * we can append 'rq' to an existing request, so we can throw 'rq' away
+ * afterwards.
+ *
+ * Returns true if we merged, false otherwise
+ */
 bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
+	/*
+	 * 先判断这个request是否允许merge,
+	 * 比如scsi passthrough或者Driver private requests不能被merge
+	 *
+	 * 然后:
+	 * Attempt to do an insertion back merge. Only check for the case where
+	 * we can append 'rq' to an existing request, so we can throw 'rq' away
+	 * afterwards.
+	 *
+	 * Returns true if we merged, false otherwise
+	 */
 	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_try_insert_merge);
 
+/*
+ * called by:
+ *   - bfq_insert_request()
+ *   - dd_insert_request()
+ */
 void blk_mq_sched_request_inserted(struct request *rq)
 {
 	trace_block_rq_insert(rq->q, rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 
+/*
+ * called only by blk_mq_sched_insert_request()
+ *
+ * 如果request满足情况, 直接添加到&hctx->dispatch
+ */
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       bool has_sched,
 				       struct request *rq)
@@ -388,6 +577,9 @@ static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
  * queues in a round-robin fashion if the tag set of @hctx is shared with other
  * hardware queues.
  */
+/*
+ * called only by blk_mq_free_request()
+ */
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *const hctx)
 {
 	struct blk_mq_tags *const tags = hctx->tags;
@@ -428,6 +620,21 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *const hctx)
 	}
 }
 
+/*
+ * called by:
+ *   - blk_execute_rq_nowait()
+ *   - blk_mq_requeue_work() 两次
+ *   - __blk_mq_try_issue_directly()
+ *   - blk_mq_try_issue_directly()
+ *   - blk_mq_make_request()
+ *
+ * 注释忽略flush和hctx->dispatch的情况
+ *
+ * 如果scheduler实现了insert方法 (也就是说有自己的队列) 就放入schedule自己的队列
+ * 否则就放入默认的ctx的队列
+ *
+ * 最后还有个run: 如果run_queue是true, 就blk_mq_run_hw_queue()
+ */
 void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 				 bool run_queue, bool async)
 {
@@ -442,11 +649,17 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		goto run;
 	}
 
+	/* 如果有scheduler, rq->tag就应该是-1 */
 	WARN_ON(e && (rq->tag != -1));
 
+	/* 如果request满足情况, 直接添加到&hctx->dispatch */
 	if (blk_mq_sched_bypass_insert(hctx, !!e, rq))
 		goto run;
 
+	/*
+	 * 如果scheduler实现了insert方法 (也就是说有自己的队列) 就放入schedule自己的队列
+	 * 否则就放入默认的ctx的队列
+	 */
 	if (e && e->type->ops.mq.insert_requests) {
 		LIST_HEAD(list);
 
@@ -454,6 +667,10 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		e->type->ops.mq.insert_requests(hctx, &list, at_head);
 	} else {
 		spin_lock(&ctx->lock);
+		/*
+		 * 把request链接到软件队列struct blk_mq_ctx的rq_list
+		 * 然后配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request
+		 */
 		__blk_mq_insert_request(hctx, rq, at_head);
 		spin_unlock(&ctx->lock);
 	}
@@ -463,6 +680,9 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 		blk_mq_run_hw_queue(hctx, async);
 }
 
+/*
+ * called only by blk_mq_flush_plug_list() 两次
+ */
 void blk_mq_sched_insert_requests(struct request_queue *q,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
@@ -470,6 +690,16 @@ void blk_mq_sched_insert_requests(struct request_queue *q,
 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
 	struct elevator_queue *e = hctx->queue->elevator;
 
+	/*
+	 * 情况1: 函数指针存在
+	 *        bfq_insert_requests()
+	 *        dd_insert_requests()
+	 *
+	 * 情况2: 函数指针不存在
+	 *        在一个循环中对于list中的没一个request, 从list删除
+	 *        把request链接到软件队列struct blk_mq_ctx的rq_list
+	 *        循环结束后配置hctx的ctx_map (sbitmap), sw的ctx在hw的hctx上有等待的request
+	 */
 	if (e && e->type->ops.mq.insert_requests)
 		e->type->ops.mq.insert_requests(hctx, list, false);
 	else
@@ -489,6 +719,11 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - blk_mq_sched_init_hctx()
+ *   - blk_mq_init_sched()
+ */
 static int blk_mq_sched_alloc_tags(struct request_queue *q,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
@@ -496,11 +731,19 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 	struct blk_mq_tag_set *set = q->tag_set;
 	int ret;
 
+	/*
+	 * 分配一个struct blk_mq_tags, 并分配其rqs和static_rqs
+	 * 每个hw queue一个
+	 */
 	hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
 					       set->reserved_tags);
 	if (!hctx->sched_tags)
 		return -ENOMEM;
 
+	/*
+	 * 不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+	 * page可能不连续 都链接在tags->page_list
+	 */
 	ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
 	if (ret)
 		blk_mq_sched_free_tags(set, hctx, hctx_idx);
@@ -518,6 +761,9 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 		blk_mq_sched_free_tags(set, hctx, i);
 }
 
+/*
+ * called only by blk_mq_init_hctx()
+ */
 int blk_mq_sched_init_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,
 			   unsigned int hctx_idx)
 {
@@ -562,6 +808,11 @@ void blk_mq_sched_exit_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,
 	blk_mq_sched_free_tags(q->tag_set, hctx, hctx_idx);
 }
 
+/*
+ * called by:
+ *   - elevator_init()
+ *   - elevator_switch_mq()
+ */
 int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -615,6 +866,11 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - blk_mq_init_sched()
+ *   - elevator_exit()
+ */
 void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -634,6 +890,16 @@ void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 	q->elevator = NULL;
 }
 
+/*
+ * called only by blk_mq_init_allocated_queue()
+ *
+ * 对于mq的设备:
+ *     如果queue的数量大于1, 默认用none (不是noop!)
+ *     如果queue的数量等于1, 默认用mq-deadline
+ *
+ * 对于sq的设备:
+ *     默认用CONFIG_DEFAULT_IOSCHED
+ */
 int blk_mq_sched_init(struct request_queue *q)
 {
 	int ret;
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index 1e9c901..40ea03e 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -35,65 +35,141 @@ void blk_mq_sched_exit_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,
 
 int blk_mq_sched_init(struct request_queue *q);
 
+/*
+ * called only by blk_mq_make_request()
+ *
+ * 先检查request_queue和bio是否都支持merge
+ * 然后如果支持e->type->ops.mq.bio_merge就用相关scheduler的函数进行merge
+ *     mq-deadline是通过struct elevator_queue的hash来查看
+ * 否则根据情况用blk_mq_attempt_merge()
+ *     遍历blk_mq_ctx.rq_list尝试bio是否可以和某一个request来merge
+ *     blk_mq_ctx.rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+ */
 static inline bool
 blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 {
+	/* 先检查request_queue和bio是否都支持merge */
 	if (blk_queue_nomerges(q) || !bio_mergeable(bio))
 		return false;
 
+	/*
+	 * 如果支持e->type->ops.mq.bio_merge就用相关scheduler的函数进行merge
+	 *     mq-deadline是通过struct elevator_queue的hash来查看
+	 * 否则根据情况用blk_mq_attempt_merge()
+	 *     遍历blk_mq_ctx.rq_list尝试bio是否可以和某一个request来merge
+	 *     blk_mq_ctx.rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+	 */
 	return __blk_mq_sched_bio_merge(q, bio);
 }
 
+/*
+ * called by:
+ *   - blk_mq_sched_try_merge() 两次
+ *   - blk_mq_attempt_merge() 两次
+ *
+ * 通过scheduler实现的.allow_merge()确认是否可以merge
+ */
 static inline bool
 blk_mq_sched_allow_merge(struct request_queue *q, struct request *rq,
 			 struct bio *bio)
 {
 	struct elevator_queue *e = q->elevator;
 
+	/*
+	 * 目前只实现了一种:
+	 * block/bfq-iosched.c --> bfq_allow_bio_merge
+	 */
 	if (e && e->type->ops.mq.allow_merge)
 		return e->type->ops.mq.allow_merge(q, rq, bio);
 
 	return true;
 }
 
+/*
+ * called only by __blk_mq_complete_request()
+ *
+ * 调用scheduler的completed_request()方法
+ */
 static inline void blk_mq_sched_completed_request(struct request *rq)
 {
 	struct elevator_queue *e = rq->q->elevator;
 
+	/*
+	 * 似乎就kyber实现了kyber_completed_request()
+	 */
 	if (e && e->type->ops.mq.completed_request)
 		e->type->ops.mq.completed_request(rq);
 }
 
+/*
+ * called only by blk_mq_start_request()
+ *
+ * 调用scheduler的.started_request()方法 (目前没人实现)
+ */
 static inline void blk_mq_sched_started_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 	struct elevator_queue *e = q->elevator;
 
+	/* 目前没人实现 */
 	if (e && e->type->ops.mq.started_request)
 		e->type->ops.mq.started_request(rq);
 }
 
+/*
+ * called only by blk_mq_requeue_request()
+ *
+ * 调用scheduler的.requeue_request()方法
+ */
 static inline void blk_mq_sched_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 	struct elevator_queue *e = q->elevator;
 
+	/*
+	 * bfq_finish_requeue_request()
+	 * kyber_finish_request()
+	 */
 	if (e && e->type->ops.mq.requeue_request)
 		e->type->ops.mq.requeue_request(rq);
 }
 
+/*
+ * called only by blk_mq_hctx_has_pending()
+ *
+ * 调用scheduler的has_work()方法查看是否有request要处理
+ */
 static inline bool blk_mq_sched_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct elevator_queue *e = hctx->queue->elevator;
 
+	/*
+	 * bfq_has_work()
+	 * kyber_has_work()
+	 * dd_has_work()
+	 */
 	if (e && e->type->ops.mq.has_work)
 		return e->type->ops.mq.has_work(hctx);
 
 	return false;
 }
 
+/*
+ * called only by blk_mq_dispatch_rq_list()
+ *
+ * 测试hctx->state是否有BLK_MQ_S_SCHED_RESTART
+ */
 static inline bool blk_mq_sched_needs_restart(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * BLK_MQ_S_SCHED_RESTART在include/linux/blk-mq.h定义
+	 *
+	 * 在以下地方set或者clear:
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1216| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &this_hctx->state);
+	 *
+	 *   - block/blk-mq-sched.c|95| <<blk_mq_sched_restart_hctx>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	return test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
 }
 
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 336dde0..9dd3812 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -13,17 +13,26 @@
 #include "blk-mq.h"
 #include "blk-mq-tag.h"
 
+/*
+ * called by only blk_mq_can_queue()
+ *
+ * Return: true if any bit in the &tags->bitmap_tags.sb is clear, false otherwise
+ */
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
 	if (!tags)
 		return true;
 
+	/* Return: true if any bit in the bitmap is clear, false otherwise */
 	return sbitmap_any_bit_clear(&tags->bitmap_tags.sb);
 }
 
 /*
  * If a previously inactive queue goes active, bump the active user count.
  */
+/*
+ * called by only blk_mq_tag_busy()
+ */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
@@ -36,8 +45,16 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 /*
  * Wakeup all potentially sleeping on tags
  */
+/*
+ * called by:
+ *   - __blk_mq_tag_idle()
+ *   - blk_mq_wake_waiters()
+ *
+ * 遍历所有的&tags->bitmap_tags(struct sbq_wait_state)并wakeup有等待事件的wait queue
+ */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
+	/* 遍历所有的struct sbq_wait_state并wakeup有等待事件的wait queue */
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
 	if (include_reserve)
 		sbitmap_queue_wake_all(&tags->breserved_tags);
@@ -47,15 +64,20 @@ void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
  * If a previously busy queue goes inactive, potential waiters could now
  * be allowed to queue. Wake them up and check.
  */
+/*
+ * called by only blk_mq_tag_idle()
+ */
 void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	struct blk_mq_tags *tags = hctx->tags;
 
+	/* Clear a bit and return its old value */
 	if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		return;
 
 	atomic_dec(&tags->active_queues);
 
+	/* 遍历所有的&tags->bitmap_tags(struct sbq_wait_state)并wakeup有等待事件的wait queue */
 	blk_mq_tag_wakeup_all(tags, false);
 }
 
@@ -63,11 +85,15 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
  */
+/*
+ * called only by __blk_mq_get_tag()
+ */
 static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 				  struct sbitmap_queue *bt)
 {
 	unsigned int depth, users;
 
+	/* 如果不是shared就返回true */
 	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return true;
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
@@ -90,9 +116,18 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 	return atomic_read(&hctx->nr_active) < depth;
 }
 
+/*
+ * 只被blk_mq_get_tag()多次调用
+ *
+ * Try to allocate a free bit
+ * 这个bit是sbitmap全局的
+ */
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+	/*
+	 * 在blk_mq_get_request()当request_queue有elevator的时候会在data->flags上mark
+	 */
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -102,8 +137,19 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 		return __sbitmap_queue_get(bt);
 }
 
+/*
+ * called by:
+ *   - blk_mq_get_request()
+ *   - blk_mq_get_driver_tag()
+ */
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+	/* 每个hw queue 一个 */
+	/*
+	 * 选择是用data->hctx->sched_tags还是data->hctx->tags
+	 *
+	 * sched_tags在在blk_mq_sched_alloc_tags()分配
+	 */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
@@ -128,9 +174,11 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	if (tag != -1)
 		goto found_tag;
 
+	/* 如果不想等待就返回 -1 */
 	if (data->flags & BLK_MQ_REQ_NOWAIT)
 		return BLK_MQ_TAG_FAIL;
 
+	/* 获取一个struct sbq_wait_state */
 	ws = bt_wait_ptr(bt, data->hctx);
 	drop_ctx = data->ctx == NULL;
 	do {
@@ -159,6 +207,7 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 		if (data->ctx)
 			blk_mq_put_ctx(data->ctx);
 
+		/* 睡眠 D (UN) state */
 		io_schedule();
 
 		data->ctx = blk_mq_get_ctx(data->q);
@@ -182,6 +231,12 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 	return tag + tag_offset;
 }
 
+/*
+ * called by:
+ *   - blk_mq_free_request()
+ *   - blk_mq_free_request()
+ *   - __blk_mq_put_driver_tag()
+ */
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
@@ -189,9 +244,23 @@ void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		const int real_tag = tag - tags->nr_reserved_tags;
 
 		BUG_ON(real_tag >= tags->nr_tags);
+		/*
+		 * 清除sbitmap_queue的sbitmap对应的bit nr
+		 * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的ws
+		 * 更新sbq->wake_index (sbq->wake_index就是当前active的那个ws)
+		 * 把ws->wait_cnt减1 如果ws->wait_cnt小于等于0了则"唤醒"ws上的wait queue的wake_batch个entry
+		 * 增加sbq->wake_index, 把dec的wake_batch恢复
+		 */
 		sbitmap_queue_clear(&tags->bitmap_tags, real_tag, ctx->cpu);
 	} else {
 		BUG_ON(tag >= tags->nr_reserved_tags);
+		/*
+		 * 清除sbitmap_queue的sbitmap对应的bit nr
+		 * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的ws
+		 * 更新sbq->wake_index (sbq->wake_index就是当前active的那个ws)
+		 * 把ws->wait_cnt减1 如果ws->wait_cnt小于等于0了则"唤醒"ws上的wait queue的wake_batch个entry
+		 * 增加sbq->wake_index, 把dec的wake_batch恢复
+		 */
 		sbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);
 	}
 }
@@ -224,6 +293,10 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	return true;
 }
 
+/*
+ * 初始化一个struct bt_iter_data作为data蚕食
+ * 对一sbitmap中每一个不为1的bit, 调用bt_iter(), 从0开始
+ */
 static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 			busy_iter_fn *fn, void *data, bool reserved)
 {
@@ -234,6 +307,7 @@ static void bt_for_each(struct blk_mq_hw_ctx *hctx, struct sbitmap_queue *bt,
 		.reserved = reserved,
 	};
 
+	/* 对一sbitmap中每一个不为1的bit, 调用bt_iter(), 从0开始 */
 	sbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);
 }
 
@@ -244,6 +318,11 @@ struct bt_tags_iter_data {
 	bool reserved;
 };
 
+/*
+ * 只被用在bt_tags_for_each()中作为sbitmap_for_each_set()的fn
+ *
+ * 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+ */
 static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 {
 	struct bt_tags_iter_data *iter_data = data;
@@ -265,6 +344,13 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	return true;
 }
 
+/*
+ * called only by blk_mq_all_tag_busy_iter()
+ *
+ * 初始化一个struct bt_tags_iter_data
+ * 对sbitmap中每一个不为1的bit, 调用bt_tags_iter(), 从0开始
+ * bt_tags_iter() 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+ */
 static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 			     busy_tag_iter_fn *fn, void *data, bool reserved)
 {
@@ -275,30 +361,71 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 		.reserved = reserved,
 	};
 
+	/*
+	 * 对sbitmap中每一个不为1的bit, 调用bt_tags_iter(), 从0开始
+	 * bt_tags_iter() 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+	 */
 	if (tags->rqs)
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
 
+/*
+ * called only by blk_mq_tagset_busy_iter()
+ *
+ * 根据情况, 对tags->nr_reserved_tags和tags->bitmap_tags调用两次如下:
+ *      初始化一个struct bt_tags_iter_data
+ *      对sbitmap中每一个不为1的bit, 调用bt_tags_iter(), 从0开始
+ *      bt_tags_iter() 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+ */
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
+	/*
+	 * 初始化一个struct bt_tags_iter_data
+	 * 对sbitmap中每一个不为1的bit, 调用bt_tags_iter(), 从0开始
+	 * bt_tags_iter() 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+	 */
 	if (tags->nr_reserved_tags)
 		bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, true);
 	bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, false);
 }
 
+/**
+ * blk_mq_tagset_busy_iter - iterate over all started requests in a tag set
+ * @tagset:        Tag set to iterate over.
+ * @fn:                Pointer to the function that will be called for each started
+ *                request. @fn will be called as follows: @fn(rq, @priv,
+ *                reserved) where rq is a pointer to a request. 'reserved'
+ *                indicates whether or not @rq is a reserved request.
+ * @priv:        Will be passed as second argument to @fn.
+ */
+/*
+ * 对于每一个hw queue, 根据情况, 对tags->nr_reserved_tags和tags->bitmap_tags调用两次如下:
+ *       初始化一个struct bt_tags_iter_data
+ *       对sbitmap中每一个不为1的bit, 调用bt_tags_iter(), 从0开始
+ *       bt_tags_iter() 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+ */
 void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 		busy_tag_iter_fn *fn, void *priv)
 {
 	int i;
 
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
+		/*
+		 * 根据情况, 对tags->nr_reserved_tags和tags->bitmap_tags调用两次如下:
+		 *      初始化一个struct bt_tags_iter_data
+		 *      对sbitmap中每一个不为1的bit, 调用bt_tags_iter(), 从0开始
+		 *      bt_tags_iter() 根据bitnr获得blk_mq_tags中的对应的request, 针对这个request调用bt_tags_iter_data的fn
+		 */
 		if (tagset->tags && tagset->tags[i])
 			blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
 	}
 }
 EXPORT_SYMBOL(blk_mq_tagset_busy_iter);
 
+/*
+ * called only by nvme_reinit_tagset()
+ */
 int blk_mq_tagset_iter(struct blk_mq_tag_set *set, void *data,
 			 int (fn)(void *, struct request *))
 {
@@ -328,6 +455,26 @@ int blk_mq_tagset_iter(struct blk_mq_tag_set *set, void *data,
 }
 EXPORT_SYMBOL_GPL(blk_mq_tagset_iter);
 
+/**
+ * blk_mq_queue_tag_busy_iter - iterate over all requests with a driver tag
+ * @q:                Request queue to examine.
+ * @fn:                Pointer to the function that will be called for each request
+ *                on @q. @fn will be called as follows: @fn(hctx, rq, @priv,
+ *                reserved) where rq is a pointer to a request and hctx points
+ *                to the hardware queue associated with the request. 'reserved'
+ *                indicates whether or not @rq is a reserved request.
+ * @priv:        Will be passed as third argument to @fn.
+ *
+ * Note: if @q->tag_set is shared with other request queues then @fn will be
+ * called for all requests on all queues that share that tag set and not only
+ * for requests associated with @q.
+ */
+/*
+ * called by:
+ *   - blk_mq_in_flight()
+ *   - blk_mq_in_flight_rw()
+ *   - blk_mq_timeout_work() 两次
+ */
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv)
 {
@@ -335,7 +482,9 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 	int i;
 
 
+	/* q->nr_hw_queues在blk_mq_realloc_hw_ctxs()初始化hctx的时候获得 */
 	queue_for_each_hw_ctx(q, hctx, i) {
+		/* hctx->tags在blk_mq_init_hctx()中指向set->tags[hctx_idx] */
 		struct blk_mq_tags *tags = hctx->tags;
 
 		/*
@@ -345,6 +494,10 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		if (!blk_mq_hw_queue_mapped(hctx))
 			continue;
 
+		/*
+		 * 初始化一个struct bt_iter_data作为data参数
+		 * 对一sbitmap中每一个不为1的bit, 调用bt_iter(), 从0开始
+		 */
 		if (tags->nr_reserved_tags)
 			bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
 		bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
@@ -352,6 +505,9 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 
 }
 
+/*
+ * called only by blk_mq_init_bitmap_tags() 两次
+ */
 static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 		    bool round_robin, int node)
 {
@@ -359,12 +515,16 @@ static int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,
 				       node);
 }
 
+/*
+ * called only by blk_mq_init_tags()
+ */
 static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 						   int node, int alloc_policy)
 {
 	unsigned int depth = tags->nr_tags - tags->nr_reserved_tags;
 	bool round_robin = alloc_policy == BLK_TAG_ALLOC_RR;
 
+	/* 初始化sbitmap */
 	if (bt_alloc(&tags->bitmap_tags, depth, round_robin, node))
 		goto free_tags;
 	if (bt_alloc(&tags->breserved_tags, tags->nr_reserved_tags, round_robin,
@@ -379,6 +539,12 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 	return NULL;
 }
 
+/*
+ * called only by blk_mq_alloc_rq_map()
+ *
+ * 函数返回一个struct blk_mq_tags指针 (一个元素)
+ * 该函数分配了实例
+ */
 struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 				     unsigned int reserved_tags,
 				     int node, int alloc_policy)
@@ -407,6 +573,9 @@ void blk_mq_free_tags(struct blk_mq_tags *tags)
 	kfree(tags);
 }
 
+/*
+ * 被blk_mq_update_nr_requests()调用两次
+ */
 int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 			    struct blk_mq_tags **tagsptr, unsigned int tdepth,
 			    bool can_grow)
@@ -472,6 +641,9 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
  * Note: When called for a request that is queued on a non-multiqueue request
  * queue, the hardware context index is set to zero.
  */
+/*
+ * 并没有被block自己调用 全是被各种block driver调用
+ */
 u32 blk_mq_unique_tag(struct request *rq)
 {
 	struct request_queue *q = rq->q;
diff --git a/block/blk-mq-tag.h b/block/blk-mq-tag.h
index 61deab0..df7df02 100644
--- a/block/blk-mq-tag.h
+++ b/block/blk-mq-tag.h
@@ -7,18 +7,40 @@
 /*
  * Tag address space map.
  */
+/* 每个hw queue 一个 属于set*/
 struct blk_mq_tags {
+	/*
+	 * 在blk_mq_alloc_rq_map()-->blk_mq_init_tags()中初始化
+	 */
 	unsigned int nr_tags;
 	unsigned int nr_reserved_tags;
 
+	/*
+	 * 在__blk_mq_tag_busy()增加
+	 * 在__blk_mq_tag_idle()减少
+	 */
 	atomic_t active_queues;
 
-	struct sbitmap_queue bitmap_tags;
-	struct sbitmap_queue breserved_tags;
-
-	struct request **rqs;
-	struct request **static_rqs;
-	struct list_head page_list;
+	/*
+	 * 在blk_mq_alloc_rq_map()-->blk_mq_init_tags()-->blk_mq_init_bitmap_tags()中初始化
+	 */
+	struct sbitmap_queue bitmap_tags;    /* 总共有tags->nr_tags - tags->nr_reserved_tags个bit */
+	struct sbitmap_queue breserved_tags; /* 总共有tags->nr_reserved_tags个bit */
+
+	/*
+	 * 可能分配实例的地方 (其实不是实例 是指向已有的实例)
+	 *   <<blk_mq_tag_set_rq>> hctx->tags->rqs[tag] = rq;
+	 *   <<blk_mq_rq_ctx_init>> data->hctx->tags->rqs[rq->tag] = rq;
+	 *   <<blk_mq_get_driver_tag>> data.hctx->tags->rqs[rq->tag] = rq;
+	 *
+	 * index是tag!!
+	 */
+	struct request **rqs;  /* depth个 (struct request *) 在blk_mq_alloc_rq_map()中分配一维指针 */
+	/*
+	 * 在blk_mq_alloc_rqs()真正分配的实例! 第一维只是指针
+	 */
+	struct request **static_rqs;  /* depth个 (struct request *) */ /* 在__blk_mq_alloc_rq_map()中初始化 */
+	struct list_head page_list; /* 链接着存放static_rqs实例的page们 */
 };
 
 
@@ -36,11 +58,17 @@ extern void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);
 void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 		void *priv);
 
+/*
+ * called by:
+ *   - blk_mq_get_tag() 两次
+ *   - blk_mq_mark_tag_wait()
+ */
 static inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,
 						 struct blk_mq_hw_ctx *hctx)
 {
 	if (!hctx)
 		return &bt->ws[0];
+	/* 返回wait_index索引的下一个sbq->ws(struct sbq_wait_state类型) 增加wait_index */
 	return sbq_wait_ptr(bt, &hctx->wait_index);
 }
 
@@ -53,6 +81,11 @@ enum {
 extern bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *);
 extern void __blk_mq_tag_idle(struct blk_mq_hw_ctx *);
 
+/*
+ * called by:
+ *   - blk_mq_rq_ctx_init()
+ *   - blk_mq_get_driver_tag()
+ */
 static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -61,6 +94,11 @@ static inline bool blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 	return __blk_mq_tag_busy(hctx);
 }
 
+/*
+ * called by:
+ *   - blk_mq_exit_hctx()
+ *   - blk_mq_timeout_work()
+ */
 static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 {
 	if (!(hctx->flags & BLK_MQ_F_TAG_SHARED))
@@ -75,12 +113,24 @@ static inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
  * in flight at the same time. The caller has to make sure the tag
  * can't be freed.
  */
+/*
+ * called by:
+ *   - flush_end_io()
+ *   - blk_kick_flush()
+ *
+ * 每个hw queue一个struct blk_mq_hw_ctx
+ */
 static inline void blk_mq_tag_set_rq(struct blk_mq_hw_ctx *hctx,
 		unsigned int tag, struct request *rq)
 {
 	hctx->tags->rqs[tag] = rq;
 }
 
+/*
+ * called by:
+ *   - blk_mq_put_tag()
+ *   - blk_mq_get_driver_tag()
+ */
 static inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,
 					  unsigned int tag)
 {
diff --git a/block/blk-mq-virtio.c b/block/blk-mq-virtio.c
index c3afbca..7cdc691 100644
--- a/block/blk-mq-virtio.c
+++ b/block/blk-mq-virtio.c
@@ -29,6 +29,11 @@
  * that maps a queue to the CPUs that have irq affinity for the corresponding
  * vector.
  */
+/*
+ * called by:
+ *   - virtblk_map_queues() -- drivers/block/virtio_blk.c
+ *   - virtscsi_map_queues() -- drivers/scsi/virtio_scsi.c
+ */
 int blk_mq_virtio_map_queues(struct blk_mq_tag_set *set,
 		struct virtio_device *vdev, int first_vec)
 {
@@ -39,6 +44,7 @@ int blk_mq_virtio_map_queues(struct blk_mq_tag_set *set,
 		goto fallback;
 
 	for (queue = 0; queue < set->nr_hw_queues; queue++) {
+		/* 和pci的实现差不多 都是获得一个affinity */
 		mask = vdev->config->get_vq_affinity(vdev, first_vec + queue);
 		if (!mask)
 			goto fallback;
diff --git a/block/blk-mq.c b/block/blk-mq.c
index 90ffd81..a111deb 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -37,10 +37,39 @@
 #include "blk-wbt.h"
 #include "blk-mq-sched.h"
 
+/*
+ * notes:
+ *   - rq->mq_ctx (struct blk_mq_ctx类型) 的rq_list放着待处理的request (也许就这里???)
+ *   - elevator_queue里也可以存放request?
+ */
+
+/*
+ * __blk_mq_insert_req_list()被调用的一个例子:
+ *
+ * [0]  blk_mq_insert_requests
+ * [0]  blk_mq_sched_insert_requests
+ * [0]  blk_mq_flush_plug_list
+ * [0]  blk_flush_plug_list
+ * [0]  blk_finish_plug
+ * [0]  __do_page_cache_readahead
+ * [0]  force_page_cache_readahead
+ * [0]  generic_file_read_iter
+ * [0]  __vfs_read
+ * [0]  vfs_read
+ * [0]  ksys_read
+ * [0]  do_syscall_64
+ * [0]  entry_SYSCALL_64_after_hwframe
+ */
+
 static bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie);
 static void blk_mq_poll_stats_start(struct request_queue *q);
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb);
 
+/*
+ * called by:
+ *   - blk_mq_init_allocated_queue()
+ *   - blk_mq_poll_nsecs()
+ */
 static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, bytes, bucket;
@@ -61,16 +90,31 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
 /*
  * Check if any of the ctx's have pending work in this hardware queue
  */
+/*
+ * called only by blk_mq_run_hw_queue()
+ *
+ * 一共检查三处:
+ *   - hctx->dispatch
+ *   - hctx是否有ctx的rq_list有request
+ *   - 通过e->type->ops.mq.has_work(hctx);
+ */
 static bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
 	return !list_empty_careful(&hctx->dispatch) ||
 		sbitmap_any_bit_set(&hctx->ctx_map) ||
-			blk_mq_sched_has_work(hctx);
+			blk_mq_sched_has_work(hctx);// --> 调用scheduler的has_work()方法查看是否有request要处理
 }
 
 /*
  * Mark this ctx as having pending work in this hardware queue
  */
+/*
+ * called by:
+ *   - blk_mq_insert_requests()
+ *   - __blk_mq_insert_request()
+ *
+ * 配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request
+ */
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
@@ -78,6 +122,9 @@ static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 		sbitmap_set_bit(&hctx->ctx_map, ctx->index_hw);
 }
 
+/*
+ * called only by blk_mq_hctx_notify_dead()
+ */
 static void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,
 				      struct blk_mq_ctx *ctx)
 {
@@ -89,6 +136,10 @@ struct mq_inflight {
 	unsigned int *inflight;
 };
 
+/*
+ * 只被如下使用:
+ *   - block/blk-mq.c|146| <<blk_mq_in_flight>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
+ */
 static void blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 				  struct request *rq, void *priv,
 				  bool reserved)
@@ -106,6 +157,9 @@ static void blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 		mi->inflight[1]++;
 }
 
+/*
+ * called only by part_in_flight()
+ */
 void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
 		      unsigned int inflight[2])
 {
@@ -115,6 +169,9 @@ void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);
 }
 
+/*
+ * called only by blk_mq_in_flight_rw()
+ */
 static void blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 				     struct request *rq, void *priv,
 				     bool reserved)
@@ -125,6 +182,10 @@ static void blk_mq_check_inflight_rw(struct blk_mq_hw_ctx *hctx,
 		mi->inflight[rq_data_dir(rq)]++;
 }
 
+/*
+ * called by only:
+ *   - block/genhd.c|89| <<part_in_flight_rw>> blk_mq_in_flight_rw(q, part, inflight);
+ */
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2])
 {
@@ -134,10 +195,24 @@ void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 	blk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight_rw, &mi);
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|704| <<blk_set_queue_dying>> blk_freeze_queue_start(q);
+ *   - block/blk-mq.c|239| <<blk_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/block/mtip32xx/mtip32xx.c|4000| <<mtip_block_remove>> blk_freeze_queue_start(dd->queue);
+ *   - drivers/nvdimm/pmem.c|284| <<pmem_freeze_queue>> blk_freeze_queue_start(q);
+ *   - drivers/nvme/host/core.c|3550| <<nvme_start_freeze>> blk_freeze_queue_start(ns->queue);
+ */
 void blk_freeze_queue_start(struct request_queue *q)
 {
 	int freeze_depth;
 
+	/*
+	 * 只在blk_freeze_queue_start() 增加
+	 * 只在blk_mq_unfreeze_queue()  减少
+	 *
+	 * 开始的默认是0
+	 */
 	freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
 	if (freeze_depth == 1) {
 		percpu_ref_kill(&q->q_usage_counter);
@@ -147,12 +222,33 @@ void blk_freeze_queue_start(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
+/*
+ * called by:
+ *   - block/blk-mq.c|242| <<blk_freeze_queue>> blk_mq_freeze_queue_wait(q);
+ *   - drivers/nvme/host/core.c|3539| <<nvme_wait_freeze>> blk_mq_freeze_queue_wait(ns->queue);
+ */
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
+	/*
+	 * 在以下等待:
+	 *   - block/blk-core.c|953| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|230| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|241| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *
+	 * 在以下唤醒:
+	 *   - block/blk-core.c|447| <<blk_clear_preempt_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|722| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|972| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|293| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
 
+/*
+ * 只被如下调用:
+ *   - drivers/nvme/host/core.c|3525| <<nvme_wait_freeze_timeout>> timeout = blk_mq_freeze_queue_wait_timeout(ns->queue, timeout);
+ */
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout)
 {
@@ -166,6 +262,11 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);
  * Guarantee no request is in use, so we can change any data structure of
  * the queue afterward.
  */
+/*
+ * called by:
+ *   - block/blk-core.c|764| <<blk_cleanup_queue>> blk_freeze_queue(q);
+ *   - block/blk-mq.c|251| <<blk_mq_freeze_queue>> blk_freeze_queue(q);
+ */
 void blk_freeze_queue(struct request_queue *q)
 {
 	/*
@@ -195,6 +296,10 @@ void blk_mq_unfreeze_queue(struct request_queue *q)
 {
 	int freeze_depth;
 
+	/*
+	 * 只在blk_freeze_queue_start() 增加
+	 * 只在blk_mq_unfreeze_queue()  减少
+	 */
 	freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
 	WARN_ON_ONCE(freeze_depth < 0);
 	if (!freeze_depth) {
@@ -208,6 +313,12 @@ EXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);
  * FIXME: replace the scsi_internal_device_*block_nowait() calls in the
  * mpt3sas driver such that this function can be removed.
  */
+/*
+ * called by:
+ *   - block/blk-mq.c|337| <<blk_mq_quiesce_queue>> blk_mq_quiesce_queue_nowait(q);
+ *   - drivers/scsi/scsi_lib.c|3124| <<scsi_internal_device_block_nowait>> blk_mq_quiesce_queue_nowait(q);
+ *   - include/linux/blk-mq.h|383| <<scsi_internal_device_block_nowait>> void blk_mq_quiesce_queue_nowait(struct request_queue *q);
+ */
 void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
@@ -258,6 +369,10 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 }
 EXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);
 
+/*
+ * called only by:
+ *   - block/blk-core.c|707| <<blk_set_queue_dying>> blk_mq_wake_waiters(q);
+ */
 void blk_mq_wake_waiters(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -274,13 +389,25 @@ bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 }
 EXPORT_SYMBOL(blk_mq_can_queue);
 
+/*
+ * called only by blk_mq_get_request()
+ */
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+	/* 如果data->flags & BLK_MQ_REQ_INTERNAL返回的则是data->hctx->sched_tags */
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
+	/*
+	 * request来自static_rqs
+	 *
+	 * 这里的request可能不是来自set, 也可能是request_queue
+	 */
 	struct request *rq = tags->static_rqs[tag];
 	req_flags_t rq_flags = 0;
 
+	/*
+	 * 在blk_mq_get_request()当request_queue有elevator的时候会在data->flags上mark BLK_MQ_REQ_INTERNAL
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL) {
 		rq->tag = -1;
 		rq->internal_tag = tag;
@@ -336,6 +463,12 @@ static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - blk_mq_alloc_request()
+ *   - blk_mq_alloc_request_hctx()
+ *   - blk_mq_make_request()
+ */
 static struct request *blk_mq_get_request(struct request_queue *q,
 		struct bio *bio, unsigned int op,
 		struct blk_mq_alloc_data *data)
@@ -345,9 +478,11 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	unsigned int tag;
 	bool put_ctx_on_error = false;
 
+	/* 增加q_usage_counter */
 	blk_queue_enter_live(q);
 	data->q = q;
 	if (likely(!data->ctx)) {
+		/* 用get_cpu()获取当前的ctx */
 		data->ctx = blk_mq_get_ctx(q);
 		put_ctx_on_error = true;
 	}
@@ -357,6 +492,11 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		data->flags |= BLK_MQ_REQ_NOWAIT;
 
 	if (e) {
+		/*
+		 * 如果有scheduler 就设置BLK_MQ_REQ_INTERNAL!!!
+		 *
+		 * 主要在blk_mq_tags_from_data()会用到
+		 */
 		data->flags |= BLK_MQ_REQ_INTERNAL;
 
 		/*
@@ -364,15 +504,25 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 		 * dispatch list.
 		 */
 		if (!op_is_flush(op) && e->type->ops.mq.limit_depth)
-			e->type->ops.mq.limit_depth(op, data);
+			e->type->ops.mq.limit_depth(op, data); // 好像只有kyber_limit_depth()实现了
 	}
 
+	/*
+	 * ## 此时data的状态 ##
+	 *
+	 * 如果从blk_mq_make_request()来的:
+	 *   data->ctx和data->hctx都设置了
+	 *   data->flags = BLK_MQ_REQ_INTERNAL (有scheduler的情况下) | BLK_MQ_REQ_NOWAIT (REQ_NOWAIT的情况下)
+	 *
+	 * 如果上面internal开了 (有scheduler), tag是从sched_tags分的
+	 */
 	tag = blk_mq_get_tag(data);
 	if (tag == BLK_MQ_TAG_FAIL) {
 		if (put_ctx_on_error) {
 			blk_mq_put_ctx(data->ctx);
 			data->ctx = NULL;
 		}
+		/* 减少q_usage_counter */
 		blk_queue_exit(q);
 		return NULL;
 	}
@@ -380,6 +530,11 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	rq = blk_mq_rq_ctx_init(data, tag, op);
 	if (!op_is_flush(op)) {
 		rq->elv.icq = NULL;
+		/*
+		 * bfq_prepare_request()
+		 * kyber_prepare_request()  ---> 把request的token设置为-1
+		 * dd_prepare_request()
+		 */
 		if (e && e->type->ops.mq.prepare_request) {
 			if (e->type->icq_cache && rq_ioc(bio))
 				blk_mq_sched_assign_ioc(rq, bio);
@@ -392,6 +547,12 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	return rq;
 }
 
+/*
+ * called by:
+ *   - block/blk-core.c|1642| <<blk_get_request_flags>> req = blk_mq_alloc_request(q, op, flags);
+ *   - drivers/block/mtip32xx/mtip32xx.c|197| <<mtip_get_int_command>> rq = blk_mq_alloc_request(dd->queue, REQ_OP_DRV_IN, BLK_MQ_REQ_RESERVED);
+ *   - drivers/nvme/host/core.c|398| <<nvme_alloc_request>> req = blk_mq_alloc_request(q, op, flags);
+ */
 struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 		blk_mq_req_flags_t flags)
 {
@@ -518,6 +679,12 @@ inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(__blk_mq_end_request);
 
+/*
+ * 中断函数调用driver的set.ops.complete:
+ *
+ *     xen-blkfront : blkif_complete_rq()     ---> blk_mq_end_request()
+ *     virtio-blk   : virtblk_request_done()  ---> blk_mq_end_request()
+ */
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
@@ -526,13 +693,44 @@ void blk_mq_end_request(struct request *rq, blk_status_t error)
 }
 EXPORT_SYMBOL(blk_mq_end_request);
 
+/*
+ * 只在__blk_mq_complete_request()中
+ */
 static void __blk_mq_complete_request_remote(void *data)
 {
 	struct request *rq = data;
 
+	/*
+	 * 在blk_queue_softirq_done()配置
+	 *   - block/blk-mq.c|3119| <<blk_mq_init_allocated_queue>> blk_queue_softirq_done(q, set->ops->complete);
+	 *   - block/bsg-lib.c|331| <<bsg_setup_queue>> blk_queue_softirq_done(q, bsg_softirq_done);
+	 *   - drivers/block/null_blk.c|1791| <<null_add_dev>> blk_queue_softirq_done(nullb->q, null_softirq_done_fn);
+	 *   - drivers/md/dm-rq.c|720| <<dm_old_init_request_queue>> blk_queue_softirq_done(md->queue, dm_softirq_done);
+	 *   - drivers/scsi/scsi_lib.c|2277| <<scsi_old_alloc_queue>> blk_queue_softirq_done(q, scsi_softirq_done);
+	 *
+	 * 关于xen和kvm, set.ops.complete被设置为:
+	 *   - xen-blkfront : blkif_complete_rq()     ---> blk_mq_end_request()
+	 *   - virtio-blk   : virtblk_request_done()  ---> blk_mq_end_request()
+	 */
+
 	rq->q->softirq_done_fn(rq);
 }
 
+/*
+ * called by:
+ *   - blk_mq_complete_request() <-- 被各种驱动(比如中断函数)调用
+ *   - blk_mq_rq_timed_out()
+ *
+ * 这个函数应该是在中断函数执行???
+ *
+ * sq的驱动中断会调用: blk_complete_request() --> __blk_complete_request()
+ *     sq的blk_complete_request()会把工作delay到softirq处理
+ *     如果在相同cpu (或相同cache的cpu)就delay到当前cpu的softirq处理
+ *     否则通过ipi来delay到相关cpu的softirq处理
+ * mq的驱动中断会调用: blk_mq_complete_request()
+ *     基于目前的实现, 要么在当前cpu的中断环境执行后续处理 要么通过ipi从其他cpu的中断环境处理
+ *     目前mq不用softirq (以后不确定)
+ */
 static void __blk_mq_complete_request(struct request *rq)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -549,21 +747,54 @@ static void __blk_mq_complete_request(struct request *rq)
 		blk_stat_add(rq);
 	}
 
+	/* 如果不强制在同一个cpu group, 直接调用softirq_done_fn()处理了 */
 	if (!test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags)) {
+		/*
+		 * 在blk_queue_softirq_done()配置
+		 *   - block/blk-mq.c|3119| <<blk_mq_init_allocated_queue>> blk_queue_softirq_done(q, set->ops->complete);
+		 *   - block/bsg-lib.c|331| <<bsg_setup_queue>> blk_queue_softirq_done(q, bsg_softirq_done);
+		 *   - drivers/block/null_blk.c|1791| <<null_add_dev>> blk_queue_softirq_done(nullb->q, null_softirq_done_fn);
+		 *   - drivers/md/dm-rq.c|720| <<dm_old_init_request_queue>> blk_queue_softirq_done(md->queue, dm_softirq_done);
+		 *   - drivers/scsi/scsi_lib.c|2277| <<scsi_old_alloc_queue>> blk_queue_softirq_done(q, scsi_softirq_done);
+		 *
+		 * 关于xen和kvm, set.ops.complete被设置为:
+		 *   - xen-blkfront : blkif_complete_rq()     ---> blk_mq_end_request()
+		 *   - virtio-blk   : virtblk_request_done()  ---> blk_mq_end_request()
+		 */
 		rq->q->softirq_done_fn(rq);
 		return;
 	}
 
 	cpu = get_cpu();
+	/* 如果QUEUE_FLAG_SAME_FORCE设置了 这个if语句就不执行了 shared就是false */
 	if (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags))
 		shared = cpus_share_cache(cpu, ctx->cpu);
 
 	if (cpu != ctx->cpu && !shared && cpu_online(ctx->cpu)) {
+		/*
+		 * 如果当前的cpu不是ctx的cpu 用ipi让目标cpu执行__blk_mq_complete_request_remote()
+		 *
+		 * csd是struct __call_single_data类型
+		 */
 		rq->csd.func = __blk_mq_complete_request_remote;
 		rq->csd.info = rq;
 		rq->csd.flags = 0;
 		smp_call_function_single_async(ctx->cpu, &rq->csd);
 	} else {
+		/*
+		 * 如果满足条件, 直接在本地cpu执行
+		 *
+		 * 在blk_queue_softirq_done()配置
+		 *   - block/blk-mq.c|3119| <<blk_mq_init_allocated_queue>> blk_queue_softirq_done(q, set->ops->complete);
+		 *   - block/bsg-lib.c|331| <<bsg_setup_queue>> blk_queue_softirq_done(q, bsg_softirq_done);
+		 *   - drivers/block/null_blk.c|1791| <<null_add_dev>> blk_queue_softirq_done(nullb->q, null_softirq_done_fn);
+		 *   - drivers/md/dm-rq.c|720| <<dm_old_init_request_queue>> blk_queue_softirq_done(md->queue, dm_softirq_done);
+		 *   - drivers/scsi/scsi_lib.c|2277| <<scsi_old_alloc_queue>> blk_queue_softirq_done(q, scsi_softirq_done);
+		 *
+		 * 关于xen和kvm, set.ops.complete被设置为:
+		 *   - xen-blkfront : blkif_complete_rq()     ---> blk_mq_end_request()
+		 *   - virtio-blk   : virtblk_request_done()  ---> blk_mq_end_request()
+		 */
 		rq->q->softirq_done_fn(rq);
 	}
 	put_cpu();
@@ -589,6 +820,11 @@ static void hctx_lock(struct blk_mq_hw_ctx *hctx, int *srcu_idx)
 		*srcu_idx = srcu_read_lock(hctx->srcu);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|1095| <<blk_mq_rq_timed_out>> blk_mq_rq_update_aborted_gstate(req, 0);
+ *   - block/blk-mq.c|1131| <<blk_mq_check_expired>> blk_mq_rq_update_aborted_gstate(rq, gstate);
+ */
 static void blk_mq_rq_update_aborted_gstate(struct request *rq, u64 gstate)
 {
 	unsigned long flags;
@@ -606,6 +842,10 @@ static void blk_mq_rq_update_aborted_gstate(struct request *rq, u64 gstate)
 	local_irq_restore(flags);
 }
 
+/*
+ * called only by:
+ *   - block/blk-mq.c|875| <<blk_mq_complete_request>> if (blk_mq_rq_aborted_gstate(rq) != rq->gstate)
+ */
 static u64 blk_mq_rq_aborted_gstate(struct request *rq)
 {
 	unsigned int start;
@@ -627,9 +867,16 @@ static u64 blk_mq_rq_aborted_gstate(struct request *rq)
  *	Ends all I/O on a request. It does not handle partial completions.
  *	The actual completion happens out-of-order, through a IPI handler.
  **/
+/*
+ * 被各种驱动(比如中断函数)调用
+ *
+ * sq的驱动中断会调用: blk_complete_request() --> __blk_complete_request()
+ * mq的驱动中断会调用: blk_mq_complete_request()
+ */
 void blk_mq_complete_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
+	/* 获得哪个hw queue下发的request */
 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, rq->mq_ctx->cpu);
 	int srcu_idx;
 
@@ -660,20 +907,34 @@ int blk_mq_request_started(struct request *rq)
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
 
+/*
+ * 都是被各自的block驱动调用的
+ */
 void blk_mq_start_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
 
+	/* 调用scheduler的.started_request()方法 (目前没人实现) */
 	blk_mq_sched_started_request(rq);
 
 	trace_block_rq_issue(q, rq);
 
+	/*
+	 * track rq completion times
+	 */
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
 		blk_stat_set_issue(&rq->issue_stat, blk_rq_sectors(rq));
 		rq->rq_flags |= RQF_STATS;
 		wbt_issue(q->rq_wb, &rq->issue_stat);
 	}
 
+	/*
+	 * 两处设置了:
+	 *   - block/blk-mq.c|643| <<blk_mq_free_request>> blk_mq_rq_update_state(rq, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|975| <<__blk_mq_requeue_request>> blk_mq_rq_update_state(rq, MQ_RQ_IDLE);
+	 *
+	 * 剩下不设置默认应该是0吧 0就是MQ_RQ_IDLE
+	 */
 	WARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);
 
 	/*
@@ -690,12 +951,22 @@ void blk_mq_start_request(struct request *rq)
 	preempt_disable();
 	write_seqcount_begin(&rq->gstate_seq);
 
+	/* 只在这一处设置MQ_RQ_IN_FLIGHT */
 	blk_mq_rq_update_state(rq, MQ_RQ_IN_FLIGHT);
+	/*
+	 * 注册的timer处理函数是blk_rq_timed_out_timer()
+	 */
 	blk_add_timer(rq);
 
 	write_seqcount_end(&rq->gstate_seq);
 	preempt_enable();
 
+	/*
+	 * 在blk_queue_dma_drain()设置
+	 *
+	 * 只有一处设置了dma_drain_size:
+	 *    - drivers/ata/libata-scsi.c|1292| <<ata_scsi_dev_config>> blk_queue_dma_drain(q, atapi_drain_needed, buf, ATAPI_MAX_DRAIN);
+	 */
 	if (q->dma_drain_size && blk_rq_bytes(rq)) {
 		/*
 		 * Make sure space for the drain appears.  We know we can do
@@ -712,6 +983,12 @@ EXPORT_SYMBOL(blk_mq_start_request);
  * to IDLE without checking @rq->aborted_gstate because we should still be
  * holding the RCU read lock and thus protected against timeout.
  */
+/*
+ * called by:
+ *   - blk_mq_requeue_request()
+ *   - blk_mq_dispatch_rq_list()
+ *   - __blk_mq_issue_directly()
+ */
 static void __blk_mq_requeue_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -728,6 +1005,9 @@ static void __blk_mq_requeue_request(struct request *rq)
 	}
 }
 
+/*
+ * 被好多驱动调用
+ */
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 {
 	__blk_mq_requeue_request(rq);
@@ -740,6 +1020,15 @@ void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 }
 EXPORT_SYMBOL(blk_mq_requeue_request);
 
+/*
+ * 被设置为:
+ *   - block/blk-mq.c|3352| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+ *
+ * 用到的地方:
+ *   - block/blk-core.c|427| <<blk_sync_queue>> cancel_delayed_work_sync(&q->requeue_work);
+ *   - block/blk-mq.c|1063| <<blk_mq_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
+ *   - block/blk-mq.c|1070| <<blk_mq_delay_kick_requeue_list>> kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,
+ */
 static void blk_mq_requeue_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -747,28 +1036,51 @@ static void blk_mq_requeue_work(struct work_struct *work)
 	LIST_HEAD(rq_list);
 	struct request *rq, *next;
 
+	/*
+	 * 以下的是添加元素的地方:
+	 *   - block/blk-mq.c|1073| <<blk_mq_add_to_requeue_list>> list_add(&rq->queuelist, &q->requeue_list);
+	 *   - block/blk-mq.c|1075| <<blk_mq_add_to_requeue_list>> list_add_tail(&rq->queuelist, &q->requeue_list);
+	 */
 	spin_lock_irq(&q->requeue_lock);
 	list_splice_init(&q->requeue_list, &rq_list);
 	spin_unlock_irq(&q->requeue_lock);
 
 	list_for_each_entry_safe(rq, next, &rq_list, queuelist) {
+		/* 没有RQF_SOFTBARRIER就略过 */
 		if (!(rq->rq_flags & RQF_SOFTBARRIER))
 			continue;
 
 		rq->rq_flags &= ~RQF_SOFTBARRIER;
 		list_del_init(&rq->queuelist);
+		/*
+		 * 如果scheduler实现了insert方法 (也就是说有自己的队列) 就放入schedule自己的队列
+		 * 否则就放入默认的ctx的队列
+		 *
+		 * 最后还有个run: 如果run_queue是true, 就blk_mq_run_hw_queue() ---> 这里run_queue是false
+		 */
 		blk_mq_sched_insert_request(rq, true, false, false);
 	}
 
 	while (!list_empty(&rq_list)) {
 		rq = list_entry(rq_list.next, struct request, queuelist);
 		list_del_init(&rq->queuelist);
+		/*
+		 * 如果scheduler实现了insert方法 (也就是说有自己的队列) 就放入schedule自己的队列
+		 * 否则就放入默认的ctx的队列
+		 *
+		 * 最后还有个run: 如果run_queue是true, 就blk_mq_run_hw_queue() ---> 这里run_queue是false
+		 */
 		blk_mq_sched_insert_request(rq, false, false, false);
 	}
 
 	blk_mq_run_hw_queues(q, false);
 }
 
+/*
+ * called by:
+ *   - block/blk-flush.c|143| <<blk_flush_queue_rq>> blk_mq_add_to_requeue_list(rq, add_front, true);
+ *   - block/blk-mq.c|1019| <<blk_mq_requeue_request>> blk_mq_add_to_requeue_list(rq, true, kick_requeue_list);
+ */
 void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 				bool kick_requeue_list)
 {
@@ -795,12 +1107,24 @@ void blk_mq_add_to_requeue_list(struct request *rq, bool at_head,
 }
 EXPORT_SYMBOL(blk_mq_add_to_requeue_list);
 
+/*
+ * called by:
+ *   - block/blk-mq-debugfs.c|176| <<queue_state_write>> blk_mq_kick_requeue_list(q);
+ *   - block/blk-mq.c|1106| <<blk_mq_add_to_requeue_list>> blk_mq_kick_requeue_list(q);
+ *   - drivers/block/xen-blkfront.c|2042| <<blkif_recover>> blk_mq_kick_requeue_list(info->rq);
+ *   - drivers/md/dm-rq.c|75| <<dm_mq_start_queue>> blk_mq_kick_requeue_list(q);
+ *   - drivers/s390/block/scm_blk.c|247| <<scm_request_requeue>> blk_mq_kick_requeue_list(bdev->rq);
+ */
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
 	kblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);
 }
 EXPORT_SYMBOL(blk_mq_kick_requeue_list);
 
+/*
+ * called by:
+ *   - drivers/md/dm-rq.c|248| <<__dm_mq_kick_requeue_list>> blk_mq_delay_kick_requeue_list(q, msecs);
+ */
 void blk_mq_delay_kick_requeue_list(struct request_queue *q,
 				    unsigned long msecs)
 {
@@ -826,13 +1150,25 @@ struct blk_mq_timeout_data {
 	unsigned int nr_expired;
 };
 
+/*
+ * called by only blk_mq_terminate_expired()
+ */
 static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 {
 	const struct blk_mq_ops *ops = req->q->mq_ops;
+	/* 在include/linux/blkdev.h定义 */
 	enum blk_eh_timer_return ret = BLK_EH_RESET_TIMER;
 
+	/* timeout is expired */
+	/* 标记这个request过期了 */
 	req->rq_flags |= RQF_MQ_TIMEOUT_EXPIRED;
 
+	/*
+	 * nvme的例子是nvme_timeout()
+	 * scsi的例子是scsi_timeout()
+	 *
+	 * xen和virtio没实现
+	 */
 	if (ops->timeout)
 		ret = ops->timeout(req, reserved);
 
@@ -847,6 +1183,7 @@ static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 		 * completions and further spurious timeouts.
 		 */
 		blk_mq_rq_update_aborted_gstate(req, 0);
+		/* timer的处理函数是blk_rq_timed_out_timer() */
 		blk_add_timer(req);
 		break;
 	case BLK_EH_NOT_HANDLED:
@@ -857,6 +1194,10 @@ static void blk_mq_rq_timed_out(struct request *req, bool reserved)
 	}
 }
 
+/*
+ * 只在一个地方被使用:
+ *   - block/blk-mq.c|1200| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &data);
+ */
 static void blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -891,6 +1232,10 @@ static void blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 	}
 }
 
+/*
+ * 只被一处使用:
+ *   - block/blk-mq.c|1234| <<blk_mq_timeout_work>> blk_mq_queue_tag_busy_iter(q, blk_mq_terminate_expired, NULL);
+ */
 static void blk_mq_terminate_expired(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, void *priv, bool reserved)
 {
@@ -906,6 +1251,10 @@ static void blk_mq_terminate_expired(struct blk_mq_hw_ctx *hctx,
 		blk_mq_rq_timed_out(rq, reserved);
 }
 
+/*
+ * 初始化为request_queue的timeout_work:
+ *   - block/blk-mq.c|3314| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+ */
 static void blk_mq_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -988,6 +1337,13 @@ struct flush_busy_ctx_data {
 	struct list_head *list;
 };
 
+/*
+ * 只在如下被使用:
+ *   - block/blk-mq.c|1337| <<blk_mq_flush_busy_ctxs>> sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
+ *
+ * 把ctx->rq_list中的request们放入flush_data->list
+ * 在sbitmap中清空对应的bit
+ */
 static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
 {
 	struct flush_busy_ctx_data *flush_data = data;
@@ -995,6 +1351,11 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
 	struct blk_mq_ctx *ctx = hctx->ctxs[bitnr];
 
 	spin_lock(&ctx->lock);
+	/*
+	 * flush_data->list是输出
+	 *
+	 * rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+	 */
 	list_splice_tail_init(&ctx->rq_list, flush_data->list);
 	sbitmap_clear_bit(sb, bitnr);
 	spin_unlock(&ctx->lock);
@@ -1005,6 +1366,13 @@ static bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)
  * Process software queues that have been marked busy, splicing them
  * to the for-dispatch
  */
+/*
+ * 参数list是输出
+ * 对一sbitmap中每一个不为1的bit, 调用flush_busy_ctx(), 从0开始
+ * 调用flush_busy_ctx会:
+ *     把ctx->rq_list中的request们放入flush_data->list (参数list)
+ *     在sbitmap中清空对应的bit
+ */
 void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 {
 	struct flush_busy_ctx_data data = {
@@ -1012,6 +1380,12 @@ void blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)
 		.list = list,
 	};
 
+	/*
+	 * 对一sbitmap中每一个不为1的bit, 调用flush_busy_ctx(), 从0开始
+	 * 调用flush_busy_ctx会:
+	 *     把ctx->rq_list中的request们放入flush_data->list (参数list)
+	 *     在sbitmap中清空对应的bit
+	 */
 	sbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);
 }
 EXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);
@@ -1021,6 +1395,12 @@ struct dispatch_rq_data {
 	struct request *rq;
 };
 
+/*
+ * 只被blk_mq_dequeue_from_ctx()用作fn()
+ *
+ * 从bitnr代表的ctx取出一个request, 放入dispatch_data->rq
+ * 如果有request则返回false, 否则返回true
+ */
 static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 		void *data)
 {
@@ -1029,7 +1409,9 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	struct blk_mq_ctx *ctx = hctx->ctxs[bitnr];
 
 	spin_lock(&ctx->lock);
+	/* rq_list上放的是用__blk_mq_insert_request()插入的待处理的request */
 	if (unlikely(!list_empty(&ctx->rq_list))) {
+		/* 从ctx->rq_list取出一个request放入dispatch_data->rq */
 		dispatch_data->rq = list_entry_rq(ctx->rq_list.next);
 		list_del_init(&dispatch_data->rq->queuelist);
 		if (list_empty(&ctx->rq_list))
@@ -1040,6 +1422,11 @@ static bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,
 	return !dispatch_data->rq;
 }
 
+/*
+ * called only by blk_mq_do_dispatch_ctx()
+ *
+ * 以start为开始, 遍历hctx的sbitmap, 如果找到一个pending的request就返回
+ */
 struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 					struct blk_mq_ctx *start)
 {
@@ -1049,6 +1436,16 @@ struct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,
 		.rq   = NULL,
 	};
 
+	/*
+	 * 对一sbitmap中每一个不为1的bit (这里一个bit是一个sw queue), 调用dispatch_rq_from_ctx()
+	 * 如果dispatch_rq_from_ctx()返回false (0), 则退出
+	 *
+	 * off是开始的bit
+	 *
+	 * dispatch_rq_from_ctx():
+	 *     从bitnr代表的ctx取出一个request, 放入dispatch_data->rq
+	 *     如果有request则返回false, 否则返回true
+	 */
 	__sbitmap_for_each_set(&hctx->ctx_map, off,
 			       dispatch_rq_from_ctx, &data);
 
@@ -1063,6 +1460,12 @@ static inline unsigned int queued_to_index(unsigned int queued)
 	return min(BLK_MQ_MAX_DISPATCH_ORDER - 1, ilog2(queued) + 1);
 }
 
+/*
+ * called by:
+ *   - blk_mq_mark_tag_wait() 两处
+ *   - blk_mq_dispatch_rq_list() 两处
+ *   - __blk_mq_try_issue_directly()
+ */
 bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 			   bool wait)
 {
@@ -1095,6 +1498,10 @@ bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 	return rq->tag != -1;
 }
 
+/*
+ * 用作dispatch_wait的func:
+ *   - block/blk-mq.c|2891| <<blk_mq_init_hctx>> init_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);
+ */
 static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,
 				int flags, void *key)
 {
@@ -1174,6 +1581,14 @@ static bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx **hctx,
 
 #define BLK_MQ_RESOURCE_DELAY	3		/* ms units */
 
+/*
+ * called by:
+ *   - blk_mq_do_dispatch_sched()
+ *   - blk_mq_do_dispatch_ctx()
+ *   - blk_mq_sched_dispatch_requests() 两处
+ *
+ * 用.queue_rq()下发list中的request
+ */
 bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			     bool got_budget)
 {
@@ -1197,6 +1612,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 
 		rq = list_first_entry(list, struct request, queuelist);
 
+		/* 根据rq的sw queue的cpu获得对应的hw queue */
 		hctx = blk_mq_map_queue(rq->q, rq->mq_ctx->cpu);
 		if (!got_budget && !blk_mq_get_dispatch_budget(hctx))
 			break;
@@ -1236,6 +1652,7 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 			bd.last = !blk_mq_get_driver_tag(nxt, NULL, false);
 		}
 
+		/* 很重要的地方!!!! */
 		ret = q->mq_ops->queue_rq(hctx, &bd);
 		if (ret == BLK_STS_RESOURCE || ret == BLK_STS_DEV_RESOURCE) {
 			/*
@@ -1309,6 +1726,13 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list,
 	return (queued + errors) != 0;
 }
 
+/*
+ * called by:
+ *   - __blk_mq_delay_run_hw_queue()
+ *   - blk_mq_run_work_fn() ----------> hctx->run_work在blk_mq_init_hctx()设置为blk_mq_run_work_fn()
+ *
+ * 这个函数很重要
+ */
 static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	int srcu_idx;
@@ -1406,12 +1830,24 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 	return next_cpu;
 }
 
+/*
+ * called by:
+ *   - blk_mq_delay_run_hw_queue()
+ *   - blk_mq_run_hw_queue()
+ *
+ * 根据情况要么立刻处理 要们在worker稍后处理
+ */
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
+	/* &hctx->state的BLK_MQ_S_STOPPED是否设置了 */
 	if (unlikely(blk_mq_hctx_stopped(hctx)))
 		return;
 
+	/*
+	 * 如果来的是如下路径则async是true!
+	 * blk_mq_make_request()-->blk_mq_run_hw_queue()-->__blk_mq_delay_run_hw_queue()
+	 */
 	if (!async && !(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		int cpu = get_cpu();
 		if (cpumask_test_cpu(cpu, hctx->cpumask)) {
@@ -1423,6 +1859,9 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 		put_cpu();
 	}
 
+	/*
+	 * run_work在blk_mq_init_hctx()设置为blk_mq_run_work_fn()
+	 */
 	kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,
 				    msecs_to_jiffies(msecs));
 }
@@ -1433,6 +1872,24 @@ void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
+/*
+ * called by:
+ *   - mq_flush_data_end_io()
+ *   - blk_mq_sched_restart_hctx()
+ *   - blk_mq_sched_insert_request()
+ *   - blk_mq_sched_insert_requests()
+ *   - blk_mq_get_tag()
+ *   - blk_mq_dispatch_wake()
+ *   - blk_mq_dispatch_rq_list()
+ *   - blk_mq_run_hw_queues()
+ *   - blk_mq_start_hw_queue()
+ *   - blk_mq_start_stopped_hw_queue()
+ *   - blk_mq_request_bypass_insert()
+ *   - blk_mq_make_request()
+ *   - blk_mq_make_request()
+ *   - blk_mq_hctx_notify_dead()
+ *   - kyber_domain_wake()
+ */
 bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
 	int srcu_idx;
@@ -1448,10 +1905,11 @@ bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 	 */
 	hctx_lock(hctx, &srcu_idx);
 	need_run = !blk_queue_quiesced(hctx->queue) &&
-		blk_mq_hctx_has_pending(hctx);
+		blk_mq_hctx_has_pending(hctx); // Check if any of the ctx's have pending work in this hardware queue
 	hctx_unlock(hctx, srcu_idx);
 
 	if (need_run) {
+		/* 根据情况要么立刻处理 要们在worker稍后处理 */
 		__blk_mq_delay_run_hw_queue(hctx, async, 0);
 		return true;
 	}
@@ -1462,6 +1920,7 @@ EXPORT_SYMBOL(blk_mq_run_hw_queue);
 
 void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 {
+	/* State for a hardware queue facing the hardware block device */
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
@@ -1530,6 +1989,9 @@ void blk_mq_stop_hw_queues(struct request_queue *q)
 }
 EXPORT_SYMBOL(blk_mq_stop_hw_queues);
 
+/*
+ * called only by blk_mq_start_hw_queues()
+ */
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
 	clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -1568,6 +2030,9 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 }
 EXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);
 
+/*
+ * hctx->run_work在blk_mq_init_hctx()设置为blk_mq_run_work_fn()
+ */
 static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -1583,6 +2048,13 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 	__blk_mq_run_hw_queue(hctx);
 }
 
+/*
+ * called by:
+ *   - __blk_mq_insert_request()
+ *   - blk_mq_insert_requests()
+ *
+ * 把request链接到软件队列struct blk_mq_ctx的rq_list
+ */
 static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    bool at_head)
@@ -1593,12 +2065,21 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 
 	trace_block_rq_insert(hctx->queue, rq);
 
+	/* rq_list上放的是用__blk_mq_insert_request()插入的待处理的request */
 	if (at_head)
 		list_add(&rq->queuelist, &ctx->rq_list);
 	else
 		list_add_tail(&rq->queuelist, &ctx->rq_list);
 }
 
+/*
+ * called by:
+ *   - blk_mq_sched_insert_request()
+ *   - blk_mq_queue_io()
+ *
+ * 把request链接到软件队列struct blk_mq_ctx的rq_list
+ * 然后配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request
+ */
 void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			     bool at_head)
 {
@@ -1606,7 +2087,9 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 
 	lockdep_assert_held(&ctx->lock);
 
+	/* 把request链接到软件队列struct blk_mq_ctx的rq_list */
 	__blk_mq_insert_req_list(hctx, rq, at_head);
+	/* 配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 }
 
@@ -1614,6 +2097,9 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
  * Should only be used carefully, when the caller knows we want to
  * bypass a potential IO scheduler on the target device.
  */
+/*
+ * called only by blk_insert_flush()
+ */
 void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
@@ -1627,6 +2113,13 @@ void blk_mq_request_bypass_insert(struct request *rq, bool run_queue)
 		blk_mq_run_hw_queue(hctx, false);
 }
 
+/*
+ * called only by blk_mq_sched_insert_requests()
+ *
+ * 在一个循环中对于list中的没一个request, 从list删除
+ * 把request链接到软件队列struct blk_mq_ctx的rq_list
+ * 循环结束后配置hctx的ctx_map (sbitmap), sw的ctx在hw的hctx上有等待的request
+ */
 void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 			    struct list_head *list)
 
@@ -1642,8 +2135,10 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 		rq = list_first_entry(list, struct request, queuelist);
 		BUG_ON(rq->mq_ctx != ctx);
 		list_del_init(&rq->queuelist);
+		/* 把request链接到软件队列struct blk_mq_ctx的rq_list */
 		__blk_mq_insert_req_list(hctx, rq, false);
 	}
+	/* 配置hctx的ctx_map (sbitmap), sw的ctx在hw的hctx上有等待的request */
 	blk_mq_hctx_mark_pending(hctx, ctx);
 	spin_unlock(&ctx->lock);
 }
@@ -1658,6 +2153,14 @@ static int plug_ctx_cmp(void *priv, struct list_head *a, struct list_head *b)
 		  blk_rq_pos(rqa) < blk_rq_pos(rqb)));
 }
 
+/*
+ * called only by blk_flush_plug_list()
+ *
+ * plug的故事似乎是这样:
+ * 1. 先blk_start_plug(), 好多好多地方都调用了blk_start_plug(), 参数plug一般是caller的本地变量
+ * 2. 做好多操作
+ * 3. 最后blk_finish_plug()
+ */
 void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 {
 	struct blk_mq_ctx *this_ctx;
@@ -1702,11 +2205,17 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	 */
 	if (this_ctx) {
 		trace_block_unplug(this_q, depth, from_schedule);
+		/*
+		 * blk_mq_sched_insert_requests()只被这里的blk_mq_flush_plug_list()调用
+		 */
 		blk_mq_sched_insert_requests(this_q, this_ctx, &ctx_list,
 						from_schedule);
 	}
 }
 
+/*
+ * 只在blk_mq_make_request()被多次使用
+ */
 static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 {
 	blk_init_request_from_bio(rq, bio);
@@ -1716,10 +2225,20 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 	blk_account_io_start(rq, true);
 }
 
+/*
+ * called only by blk_mq_make_request()
+ *
+ * 把request链接到软件队列struct blk_mq_ctx的rq_list
+ * 然后配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request
+ */
 static inline void blk_mq_queue_io(struct blk_mq_hw_ctx *hctx,
 				   struct blk_mq_ctx *ctx,
 				   struct request *rq)
 {
+	/*
+	 * 把request链接到软件队列struct blk_mq_ctx的rq_list
+	 * 然后配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request
+	 */
 	spin_lock(&ctx->lock);
 	__blk_mq_insert_request(hctx, rq, false);
 	spin_unlock(&ctx->lock);
@@ -1733,6 +2252,11 @@ static blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx, struct request *rq)
 	return blk_tag_to_qc_t(rq->internal_tag, hctx->queue_num, true);
 }
 
+/*
+ * called by only __blk_mq_try_issue_directly()
+ *
+ * 直接调用 .queue_rq()
+ */
 static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 					    struct request *rq,
 					    blk_qc_t *cookie)
@@ -1769,6 +2293,11 @@ static blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - blk_mq_try_issue_directly()
+ *   - blk_mq_request_issue_directly()
+ */
 static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 						struct request *rq,
 						blk_qc_t *cookie,
@@ -1801,15 +2330,27 @@ static blk_status_t __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		goto insert;
 	}
 
+	/*
+	 * 直接调用 .queue_rq()
+	 */
 	return __blk_mq_issue_directly(hctx, rq, cookie);
 insert:
 	if (bypass_insert)
 		return BLK_STS_RESOURCE;
 
+	/*
+	 * 如果scheduler实现了insert方法 (也就是说有自己的队列) 就放入schedule自己的队列
+	 * 否则就放入默认的ctx的队列
+	 *
+	 * 最后还有个run: 如果run_queue是true, 就blk_mq_run_hw_queue()
+	 */
 	blk_mq_sched_insert_request(rq, false, run_queue, false);
 	return BLK_STS_OK;
 }
 
+/*
+ * 在两处被blk_mq_make_request()调用
+ */
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
@@ -1829,6 +2370,9 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 	hctx_unlock(hctx, srcu_idx);
 }
 
+/*
+ * called by only blk_insert_cloned_request()
+ */
 blk_status_t blk_mq_request_issue_directly(struct request *rq)
 {
 	blk_status_t ret;
@@ -1860,20 +2404,52 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	blk_queue_split(q, &bio);
 
+	/*
+	 * 如果硬件不支持 可以忽略
+	 */
 	if (!bio_integrity_prep(bio))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * blk_attempt_plug_merge():
+	 *     试一下能不能和current->plug里的已经存在的request们merge
+	 *     如果q->mq_ops存在, 使用mq_list, 否则使用list
+	 */
 	if (!is_flush_fua && !blk_queue_nomerges(q) &&
 	    blk_attempt_plug_merge(q, bio, &request_count, &same_queue_rq))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * 先检查request_queue和bio是否都支持merge
+	 *     mq-deadline是通过struct elevator_queue的hash来查看
+	 * 然后如果支持e->type->ops.mq.bio_merge就用相关scheduler的函数进行merge
+	 *     遍历blk_mq_ctx.rq_list尝试bio是否可以和某一个request来merge
+	 *     blk_mq_ctx.rq_list上放的是用__blk_mq_insert_request()插入的待处理的request
+	 * 否则根据情况用blk_mq_attempt_merge()
+	 *
+	 * .bio_merge:
+	 *   - bfq_bio_merge()
+	 *   - dd_bio_merge()
+	 *
+	 * 总之 如果用mq-deadline的话:
+	 * 1. 先查看struct elevator_queue可否merge
+	 * 2. 再遍历blk_mq_ctx.rq_list上是否可merge
+	 */
 	if (blk_mq_sched_bio_merge(q, bio))
 		return BLK_QC_T_NONE;
 
+	/*
+	 * buffered writeback throttling
+	 */
 	wb_acct = wbt_wait(q->rq_wb, bio, NULL);
 
 	trace_block_getrq(q, bio, bio->bi_opf);
 
+	/*
+	 * 这个函数对data进行了修改
+	 *
+	 * data一开始什么都没有 就一个.flag=0
+	 */
 	rq = blk_mq_get_request(q, bio, bio->bi_opf, &data);
 	if (unlikely(!rq)) {
 		__wbt_done(q->rq_wb, wb_acct);
@@ -1886,6 +2462,21 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 
 	cookie = request_to_qc_t(data.hctx, rq);
 
+	/*
+	 * 此时如果有scheduler:
+	 *     tag从data->hctx->sched_tags分配 (sched_tags在blk_mq_sched_alloc_tags()分配)
+	 *     request来自tags->static_rqs[tag]  ---> 注意tags的来源, 是data->hctx->sched_tags
+	 *     rq->tag = -1;
+	 *     rq->internal_tag = tag;
+	 *
+	 * 此时如果没有scheduler:
+	 *     tag从data->hctx->tags分配 (tags在blk_mq_init_hctx()中指向set->tags[hctx_idx])
+	 *     request来自tags->static_rqs[tag]  ---> 注意tags的来源, 是data->hctx->tags
+	 *     rq->tag = tag;
+	 *     rq->internal_tag = -1;
+	 *     data->hctx->tags->rqs[rq->tag] = rq;
+	 */
+
 	plug = current->plug;
 	if (unlikely(is_flush_fua)) {
 		blk_mq_put_ctx(data.ctx);
@@ -1932,7 +2523,7 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 		 * the plug list is empty, and same_queue_rq is invalid.
 		 */
 		if (list_empty(&plug->mq_list))
-			same_queue_rq = NULL;
+			same_queue_rq = NULL; // 是struct request类型
 		if (same_queue_rq)
 			list_del_init(&same_queue_rq->queuelist);
 		list_add_tail(&rq->queuelist, &plug->mq_list);
@@ -1950,13 +2541,47 @@ static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 		blk_mq_bio_to_request(rq, bio);
 		blk_mq_try_issue_directly(data.hctx, rq, &cookie);
 	} else if (q->elevator) {
+		/*
+		 * 能到这里说明一定一定支持elevaor!!
+		 *
+		 * 此时如果有scheduler:
+		 *       tag从data->hctx->sched_tags分配 (sched_tags在blk_mq_sched_alloc_tags()分配)
+		 *       request来自tags->static_rqs[tag]
+		 *       rq->tag = -1;
+		 *       rq->internal_tag = tag;
+		 */
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 注释忽略flush和hctx->dispatch的情况
+		 *
+		 * 如果scheduler实现了insert方法 (也就是说有自己的队列) 就放入schedule自己的队列
+		 * 否则就放入默认的ctx的队列
+		 *
+		 * 最后还有个run: 如果run_queue是true, 就blk_mq_run_hw_queue()
+		 */
 		blk_mq_sched_insert_request(rq, false, true, true);
 	} else {
+		/*
+		 * 能到这里说明不支持elevator了吧?!!!
+		 *
+		 * 如果没有scheduler:
+		 *       tag从data->hctx->tags分配 (tags在blk_mq_init_hctx()中指向set->tags[hctx_idx])
+		 *       request来自tags->static_rqs[tag]
+		 *       rq->tag = tag;
+		 *       rq->internal_tag = -1;
+		 *       data->hctx->tags->rqs[rq->tag] = rq;
+		 */
 		blk_mq_put_ctx(data.ctx);
 		blk_mq_bio_to_request(rq, bio);
+		/*
+		 * 把request链接到软件队列struct blk_mq_ctx的rq_list
+		 * 然后配置hctx的ctx_map (sbitmap), 标明sw的ctx在hw的hctx上有等待的request
+		 */
 		blk_mq_queue_io(data.hctx, data.ctx, rq);
+		/*
+		 * 这里async是true!
+		 */
 		blk_mq_run_hw_queue(data.hctx, true);
 	}
 
@@ -2003,6 +2628,15 @@ void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 	blk_mq_free_tags(tags);
 }
 
+/*
+ * called by:
+ *   - blk_mq_sched_alloc_tags()
+ *   - blk_mq_tag_update_depth()
+ *   - __blk_mq_alloc_rq_map()
+ *
+ * 分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+ * 每个hw queue一个
+ */
 struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 					unsigned int hctx_idx,
 					unsigned int nr_tags,
@@ -2011,10 +2645,15 @@ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 	struct blk_mq_tags *tags;
 	int node;
 
+	/* 返回hw queue index (hctx_idx)对应的numa node */
 	node = blk_mq_hw_queue_to_node(set->mq_map, hctx_idx);
 	if (node == NUMA_NO_NODE)
 		node = set->numa_node;
 
+	/*
+	 * 函数返回一个struct blk_mq_tags指针 (一个元素)
+	 * 该函数分配了实例
+	 */
 	tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
 				BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
 	if (!tags)
@@ -2045,6 +2684,11 @@ static size_t order_to_size(unsigned int order)
 	return (size_t)PAGE_SIZE << order;
 }
 
+/*
+ * called by:
+ *   - blk_mq_alloc_rqs()
+ *   - blk_mq_init_hctx()
+ */
 static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 			       unsigned int hctx_idx, int node)
 {
@@ -2068,6 +2712,19 @@ static int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - blk_mq_sched_alloc_tags()
+ *   - blk_mq_tag_update_depth()
+ *   - __blk_mq_alloc_rq_map()
+ *
+ *   set: request_queue一个
+ *   tags: 某一个hw queue的struct blk_mq_tags
+ *   hctx_idx: 某一个hw queue
+ *
+ *   不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+ *   page可能不连续 都链接在tags->page_list
+ */
 int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 		     unsigned int hctx_idx, unsigned int depth)
 {
@@ -2085,11 +2742,16 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 	 * rq_size is the size of the request plus driver payload, rounded
 	 * to the cacheline size
 	 */
+	/*
+	 * struct request后面会为不同的block driver附属不同的cmd_size
+	 * 比如xen-blkfront是sizeof(struct blkif_req)
+	 */
 	rq_size = round_up(sizeof(struct request) + set->cmd_size,
 				cache_line_size());
 	left = rq_size * depth;
 
 	for (i = 0; i < depth; ) {
+		/* max_order初始化是4 */
 		int this_order = max_order;
 		struct page *page;
 		int to_do;
@@ -2160,6 +2822,7 @@ static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 	ctx = __blk_mq_get_ctx(hctx->queue, cpu);
 
 	spin_lock(&ctx->lock);
+	/* rq_list上放的是用__blk_mq_insert_request()插入的待处理的request */
 	if (!list_empty(&ctx->rq_list)) {
 		list_splice_init(&ctx->rq_list, &tmp);
 		blk_mq_hctx_clear_pending(hctx, ctx);
@@ -2184,6 +2847,11 @@ static void blk_mq_remove_cpuhp(struct blk_mq_hw_ctx *hctx)
 }
 
 /* hctx->ctxs will be freed in queue's release handler */
+/*
+ * called by:
+ *   - blk_mq_exit_hw_queues()
+ *   - blk_mq_realloc_hw_ctxs()
+ */
 static void blk_mq_exit_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
@@ -2209,6 +2877,9 @@ static void blk_mq_exit_hctx(struct request_queue *q,
 	sbitmap_free(&hctx->ctx_map);
 }
 
+/*
+ * called only by blk_mq_free_queue()
+ */
 static void blk_mq_exit_hw_queues(struct request_queue *q,
 		struct blk_mq_tag_set *set, int nr_queue)
 {
@@ -2222,6 +2893,11 @@ static void blk_mq_exit_hw_queues(struct request_queue *q,
 	}
 }
 
+/*
+ * called only by blk_mq_realloc_hw_ctxs()
+ *
+ * 参数hctx其实是q->queue_hw_ctx[hctx_idx]
+ */
 static int blk_mq_init_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)
@@ -2297,6 +2973,9 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	return -1;
 }
 
+/*
+ * called by blk_mq_init_allocated_queue()
+ */
 static void blk_mq_init_cpu_queues(struct request_queue *q,
 				   unsigned int nr_hw_queues)
 {
@@ -2308,6 +2987,7 @@ static void blk_mq_init_cpu_queues(struct request_queue *q,
 
 		__ctx->cpu = i;
 		spin_lock_init(&__ctx->lock);
+		/* rq_list上放的是用__blk_mq_insert_request()插入的待处理的request */
 		INIT_LIST_HEAD(&__ctx->rq_list);
 		__ctx->queue = q;
 
@@ -2321,15 +3001,36 @@ static void blk_mq_init_cpu_queues(struct request_queue *q,
 	}
 }
 
+/*
+ * called by: 重要!! 注意request_queue和blk_mq_tag_set之间的关系!
+ *   - blk_mq_map_swqueue()      ----> 输入的参数是request_queue
+ *   - __blk_mq_alloc_rq_maps()  ----> 输入的参数是blk_mq_tag_set
+ *
+ *
+ * 分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+ * 每个hw queue一个
+ * 不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+ * page可能不连续 都链接在tags->page_list
+ */
 static bool __blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, int hctx_idx)
 {
 	int ret = 0;
 
+	/*
+	 * tags[i]一开始只分配了指针
+	 *
+	 * 分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+	 * 每个hw queue一个
+	 */
 	set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
 					set->queue_depth, set->reserved_tags);
 	if (!set->tags[hctx_idx])
 		return false;
 
+	/*
+	 * 不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+	 * page可能不连续 都链接在tags->page_list
+	 */
 	ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
 				set->queue_depth);
 	if (!ret)
@@ -2340,6 +3041,12 @@ static bool __blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, int hctx_idx)
 	return false;
 }
 
+/*
+ * called by:
+ *   - blk_mq_map_swqueue()
+ *   - blk_mq_realloc_hw_ctxs()
+ *   - blk_mq_free_tag_set()
+ */
 static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 					 unsigned int hctx_idx)
 {
@@ -2350,11 +3057,19 @@ static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called by:
+ *   - blk_mq_init_allocated_queue()
+ *   - blk_mq_queue_reinit()
+ */
 static void blk_mq_map_swqueue(struct request_queue *q)
 {
 	unsigned int i, hctx_idx;
+	/* State for a hardware queue facing the hardware block device */
 	struct blk_mq_hw_ctx *hctx;
+	/* State for a software queue facing the submitting CPUs */
 	struct blk_mq_ctx *ctx;
+	/* 多个request_queue可能共享tag_set, 比如nvme controller有多个namespace */
 	struct blk_mq_tag_set *set = q->tag_set;
 
 	/*
@@ -2362,6 +3077,15 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 */
 	mutex_lock(&q->sysfs_lock);
 
+	/*
+	 * cpu的数量是sw queue的数量
+	 * 一个hw queue可能map多个sw queue
+	 */
+
+	/*
+	 * 对于每一个hw dispatch queue
+	 * (q)->queue_hw_ctx[i]
+	 */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		cpumask_clear(hctx->cpumask);
 		hctx->nr_ctx = 0;
@@ -2373,8 +3097,21 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 	 * If the cpu isn't present, the cpu is mapped to first hctx.
 	 */
 	for_each_possible_cpu(i) {
+		/*
+		 * mq_map应该是有cpu个元素的
+		 * hctx_idx是cpu i 要map到的hw queue在q->queue_hw_ctx[q->mq_map[cpu]]的index
+		 *
+		 * 在blk_mq_init_allocated_queue()把request_queue的mq_map指向set的mq_map
+		 */
 		hctx_idx = q->mq_map[i];
 		/* unmapped hw queue can be remapped after CPU topo changed */
+		/*
+		 *
+		 * set->tags[hctx_idx]在blk_mq_alloc_tag_set()分配第一维数组指针
+		 *
+		 * set->tags[hctx_idx]分配实例在: blk_mq_alloc_tag_set()-->blk_mq_alloc_rq_maps()-->__blk_mq_alloc_rq_maps()-->__blk_mq_alloc_rq_map()
+		 *
+		 */
 		if (!set->tags[hctx_idx] &&
 		    !__blk_mq_alloc_rq_map(set, hctx_idx)) {
 			/*
@@ -2383,19 +3120,24 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			 * case, remap the current ctx to hctx[0] which
 			 * is guaranteed to always have tags allocated
 			 */
+			/* 默认使用queue 0 */
 			q->mq_map[i] = 0;
 		}
 
 		ctx = per_cpu_ptr(q->queue_ctx, i);
+		/* 其实就是q->queue_hw_ctx[q->mq_map[cpu]] 获得sw queue i对于的hw queue */
 		hctx = blk_mq_map_queue(q, i);
 
+		/* 因为sw queue (cpu) i 要map到hctx这个硬件队列上, 把hctx的cpumask相应的bit置上 */
 		cpumask_set_cpu(i, hctx->cpumask);
+		/* sw queue的index_hw是其在hctx->ctxs[]的index */
 		ctx->index_hw = hctx->nr_ctx;
 		hctx->ctxs[hctx->nr_ctx++] = ctx;
 	}
 
 	mutex_unlock(&q->sysfs_lock);
 
+	/* 对于每一个hw dispatch queue */
 	queue_for_each_hw_ctx(q, hctx, i) {
 		/*
 		 * If no software queues are mapped to this hardware queue,
@@ -2421,6 +3163,9 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 		 * This is more accurate and more efficient than looping
 		 * over all possibly mapped software queues.
 		 */
+		/*
+		 * hctx->nr_ctx: @depth: Number of bits used in the whole bitmap
+		 */
 		sbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);
 
 		/*
@@ -2435,6 +3180,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
  * Caller needs to ensure that we're either frozen/quiesced, or that
  * the queue isn't live yet.
  */
+/*
+ * called by:
+ *   - blk_mq_update_tag_set_depth()
+ *   - blk_mq_add_queue_tag_set()
+ */
 static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2453,6 +3203,11 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	}
 }
 
+/*
+ * called by:
+ *   - block/blk-mq.c|2934| <<blk_mq_del_queue_tag_set>> blk_mq_update_tag_set_depth(set, false);
+ *   - block/blk-mq.c|2959| <<blk_mq_add_queue_tag_set>> blk_mq_update_tag_set_depth(set, true);
+ */
 static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 					bool shared)
 {
@@ -2467,6 +3222,9 @@ static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 	}
 }
 
+/*
+ * called only by blk_mq_free_queue()
+ */
 static void blk_mq_del_queue_tag_set(struct request_queue *q)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -2481,9 +3239,13 @@ static void blk_mq_del_queue_tag_set(struct request_queue *q)
 	}
 	mutex_unlock(&set->tag_list_lock);
 	synchronize_rcu();
+	/* 在blk_mq_add_queue_tag_set() 作为元素链接在blk_mq_tag_set->tag_list */
 	INIT_LIST_HEAD(&q->tag_set_list);
 }
 
+/*
+ * called only by blk_mq_init_allocated_queue()
+ */
 static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 				     struct request_queue *q)
 {
@@ -2502,6 +3264,10 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 	}
 	if (set->flags & BLK_MQ_F_TAG_SHARED)
 		queue_set_hctx_shared(q, true);
+	/*
+	 * 多个request_queue可能共用一个struct blk_mq_tag_set,
+	 * 比如一个nvme的controller (struct nvme_ctrl) 可以有多个nvme的namespace (struct nvme_ns)
+	 */
 	list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
 
 	mutex_unlock(&set->tag_list_lock);
@@ -2513,6 +3279,10 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
  * and headache because q->mq_kobj shouldn't have been introduced,
  * but we can't group ctx/kctx kobj without it.
  */
+/*
+ * called only by:
+ *   - block/blk-sysfs.c|808| <<__blk_release_queue>> blk_mq_release(q);
+ */
 void blk_mq_release(struct request_queue *q)
 {
 	struct blk_mq_hw_ctx *hctx;
@@ -2538,14 +3308,21 @@ void blk_mq_release(struct request_queue *q)
 	free_percpu(q->queue_ctx);
 }
 
+/*
+ * 被各种block driver调用初始化
+ */
 struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 {
 	struct request_queue *uninit_q, *q;
 
+	/* 分配request_queue */
 	uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node, NULL);
 	if (!uninit_q)
 		return ERR_PTR(-ENOMEM);
 
+	/*
+	 * 根据struct blk_mq_tag_set初始化request_queue
+	 */
 	q = blk_mq_init_allocated_queue(set, uninit_q);
 	if (IS_ERR(q))
 		blk_cleanup_queue(uninit_q);
@@ -2554,6 +3331,9 @@ struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_init_queue);
 
+/*
+ * called only by blk_mq_realloc_hw_ctxs()
+ */
 static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 {
 	int hw_ctx_size = sizeof(struct blk_mq_hw_ctx);
@@ -2568,10 +3348,18 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 	return hw_ctx_size;
 }
 
+/*
+ * called by:
+ *   - blk_mq_init_allocated_queue()
+ *   - __blk_mq_update_nr_hw_queues()
+ *
+ *  分配并初始化q->queue_hw_ctx
+ */
 static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 						struct request_queue *q)
 {
 	int i, j;
+	/* 在blk_mq_init_allocated_queue()分配过指针数组 */
 	struct blk_mq_hw_ctx **hctxs = q->queue_hw_ctx;
 
 	blk_mq_sysfs_unregister(q);
@@ -2585,6 +3373,7 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 			continue;
 
 		node = blk_mq_hw_queue_to_node(q->mq_map, i);
+		/* 分配实例 struct blk_mq_hw_ctx */
 		hctxs[i] = kzalloc_node(blk_mq_hw_ctx_size(set),
 					GFP_KERNEL, node);
 		if (!hctxs[i])
@@ -2609,6 +3398,7 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 		}
 		blk_mq_hctx_kobj_init(hctxs[i]);
 	}
+	/* 下面是针对那些不需要初始化的 */
 	for (j = i; j < q->nr_hw_queues; j++) {
 		struct blk_mq_hw_ctx *hctx = hctxs[j];
 
@@ -2626,18 +3416,36 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 	blk_mq_sysfs_register(q);
 }
 
+/*
+ * called by:
+ *   - blk_mq_init_queue()
+ *   - dm_mq_init_request_queue()
+ *   - blk_eh_timer_return()
+ *
+ * 根据struct blk_mq_tag_set初始化request_queue
+ */
 struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 						  struct request_queue *q)
 {
 	/* mark the queue as mq asap */
 	q->mq_ops = set->ops;
 
+	/*
+	 * Block statistics callback
+	 */
+	/*
+	 * cb->timer_fn = blk_mq_poll_stats_fn;
+	 * cb->bucket_fn = blk_mq_poll_stats_bkt;
+	 * cb->data = q;
+	 * cb->buckets = BLK_MQ_POLL_STATS_BKTS;
+	 */
 	q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
 					     blk_mq_poll_stats_bkt,
 					     BLK_MQ_POLL_STATS_BKTS, q);
 	if (!q->poll_cb)
 		goto err_exit;
 
+	/* 类型是struct blk_mq_ctx, 分配sw queue (percpu的) */
 	q->queue_ctx = alloc_percpu(struct blk_mq_ctx);
 	if (!q->queue_ctx)
 		goto err_exit;
@@ -2645,6 +3453,7 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	/* init q->mq_kobj and sw queues' kobjects */
 	blk_mq_sysfs_init(q);
 
+	/* 这里分配的是指针数组 cpu个 */
 	q->queue_hw_ctx = kzalloc_node(nr_cpu_ids * sizeof(*(q->queue_hw_ctx)),
 						GFP_KERNEL, set->numa_node);
 	if (!q->queue_hw_ctx)
@@ -2652,6 +3461,9 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 
 	q->mq_map = set->mq_map;
 
+	/*
+	 * 分配并初始化q->queue_hw_ctx
+	 */
 	blk_mq_realloc_hw_ctxs(set, q);
 	if (!q->nr_hw_queues)
 		goto err_hctxs;
@@ -2696,6 +3508,14 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	if (!(set->flags & BLK_MQ_F_NO_SCHED)) {
 		int ret;
 
+		/*
+		 * 对于mq的设备:
+		 *     如果queue的数量大于1, 默认用none (不是noop!)
+		 *     如果queue的数量等于1, 默认用mq-deadline
+		 *
+		 * 对于sq的设备:
+		 *     默认用CONFIG_DEFAULT_IOSCHED
+		 */
 		ret = blk_mq_sched_init(q);
 		if (ret)
 			return ERR_PTR(ret);
@@ -2713,6 +3533,10 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 }
 EXPORT_SYMBOL(blk_mq_init_allocated_queue);
 
+/*
+ * called only by:
+ *   - block/blk-core.c|817| <<blk_cleanup_queue>> blk_mq_free_queue(q);
+ */
 void blk_mq_free_queue(struct request_queue *q)
 {
 	struct blk_mq_tag_set	*set = q->tag_set;
@@ -2740,11 +3564,27 @@ static void blk_mq_queue_reinit(struct request_queue *q)
 	blk_mq_debugfs_register_hctxs(q);
 }
 
+/*
+ * called only by blk_mq_alloc_rq_maps()
+ *
+ * 针对每一个hw queue:
+ *     分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+ *     每个hw queue一个
+ *     不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+ *     page可能不连续 都链接在tags->page_list
+ */
 static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 {
 	int i;
 
+	/* nr_hw_queues 应该都是由block driver设置的 */
 	for (i = 0; i < set->nr_hw_queues; i++)
+		/*
+		 * 分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+		 * 每个hw queue一个
+		 * 不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+		 * age可能不连续 都链接在tags->page_list
+		 */
 		if (!__blk_mq_alloc_rq_map(set, i))
 			goto out_unwind;
 
@@ -2762,17 +3602,30 @@ static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
  * may reduce the depth asked for, if memory is tight. set->queue_depth
  * will be updated to reflect the allocated depth.
  */
+/*
+ * called only by blk_mq_alloc_tag_set()
+ */
 static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 {
 	unsigned int depth;
 	int err;
 
+	/* xen: info->tag_set.queue_depth = BLK_RING_SIZE(info); */
 	depth = set->queue_depth;
+	/* 虽然是个循环 可能就执行一次 */
 	do {
+		/*
+		 * 针对每一个hw queue:
+		 *      分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+		 *      每个hw queue一个
+		 *      不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+		 *      page可能不连续 都链接在tags->page_list
+		 */
 		err = __blk_mq_alloc_rq_maps(set);
 		if (!err)
 			break;
 
+		/* 分配不了这么多 把depth除以2 */
 		set->queue_depth >>= 1;
 		if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {
 			err = -ENOMEM;
@@ -2792,6 +3645,14 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - blk_mq_alloc_tag_set()
+ *   - __blk_mq_update_nr_hw_queues()
+ *
+ * 设置set->mq_map, 初始化里面每一个元素
+ * 有些是根据中断的affinity设置map
+ */
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues) {
@@ -2813,9 +3674,9 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 		for_each_possible_cpu(cpu)
 			set->mq_map[cpu] = 0;
 
-		return set->ops->map_queues(set);
+		return set->ops->map_queues(set); // 有些是根据中断的affinity设置map
 	} else
-		return blk_mq_map_queues(set);
+		return blk_mq_map_queues(set); // 设置set->mq_map, 初始化里面每一个元素
 }
 
 /*
@@ -2824,6 +3685,9 @@ static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
  * requested depth down, if if it too large. In that case, the set
  * value will be stored in set->queue_depth.
  */
+/*
+ * 被各种block的driver调用
+ */
 int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int ret;
@@ -2843,6 +3707,10 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	if (!set->ops->get_budget ^ !set->ops->put_budget)
 		return -EINVAL;
 
+	/*
+	 * BLK_MQ_MAX_DEPTH        = 10240,
+	 * include/linux/blk-mq.h
+	 */
 	if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
 		pr_info("blk-mq: reduced tag depth to %u\n",
 			BLK_MQ_MAX_DEPTH);
@@ -2861,29 +3729,47 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	/*
 	 * There is no use for more h/w queues than cpus.
 	 */
+	/* hw queue的数量不可以多于 cpu */
 	if (set->nr_hw_queues > nr_cpu_ids)
 		set->nr_hw_queues = nr_cpu_ids;
 
+	/*
+	 * 分配的是数组指针
+	 * 数组有cpu个(struct blk_mq_tags *) 用的时候是nr_hw_queues个
+	 */
 	set->tags = kzalloc_node(nr_cpu_ids * sizeof(struct blk_mq_tags *),
 				 GFP_KERNEL, set->numa_node);
 	if (!set->tags)
 		return -ENOMEM;
 
 	ret = -ENOMEM;
+	/* 分配的是cpu个元素 不是指针 */
 	set->mq_map = kzalloc_node(sizeof(*set->mq_map) * nr_cpu_ids,
 			GFP_KERNEL, set->numa_node);
 	if (!set->mq_map)
 		goto out_free_tags;
 
+	/*
+	 * 设置set->mq_map, 初始化里面每一个元素
+	 * 有些是根据中断的affinity设置map
+	 */
 	ret = blk_mq_update_queue_map(set);
 	if (ret)
 		goto out_free_mq_map;
 
+	/*
+	 * 针对set的每一个hw queue:
+	 *     分配一个struct blk_mq_tags, 并分配其rqs和static_rqs (只是struct request *, 不是实体)
+	 *     每个hw queue一个
+	 *     不停的分配不同order的page们 然后让tags->static_rqs[i]指向这些page中的rq_size
+	 *     page可能不连续 都链接在tags->page_list
+	 */
 	ret = blk_mq_alloc_rq_maps(set);
 	if (ret)
 		goto out_free_mq_map;
 
 	mutex_init(&set->tag_list_lock);
+	/* 很想知道这个list干什么的 */
 	INIT_LIST_HEAD(&set->tag_list);
 
 	return 0;
@@ -2898,6 +3784,9 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_alloc_tag_set);
 
+/*
+ * 很多驱动调用这里
+ */
 void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 {
 	int i;
@@ -2913,6 +3802,9 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 }
 EXPORT_SYMBOL(blk_mq_free_tag_set);
 
+/*
+ * called only by queue_requests_store()
+ */
 int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct blk_mq_tag_set *set = q->tag_set;
@@ -2953,11 +3845,15 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 	return ret;
 }
 
+/*
+ * called only by blk_mq_update_nr_hw_queues()
+ */
 static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 							int nr_hw_queues)
 {
 	struct request_queue *q;
 
+	/* 调用__blk_mq_update_nr_hw_queues()必须抓住mutex lock: &set->tag_list_lock */
 	lockdep_assert_held(&set->tag_list_lock);
 
 	if (nr_hw_queues > nr_cpu_ids)
@@ -2965,6 +3861,13 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 	if (nr_hw_queues < 1 || nr_hw_queues == set->nr_hw_queues)
 		return;
 
+	/*
+	 * 多个request_queue可能共用一个struct blk_mq_tag_set,
+	 * 比如一个nvme的controller (struct nvme_ctrl) 可以有多个nvme的namespace (struct nvme_ns)
+	 *      -----> list_add_tail(&ns->list, &ctrl->namespaces);
+	 *
+	 * BLK_MQ_F_TAG_SHARED在include/linux/blk-mq.h
+	 */
 	list_for_each_entry(q, &set->tag_list, tag_set_list)
 		blk_mq_freeze_queue(q);
 
@@ -2979,6 +3882,15 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 		blk_mq_unfreeze_queue(q);
 }
 
+/*
+ * called by:
+ *   - drivers/block/nbd.c|1081| <<nbd_start_device>> blk_mq_update_nr_hw_queues(&nbd->tag_set, config->num_connections);
+ *   - drivers/block/xen-blkfront.c|2107| <<blkfront_resume>> blk_mq_update_nr_hw_queues(&info->tag_set, info->nr_rings);
+ *   - drivers/nvme/host/fc.c|2530| <<nvme_fc_reinit_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+ *   - drivers/nvme/host/pci.c|2100| <<nvme_dev_add>> blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+ *   - drivers/nvme/host/rdma.c|851| <<nvme_rdma_configure_io_queues>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ *   - drivers/nvme/target/loop.c|490| <<nvme_loop_reset_ctrl_work>> blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+ */
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
 	mutex_lock(&set->tag_list_lock);
@@ -2988,6 +3900,9 @@ void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 
 /* Enable polling stats and return whether they were already enabled. */
+/*
+ * called only by blk_mq_poll_nsecs()
+ */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
@@ -2997,6 +3912,9 @@ static bool blk_poll_stats_enable(struct request_queue *q)
 	return false;
 }
 
+/*
+ * called only by __blk_mq_complete_request()
+ */
 static void blk_mq_poll_stats_start(struct request_queue *q)
 {
 	/*
@@ -3010,6 +3928,10 @@ static void blk_mq_poll_stats_start(struct request_queue *q)
 	blk_stat_activate_msecs(q->poll_cb, 100);
 }
 
+/*
+ * 只被如下使用:
+ *   block/blk-mq.c|3131| <<blk_mq_init_allocated_queue>> q->poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn,
+ */
 static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 {
 	struct request_queue *q = cb->data;
@@ -3021,6 +3943,9 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	}
 }
 
+/*
+ * called by only blk_mq_poll_hybrid_sleep()
+ */
 static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 				       struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
@@ -3054,6 +3979,9 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	return ret;
 }
 
+/*
+ * called only by __blk_mq_poll()
+ */
 static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 				     struct blk_mq_hw_ctx *hctx,
 				     struct request *rq)
@@ -3097,6 +4025,10 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 
 	hrtimer_init_sleeper(&hs, current);
 	do {
+		/*
+		 * 只在一处设置:
+		 *   - block/blk-mq.c|727| <<__blk_mq_complete_request>> blk_mq_rq_update_state(rq, MQ_RQ_COMPLETE);
+		 */
 		if (blk_mq_rq_state(rq) == MQ_RQ_COMPLETE)
 			break;
 		set_current_state(TASK_UNINTERRUPTIBLE);
@@ -3112,6 +4044,9 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	return true;
 }
 
+/*
+ * 只被blk_mq_poll()调用
+ */
 static bool __blk_mq_poll(struct blk_mq_hw_ctx *hctx, struct request *rq)
 {
 	struct request_queue *q = hctx->queue;
@@ -3156,6 +4091,18 @@ static bool __blk_mq_poll(struct blk_mq_hw_ctx *hctx, struct request *rq)
 	return false;
 }
 
+/*
+ * 用作poll_fn:
+ *     block/blk-mq.c|3178| <<blk_mq_init_allocated_queue>> q->poll_fn = blk_mq_poll;
+ *
+ * 设置的地方:
+ *   - block/blk-mq.c|3178| <<blk_mq_init_allocated_queue>> q->poll_fn = blk_mq_poll;
+ *   - drivers/nvme/host/multipath.c|192| <<nvme_mpath_alloc_disk>> q->poll_fn = nvme_ns_head_poll;
+ *
+ * 调用的地方:
+ *   - block/blk-core.c|2644| <<blk_poll>> return q->poll_fn(q, cookie);
+ *   - drivers/nvme/host/multipath.c|142| <<nvme_ns_head_poll>> found = ns->queue->poll_fn(q, qc);
+ */
 static bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index e1bb420..1ba2b50 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -13,7 +13,8 @@ struct blk_mq_tag_set;
 struct blk_mq_ctx {
 	struct {
 		spinlock_t		lock;
-		struct list_head	rq_list;
+		/* rq_list上放的是用__blk_mq_insert_request()插入的待处理的request */
+		struct list_head	rq_list;  // 软件队列链接一群request待处理, 在__blk_mq_insert_request()插入的
 	}  ____cacheline_aligned_in_smp;
 
 	unsigned int		cpu;
@@ -35,8 +36,20 @@ struct blk_mq_ctx {
  * and the upper bits the generation number.
  */
 enum mq_rq_state {
+	/*
+	 * 两处设置了:
+	 *   - block/blk-mq.c|643| <<blk_mq_free_request>> blk_mq_rq_update_state(rq, MQ_RQ_IDLE);
+	 *   - block/blk-mq.c|975| <<__blk_mq_requeue_request>> blk_mq_rq_update_state(rq, MQ_RQ_IDLE);
+	 */
 	MQ_RQ_IDLE		= 0,
+	/*
+	 * 只在blk_mq_start_request()设置MQ_RQ_IN_FLIGHT (request->gstate)
+	 */
 	MQ_RQ_IN_FLIGHT		= 1,
+	/*
+	 * 只在一处设置:
+	 *   - block/blk-mq.c|727| <<__blk_mq_complete_request>> blk_mq_rq_update_state(rq, MQ_RQ_COMPLETE);
+	 */
 	MQ_RQ_COMPLETE		= 2,
 
 	MQ_RQ_STATE_BITS	= 2,
@@ -88,6 +101,7 @@ extern int blk_mq_hw_queue_to_node(unsigned int *map, unsigned int);
 static inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,
 		int cpu)
 {
+	/* 指向指向struct blk_mq_tag_set 的mq_map */
 	return q->queue_hw_ctx[q->mq_map[cpu]];
 }
 
@@ -169,11 +183,24 @@ struct blk_mq_alloc_data {
 	struct blk_mq_hw_ctx *hctx;
 };
 
+/*
+ * called by:
+ *   - bfq_limit_depth()
+ *   - blk_mq_get_tag() 两处
+ *   - blk_mq_rq_ctx_init()
+ */
 static inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)
 {
+	/*
+	 * sched_tags在blk_mq_sched_alloc_tags()分配的 专门为hctx分配的
+	 * 似乎和INTERNAL和driver tag有某种关系
+	 *
+	 * 在blk_mq_get_request()当request_queue有elevator的时候会在data->flags上mark
+	 */
 	if (data->flags & BLK_MQ_REQ_INTERNAL)
 		return data->hctx->sched_tags;
 
+	/* 指向set->tags[hctx_idx] */
 	return data->hctx->tags;
 }
 
@@ -184,6 +211,9 @@ static inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)
 
 static inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)
 {
+	/*
+	 * hctx->tags: 在blk_mq_init_hctx()中指向set->tags[hctx_idx]
+	 */
 	return hctx->nr_ctx && hctx->tags;
 }
 
@@ -192,6 +222,13 @@ void blk_mq_in_flight(struct request_queue *q, struct hd_struct *part,
 void blk_mq_in_flight_rw(struct request_queue *q, struct hd_struct *part,
 			 unsigned int inflight[2]);
 
+/*
+ * called by:
+ *   - blk_mq_do_dispatch_sched()
+ *   - blk_mq_do_dispatch_ctx()
+ *   - blk_mq_dispatch_rq_list()
+ *   - __blk_mq_try_issue_directly()
+ */
 static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -200,6 +237,13 @@ static inline void blk_mq_put_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 		q->mq_ops->put_budget(hctx);
 }
 
+/*
+ * called by:
+ *   - blk_mq_do_dispatch_sched()
+ *   - blk_mq_do_dispatch_ctx()
+ *   - blk_mq_dispatch_rq_list()
+ *   - __blk_mq_try_issue_directly()
+ */
 static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
@@ -209,6 +253,11 @@ static inline bool blk_mq_get_dispatch_budget(struct blk_mq_hw_ctx *hctx)
 	return true;
 }
 
+/*
+ * called by:
+ *   - blk_mq_put_driver_tag_hctx()
+ *   - blk_mq_put_driver_tag()
+ */
 static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 					   struct request *rq)
 {
@@ -221,6 +270,11 @@ static inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 	}
 }
 
+/*
+ * called by:
+ *   - flush_end_io()
+ *   - mq_flush_data_end_io()
+ */
 static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
 {
@@ -230,6 +284,11 @@ static inline void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 	__blk_mq_put_driver_tag(hctx, rq);
 }
 
+/*
+ * called by:
+ *   - __blk_mq_requeue_request()
+ *   - blk_mq_dispatch_rq_list()
+ */
 static inline void blk_mq_put_driver_tag(struct request *rq)
 {
 	struct blk_mq_hw_ctx *hctx;
diff --git a/block/blk-settings.c b/block/blk-settings.c
index d1de711..b07e4be 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -54,6 +54,14 @@ void blk_queue_unprep_rq(struct request_queue *q, unprep_rq_fn *ufn)
 }
 EXPORT_SYMBOL(blk_queue_unprep_rq);
 
+/*
+ * called by:
+ *   - blk_mq_init_allocated_queue()
+ *   - bsg_setup_queue()
+ *   - null_add_dev()
+ *   - dm_old_init_request_queue()
+ *   - scsi_old_alloc_queue()
+ */
 void blk_queue_softirq_done(struct request_queue *q, softirq_done_fn *fn)
 {
 	q->softirq_done_fn = fn;
@@ -774,6 +782,10 @@ EXPORT_SYMBOL(blk_queue_update_dma_pad);
  * this routine, you must set the limit to one fewer than your device
  * can support otherwise there won't be room for the drain buffer.
  */
+/*
+ * 只有一处设置了dma_drain_size:
+ *   - drivers/ata/libata-scsi.c|1292| <<ata_scsi_dev_config>> blk_queue_dma_drain(q, atapi_drain_needed, buf, ATAPI_MAX_DRAIN);
+ */
 int blk_queue_dma_drain(struct request_queue *q,
 			       dma_drain_needed_fn *dma_drain_needed,
 			       void *buf, unsigned int size)
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 01e2b35..fa764d7 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -39,6 +39,9 @@ static __latent_entropy void blk_done_softirq(struct softirq_action *h)
 }
 
 #ifdef CONFIG_SMP
+/*
+ * 在raise_blk_irq()设置为smo call的data的func
+ */
 static void trigger_softirq(void *data)
 {
 	struct request *rq = data;
@@ -58,6 +61,12 @@ static void trigger_softirq(void *data)
 /*
  * Setup and invoke a run of 'trigger_softirq' on the given cpu.
  */
+/*
+ * called only by __blk_complete_request()
+ *
+ * 返回0: ipi send一个trigger_softirq()完成了
+ * 返回1: 没下发ipi, 因为目的cpu不online
+ */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
 	if (cpu_online(cpu)) {
@@ -95,6 +104,14 @@ static int blk_softirq_cpu_dead(unsigned int cpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - blk_complete_request()
+ *   - blk_rq_timed_out()
+ *
+ * sq的驱动中断会调用: blk_complete_request() --> __blk_complete_request()
+ * mq的驱动中断会调用: blk_mq_complete_request()
+ */
 void __blk_complete_request(struct request *req)
 {
 	int ccpu, cpu;
@@ -102,9 +119,22 @@ void __blk_complete_request(struct request *req)
 	unsigned long flags;
 	bool shared = false;
 
+	/*
+	 * 在blk_queue_softirq_done()配置
+	 *   - block/blk-mq.c|3119| <<blk_mq_init_allocated_queue>> blk_queue_softirq_done(q, set->ops->complete);
+	 *   - block/bsg-lib.c|331| <<bsg_setup_queue>> blk_queue_softirq_done(q, bsg_softirq_done);
+	 *   - drivers/block/null_blk.c|1791| <<null_add_dev>> blk_queue_softirq_done(nullb->q, null_softirq_done_fn);
+	 *   - drivers/md/dm-rq.c|720| <<dm_old_init_request_queue>> blk_queue_softirq_done(md->queue, dm_softirq_done);
+	 *   - drivers/scsi/scsi_lib.c|2277| <<scsi_old_alloc_queue>> blk_queue_softirq_done(q, scsi_softirq_done);
+	 *
+	 * 关于xen和kvm, set.ops.complete被设置为:
+	 *   - xen-blkfront : blkif_complete_rq()     ---> blk_mq_end_request()
+	 *   - virtio-blk   : virtblk_request_done()  ---> blk_mq_end_request()
+	 */
 	BUG_ON(!q->softirq_done_fn);
 
 	local_irq_save(flags);
+	/* 获取当前中断上下文的cpu */
 	cpu = smp_processor_id();
 
 	/*
@@ -112,12 +142,21 @@ void __blk_complete_request(struct request *req)
 	 */
 	if (req->cpu != -1) {
 		ccpu = req->cpu;
+		/*
+		 * QUEUE_FLAG_SAME_FORCE: force complete on same CPU
+		 */
 		if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
-			shared = cpus_share_cache(cpu, ccpu);
+			shared = cpus_share_cache(cpu, ccpu);  // 检查两个cpu是否在相同的cache
 	} else
 		ccpu = cpu;
 
 	/*
+	 * 上面的if-else:
+	 *   如果req->cpu有cpu, 则把ccpu设置成req->cpu
+	 *   否则使用当前中断触发的cpu
+	 */
+
+	/*
 	 * If current CPU and requested CPU share a cache, run the softirq on
 	 * the current CPU. One might concern this is just like
 	 * QUEUE_FLAG_SAME_FORCE, but actually not. blk_complete_request() is
@@ -125,6 +164,9 @@ void __blk_complete_request(struct request *req)
 	 * support multiple interrupts, so current CPU is unique actually. This
 	 * avoids IPI sending from current CPU to the first CPU of a group.
 	 */
+	/*
+	 * 上面的shared: 中断的cpu和下发request的cpu是否在一个cache上
+	 */
 	if (ccpu == cpu || shared) {
 		struct list_head *list;
 do_local:
@@ -139,7 +181,12 @@ void __blk_complete_request(struct request *req)
 		 */
 		if (list->next == &req->ipi_list)
 			raise_softirq_irqoff(BLOCK_SOFTIRQ);
-	} else if (raise_blk_irq(ccpu, req))
+	} else if (raise_blk_irq(ccpu, req)) // ---> 返回0就可以退出了!
+		/*
+		 * raise_blk_irq():
+		 *   返回0: ipi send一个trigger_softirq()完成了
+		 *   返回1: 没下发ipi, 因为目的cpu不online
+		 */
 		goto do_local;
 
 	local_irq_restore(flags);
@@ -156,6 +203,22 @@ void __blk_complete_request(struct request *req)
  *     through a softirq handler. The user must have registered a completion
  *     callback through blk_queue_softirq_done().
  **/
+/*
+ * called by (被sq的驱动们的中断函数调用):
+ *   - block/bsg-lib.c|161| <<bsg_job_done>> blk_complete_request(blk_mq_rq_from_pdu(job));
+ *   - drivers/block/null_blk.c|1295| <<null_handle_cmd>> blk_complete_request(cmd->rq);
+ *   - drivers/md/dm-rq.c|367| <<dm_complete_request>> blk_complete_request(rq);
+ *   - drivers/scsi/scsi_lib.c|1655| <<scsi_kill_request>> blk_complete_request(req);
+ *   - drivers/scsi/scsi_lib.c|1792| <<scsi_done>> blk_complete_request(cmd->request);
+ *
+ * sq的驱动中断会调用: blk_complete_request() --> __blk_complete_request()
+ *     sq的blk_complete_request()会把工作delay到softirq处理
+ *     如果在相同cpu (或相同cache的cpu)就delay到当前cpu的softirq处理
+ *     否则通过ipi来delay到相关cpu的softirq处理
+ * mq的驱动中断会调用: blk_mq_complete_request()
+ *     基于目前的实现, 要么在当前cpu的中断环境执行后续处理 要么通过ipi从其他cpu的中断环境处理
+ *     目前mq不用softirq (以后不确定)
+ */
 void blk_complete_request(struct request *req)
 {
 	if (unlikely(blk_should_fake_timeout(req->q)))
@@ -172,6 +235,12 @@ static __init int blk_softirq_init(void)
 	for_each_possible_cpu(i)
 		INIT_LIST_HEAD(&per_cpu(blk_cpu_done, i));
 
+	/*
+	 * 被如下的地方raise:
+	 *   - block/blk-softirq.c|53| <<trigger_softirq>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-softirq.c|92| <<blk_softirq_cpu_dead>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 *   - block/blk-softirq.c|141| <<__blk_complete_request>> raise_softirq_irqoff(BLOCK_SOFTIRQ);
+	 */
 	open_softirq(BLOCK_SOFTIRQ, blk_done_softirq);
 	cpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,
 				  "block/softirq:dead", NULL,
diff --git a/block/blk-stat.c b/block/blk-stat.c
index bd365a9..28d3297 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -11,12 +11,22 @@
 #include "blk-mq.h"
 #include "blk.h"
 
+/* 被struct request_queue.stats所指 */
 struct blk_queue_stats {
-	struct list_head callbacks;
+	struct list_head callbacks;  /* 挂载着request_queue所有的blk_stat_callback */
 	spinlock_t lock;
+	/*
+	 * 在blk_alloc_queue_stats()中初始化为false
+	 * 在blk_stat_enable_accounting()设置为true
+	 */
 	bool enable_accounting;
 };
 
+/*
+ * called by:
+ *   - blk_stat_timer_fn() 两次
+ *   - blk_stat_add_callback()
+ */
 static void blk_stat_init(struct blk_rq_stat *stat)
 {
 	stat->min = -1ULL;
@@ -25,6 +35,9 @@ static void blk_stat_init(struct blk_rq_stat *stat)
 }
 
 /* src is a per-cpu stat, mean isn't initialized */
+/*
+ * called by only blk_stat_timer_fn()
+ */
 static void blk_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
 	if (!src->nr_samples)
@@ -33,12 +46,16 @@ static void blk_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 	dst->min = min(dst->min, src->min);
 	dst->max = max(dst->max, src->max);
 
+	/* 第一个除以第二个 */
 	dst->mean = div_u64(src->batch + dst->mean * dst->nr_samples,
 				dst->nr_samples + src->nr_samples);
 
 	dst->nr_samples += src->nr_samples;
 }
 
+/*
+ * called only by blk_stat_add()
+ */
 static void __blk_stat_add(struct blk_rq_stat *stat, u64 value)
 {
 	stat->min = min(stat->min, value);
@@ -47,6 +64,11 @@ static void __blk_stat_add(struct blk_rq_stat *stat, u64 value)
 	stat->nr_samples++;
 }
 
+/*
+ * called by:
+ *   - blk_finish_request()
+ *   - __blk_mq_complete_request()
+ */
 void blk_stat_add(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -79,8 +101,12 @@ void blk_stat_add(struct request *rq)
 	rcu_read_unlock();
 }
 
+/*
+ * 设置为blk_stat_callback的timer
+ */
 static void blk_stat_timer_fn(struct timer_list *t)
 {
+	/* struct blk_stat_callback包含了timer */
 	struct blk_stat_callback *cb = from_timer(cb, t, timer);
 	unsigned int bucket;
 	int cpu;
@@ -101,6 +127,12 @@ static void blk_stat_timer_fn(struct timer_list *t)
 	cb->timer_fn(cb);
 }
 
+/*
+ * called by:
+ *   - blk_mq_init_allocated_queue()
+ *   - wbt_init()
+ *   - kyber_queue_data_alloc()
+ */
 struct blk_stat_callback *
 blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 			int (*bucket_fn)(const struct request *),
@@ -118,6 +150,7 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 		kfree(cb);
 		return NULL;
 	}
+	/* 每个cpu不只一个元素 而是一个数组! */
 	cb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),
 				      __alignof__(struct blk_rq_stat));
 	if (!cb->cpu_stat) {
@@ -136,6 +169,14 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 }
 EXPORT_SYMBOL_GPL(blk_stat_alloc_callback);
 
+/*
+ * called by:
+ *   - blk_poll_stats_enable()
+ *   - wbt_init()
+ *   - kyber_init_sched()
+ *
+ * Add a block statistics callback to be run on a request queue.
+ */
 void blk_stat_add_callback(struct request_queue *q,
 			   struct blk_stat_callback *cb)
 {
@@ -151,12 +192,21 @@ void blk_stat_add_callback(struct request_queue *q,
 	}
 
 	spin_lock(&q->stats->lock);
+	/* 把&cb->list加到&q->stats->callbacks后面 */
 	list_add_tail_rcu(&cb->list, &q->stats->callbacks);
 	blk_queue_flag_set(QUEUE_FLAG_STATS, q);
 	spin_unlock(&q->stats->lock);
 }
 EXPORT_SYMBOL_GPL(blk_stat_add_callback);
 
+/*
+ * called by:
+ *   - __blk_release_queue()
+ *   - wbt_exit()
+ *   - kyber_exit_sched()
+ *
+ * Remove a block statistics callback from a request queue.
+ */
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb)
 {
@@ -170,6 +220,9 @@ void blk_stat_remove_callback(struct request_queue *q,
 }
 EXPORT_SYMBOL_GPL(blk_stat_remove_callback);
 
+/*
+ * 下面设置为struct blk_stat_callback的rcu的回调函数
+ */
 static void blk_stat_free_callback_rcu(struct rcu_head *head)
 {
 	struct blk_stat_callback *cb;
@@ -180,6 +233,15 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 	kfree(cb);
 }
 
+/*
+ * called by:
+ *   - __blk_release_queue()
+ *   - wbt_exit()
+ *   - kyber_queue_data_alloc()
+ *   - kyber_exit_sched()
+ *
+ * Free a block statistics callback
+ */
 void blk_stat_free_callback(struct blk_stat_callback *cb)
 {
 	if (cb)
@@ -187,6 +249,9 @@ void blk_stat_free_callback(struct blk_stat_callback *cb)
 }
 EXPORT_SYMBOL_GPL(blk_stat_free_callback);
 
+/*
+ * called only by blk_throtl_register_queue()
+ */
 void blk_stat_enable_accounting(struct request_queue *q)
 {
 	spin_lock(&q->stats->lock);
@@ -195,6 +260,9 @@ void blk_stat_enable_accounting(struct request_queue *q)
 	spin_unlock(&q->stats->lock);
 }
 
+/*
+ * called only by blk_alloc_queue_node()
+ */
 struct blk_queue_stats *blk_alloc_queue_stats(void)
 {
 	struct blk_queue_stats *stats;
@@ -203,6 +271,7 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	if (!stats)
 		return NULL;
 
+	/* 挂载着request_queue所有的blk_stat_callback */
 	INIT_LIST_HEAD(&stats->callbacks);
 	spin_lock_init(&stats->lock);
 	stats->enable_accounting = false;
@@ -210,11 +279,17 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 	return stats;
 }
 
+/*
+ * called by:
+ *   - blk_alloc_queue_node() 错误处理
+ *   - __blk_release_queue()
+ */
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
 	if (!stats)
 		return;
 
+	/* 挂载着request_queue所有的blk_stat_callback */
 	WARN_ON(!list_empty(&stats->callbacks));
 
 	kfree(stats);
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 2dd3634..6a8cd68 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -40,12 +40,12 @@ struct blk_stat_callback {
 	/**
 	 * @timer: Timer for the next callback invocation.
 	 */
-	struct timer_list timer;
+	struct timer_list timer; // 设置为blk_stat_timer_fn()
 
 	/**
 	 * @cpu_stat: Per-cpu statistics buckets.
 	 */
-	struct blk_rq_stat __percpu *cpu_stat;
+	struct blk_rq_stat __percpu *cpu_stat;  // 每个cpu不只一个元素 而是一个数组!
 
 	/**
 	 * @bucket_fn: Given a request, returns which statistics bucket it
@@ -169,8 +169,17 @@ void blk_stat_free_callback(struct blk_stat_callback *cb);
  * gathering statistics.
  * @cb: The callback.
  */
+/*
+ * called by:
+ *   - blk_mq_poll_stats_start()
+ *   - blk_stat_add()
+ *   - wbt_wait()
+ *   - kyber_stat_timer_fn()
+ *   - kyber_completed_request()
+ */
 static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
 {
+	/* timer是struct timer_list */
 	return timer_pending(&cb->timer);
 }
 
@@ -182,6 +191,9 @@ static inline bool blk_stat_is_active(struct blk_stat_callback *cb)
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called only by rwb_arm_timer()
+ */
 static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
 					   u64 nsecs)
 {
@@ -196,6 +208,12 @@ static inline void blk_stat_activate_nsecs(struct blk_stat_callback *cb,
  *
  * The timer callback will be called when the window expires.
  */
+/*
+ * called by:
+ *   - blk_mq_poll_stats_start()
+ *   - kyber_stat_timer_fn()
+ *   - kyber_completed_request()
+ */
 static inline void blk_stat_activate_msecs(struct blk_stat_callback *cb,
 					   unsigned int msecs)
 {
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index c5a1316..c21e423 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -2406,6 +2406,9 @@ void blk_throtl_drain(struct request_queue *q)
 	spin_lock_irq(q->queue_lock);
 }
 
+/*
+ * called only by blkcg_init_queue()
+ */
 int blk_throtl_init(struct request_queue *q)
 {
 	struct throtl_data *td;
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 652d4d4..91b5a3c 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -73,11 +73,22 @@ ssize_t part_timeout_store(struct device *dev, struct device_attribute *attr,
  * @req:	request that we are canceling timer for
  *
  */
+/*
+ * called by:
+ *   - blk_requeue_request()
+ *   - blk_finish_request()
+ *   - blk_abort_request()
+ */
 void blk_delete_timer(struct request *req)
 {
 	list_del_init(&req->timeout_list);
 }
 
+/*
+ * called by:
+ *   - blk_rq_check_expired()
+ *   - blk_abort_request()
+ */
 static void blk_rq_timed_out(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -126,6 +137,9 @@ static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout
 	}
 }
 
+/*
+ * 设置为request_queue的INIT_WORK(&q->timeout_work, blk_timeout_work);
+ */
 void blk_timeout_work(struct work_struct *work)
 {
 	struct request_queue *q =
@@ -154,6 +168,14 @@ void blk_timeout_work(struct work_struct *work)
  * LLDDs who implement their own error recovery MAY ignore the timeout
  * event if they generated blk_abort_req. Must hold queue lock.
  */
+/*
+ * 不太常用:
+ *   - drivers/ata/libata-eh.c|986| <<ata_qc_schedule_eh>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/block/mtip32xx/mtip32xx.c|2754| <<mtip_queue_cmd>> blk_abort_request(req);
+ *   - drivers/s390/block/dasd_ioctl.c|169| <<dasd_ioctl_abortio>> blk_abort_request(cqr->callback_data);
+ *   - drivers/scsi/libsas/sas_ata.c|596| <<sas_ata_task_abort>> blk_abort_request(qc->scsicmd->request);
+ *   - drivers/scsi/libsas/sas_scsi_host.c|940| <<sas_task_abort>> blk_abort_request(sc->request);
+ */
 void blk_abort_request(struct request *req)
 {
 	if (req->q->mq_ops) {
@@ -178,6 +200,7 @@ unsigned long blk_rq_timeout(unsigned long timeout)
 	unsigned long maxt;
 
 	maxt = round_jiffies_up(jiffies + BLK_MAX_TIMEOUT);
+	/* time_after(a,b) returns true if the time a is after time b. */
 	if (time_after(timeout, maxt))
 		timeout = maxt;
 
@@ -192,6 +215,13 @@ unsigned long blk_rq_timeout(unsigned long timeout)
  *    Each request has its own timer, and as it is added to the queue, we
  *    set up the timer. When the request completes, we cancel the timer.
  */
+/*
+ * called by:
+ *   - blk_start_request()
+ *   - blk_mq_start_request()
+ *   - blk_mq_rq_timed_out()
+ *   - blk_rq_timed_out()
+ */
 void blk_add_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
@@ -204,15 +234,20 @@ void blk_add_timer(struct request *req)
 	if (!q->mq_ops && !q->rq_timed_out_fn)
 		return;
 
+	/* timeout_list不能被使用中 */
 	BUG_ON(!list_empty(&req->timeout_list));
 
 	/*
 	 * Some LLDs, like scsi, peek at the timeout to prevent a
 	 * command from being retried forever.
 	 */
+	/*
+	 * timeout是unsigned int
+	 */
 	if (!req->timeout)
 		req->timeout = q->rq_timeout;
 
+	/* 设置rq->__deadline */
 	blk_rq_set_deadline(req, jiffies + req->timeout);
 	req->rq_flags &= ~RQF_MQ_TIMEOUT_EXPIRED;
 
@@ -228,8 +263,14 @@ void blk_add_timer(struct request *req)
 	 * than an existing one, modify the timer. Round up to next nearest
 	 * second.
 	 */
+	/*
+	 * expiry是unsigned long
+	 */
 	expiry = blk_rq_timeout(round_jiffies_up(blk_rq_deadline(req)));
 
+	/*
+	 * q->timeout是timer_list, 注册的是blk_rq_timed_out_timer()
+	 */
 	if (!timer_pending(&q->timeout) ||
 	    time_before(expiry, q->timeout.expires)) {
 		unsigned long diff = q->timeout.expires - expiry;
@@ -242,7 +283,7 @@ void blk_add_timer(struct request *req)
 		 * will be X + something.
 		 */
 		if (!timer_pending(&q->timeout) || (diff >= HZ / 2))
-			mod_timer(&q->timeout, expiry);
+			mod_timer(&q->timeout, expiry);  // 注册的是blk_rq_timed_out_timer()
 	}
 
 }
diff --git a/block/blk.h b/block/blk.h
index b034fd2..811173e 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -211,6 +211,10 @@ static inline bool blk_rq_is_complete(struct request *rq)
 /*
  * Internal elevator interface
  */
+/*
+ * on IO scheduler merge hash
+ * 只在elv_rqhash_add()和__elv_rqhash_del()修改
+ */
 #define ELV_ON_HASH(rq) ((rq)->rq_flags & RQF_HASHED)
 
 void blk_insert_flush(struct request *rq);
diff --git a/block/elevator.c b/block/elevator.c
index e87e9b4..c5fa138 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -43,25 +43,38 @@
 #include "blk-mq-sched.h"
 #include "blk-wbt.h"
 
+/* 链接着全部的evevator (elevator_type) */
 static DEFINE_SPINLOCK(elv_list_lock);
 static LIST_HEAD(elv_list);
 
 /*
  * Merge hash stuff.
  */
+/*
+ * called by:
+ *   - elv_rqhash_add()
+ *   - elv_rqhash_find()
+ *
+ * 这里的sum是request的下一个起始的sector 用来判断backend merge
+ */
 #define rq_hash_key(rq)		(blk_rq_pos(rq) + blk_rq_sectors(rq))
 
 /*
  * Query io scheduler to see if the current process issuing bio may be
  * merged with rq.
  */
+/*
+ * called only by elv_bio_merge_ok()
+ *
+ * 通过elevator的.allow_merge()或者.elevator_allow_bio_merge_fn()查看可否merge
+ */
 static int elv_iosched_allow_bio_merge(struct request *rq, struct bio *bio)
 {
 	struct request_queue *q = rq->q;
 	struct elevator_queue *e = q->elevator;
 
 	if (e->uses_mq && e->type->ops.mq.allow_merge)
-		return e->type->ops.mq.allow_merge(q, rq, bio);
+		return e->type->ops.mq.allow_merge(q, rq, bio); // 只有bfq_allow_bio_merge()实现了
 	else if (!e->uses_mq && e->type->ops.sq.elevator_allow_bio_merge_fn)
 		return e->type->ops.sq.elevator_allow_bio_merge_fn(q, rq, bio);
 
@@ -71,11 +84,27 @@ static int elv_iosched_allow_bio_merge(struct request *rq, struct bio *bio)
 /*
  * can we safely merge with this request?
  */
+/*
+ * called by:
+ *   - bfq_request_merge()
+ *   - cfq_merge()
+ *   - deadline_merge()
+ *   - elv_merge() 两次
+ *   - dd_request_merge
+ *
+ * 确认bio是否能merge到request去
+ *     先通过request和bio的属性确认他们能否merge
+ *     再通过elevator的.allow_merge()或者.elevator_allow_bio_merge_fn()查看可否merge
+ */
 bool elv_bio_merge_ok(struct request *rq, struct bio *bio)
 {
+	/* 通过request和bio的属性确认他们能否merge */
 	if (!blk_rq_merge_ok(rq, bio))
 		return false;
 
+	/*
+	 * 通过elevator的.allow_merge()或者.elevator_allow_bio_merge_fn()查看可否merge
+	 */
 	if (!elv_iosched_allow_bio_merge(rq, bio))
 		return false;
 
@@ -83,6 +112,13 @@ bool elv_bio_merge_ok(struct request *rq, struct bio *bio)
 }
 EXPORT_SYMBOL(elv_bio_merge_ok);
 
+/*
+ * called by:
+ *   - elevator_find()
+ *   - elv_register() 两次
+ *   - __elevator_change()
+ *   - elv_iosched_show()
+ */
 static bool elevator_match(const struct elevator_type *e, const char *name)
 {
 	if (!strcmp(e->elevator_name, name))
@@ -113,6 +149,11 @@ static void elevator_put(struct elevator_type *e)
 	module_put(e->elevator_owner);
 }
 
+/*
+ * called by:
+ *   - elevator_init()
+ *   - __elevator_change()
+ */
 static struct elevator_type *elevator_get(struct request_queue *q,
 					  const char *name, bool try_loading)
 {
@@ -123,6 +164,10 @@ static struct elevator_type *elevator_get(struct request_queue *q,
 	e = elevator_find(name, q->mq_ops != NULL);
 	if (!e && try_loading) {
 		spin_unlock(&elv_list_lock);
+		/*
+		 * try to load a kernel module
+		 * 如果没找到 加载模块再找一次
+		 */
 		request_module("%s-iosched", name);
 		spin_lock(&elv_list_lock);
 		e = elevator_find(name, q->mq_ops != NULL);
@@ -171,6 +216,17 @@ void __init load_default_elevator_module(void)
 
 static struct kobj_type elv_ktype;
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|5156| <<bfq_init_queue>> eq = elevator_alloc(q, e);
+ *   - block/cfq-iosched.c|4579| <<cfq_init_queue>> eq = elevator_alloc(q, e);
+ *   - block/deadline-iosched.c|430| <<deadline_init_queue>> eq = elevator_alloc(q, e);
+ *   - block/kyber-iosched.c|397| <<kyber_init_sched>> eq = elevator_alloc(q, e);
+ *   - block/mq-deadline.c|618| <<dd_init_queue>> eq = elevator_alloc(q, e);
+ *   - block/noop-iosched.c|88| <<noop_init_queue>> eq = elevator_alloc(q, e);
+ *
+ * 分配struct elevator_queue并初始化一些特性
+ */
 struct elevator_queue *elevator_alloc(struct request_queue *q,
 				  struct elevator_type *e)
 {
@@ -199,6 +255,18 @@ static void elevator_release(struct kobject *kobj)
 	kfree(e);
 }
 
+/*
+ * called by:
+ *   - blk_init_allocated_queue()
+ *   - blk_mq_sched_init()
+ *
+ * 对于mq的设备:
+ *     如果queue的数量大于1, 默认用none (不是noop!)
+ *     如果queue的数量等于1, 默认用mq-deadline
+ *
+ * 对于sq的设备:
+ *     默认用CONFIG_DEFAULT_IOSCHED
+ */
 int elevator_init(struct request_queue *q, char *name)
 {
 	struct elevator_type *e = NULL;
@@ -230,6 +298,7 @@ int elevator_init(struct request_queue *q, char *name)
 	 * as we could be running off async and request_module() isn't
 	 * allowed from async.
 	 */
+	/* name没有设置就用default scheduler */
 	if (!e && !q->mq_ops && *chosen_elevator) {
 		e = elevator_get(q, chosen_elevator, false);
 		if (!e)
@@ -245,6 +314,7 @@ int elevator_init(struct request_queue *q, char *name)
 		 * to "none".
 		 */
 		if (q->mq_ops) {
+			/* 在blk_mq_realloc_hw_ctxs()初始化hctx的时候获得 */
 			if (q->nr_hw_queues == 1)
 				e = elevator_get(q, "mq-deadline", false);
 			if (!e)
@@ -283,49 +353,122 @@ void elevator_exit(struct request_queue *q, struct elevator_queue *e)
 }
 EXPORT_SYMBOL(elevator_exit);
 
+/*
+ * called by:
+ *   - elv_rqhash_del()
+ *   - elv_rqhash_reposition()
+ *   - elv_rqhash_find()
+ *
+ * 然后把&rq->hash从elevator_queue->hash删除
+ * 然后取消RQD_HASHED (on IO scheduler merge hash)
+ */
 static inline void __elv_rqhash_del(struct request *rq)
 {
 	hash_del(&rq->hash);
 	rq->rq_flags &= ~RQF_HASHED;
 }
 
+/*
+ * called by:
+ *   - bfq_remove_request()
+ *   - elv_dispatch_sort()
+ *   - elv_dispatch_add_tail()
+ *   - elv_merge_requests()
+ *   - deadline_remove_request()
+ *
+ * 如果不在elevator_queue->hash上就算了
+ * 否则把&rq->hash从elevator_queue->hash删除
+ * 然后取消RQD_HASHED (on IO scheduler merge hash)
+ */
 void elv_rqhash_del(struct request_queue *q, struct request *rq)
 {
+	/*
+	 * on IO scheduler merge hash
+	 */
 	if (ELV_ON_HASH(rq))
 		__elv_rqhash_del(rq);
 }
 EXPORT_SYMBOL_GPL(elv_rqhash_del);
 
+/*
+ * called by:
+ *   - bfq_insert_request()
+ *   - elv_rqhash_reposition()
+ *   - __elv_add_request()
+ *   - dd_insert_request()
+ *
+ * 计算rq_hash_key(rq): request的下一个起始的sector 用来判断backend merge
+ * 然后把&rq->hash添加到elevator_queue->hash
+ * 然后设置RQD_HASHED (on IO scheduler merge hash)
+ */
 void elv_rqhash_add(struct request_queue *q, struct request *rq)
 {
 	struct elevator_queue *e = q->elevator;
 
 	BUG_ON(ELV_ON_HASH(rq));
+	/*
+	 * rq_hash_key(rq): request的下一个起始的sector 用来判断backend merge
+	 */
 	hash_add(e->hash, &rq->hash, rq_hash_key(rq));
 	rq->rq_flags |= RQF_HASHED;
 }
 EXPORT_SYMBOL_GPL(elv_rqhash_add);
 
+/*
+ * called by:
+ *   - elv_merged_request()
+ *   - elv_merge_requests()
+ *
+ * 先把rq->hash从elevator_queue->hash删除
+ * 在把rq->hash添加到elevator_queue->hash
+ */
 void elv_rqhash_reposition(struct request_queue *q, struct request *rq)
 {
 	__elv_rqhash_del(rq);
 	elv_rqhash_add(q, rq);
 }
 
+/*
+ * called by:
+ *   - elv_merge()
+ *   - elv_attempt_insert_merge()
+ *
+ * 在request_queue中寻找和offset可以back merge的request 否则返回NULL
+ */
 struct request *elv_rqhash_find(struct request_queue *q, sector_t offset)
 {
 	struct elevator_queue *e = q->elevator;
 	struct hlist_node *next;
 	struct request *rq;
 
+	/**
+	 * hash_for_each_possible_safe - iterate over all possible objects hashing to the
+	 * same bucket safe against removals
+	 * @name: hashtable to iterate
+	 * @obj: the type * to use as a loop cursor for each entry
+	 * @tmp: a &struct used for temporary storage
+	 * @member: the name of the hlist_node within the struct
+	 * @key: the key of the objects to iterate over
+	 */
 	hash_for_each_possible_safe(e->hash, rq, next, hash, offset) {
 		BUG_ON(!ELV_ON_HASH(rq));
 
+		/*
+		 * 判断这个request是否允许merge,
+		 * 比如scsi passthrough或者Driver private requests不能被merge
+		 *
+		 * 这里个人猜测
+		 * 既然rq都不允许merge 干什么还放在hash上呢??
+		 */
 		if (unlikely(!rq_mergeable(rq))) {
 			__elv_rqhash_del(rq);
 			continue;
 		}
 
+		/*
+		 * rq_hash_key(rq)获取的是当前request的末尾(其实是下一个的sector)
+		 * 这里如果相等说明bio可以backend merge
+		 */
 		if (rq_hash_key(rq) == offset)
 			return rq;
 	}
@@ -337,6 +480,16 @@ struct request *elv_rqhash_find(struct request_queue *q, sector_t offset)
  * RB-tree support functions for inserting/lookup/removal of requests
  * in a sorted RB tree.
  */
+/*
+ * called by:
+ *   - bfq_add_request()
+ *   - bfq_request_merged()
+ *   - cfq_add_rq_rb()
+ *   - deadline_add_rq_rb()
+ *   - deadline_add_rq_rb()
+ *
+ * 根据request的blk_rq_pos() (rq->__sector)把rq->rb_node添加到rbtree
+ */
 void elv_rb_add(struct rb_root *root, struct request *rq)
 {
 	struct rb_node **p = &root->rb_node;
@@ -358,6 +511,19 @@ void elv_rb_add(struct rb_root *root, struct request *rq)
 }
 EXPORT_SYMBOL(elv_rb_add);
 
+/*
+ * called by:
+ *   - bfq_remove_request()
+ *   - bfq_request_merged()
+ *   - cfq_del_rq_rb()
+ *   - cfq_reposition_rq_rb()
+ *   - deadline_del_rq_rb() -- block/deadline-iosched.c
+ *   - deadline_merged_request()
+ *   - deadline_del_rq_rb() -- block/mq-deadline.c
+ *   - dd_request_merged()
+ *
+ * 把request->rb_node从rbtree删除
+ */
 void elv_rb_del(struct rb_root *root, struct request *rq)
 {
 	BUG_ON(RB_EMPTY_NODE(&rq->rb_node));
@@ -366,6 +532,15 @@ void elv_rb_del(struct rb_root *root, struct request *rq)
 }
 EXPORT_SYMBOL(elv_rb_del);
 
+/*
+ * called by:
+ *   - bfq_find_rq_fmerge()
+ *   - cfq_find_rq_fmerge()
+ *   - deadline_merge()
+ *   - dd_request_merge()
+ *
+ * 在这个函数中 找到的request的blk_rq_pos()也就是the current sector必须完全等于sector
+ */
 struct request *elv_rb_find(struct rb_root *root, sector_t sector)
 {
 	struct rb_node *n = root->rb_node;
@@ -374,6 +549,9 @@ struct request *elv_rb_find(struct rb_root *root, sector_t sector)
 	while (n) {
 		rq = rb_entry(n, struct request, rb_node);
 
+		/*
+		 * blk_rq_pos(rq)返回the current sector
+		 */
 		if (sector < blk_rq_pos(rq))
 			n = n->rb_left;
 		else if (sector > blk_rq_pos(rq))
@@ -391,6 +569,13 @@ EXPORT_SYMBOL(elv_rb_find);
  * entry.  rq is sort instead into the dispatch queue. To be used by
  * specific elevators.
  */
+/*
+ * called by: 这两个都只用在single queue
+ *   - block/cfq-iosched.c|2996| <<cfq_dispatch_insert>> elv_dispatch_sort(q, rq);
+ *   - block/noop-iosched.c|38| <<noop_dispatch>> elv_dispatch_sort(q, rq);
+ *
+ * 核心是把request添加到request_queue->queue_head
+ */
 void elv_dispatch_sort(struct request_queue *q, struct request *rq)
 {
 	sector_t boundary;
@@ -399,11 +584,28 @@ void elv_dispatch_sort(struct request_queue *q, struct request *rq)
 	if (q->last_merge == rq)
 		q->last_merge = NULL;
 
+	/*
+	 * 如果不在elevator_queue->hash上就算了
+	 * 否则把&rq->hash从elevator_queue->hash删除
+	 * 然后取消RQD_HASHED (on IO scheduler merge hash)
+	 */
 	elv_rqhash_del(q, rq);
 
+	/*
+	 * 增加的地方:
+	 *   - __elv_add_request()
+	 *
+	 * 减少的地方:
+	 *   - elv_dispatch_sort()
+	 *   - elv_dispatch_add_tail()
+	 *   - elv_merge_requests() 
+	 */
 	q->nr_sorted--;
 
 	boundary = q->end_sector;
+	/*
+	 * 把request添加到queue_head, 下面属于在queue_head找个位置
+	 */
 	list_for_each_prev(entry, &q->queue_head) {
 		struct request *pos = list_entry_rq(entry);
 
@@ -433,11 +635,23 @@ EXPORT_SYMBOL(elv_dispatch_sort);
  * entry.  rq is added to the back of the dispatch queue. To be used by
  * specific elevators.
  */
+/*
+ * called by:
+ *   - deadline_move_to_dispatch() -- block/deadline-iosched.c
+ *   - elv_merge()
+ *
+ * 核心思想是把request添加到q->queue_head
+ */
 void elv_dispatch_add_tail(struct request_queue *q, struct request *rq)
 {
 	if (q->last_merge == rq)
 		q->last_merge = NULL;
 
+	/*
+	 * 如果不在elevator_queue->hash上就算了
+	 * 否则把&rq->hash从elevator_queue->hash删除
+	 * 然后取消RQD_HASHED (on IO scheduler merge hash)
+	 */
 	elv_rqhash_del(q, rq);
 
 	q->nr_sorted--;
@@ -448,6 +662,21 @@ void elv_dispatch_add_tail(struct request_queue *q, struct request *rq)
 }
 EXPORT_SYMBOL(elv_dispatch_add_tail);
 
+/*
+ * called by:
+ *   - blk_mq_sched_try_merge()
+ *   - blk_queue_bio()
+ *
+ * 该函数返回值可能是:
+ *   ELEVATOR_BACK_MERGE
+ *   ELEVATOR_FRONT_MERGE
+ *   ELEVATOR_DISCARD_MERGE
+ *   其他?
+ *
+ * 这个函数通过elv自己的和相关scheduler的函数查询是否可以front或者back merge
+ * 把可以merge的request返回到参数req
+ * 返回值为ELEVATOR_BACK_MERGE, ELEVATOR_FRONT_MERGE, ELEVATOR_DISCARD_MERGE等
+ */
 enum elv_merge elv_merge(struct request_queue *q, struct request **req,
 		struct bio *bio)
 {
@@ -460,33 +689,57 @@ enum elv_merge elv_merge(struct request_queue *q, struct request **req,
 	 * 	noxmerges: Only simple one-hit cache try
 	 * 	merges:	   All merge tries attempted
 	 */
+	/*
+	 * 如果merge attempts is disabled或者bio不允许merge
+	 * 返回ELEVATOR_NO_MERGE
+	 */
 	if (blk_queue_nomerges(q) || !bio_mergeable(bio))
 		return ELEVATOR_NO_MERGE;
 
 	/*
 	 * First try one-hit cache.
 	 */
+	/*
+	 * can we safely merge bio with this q->last_merge???
+	 *
+	 * last_merge: 保存上一次合并的请求指针
+	 */
 	if (q->last_merge && elv_bio_merge_ok(q->last_merge, bio)) {
 		enum elv_merge ret = blk_try_merge(q->last_merge, bio);
 
+		/* 如果允许把bio给merge到q->last_merge */
 		if (ret != ELEVATOR_NO_MERGE) {
 			*req = q->last_merge;
 			return ret;
 		}
 	}
 
+	/* No extended merges */
 	if (blk_queue_noxmerges(q))
 		return ELEVATOR_NO_MERGE;
 
 	/*
 	 * See if our hash lookup can find a potential backmerge.
 	 */
+	/* 在request_queue中寻找和offset可以back merge的request 否则返回NULL */
 	__rq = elv_rqhash_find(q, bio->bi_iter.bi_sector);
+	/* 确认bio是否能merge到request去 */
 	if (__rq && elv_bio_merge_ok(__rq, bio)) {
 		*req = __rq;
 		return ELEVATOR_BACK_MERGE;
 	}
 
+	/*
+	 * 猜测上面主要是用elv自己的函数查看是否能MERGE
+	 * 然后下面用每个scheduler的request_merge查看是否可以MERGE
+	 */
+
+	/*
+	 * mq:
+	 *   dd_request_merge(): ----> 期待返回ELEVATOR_FRONT_MERGE
+	 *       判断bio是否可以在request_queue中front merge, 如果可以找到对应
+	 *       的request (存到参数rq), 并返回ELEVATOR_NO_MERGE
+	 */
 	if (e->uses_mq && e->type->ops.mq.request_merge)
 		return e->type->ops.mq.request_merge(q, req, bio);
 	else if (!e->uses_mq && e->type->ops.sq.elevator_merge_fn)
@@ -502,20 +755,30 @@ enum elv_merge elv_merge(struct request_queue *q, struct request **req,
  *
  * Returns true if we merged, false otherwise
  */
+/*
+ * called by:
+ *   - blk_mq_sched_try_insert_merge()
+ *   - __elv_add_request()
+ *
+ * 核心思想是和request_queue的struct elevator_queue中hash中已有的request来merge
+ */
 bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq)
 {
 	struct request *__rq;
 	bool ret;
 
+	/* check if: disable merge attempts */
 	if (blk_queue_nomerges(q))
 		return false;
 
 	/*
 	 * First try one-hit cache.
 	 */
+	/* 把q->last_merge和rq合并 释放rq */
 	if (q->last_merge && blk_attempt_req_merge(q, q->last_merge, rq))
 		return true;
 
+	/* No extended merges */
 	if (blk_queue_noxmerges(q))
 		return false;
 
@@ -524,7 +787,9 @@ bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq)
 	 * See if our hash lookup can find a potential backmerge.
 	 */
 	while (1) {
+		/* 在request_queue中寻找和offset可以back merge的request 否则返回NULL */
 		__rq = elv_rqhash_find(q, blk_rq_pos(rq));
+		/* 把__rq和rq合并 释放rq */
 		if (!__rq || !blk_attempt_req_merge(q, __rq, rq))
 			break;
 
@@ -536,22 +801,38 @@ bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - blk_queue_bio() 两处 (ELEVATOR_BACK_MERGE和ELEVATOR_FRONT_MERGE)
+ *   - blk_mq_sched_try_merge() 两处 (ELEVATOR_BACK_MERGE和ELEVATOR_FRONT_MERGE)
+ * 对于mq-deadline, 合并bio到请求后的处理, 向前合并后, 将请求从红黑树中删除, 重新插入
+ */
 void elv_merged_request(struct request_queue *q, struct request *rq,
 		enum elv_merge type)
 {
 	struct elevator_queue *e = q->elevator;
 
+	/* 对于mq-deadline, 合并bio到请求后的处理, 向前合并后, 将请求从红黑树中删除, 重新插入 */
+
 	if (e->uses_mq && e->type->ops.mq.request_merged)
 		e->type->ops.mq.request_merged(q, rq, type);
 	else if (!e->uses_mq && e->type->ops.sq.elevator_merged_fn)
 		e->type->ops.sq.elevator_merged_fn(q, rq, type);
 
+	/*
+	 * 先把rq->hash从elevator_queue->hash删除
+	 * 在把rq->hash添加到elevator_queue->hash
+	 */
 	if (type == ELEVATOR_BACK_MERGE)
 		elv_rqhash_reposition(q, rq);
 
 	q->last_merge = rq;
 }
 
+/*
+ * called by only:
+ *   - attempt_merge()
+ */
 void elv_merge_requests(struct request_queue *q, struct request *rq,
 			     struct request *next)
 {
@@ -566,6 +847,10 @@ void elv_merge_requests(struct request_queue *q, struct request *rq,
 			e->type->ops.sq.elevator_merge_req_fn(q, rq, next);
 	}
 
+	/*
+	 * 先把rq->hash从elevator_queue->hash删除
+	 * 在把rq->hash添加到elevator_queue->hash
+	 */
 	elv_rqhash_reposition(q, rq);
 
 	if (next_sorted) {
@@ -576,6 +861,11 @@ void elv_merge_requests(struct request_queue *q, struct request *rq,
 	q->last_merge = rq;
 }
 
+/*
+ * called only by blk_queue_bio()
+ *
+ * 这个函数不能被mq使用 会warn
+ */
 void elv_bio_merged(struct request_queue *q, struct request *rq,
 			struct bio *bio)
 {
@@ -595,6 +885,9 @@ static void blk_pm_requeue_request(struct request *rq)
 		rq->q->nr_pending--;
 }
 
+/*
+ * called only by __elv_add_request()
+ */
 static void blk_pm_add_request(struct request_queue *q, struct request *rq)
 {
 	if (q->dev && !(rq->rq_flags & RQF_PM) && q->nr_pending++ == 0 &&
@@ -628,6 +921,13 @@ void elv_requeue_request(struct request_queue *q, struct request *rq)
 	__elv_add_request(q, rq, ELEVATOR_INSERT_REQUEUE);
 }
 
+/*
+ * called by:
+ *   - __blk_drain_queue()
+ *   - __elv_add_request()
+ *
+ * 应该是只用在sq
+ */
 void elv_drain_elevator(struct request_queue *q)
 {
 	struct elevator_queue *e = q->elevator;
@@ -647,6 +947,25 @@ void elv_drain_elevator(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - add_acct_request()
+ *   - blk_flush_plug_list() 两次
+ *   - blk_execute_rq_nowait()
+ *   - elv_requeue_request()
+ *   - elv_add_request()
+ *   - ide_pm_execute_rq()
+ *
+ * where的选择:
+ *   #define ELEVATOR_INSERT_FRONT   1
+ *   #define ELEVATOR_INSERT_BACK    2
+ *   #define ELEVATOR_INSERT_SORT    3
+ *   #define ELEVATOR_INSERT_REQUEUE 4
+ *   #define ELEVATOR_INSERT_FLUSH   5
+ *   #define ELEVATOR_INSERT_SORT_MERGE      6
+ *
+ * 感觉应该只用在sq
+ */
 void __elv_add_request(struct request_queue *q, struct request *rq, int where)
 {
 	trace_block_rq_insert(q, rq);
@@ -729,6 +1048,11 @@ void __elv_add_request(struct request_queue *q, struct request *rq, int where)
 }
 EXPORT_SYMBOL(__elv_add_request);
 
+/*
+ * called by:
+ *   - ide_queue_sense_rq() -- drivers/ide/ide-atapi.c
+ *   - issue_park_cmd()     -- drivers/ide/ide-park.c
+ */
 void elv_add_request(struct request_queue *q, struct request *rq, int where)
 {
 	unsigned long flags;
@@ -739,6 +1063,9 @@ void elv_add_request(struct request_queue *q, struct request *rq, int where)
 }
 EXPORT_SYMBOL(elv_add_request);
 
+/*
+ * 取出rq在request_queue中(比如rbtree)的下一个request
+ */
 struct request *elv_latter_request(struct request_queue *q, struct request *rq)
 {
 	struct elevator_queue *e = q->elevator;
@@ -751,6 +1078,9 @@ struct request *elv_latter_request(struct request_queue *q, struct request *rq)
 	return NULL;
 }
 
+/*
+ * 取出rq在request_queue中的上一个request
+ */
 struct request *elv_former_request(struct request_queue *q, struct request *rq)
 {
 	struct elevator_queue *e = q->elevator;
@@ -762,6 +1092,11 @@ struct request *elv_former_request(struct request_queue *q, struct request *rq)
 	return NULL;
 }
 
+/*
+ * called only by __get_request()
+ *
+ * mq使用会warn
+ */
 int elv_set_request(struct request_queue *q, struct request *rq,
 		    struct bio *bio, gfp_t gfp_mask)
 {
@@ -775,6 +1110,11 @@ int elv_set_request(struct request_queue *q, struct request *rq,
 	return 0;
 }
 
+/*
+ * called only by blk_free_request()
+ *
+ * mq使用会warn
+ */
 void elv_put_request(struct request_queue *q, struct request *rq)
 {
 	struct elevator_queue *e = q->elevator;
@@ -786,6 +1126,11 @@ void elv_put_request(struct request_queue *q, struct request *rq)
 		e->type->ops.sq.elevator_put_req_fn(rq);
 }
 
+/*
+ * called only by __get_request()
+ *
+ * mq使用会warn
+ */
 int elv_may_queue(struct request_queue *q, unsigned int op)
 {
 	struct elevator_queue *e = q->elevator;
@@ -799,6 +1144,14 @@ int elv_may_queue(struct request_queue *q, unsigned int op)
 	return ELV_MQUEUE_MAY;
 }
 
+/*
+ * called by:
+ *   - __blk_put_request()
+ *   - flush_end_io()
+ *   - flush_data_end_io()
+ *
+ * mq使用会warn
+ */
 void elv_completed_request(struct request_queue *q, struct request *rq)
 {
 	struct elevator_queue *e = q->elevator;
@@ -864,6 +1217,13 @@ static struct kobj_type elv_ktype = {
 	.release	= elevator_release,
 };
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|901| <<blk_register_queue>>
+ *   - block/elevator.c|1334| <<elevator_switch_mq>>
+ *   - block/elevator.c|1396| <<elevator_switch>>
+ *   - block/elevator.c|1416| <<elevator_switch>>
+ */
 int elv_register_queue(struct request_queue *q)
 {
 	struct elevator_queue *e = q->elevator;
@@ -889,6 +1249,12 @@ int elv_register_queue(struct request_queue *q)
 	return error;
 }
 
+/*
+ * called by:
+ *   - block/blk-sysfs.c|961| <<blk_unregister_queue>>
+ *   - block/elevator.c|1324| <<elevator_switch_mq>>
+ *   - block/elevator.c|1386| <<elevator_switch>>
+ */
 void elv_unregister_queue(struct request_queue *q)
 {
 	lockdep_assert_held(&q->sysfs_lock);
@@ -904,6 +1270,15 @@ void elv_unregister_queue(struct request_queue *q)
 	}
 }
 
+/*
+ * called by:
+ *   - block/bfq-iosched.c|5587| <<bfq_init>> ret = elv_register(&iosched_bfq_mq);
+ *   - block/cfq-iosched.c|4884| <<cfq_init>> ret = elv_register(&iosched_cfq);
+ *   - block/deadline-iosched.c|548| <<deadline_init>> return elv_register(&iosched_deadline);
+ *   - block/kyber-iosched.c|1045| <<kyber_init>> return elv_register(&kyber_sched);
+ *   - block/mq-deadline.c|1101| <<deadline_init>> return elv_register(&mq_deadline);
+ *   - block/noop-iosched.c|137| <<noop_init>> return elv_register(&elevator_noop);
+ */
 int elv_register(struct elevator_type *e)
 {
 	char *def = "";
@@ -964,6 +1339,9 @@ void elv_unregister(struct elevator_type *e)
 }
 EXPORT_SYMBOL_GPL(elv_unregister);
 
+/*
+ * called only by elevator_switch()
+ */
 static int elevator_switch_mq(struct request_queue *q,
 			      struct elevator_type *new_e)
 {
@@ -1010,6 +1388,9 @@ static int elevator_switch_mq(struct request_queue *q,
  * need for the new one. this way we have a chance of going back to the old
  * one, if the new one fails init for some reason.
  */
+/*
+ * 两次被__elevator_change()调用
+ */
 static int elevator_switch(struct request_queue *q, struct elevator_type *new_e)
 {
 	struct elevator_queue *old = q->elevator;
@@ -1075,6 +1456,9 @@ static int elevator_switch(struct request_queue *q, struct elevator_type *new_e)
 /*
  * Switch this queue to the given IO scheduler.
  */
+/*
+ * 被sysfs的接口elv_iosched_store()调用
+ */
 static int __elevator_change(struct request_queue *q, const char *name)
 {
 	char elevator_name[ELV_NAME_MAX];
@@ -1103,6 +1487,11 @@ static int __elevator_change(struct request_queue *q, const char *name)
 	return elevator_switch(q, e);
 }
 
+/*
+ * called by:
+ *   - elv_iosched_store()
+ *   - elv_iosched_show()
+ */
 static inline bool elv_support_iosched(struct request_queue *q)
 {
 	if (q->mq_ops && q->tag_set && (q->tag_set->flags &
@@ -1163,9 +1552,15 @@ ssize_t elv_iosched_show(struct request_queue *q, char *name)
 	return len;
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.former_request = elv_rb_former_request()
+ *
+ * 在rbtree中取出rq的上一个request
+ */
 struct request *elv_rb_former_request(struct request_queue *q,
 				      struct request *rq)
 {
+	/* 根据rq->rb_node获取struct rb_node, 说明request已经在request_queue了 */
 	struct rb_node *rbprev = rb_prev(&rq->rb_node);
 
 	if (rbprev)
@@ -1175,6 +1570,11 @@ struct request *elv_rb_former_request(struct request_queue *q,
 }
 EXPORT_SYMBOL(elv_rb_former_request);
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.next_request = elv_rb_latter_request()
+ *
+ * 在rbtree中取出rq的下一个request
+ */
 struct request *elv_rb_latter_request(struct request_queue *q,
 				      struct request *rq)
 {
diff --git a/block/kyber-iosched.c b/block/kyber-iosched.c
index 0d6d25e..b9735e0 100644
--- a/block/kyber-iosched.c
+++ b/block/kyber-iosched.c
@@ -31,6 +31,20 @@
 #include "blk-mq-tag.h"
 #include "blk-stat.h"
 
+/*
+ * 申请token
+ *     kyber_get_domain_token()  -- 只被kyber_dispatch_cur_domain()调用
+ *
+ * 获得request的token
+ *     rq_get_domain_token()  -- 只在释放token的rq_clear_domain_token()被用到
+ *
+ * 设置token
+ *     rq_set_domain_token()  -- 被kyber_prepare_request()清空为-1, 被kyber_dispatch_cur_domain()设置为申请到的token nr
+ *
+ * 释放token
+ *     rq_clear_domain_token()  -- 只被kyber_finish_request()调用
+ */
+
 /* Scheduling domains. */
 enum {
 	KYBER_READ,
@@ -40,6 +54,10 @@ enum {
 };
 
 enum {
+	/*
+	 * kyber_queue_data_alloc()中初始化kyber_queue_data.domain_tokens[i]的depth的最小值
+	 * 只在kyber_queue_data_alloc()用到
+	 */
 	KYBER_MIN_DEPTH = 256,
 
 	/*
@@ -47,6 +65,10 @@ enum {
 	 * asynchronous requests, we reserve 25% of requests for synchronous
 	 * operations.
 	 */
+	/*
+	 * kyber_queue_data_alloc()中初始化kyber_queue_data.async_depth
+	 * 只在kyber_queue_data_alloc()中用到
+	 */
 	KYBER_ASYNC_PERCENT = 75,
 };
 
@@ -57,6 +79,16 @@ enum {
  * the device with only a fraction of the maximum possible queue depth.
  * So, we cap these to a reasonable value.
  */
+/*
+ * 为什么不使用block driver给的depth呢???
+ *
+ * 被以下用到:
+ *   - <<kyber_adjust_rw_depth>> depth = clamp(depth, 1U, kyber_depth[sched_domain]);
+ *   - <<kyber_adjust_other_depth>> depth = clamp(depth, 1U, kyber_depth[KYBER_OTHER]);
+ *   - <<kyber_stat_timer_fn>> kqd->domain_tokens[KYBER_OTHER].sb.depth < kyber_depth[KYBER_OTHER])))
+ *   - <<kyber_queue_data_alloc>> WARN_ON(!kyber_depth[i]);
+ *   - <<kyber_queue_data_alloc>> sbitmap_queue_resize(&kqd->domain_tokens[i], kyber_depth[i]);
+ */
 static const unsigned int kyber_depth[] = {
 	[KYBER_READ] = 256,
 	[KYBER_SYNC_WRITE] = 128,
@@ -66,12 +98,20 @@ static const unsigned int kyber_depth[] = {
 /*
  * Scheduling domain batch sizes. We favor reads.
  */
+/*
+ * 被以下用到:
+ *   - <<kyber_queue_data_alloc>> WARN_ON(!kyber_batch_size[i]);
+ *   - <<kyber_dispatch_request>> if (khd->batching < kyber_batch_size[khd->cur_domain]) {
+ */
 static const unsigned int kyber_batch_size[] = {
 	[KYBER_READ] = 16,
 	[KYBER_SYNC_WRITE] = 8,
 	[KYBER_OTHER] = 8,
 };
 
+/*
+ * 每个request_queue一个kyber_queue_data
+ */
 struct kyber_queue_data {
 	struct request_queue *q;
 
@@ -82,31 +122,68 @@ struct kyber_queue_data {
 	 * request type. Each domain has a fixed number of in-flight requests of
 	 * that type device-wide, limited by these tokens.
 	 */
+	/*
+	 * struct sbitmap_queue中包含一个struct sbq_wait_state的数组
+	 *
+	 * sbitmap的depth会在以下的地方被修改
+	 *   - <<kyber_adjust_rw_depth>> sbitmap_queue_resize(&kqd->domain_tokens[sched_domain], depth);
+	 *   - <<kyber_adjust_other_depth>> sbitmap_queue_resize(&kqd->domain_tokens[KYBER_OTHER], depth);
+	 *   - <<kyber_queue_data_alloc>> ret = sbitmap_queue_init_node(&kqd->domain_tokens[i],
+	 *   - <<kyber_queue_data_alloc>> sbitmap_queue_resize(&kqd->domain_tokens[i], kyber_depth[i]);
+	 */
 	struct sbitmap_queue domain_tokens[KYBER_NUM_DOMAINS];
 
 	/*
 	 * Async request percentage, converted to per-word depth for
 	 * sbitmap_get_shallow().
 	 */
+	/*
+	 * 在以下被使用:
+	 *   - <<kyber_queue_data_alloc>> kqd->async_depth = (1U << shift) * KYBER_ASYNC_PERCENT / 100U;
+	 *   - <<kyber_limit_depth>> data->shallow_depth = kqd->async_depth;
+	 *   - <<kyber_async_depth_show>> seq_printf(m, "%u\n", kqd->async_depth);
+	 */
 	unsigned int async_depth;
 
 	/* Target latencies in nanoseconds. */
+	/*
+	 * read_lat_nsec初始化后就没改过:
+	 *   - <<kyber_queue_data_alloc>> kqd->read_lat_nsec = 2000000ULL;
+	 *
+	 * write_lat_nsec初始化后就没改过:
+	 *   - <<kyber_queue_data_alloc>> kqd->write_lat_nsec = 10000000ULL;
+	 */
 	u64 read_lat_nsec, write_lat_nsec;
 };
 
+/*
+ * 设置为blk_mq_hw_ctx.sched_data
+ */
 struct kyber_hctx_data {
 	spinlock_t lock;
-	struct list_head rqs[KYBER_NUM_DOMAINS];
+	struct list_head rqs[KYBER_NUM_DOMAINS];  // 只在kyber_dispatch_cur_domain()-->kyber_flush_busy_ctxs()添加request
+	/*
+	 * 只在kyber_dispatch_request()中增加 (会wrap around)
+	 */
 	unsigned int cur_domain;
-	unsigned int batching;
-	wait_queue_entry_t domain_wait[KYBER_NUM_DOMAINS];
-	struct sbq_wait_state *domain_ws[KYBER_NUM_DOMAINS];
+	unsigned int batching;  // batching只在kyber_dispatch_cur_domain()增加
+	wait_queue_entry_t domain_wait[KYBER_NUM_DOMAINS];  // 在kyber_init_hctx()中初始化为kyber_domain_wake()
+	struct sbq_wait_state *domain_ws[KYBER_NUM_DOMAINS]; // 每一个有一个waitqueue
 	atomic_t wait_index[KYBER_NUM_DOMAINS];
 };
 
 static int kyber_domain_wake(wait_queue_entry_t *wait, unsigned mode, int flags,
 			     void *key);
 
+/*
+ * called by:
+ *   - kyber_queue_data_alloc() --- 设置为struct blk_stat_callback的bucket_fn
+ *   - rq_clear_domain_token()
+ *   - kyber_completed_request()
+ *   - kyber_flush_busy_ctxs()
+ *
+ * 根据request的cmd_flags获取对应的KYBER DOMAIN
+ */
 static int rq_sched_domain(const struct request *rq)
 {
 	unsigned int op = rq->cmd_flags;
@@ -130,6 +207,9 @@ enum {
 #define IS_GOOD(status) ((status) > 0)
 #define IS_BAD(status) ((status) < 0)
 
+/*
+ * called only by kyber_stat_timer_fn()
+ */
 static int kyber_lat_status(struct blk_stat_callback *cb,
 			    unsigned int sched_domain, u64 target)
 {
@@ -169,6 +249,9 @@ static void kyber_adjust_rw_depth(struct kyber_queue_data *kqd,
 	    (IS_BAD(this_status) && IS_BAD(other_status)))
 		return;
 
+	/*
+	 * 一个kyber_queue_data有多个sbitmap_queue
+	 */
 	orig_depth = depth = kqd->domain_tokens[sched_domain].sb.depth;
 
 	if (other_status == NONE) {
@@ -284,19 +367,43 @@ static unsigned int kyber_sched_tags_shift(struct kyber_queue_data *kqd)
 	return kqd->q->queue_hw_ctx[0]->sched_tags->bitmap_tags.sb.shift;
 }
 
+/*
+ * called by only kyber_init_sched()
+ *
+ * struct kyber_queue_data的sbitmap里的bit就是token
+ *
+ * token干什么用的????
+ */
 static struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)
 {
+	/*
+	 * 每个request_queue一个kyber_queue_data
+	 */
 	struct kyber_queue_data *kqd;
 	unsigned int max_tokens;
 	unsigned int shift;
 	int ret = -ENOMEM;
 	int i;
 
+	/* 分配struct kyber_queue_data */
 	kqd = kmalloc_node(sizeof(*kqd), GFP_KERNEL, q->node);
 	if (!kqd)
 		goto err;
 	kqd->q = q;
 
+	/*
+	 * cb->timer_fn = kyber_stat_timer_fn;
+	 * cb->bucket_fn = rq_sched_domain;
+	 * cb->data = kqd;
+	 * cb->buckets = KYBER_NUM_DOMAINS;
+	 *
+	 * 就三个domain:
+	 *    KYBER_SYNC_READ
+	 *    KYBER_SYNC_WRITE
+	 *    KYBER_SYNC_OTHER
+	 *
+	 * 为kyber_queue_data分配blk_stat_callback
+	 */
 	kqd->cb = blk_stat_alloc_callback(kyber_stat_timer_fn, rq_sched_domain,
 					  KYBER_NUM_DOMAINS, kqd);
 	if (!kqd->cb)
@@ -307,12 +414,23 @@ static struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)
 	 * the queue depth of a single hardware queue. If the hardware doesn't
 	 * have many tags, still provide a reasonable number.
 	 */
+	/*
+	 * BLK_MQ_MAX_DEPTH是10240在include/linux/blk-mq.h定义
+	 *
+	 * queue_depth的值
+	 *    xen: BLK_RING_SIZE(info)
+	 *    kvm: virtblk_queue_depth = vblk->vqs[0].vq->num_free
+	 */
 	max_tokens = max_t(unsigned int, q->tag_set->queue_depth,
 			   KYBER_MIN_DEPTH);
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++) {
 		WARN_ON(!kyber_depth[i]);
 		WARN_ON(!kyber_batch_size[i]);
-		ret = sbitmap_queue_init_node(&kqd->domain_tokens[i],
+		/*
+		 * 每个&kqd->domain_tokens[i]的sbitmap一共有q->tag_set->queue_depth个token!
+		 * 下面会重新设计为kyber_depth[i]
+		 */
+		ret = sbitmap_queue_init_node(&kqd->domain_tokens[i], // ---> struct sbitmap_queue
 					      max_tokens, -1, false, GFP_KERNEL,
 					      q->node);
 		if (ret) {
@@ -324,6 +442,11 @@ static struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)
 	}
 
 	shift = kyber_sched_tags_shift(kqd);
+	/*
+	 * In order to prevent starvation of synchronous requests by a flood of
+	 * asynchronous requests, we reserve 25% of requests for synchronous
+	 * operations.
+	 */
 	kqd->async_depth = (1U << shift) * KYBER_ASYNC_PERCENT / 100U;
 
 	kqd->read_lat_nsec = 2000000ULL;
@@ -339,15 +462,24 @@ static struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)
 	return ERR_PTR(ret);
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mp.init_sched = kyber_init_sched()
+ *
+ * called only by blk_mq_init_sched()
+ */
 static int kyber_init_sched(struct request_queue *q, struct elevator_type *e)
 {
 	struct kyber_queue_data *kqd;
 	struct elevator_queue *eq;
 
+	/* 分配struct elevator_queue并初始化一些特性 */
 	eq = elevator_alloc(q, e);
 	if (!eq)
 		return -ENOMEM;
 
+	/*
+	 * struct kyber_queue_data的sbitmap里的bit就是token
+	 */
 	kqd = kyber_queue_data_alloc(q);
 	if (IS_ERR(kqd)) {
 		kobject_put(&eq->kobj);
@@ -357,11 +489,17 @@ static int kyber_init_sched(struct request_queue *q, struct elevator_type *e)
 	eq->elevator_data = kqd;
 	q->elevator = eq;
 
+	/*
+	 * 主要把&cb->list加到&q->stats->callbacks后面
+	 */
 	blk_stat_add_callback(q, kqd->cb);
 
 	return 0;
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.exit_sched = kyber_exit_sched()
+ */
 static void kyber_exit_sched(struct elevator_queue *e)
 {
 	struct kyber_queue_data *kqd = e->elevator_data;
@@ -376,6 +514,14 @@ static void kyber_exit_sched(struct elevator_queue *e)
 	kfree(kqd);
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.init_hctx = kyber_init_hctx()
+ *
+ * called by:
+ *   - blk_mq_sched_init_hctx()
+ *   - blk_mq_init_sched()
+ *   - blk_mq_init_hctx()
+ */
 static int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
 	struct kyber_hctx_data *khd;
@@ -389,6 +535,7 @@ static int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++) {
 		INIT_LIST_HEAD(&khd->rqs[i]);
+		/* 很重要 */
 		init_waitqueue_func_entry(&khd->domain_wait[i],
 					  kyber_domain_wake);
 		khd->domain_wait[i].private = hctx;
@@ -404,35 +551,76 @@ static int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 	return 0;
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mp.exit_hctx = kyber_exit_hctx()
+ */
 static void kyber_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
 	kfree(hctx->sched_data);
 }
 
+/*
+ * called only by rq_clear_domain_token()
+ */
 static int rq_get_domain_token(struct request *rq)
 {
 	return (long)rq->elv.priv[0];
 }
 
+/*
+ * called by:
+ *   - kyber_prepare_request()     -- rq_set_domain_token(rq, -1);
+ *   - kyber_dispatch_cur_domain() -- rq_set_domain_token(rq, nr);
+ *
+ * 被kyber_prepare_request()清空为-1, 被kyber_dispatch_cur_domain()设置为申请到的token n
+ */
 static void rq_set_domain_token(struct request *rq, int token)
 {
 	rq->elv.priv[0] = (void *)(long)token;
 }
 
+/*
+ * called only by kyber_finish_request()
+ *
+ * 这里并不会修改rq->elv.priv[0]
+ */
 static void rq_clear_domain_token(struct kyber_queue_data *kqd,
 				  struct request *rq)
 {
 	unsigned int sched_domain;
 	int nr;
 
+	/*
+	 * 申请的token会放在request的私有数据
+	 * (long)rq->elv.priv[0]
+	 */
 	nr = rq_get_domain_token(rq);
 	if (nr != -1) {
+		/* 根据request的cmd_flags获取对应的KYBER DOMAIN */
 		sched_domain = rq_sched_domain(rq);
+		/*
+		 * 清除sbitmap_queue的sbitmap对应的bit nr
+		 * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的ws
+		 * 更新sbq->wake_index (sbq->wake_index就是当前active的那个ws)
+		 * 把ws->wait_cnt减1 如果ws->wait_cnt小于等于0了则"唤醒"ws上的wait queue的wake_batch个entry
+		 * 增加sbq->wake_index, 把dec的wake_batch恢复
+		 *
+		 * 会唤醒ws上的wait queue的wake_batch个entry
+		 *
+		 * 唤醒的是kyber_domain_wake()
+		 *
+		 * 这里其实就是把token还回去
+		 */
 		sbitmap_queue_clear(&kqd->domain_tokens[sched_domain], nr,
 				    rq->mq_ctx->cpu);
 	}
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.limit_depth = kyber_limit_depth()
+ *
+ * called only by blk_mq_get_request()
+ */
 static void kyber_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)
 {
 	/*
@@ -446,11 +634,28 @@ static void kyber_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)
 	}
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.prepare_request = kyber_prepare_request()
+ *
+ * called only by blk_mq_get_request()
+ *
+ * 初始化request的domain token为-1
+ */
 static void kyber_prepare_request(struct request *rq, struct bio *bio)
 {
 	rq_set_domain_token(rq, -1);
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.finish_request = kyber_finish_request()
+ * struct elevator_type kyber_sched.ops.mq.requeue_request = kyber_finish_request()
+ *
+ * finish_request():
+ *     - blk_mq_free_request()
+ *
+ * requeue_request():
+ *     - blk_mq_sched_requeue_request()
+ */
 static void kyber_finish_request(struct request *rq)
 {
 	struct kyber_queue_data *kqd = rq->q->elevator->elevator_data;
@@ -458,6 +663,11 @@ static void kyber_finish_request(struct request *rq)
 	rq_clear_domain_token(kqd, rq);
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.completed_request = kyber_completed_request()
+ *
+ * called only by blk_mq_sched_completed_request()
+ */
 static void kyber_completed_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -495,21 +705,37 @@ static void kyber_completed_request(struct request *rq)
 		blk_stat_activate_msecs(kqd->cb, 10);
 }
 
+/*
+ * called by only kyber_dispatch_cur_domain()
+ *
+ * 把struct blk_mq_hw_ctx (硬件队列)的一些(某种条件的)request
+ * 放入对应的kyber_hctx_data (sched_domain) 的rqs链表
+ */
 static void kyber_flush_busy_ctxs(struct kyber_hctx_data *khd,
 				  struct blk_mq_hw_ctx *hctx)
 {
 	LIST_HEAD(rq_list);
 	struct request *rq, *next;
 
+	/* Process software queues that have been marked busy, splicing them to rq_list */
 	blk_mq_flush_busy_ctxs(hctx, &rq_list);
 	list_for_each_entry_safe(rq, next, &rq_list, queuelist) {
 		unsigned int sched_domain;
 
+		/* 根据request的cmd_flags获取对应的KYBER DOMAIN */
 		sched_domain = rq_sched_domain(rq);
+		/*
+		 * 把rq (request)从rq_list移到对应domain的rqs
+		 *
+		 * 只在这里修改rqs
+		 */
 		list_move_tail(&rq->queuelist, &khd->rqs[sched_domain]);
 	}
 }
 
+/*
+ * 设置为kyber_hctx_data (&khd->domain_wait[i]) 的func
+ */
 static int kyber_domain_wake(wait_queue_entry_t *wait, unsigned mode, int flags,
 			     void *key)
 {
@@ -520,16 +746,29 @@ static int kyber_domain_wake(wait_queue_entry_t *wait, unsigned mode, int flags,
 	return 1;
 }
 
+/*
+ * called only by kyber_dispatch_cur_domain()
+ */
 static int kyber_get_domain_token(struct kyber_queue_data *kqd,
 				  struct kyber_hctx_data *khd,
 				  struct blk_mq_hw_ctx *hctx)
 {
 	unsigned int sched_domain = khd->cur_domain;
 	struct sbitmap_queue *domain_tokens = &kqd->domain_tokens[sched_domain];
+	/*
+	 * khd是kyber_hctx_data
+	 *
+	 * wait来自kyber_hctx_data, 是属于kyber_hctx_data的wait_queue_entry_t实体
+	 */
 	wait_queue_entry_t *wait = &khd->domain_wait[sched_domain];
 	struct sbq_wait_state *ws;
 	int nr;
 
+	/*
+	 * 从sbitmap_queue中分配一个bit (bit是全局的)
+	 *
+	 * domain_tokens来自kyber_queue_data, kyber_queue_data是全局的 一个request_queue一个
+	 */
 	nr = __sbitmap_queue_get(domain_tokens);
 
 	/*
@@ -537,10 +776,31 @@ static int kyber_get_domain_token(struct kyber_queue_data *kqd,
 	 * run when one becomes available. Note that this is serialized on
 	 * khd->lock, but we still need to be careful about the waker.
 	 */
+	/* tests whether a list is empty and not being modified */
+	/*
+	 * wait来自kyber_hctx_data, 是属于kyber_hctx_data的wait_queue_entry_t实体
+	 */
 	if (nr < 0 && list_empty_careful(&wait->entry)) {
+		/*
+		 * 返回wait_index (khd->wait_index[sched_domain])索引的下一个sbq->ws(struct sbq_wait_state类型) 增加wait_index
+		 *
+		 * domain_tokens是上面从kqd->domain_tokens[sched_domain]获得的sbitmap_queue
+		 * domain_tokens来自kyber_queue_data, kyber_queue_data是全局的 一个request_queue一个
+		 */
+		/*
+		 * ws来自domain_tokens的sbitmap, 包含一个wait_queue_head_t
+		 */
 		ws = sbq_wait_ptr(domain_tokens,
 				  &khd->wait_index[sched_domain]);
 		khd->domain_ws[sched_domain] = ws;
+		/*
+		 * ws->wait是wait_queue_head_t类型, 是khd->domain_wait[sched_domain]
+		 *
+		 * wait的func在kyber_init_hctx()中初始化为kyber_domain_wake()
+		 * 把wait挂载到了ws->wait
+		 *
+		 * 因为这里不睡眠 所以kyber_finish_request()-->rq_clear_domain_token()唤醒的时候kyber_domain_wake()会调用blk_mq_run_hw_queue()
+		 */
 		add_wait_queue(&ws->wait, wait);
 
 		/*
@@ -567,6 +827,9 @@ static int kyber_get_domain_token(struct kyber_queue_data *kqd,
 	return nr;
 }
 
+/*
+ * 只被kyber_dispatch_request()调用两次
+ */
 static struct request *
 kyber_dispatch_cur_domain(struct kyber_queue_data *kqd,
 			  struct kyber_hctx_data *khd,
@@ -577,7 +840,13 @@ kyber_dispatch_cur_domain(struct kyber_queue_data *kqd,
 	struct request *rq;
 	int nr;
 
+	/*
+	 * rqs是链表
+	 *
+	 * 只在kyber_dispatch_cur_domain()-->kyber_flush_busy_ctxs()添加request
+	 */
 	rqs = &khd->rqs[khd->cur_domain];
+	/* get the first element from a list. Note that if the list is empty, it returns NULL. */
 	rq = list_first_entry_or_null(rqs, struct request, queuelist);
 
 	/*
@@ -585,16 +854,26 @@ kyber_dispatch_cur_domain(struct kyber_queue_data *kqd,
 	 * software queues yet, flush the software queues and check again.
 	 */
 	if (!rq && !*flushed) {
+		/*
+		 * 把struct blk_mq_hw_ctx (硬件队列)的一些(某种条件的)request
+		 * 放入对应的kyber_hctx_data (sched_domain) 的rqs链表
+		 */
 		kyber_flush_busy_ctxs(khd, hctx);
 		*flushed = true;
 		rq = list_first_entry_or_null(rqs, struct request, queuelist);
 	}
 
 	if (rq) {
+		/* 如果token没了就等待吧 */
 		nr = kyber_get_domain_token(kqd, khd, hctx);
 		if (nr >= 0) {
+			/*
+			 * request是从current domain上拿的 batching超过了就要在别的地方换一个current domain了
+			 */
 			khd->batching++;
+			/* 给request设置上面申请的token nr */
 			rq_set_domain_token(rq, nr);
+			/* 把request从&khd->rqs[khd->cur_domain]上删除 */
 			list_del_init(&rq->queuelist);
 			return rq;
 		}
@@ -604,6 +883,11 @@ kyber_dispatch_cur_domain(struct kyber_queue_data *kqd,
 	return NULL;
 }
 
+/*
+ * called by only blk_mq_do_dispatch_sched()
+ *
+ * struct elevator_type kyber_sched.ops.mq.dispatch_request = kyber_dispatch_request()
+ */
 static struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)
 {
 	struct kyber_queue_data *kqd = hctx->queue->elevator->elevator_data;
@@ -618,6 +902,9 @@ static struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	 * First, if we are still entitled to batch, try to dispatch a request
 	 * from the batch.
 	 */
+	/*
+	 * batching只在kyber_dispatch_cur_domain()增加
+	 */
 	if (khd->batching < kyber_batch_size[khd->cur_domain]) {
 		rq = kyber_dispatch_cur_domain(kqd, khd, hctx, &flushed);
 		if (rq)
@@ -634,6 +921,7 @@ static struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	 * domain if no other domains have requests or tokens.
 	 */
 	khd->batching = 0;
+	/* 如果current domain的batching满了 换下一个current domain */
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++) {
 		if (khd->cur_domain == KYBER_NUM_DOMAINS - 1)
 			khd->cur_domain = 0;
@@ -651,15 +939,30 @@ static struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	return rq;
 }
 
+/*
+ * struct elevator_type kyber_sched.ops.mq.has_work = kyber_has_work()
+ *
+ * 查看某一个硬件队列是否还有request
+ * 通过查看硬件队列struct kyber_hctx_data的每一个sched_domain的rqs和ctx_map确认是否有work
+ */
 static bool kyber_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct kyber_hctx_data *khd = hctx->sched_data;
 	int i;
 
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++) {
+		/*
+		 * tests whether a list is empty and not being modified
+		 * 判断是否所有的rqs[i]都是空的
+		 */
 		if (!list_empty_careful(&khd->rqs[i]))
 			return true;
 	}
+	/*
+	 * 在blk_mq_init_hctx()初始化, depth是nr_ctx (nr_ctx记录着这个hw queue当前map着几个sw queue)
+	 *
+	 * bit在blk_mq_hctx_mark_pending()被置位
+	 */
 	return sbitmap_any_bit_set(&hctx->ctx_map);
 }
 
diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 8ec0ba9..0807b8d 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -24,11 +24,40 @@
 #include "blk-mq-sched.h"
 
 /*
+ * Deadline算法的核心在于保证每个IO请求在一定的时间内一定
+ * 要被服务到, 以此来避免某个请求饥饿.
+ *
+ * Deadline算法中引入了四个队列,  这四个队列可以分为两类,每
+ * 一类都由读和写两类队列组成,一类队列用来对请求按起始扇区
+ * 序号进行排序, 通过红黑树来组织, 称为sort_list;
+ * 另一类对请求按它们的生成时间进行排序, 由链表来组织, 称为fifo_list.
+ *
+ * 每当确定了一个传输方向(读或写), 那么将会从相应的sort_list
+ * 中将一批连续请求dispatch到requst_queue的请求队列里, 具体的
+ * 数目由fifo_batch来确定.
+ *
+ * 只有下面三种情况才会导致一次批量传输的结束:
+ *
+ * 1) 对应的sort_list中已经没有请求了
+ *
+ * 2) 下一个请求的扇区不满足递增的要求
+ *
+ * 3) 上一个请求已经是批量传输的最后一个请求了。
+ */
+
+/*
  * See Documentation/block/deadline-iosched.txt
  */
+/* 设置为dd->fifo_expire[READ] */
 static const int read_expire = HZ / 2;  /* max time before a read is submitted. */
+/* 设置为dd->fifo_expire[WRITE] */
 static const int write_expire = 5 * HZ; /* ditto for writes, these limits are SOFT! */
+/* 不可以每次让READ优先让WRITE等太久 */
 static const int writes_starved = 2;    /* max times reads can starve a write */
+/*
+ * 每当确定了一个传输方向(读或写), 那么将会从相应的sort_list中将一批连续请求dispatch
+ * 到request_queue的请求队列里, 具体的数目由fifo_batch(default:16)来确定.
+ */
 static const int fifo_batch = 16;       /* # of sequential requests treated as one
 				     by the above parameters. For throughput. */
 
@@ -40,29 +69,58 @@ struct deadline_data {
 	/*
 	 * requests (deadline_rq s) are present on both sort_list and fifo_list
 	 */
+	/*
+	 * 通过红黑树来对请求按起始扇区序号进行排序
+	 * key是request->__sector (所以front merge后要重新删除添加)
+	 */
 	struct rb_root sort_list[2];
-	struct list_head fifo_list[2];
+	/*
+	 * insert的时候是add tail, 因为是FIFO!
+	 */
+	struct list_head fifo_list[2]; /* 0是READ 1是WRITE */
 
 	/*
 	 * next in sort order. read, write or both are NULL
 	 */
+	/*
+	 * 被deadline_del_rq_rb()和deadline_move_request()修改
+	 */
 	struct request *next_rq[2];
-	unsigned int batching;		/* number of sequential requests made */
-	unsigned int starved;		/* times reads have starved writes */
+	unsigned int batching;		/* number of sequential requests made */ /* 不能大于fifo_batch */
+	unsigned int starved;		/* times reads have starved writes */ /* 写饥饿的次数 */
 
 	/*
 	 * settings that change how the i/o scheduler behaves
 	 */
-	int fifo_expire[2];
+	int fifo_expire[2]; /* 读写超时时间 */
+	/*
+	 * 每当确定了一个传输方向(读或写)，那么将会从相应的sort_list中将一批连续请求dispatch
+	 * 到request_queue的请求队列里，具体的数目由fifo_batch(default:16)来确定。
+	 */
 	int fifo_batch;
-	int writes_starved;
+	int writes_starved; /* 写饥饿的最大限制 */
 	int front_merges;
 
 	spinlock_t lock;
 	spinlock_t zone_lock;
+	/*
+	 * 在dd_insert_request()中, 只有at_head || blk_rq_is_passthrough(rq)的
+	 * request, 才会被放到这个list
+	 * at_head时放在head, 否则add tail
+	 *
+	 * 只在__dd_dispatch_request()被优先取出
+	 */
 	struct list_head dispatch;
 };
 
+/*
+ * called by:
+ *   - deadline_add_rq_rb()
+ *   - deadline_del_rq_rb()
+ *   - dd_request_merged()
+ *
+ * 根据request的READ或WRITE返回对应的sort_list (rb_root)
+ */
 static inline struct rb_root *
 deadline_rb_root(struct deadline_data *dd, struct request *rq)
 {
@@ -72,6 +130,15 @@ deadline_rb_root(struct deadline_data *dd, struct request *rq)
 /*
  * get the request after `rq' in sector-sorted order
  */
+/*
+ * called by:
+ *   - deadline_del_rq_rb()
+ *   - deadline_move_request()
+ *   - deadline_next_request()
+ *
+ * 返回request在rbtree的下一个request
+ * rbtree是按照sector存的!
+ */
 static inline struct request *
 deadline_latter_request(struct request *rq)
 {
@@ -83,45 +150,88 @@ deadline_latter_request(struct request *rq)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - dd_request_merged()
+ *   - dd_insert_request()
+ *
+ * 根据request的READ或WRITE返回对应的sort_list (rb_root)
+ * 根据request的blk_rq_pos() (rq->__sector)添加到rbtree
+ */
 static void
 deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
 {
+	/* 根据request的READ或WRITE返回对应的sort_list (rb_root) */
 	struct rb_root *root = deadline_rb_root(dd, rq);
 
+	/* 根据request的blk_rq_pos() (rq->__sector)添加到rbtree */
 	elv_rb_add(root, rq);
 }
 
+/*
+ * called only by deadline_remove_request()
+ *
+ * 根据request的READ或WRITE返回对应的sort_list (rb_root)
+ * 把request从rbtree删除
+ */
 static inline void
 deadline_del_rq_rb(struct deadline_data *dd, struct request *rq)
 {
 	const int data_dir = rq_data_dir(rq);
 
+	/* 返回request在rbtree的下一个request */
 	if (dd->next_rq[data_dir] == rq)
 		dd->next_rq[data_dir] = deadline_latter_request(rq);
 
+	/*
+	 * deadline_rb_root(): 根据request的READ或WRITE返回对应的sort_list (rb_root)
+	 *
+	 * 把request从rbtree删除
+	 */
 	elv_rb_del(deadline_rb_root(dd, rq), rq);
 }
 
 /*
  * remove rq from rbtree and fifo.
+ * 从hash上也要删除
+ */
+/*
+ * called by:
+ *   - dd_merged_requests()
+ *   - deadline_move_request()
  */
 static void deadline_remove_request(struct request_queue *q, struct request *rq)
 {
 	struct deadline_data *dd = q->elevator->elevator_data;
 
+	/* 在fifo删掉 */
 	list_del_init(&rq->queuelist);
 
 	/*
 	 * We might not be on the rbtree, if we are doing an insert merge
 	 */
+	/*
+	 * 根据request的READ或WRITE返回对应的sort_list (rb_root)
+	 * 把request从rbtree删除
+	 */
 	if (!RB_EMPTY_NODE(&rq->rb_node))
 		deadline_del_rq_rb(dd, rq);
 
+	/*
+	 * 从hash上也要删除!!!
+	 */
 	elv_rqhash_del(q, rq);
 	if (q->last_merge == rq)
 		q->last_merge = NULL;
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.request_merged = dd_request_merged()
+ *
+ * called only by elv_merged_request()
+ *
+ * 合并bio到请求后的处理, 向前合并后, 将请求从红黑树中删除, 重新插入
+ */
 static void dd_request_merged(struct request_queue *q, struct request *req,
 			      enum elv_merge type)
 {
@@ -131,11 +241,27 @@ static void dd_request_merged(struct request_queue *q, struct request *req,
 	 * if the merge was a front merge, we need to reposition request
 	 */
 	if (type == ELEVATOR_FRONT_MERGE) {
+		/*
+		 * deadline_rb_root(): 根据request的READ或WRITE返回对应的sort_list (rb_root)
+		 * 把request在rbtree删除
+		 */
 		elv_rb_del(deadline_rb_root(dd, req), req);
+		/*
+		 * 根据request的READ或WRITE返回对应的sort_list (rb_root)
+		 * 根据request的blk_rq_pos() (rq->__sector)添加到rbtree
+		 *
+		 * rbtree用的是rq->__sector
+		 * 上面如果是front merge的话rq->__sector会变的!
+		 */
 		deadline_add_rq_rb(dd, req);
 	}
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.requests_merged = dd_merged_requests()
+ *
+ * called only by elv_merge_requests()
+ */
 static void dd_merged_requests(struct request_queue *q, struct request *req,
 			       struct request *next)
 {
@@ -146,6 +272,7 @@ static void dd_merged_requests(struct request_queue *q, struct request *req,
 	if (!list_empty(&req->queuelist) && !list_empty(&next->queuelist)) {
 		if (time_before((unsigned long)next->fifo_time,
 				(unsigned long)req->fifo_time)) {
+			/* 把req->queuelist移到next->queuelist后面 */
 			list_move(&req->queuelist, &next->queuelist);
 			req->fifo_time = next->fifo_time;
 		}
@@ -154,12 +281,22 @@ static void dd_merged_requests(struct request_queue *q, struct request *req,
 	/*
 	 * kill knowledge of next, this one is a goner
 	 */
+	/*
+	 * remove rq from rbtree and fifo
+	 * 从hash上也要删除
+	 */
 	deadline_remove_request(q, next);
 }
 
 /*
  * move an entry to dispatch queue
  */
+/*
+ * called only by __dd_dispatch_request()
+ *
+ * 把dd->next_rq[data_dir]更新为rq的下一个request
+ * 移除rq
+ */
 static void
 deadline_move_request(struct deadline_data *dd, struct request *rq)
 {
@@ -167,11 +304,19 @@ deadline_move_request(struct deadline_data *dd, struct request *rq)
 
 	dd->next_rq[READ] = NULL;
 	dd->next_rq[WRITE] = NULL;
+	/*
+	 * 返回rq在rbtree的下一个request, 存入next_rq
+	 * rbtree是按照sector存的!
+	 */
 	dd->next_rq[data_dir] = deadline_latter_request(rq);
 
 	/*
 	 * take it off the sort and fifo list
 	 */
+	/*
+	 * remove rq from rbtree and fifo
+	 * 在hash上也删除
+	 */
 	deadline_remove_request(rq->q, rq);
 }
 
@@ -179,6 +324,12 @@ deadline_move_request(struct deadline_data *dd, struct request *rq)
  * deadline_check_fifo returns 0 if there are no expired requests on the fifo,
  * 1 otherwise. Requires !list_empty(&dd->fifo_list[data_dir])
  */
+/*
+ * called only by __dd_dispatch_request()
+ *
+ * 返回0: 没有过期的request
+ * 返回1: 有过期的request
+ */
 static inline int deadline_check_fifo(struct deadline_data *dd, int ddir)
 {
 	struct request *rq = rq_entry_fifo(dd->fifo_list[ddir].next);
@@ -196,6 +347,11 @@ static inline int deadline_check_fifo(struct deadline_data *dd, int ddir)
  * For the specified data direction, return the next request to
  * dispatch using arrival ordered lists.
  */
+/*
+ * called by __dd_dispatch_request() 两次
+ *
+ * 根据data direction, 在对应的fifo list中返回下一个request
+ */
 static struct request *
 deadline_fifo_request(struct deadline_data *dd, int data_dir)
 {
@@ -232,6 +388,11 @@ deadline_fifo_request(struct deadline_data *dd, int data_dir)
  * For the specified data direction, return the next request to
  * dispatch using sector position sorted lists.
  */
+/*
+ * called by __dd_dispatch_request() 三次
+ *
+ * 返回dd->next_rq[data_dir] 这是在rbtree基于sector排序的
+ */
 static struct request *
 deadline_next_request(struct deadline_data *dd, int data_dir)
 {
@@ -267,12 +428,21 @@ deadline_next_request(struct deadline_data *dd, int data_dir)
  * deadline_dispatch_requests selects the best request according to
  * read/write expire, fifo_batch, etc
  */
+/*
+ * 只被dd_dispatch_request()调用
+ */
 static struct request *__dd_dispatch_request(struct deadline_data *dd)
 {
 	struct request *rq, *next_rq;
 	bool reads, writes;
 	int data_dir;
 
+	/* 如果dispatch不为空就取出一个request */
+	/*
+	 * 在dd_insert_request()中, 只有at_head || blk_rq_is_passthrough(rq)的
+	 * request, 才会被放到这个list
+	 * at_head时放在head, 否则add tail
+	 */
 	if (!list_empty(&dd->dispatch)) {
 		rq = list_first_entry(&dd->dispatch, struct request, queuelist);
 		list_del_init(&rq->queuelist);
@@ -285,10 +455,15 @@ static struct request *__dd_dispatch_request(struct deadline_data *dd)
 	/*
 	 * batches are currently reads XOR writes
 	 */
+	/*
+	 * rq 一开始应该是空的
+	 * 返回dd->next_rq[data_dir] 这是在rbtree基于sector排序的
+	 */
 	rq = deadline_next_request(dd, WRITE);
 	if (!rq)
-		rq = deadline_next_request(dd, READ);
+		rq = deadline_next_request(dd, READ); /* rq一开始应该是空的 */
 
+	/* 如果rq不为NULL 是刚才batch的结果 */
 	if (rq && dd->batching < dd->fifo_batch)
 		/* we have a next request are still entitled to batch */
 		goto dispatch_request;
@@ -297,10 +472,15 @@ static struct request *__dd_dispatch_request(struct deadline_data *dd)
 	 * at this point we are not running a batch. select the appropriate
 	 * data direction (read / write)
 	 */
+	/*
+	 * 如果到了这里说明没有在用的batch
+	 */
 
+	/* 如果有read的话read优先 */
 	if (reads) {
 		BUG_ON(RB_EMPTY_ROOT(&dd->sort_list[READ]));
 
+		/* 不可以每次read优先让write等太久 */
 		if (deadline_fifo_request(dd, WRITE) &&
 		    (dd->starved++ >= dd->writes_starved))
 			goto dispatch_writes;
@@ -331,13 +511,21 @@ static struct request *__dd_dispatch_request(struct deadline_data *dd)
 	/*
 	 * we are not running a batch, find best request for selected data_dir
 	 */
+	/* 返回dd->next_rq[data_dir] 这是在rbtree基于sector排序的 */
 	next_rq = deadline_next_request(dd, data_dir);
-	if (deadline_check_fifo(dd, data_dir) || !next_rq) {
+	/*
+	 * 返回0: 没有过期的request
+	 * 返回1: 有过期的request
+	 */
+	if (deadline_check_fifo(dd, data_dir) || !next_rq) { // 有过期的request或者没有积攒的batch了 优先查看是否有过期的request
 		/*
 		 * A deadline has expired, the last request was in the other
 		 * direction, or we have run out of higher-sectored requests.
 		 * Start again from the request with the earliest expiry time.
 		 */
+		/*
+		 * 根据data direction, 在对应的fifo list()基于时间不是基于sector中返回下一个request
+		 */
 		rq = deadline_fifo_request(dd, data_dir);
 	} else {
 		/*
@@ -361,6 +549,10 @@ static struct request *__dd_dispatch_request(struct deadline_data *dd)
 	 * rq is the selected appropriate request.
 	 */
 	dd->batching++;
+	/*
+	 * 把dd->next_rq[data_dir]更新为rq的下一个request
+	 * 移除rq
+	 */
 	deadline_move_request(dd, rq);
 done:
 	/*
@@ -377,6 +569,11 @@ static struct request *__dd_dispatch_request(struct deadline_data *dd)
  * different hardware queue. This is because mq-deadline has shared
  * state for all hardware queues, in terms of sorting, FIFOs, etc.
  */
+/*
+ * struct elevator_type mq_deadline.ops.mq.dispatch_request = dd_dispatch_request()
+ *
+ * called only by blk_mq_do_dispatch_sched()
+ */
 static struct request *dd_dispatch_request(struct blk_mq_hw_ctx *hctx)
 {
 	struct deadline_data *dd = hctx->queue->elevator->elevator_data;
@@ -389,6 +586,11 @@ static struct request *dd_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	return rq;
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.exit_sched = dd_exit_queue()
+ *
+ * called only by blk_mq_exit_sched()
+ */
 static void dd_exit_queue(struct elevator_queue *e)
 {
 	struct deadline_data *dd = e->elevator_data;
@@ -402,11 +604,17 @@ static void dd_exit_queue(struct elevator_queue *e)
 /*
  * initialize elevator private data (deadline_data).
  */
+/*
+ * struct elevator_type mq_deadline.ops.mq.init_sched = dd_init_queue()
+ *
+ * called by only blk_mq_init_sched()
+ */
 static int dd_init_queue(struct request_queue *q, struct elevator_type *e)
 {
 	struct deadline_data *dd;
 	struct elevator_queue *eq;
 
+	/* 下面被设为q->elevator */
 	eq = elevator_alloc(q, e);
 	if (!eq)
 		return -ENOMEM;
@@ -422,19 +630,42 @@ static int dd_init_queue(struct request_queue *q, struct elevator_type *e)
 	INIT_LIST_HEAD(&dd->fifo_list[WRITE]);
 	dd->sort_list[READ] = RB_ROOT;
 	dd->sort_list[WRITE] = RB_ROOT;
-	dd->fifo_expire[READ] = read_expire;
-	dd->fifo_expire[WRITE] = write_expire;
-	dd->writes_starved = writes_starved;
-	dd->front_merges = 1;
-	dd->fifo_batch = fifo_batch;
+	dd->fifo_expire[READ] = read_expire;    // 除了sysfs就在这里被修改
+	dd->fifo_expire[WRITE] = write_expire;  // 除了sysfs就在这里被修改
+	dd->writes_starved = writes_starved;    // 不断被增加 不可以每次让READ优先让WRITE等太久
+	dd->front_merges = 1;                   // 在这里设置后只能sysfs修改
+	/*
+	 * 每当确定了一个传输方向(读或写), 那么将会从相应的sort_list中将一批连续请求dispatch
+	 * 到request_queue的请求队列里, 具体的数目由fifo_batch(default:16)来确定.
+	 */
+	dd->fifo_batch = fifo_batch;            // 除了sysfs就在这里修改
 	spin_lock_init(&dd->lock);
 	spin_lock_init(&dd->zone_lock);
+	/*
+	 * 在dd_insert_request()中, 只有at_head || blk_rq_is_passthrough(rq)的
+	 * request, 才会被放到这个list
+	 * at_head时放在head, 否则add tail
+	 */
 	INIT_LIST_HEAD(&dd->dispatch);
 
 	q->elevator = eq;
 	return 0;
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.request_merge = dd_request_merge()
+ *
+ * called by only elv_merge()
+ *
+ * 在这个函数中 找到的request的blk_rq_pos()也就是the current sector必须完全等于sector
+ * 这里sector是bio的末尾+1的sector
+ * 也就是找到的request的第一个sector
+ *
+ * 然后可以返回ELEVATOR_FRONT_MERGE
+ *
+ * 判断bio是否可以在request_queue中front merge, 如果可以找到对应
+ * 的request (存到参数rq), 并返回ELEVATOR_NO_MERGE
+ */
 static int dd_request_merge(struct request_queue *q, struct request **rq,
 			    struct bio *bio)
 {
@@ -445,6 +676,11 @@ static int dd_request_merge(struct request_queue *q, struct request **rq,
 	if (!dd->front_merges)
 		return ELEVATOR_NO_MERGE;
 
+	/*
+	 * 在这个函数中 找到的request的blk_rq_pos()也就是the current sector必须完全等于sector
+	 * 这里sector是bio的末尾+1的sector
+	 * 也就是找到的request的第一个sector
+	 */
 	__rq = elv_rb_find(&dd->sort_list[bio_data_dir(bio)], sector);
 	if (__rq) {
 		BUG_ON(sector != blk_rq_pos(__rq));
@@ -458,6 +694,15 @@ static int dd_request_merge(struct request_queue *q, struct request **rq,
 	return ELEVATOR_NO_MERGE;
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.bio_merge = dd_bio_merge()
+ *
+ * called only by __blk_mq_sched_bio_merge() <<--- blk_mq_sched_bio_merge()
+ *
+ * 把bio给merge到request_queue中
+ * 合并到某个request后, 如果该request可以和前面或后面继续合并就再合并一次
+ * 否则因为request的起始sector变了, 把request从红黑树中删除, 重新插入
+ */
 static bool dd_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio)
 {
 	struct request_queue *q = hctx->queue;
@@ -465,6 +710,12 @@ static bool dd_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio)
 	struct request *free = NULL;
 	bool ret;
 
+	/*
+	 * 把bio给merge到request_queue中
+	 * 合并到某个request后, 如果该request可以和前面或后面继续合并就再合并一次
+	 *     把再合并一次的request返回到参数merged_request
+	 * 否则因为request的起始sector变了, 把request从红黑树中删除, 重新插入
+	 */
 	spin_lock(&dd->lock);
 	ret = blk_mq_sched_try_merge(q, bio, &free);
 	spin_unlock(&dd->lock);
@@ -478,6 +729,12 @@ static bool dd_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio)
 /*
  * add rq to rbtree and fifo
  */
+/*
+ * called only by dd_insert_requests()
+ *
+ * 把request (rq)插入到rbtree和fifo
+ * at_head有时候true有时候false 不知道怎么设定的
+ */
 static void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			      bool at_head)
 {
@@ -491,19 +748,42 @@ static void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	 */
 	blk_req_zone_write_unlock(rq);
 
+	/*
+	 * 先判断这个request是否允许merge,
+	 * 比如scsi passthrough或者Driver private requests不能被merge
+	 *
+	 * 然后:
+	 * Attempt to do an insertion back merge. Only check for the case where
+	 * we can append 'rq' to an existing request, so we can throw 'rq' away
+	 * afterwards.
+	 *
+	 * 总结: 这里先判断能否back merge! 返回true说明back merge完了 直接退出吧
+	 */
 	if (blk_mq_sched_try_insert_merge(q, rq))
 		return;
 
+	/* 一个trace 忽略 */
 	blk_mq_sched_request_inserted(rq);
 
+	/* scsi passthrough或者Driver private requests */
 	if (at_head || blk_rq_is_passthrough(rq)) {
 		if (at_head)
 			list_add(&rq->queuelist, &dd->dispatch);
 		else
 			list_add_tail(&rq->queuelist, &dd->dispatch);
 	} else {
+		/*
+		 * 根据request的READ或WRITE返回对应的sort_list (rb_root)
+		 * 根据request的blk_rq_pos() (rq->__sector)添加到rbtree
+		 */
 		deadline_add_rq_rb(dd, rq);
 
+		/*
+		 * 判断这个request是否允许merge,
+		 * 比如scsi passthrough或者Driver private requests不能被merge
+		 *
+		 * 似乎能merge的要加入到hash, 然后可能更新last_merge
+		 */
 		if (rq_mergeable(rq)) {
 			elv_rqhash_add(q, rq);
 			if (!q->last_merge)
@@ -514,10 +794,21 @@ static void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 		 * set expire time and add to fifo list
 		 */
 		rq->fifo_time = jiffies + dd->fifo_expire[data_dir];
+		/* insert的时候是add tail, 因为是FIFO! */
 		list_add_tail(&rq->queuelist, &dd->fifo_list[data_dir]);
 	}
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.insert_requests = dd_insert_requests()
+ *
+ * called by:
+ *   - blk_mq_sched_insert_request()
+ *   - blk_mq_sched_insert_requests()
+ *
+ * 把链表list中的request一个一个插入到rbtree和fifo
+ * at_head有时候true有时候false 不知道怎么设定的
+ */
 static void dd_insert_requests(struct blk_mq_hw_ctx *hctx,
 			       struct list_head *list, bool at_head)
 {
@@ -539,6 +830,9 @@ static void dd_insert_requests(struct blk_mq_hw_ctx *hctx,
  * Nothing to do here. This is defined only to ensure that .finish_request
  * method is called upon request completion.
  */
+/*
+ * struct elevator_type mq_deadline.ops.mq.prepare_request = dd_prepare_request()
+ */
 static void dd_prepare_request(struct request *rq, struct bio *bio)
 {
 }
@@ -550,6 +844,11 @@ static void dd_prepare_request(struct request *rq, struct bio *bio)
  * or deadline_next_request() are executing. This function is called for
  * all requests, whether or not these requests complete successfully.
  */
+/*
+ * struct elevator_type mq_deadline.ops.mq.finish_request = dd_finish_request()
+ *
+ * called only by blk_mq_free_request()
+ */
 static void dd_finish_request(struct request *rq)
 {
 	struct request_queue *q = rq->q;
@@ -564,10 +863,18 @@ static void dd_finish_request(struct request *rq)
 	}
 }
 
+/*
+ * struct elevator_type mq_deadline.ops.mq.has_work = dd_has_work()
+ *
+ * 通过dispatch和fifo_list判断是否还有work
+ */
 static bool dd_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct deadline_data *dd = hctx->queue->elevator->elevator_data;
 
+	/*
+	 * tests whether a list is empty and not being modified
+	 */
 	return !list_empty_careful(&dd->dispatch) ||
 		!list_empty_careful(&dd->fifo_list[0]) ||
 		!list_empty_careful(&dd->fifo_list[1]);
diff --git a/block/noop-iosched.c b/block/noop-iosched.c
index 2d1b15d..c2f17d8 100644
--- a/block/noop-iosched.c
+++ b/block/noop-iosched.c
@@ -12,12 +12,18 @@ struct noop_data {
 	struct list_head queue;
 };
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_merge_req_fn = noop_merged_requests()
+ */
 static void noop_merged_requests(struct request_queue *q, struct request *rq,
 				 struct request *next)
 {
 	list_del_init(&next->queuelist);
 }
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_dispatch_fn = noop_dispatch()
+ */
 static int noop_dispatch(struct request_queue *q, int force)
 {
 	struct noop_data *nd = q->elevator->elevator_data;
@@ -26,12 +32,18 @@ static int noop_dispatch(struct request_queue *q, int force)
 	rq = list_first_entry_or_null(&nd->queue, struct request, queuelist);
 	if (rq) {
 		list_del_init(&rq->queuelist);
+		/*
+		 * 刚取出的rq放入到系统的请求队列
+		 */
 		elv_dispatch_sort(q, rq);
 		return 1;
 	}
 	return 0;
 }
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_add_req_fn = noop_add_request()
+ */
 static void noop_add_request(struct request_queue *q, struct request *rq)
 {
 	struct noop_data *nd = q->elevator->elevator_data;
@@ -39,6 +51,9 @@ static void noop_add_request(struct request_queue *q, struct request *rq)
 	list_add_tail(&rq->queuelist, &nd->queue);
 }
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_former_req_fn = noop_former_request()
+ */
 static struct request *
 noop_former_request(struct request_queue *q, struct request *rq)
 {
@@ -49,6 +64,9 @@ noop_former_request(struct request_queue *q, struct request *rq)
 	return list_prev_entry(rq, queuelist);
 }
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_latter_req_fn = noop_latter_request()
+ */
 static struct request *
 noop_latter_request(struct request_queue *q, struct request *rq)
 {
@@ -59,6 +77,9 @@ noop_latter_request(struct request_queue *q, struct request *rq)
 	return list_next_entry(rq, queuelist);
 }
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_init_fn = noop_init_queue()
+ */
 static int noop_init_queue(struct request_queue *q, struct elevator_type *e)
 {
 	struct noop_data *nd;
@@ -83,6 +104,9 @@ static int noop_init_queue(struct request_queue *q, struct elevator_type *e)
 	return 0;
 }
 
+/*
+ * struct elevator_type elevator_noop.ops.sq.elevator_exit_fn = noop_exit_queue()
+ */
 static void noop_exit_queue(struct elevator_queue *e)
 {
 	struct noop_data *nd = e->elevator_data;
@@ -91,6 +115,9 @@ static void noop_exit_queue(struct elevator_queue *e)
 	kfree(nd);
 }
 
+/*
+ * 不存在bool uses_mq
+ */
 static struct elevator_type elevator_noop = {
 	.ops.sq = {
 		.elevator_merge_req_fn		= noop_merged_requests,
diff --git a/block/partition-generic.c b/block/partition-generic.c
index db57cce..c429443 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -298,6 +298,11 @@ static DEVICE_ATTR(whole_disk, S_IRUSR | S_IRGRP | S_IROTH,
  * Must be called either with bd_mutex held, before a disk can be opened or
  * after all disk users are gone.
  */
+/*
+ * called by:
+ *   - rescan_partitions()
+ *   - blkpg_ioctl()
+ */
 struct hd_struct *add_partition(struct gendisk *disk, int partno,
 				sector_t start, sector_t len, int flags,
 				struct partition_meta_info *info)
@@ -501,6 +506,11 @@ static bool part_zone_aligned(struct gendisk *disk,
 	return true;
 }
 
+/*
+ * called by:
+ *   - __blkdev_reread_part()
+ *   - __blkdev_get() 两次
+ */
 int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 {
 	struct parsed_partitions *state = NULL;
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index ebc34a5..1ff81cb 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -12,47 +12,75 @@ struct blk_flush_queue;
 /**
  * struct blk_mq_hw_ctx - State for a hardware queue facing the hardware block device
  */
+/*
+ * 一个hw queue可能map多个sw queue
+ */
 struct blk_mq_hw_ctx {
 	struct {
 		spinlock_t		lock;
+		/*
+		 * 以下是为dispatch添加request的地方:
+		 *   - block/blk-mq-sched.c|523| <<blk_mq_sched_bypass_insert>> list_add(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|1396| <<blk_mq_dispatch_rq_list>> list_splice_init(list, &hctx->dispatch);
+		 *   - block/blk-mq.c|1803| <<blk_mq_request_bypass_insert>> list_add_tail(&rq->queuelist, &hctx->dispatch);
+		 *   - block/blk-mq.c|2466| <<blk_mq_hctx_notify_dead>> list_splice_tail_init(&tmp, &hctx->dispatch);
+		 */
 		struct list_head	dispatch;
 		unsigned long		state;		/* BLK_MQ_S_* flags */
 	} ____cacheline_aligned_in_smp;
 
 	struct delayed_work	run_work;
-	cpumask_var_t		cpumask;
+	cpumask_var_t		cpumask;  // 当前hw queue都map到了哪些cpu
 	int			next_cpu;
 	int			next_cpu_batch;
 
 	unsigned long		flags;		/* BLK_MQ_F_* flags */
 
 	void			*sched_data;
-	struct request_queue	*queue;
+	struct request_queue	*queue;  // 所属的request_queue
 	struct blk_flush_queue	*fq;
 
 	void			*driver_data;
 
-	struct sbitmap		ctx_map;
+	/*
+	 * 在blk_mq_hctx_mark_pending()被置位
+	 *
+	 * bit被set说明对应的sw queue有等待的request (在struct blk_mq_ctx的rq_list)
+	 */
+	struct sbitmap		ctx_map;  // 在blk_mq_init_hctx()初始化, depth是nr_ctx (记录着这个hw queue当前map着几个sw queue)
 
+	/*
+	 * 只在blk_mq_do_dispatch_ctx()这一个函数用到
+	 */
 	struct blk_mq_ctx	*dispatch_from;
 
-	struct blk_mq_ctx	**ctxs;
-	unsigned int		nr_ctx;
+	struct blk_mq_ctx	**ctxs;  // 在blk_mq_init_hctx()分配为指针列表
+	unsigned int		nr_ctx;  // 记录着这个hw queue当前map着几个sw queue
 
 	wait_queue_entry_t	dispatch_wait;
 	atomic_t		wait_index;
 
-	struct blk_mq_tags	*tags;
-	struct blk_mq_tags	*sched_tags;
+	struct blk_mq_tags	*tags;  // 在blk_mq_init_hctx()中指向set->tags[hctx_idx]
+	/* 似乎和INTERNAL和driver tag有某种关系 */
+	struct blk_mq_tags	*sched_tags; // 在blk_mq_sched_alloc_tags()分配
 
 	unsigned long		queued;
+	/*
+	 * 被如下的引用:
+	 *   - block/blk-mq-debugfs.c|620| <<hctx_run_show>> seq_printf(m, "%lu\n", hctx->run);
+	 *   - block/blk-mq-debugfs.c|629| <<hctx_run_write>> hctx->run = 0;
+	 *   - block/blk-mq-sched.c|238| <<blk_mq_sched_dispatch_requests>> hctx->run++;
+	 */
 	unsigned long		run;
 #define BLK_MQ_MAX_DISPATCH_ORDER	7
 	unsigned long		dispatched[BLK_MQ_MAX_DISPATCH_ORDER];
 
 	unsigned int		numa_node;
-	unsigned int		queue_num;
+	unsigned int		queue_num;  // hw queue的index
 
+	/*
+	 * 在blk_mq_realloc_hw_ctxs()初始化为0
+	 */
 	atomic_t		nr_active;
 	unsigned int		nr_expired;
 
@@ -72,22 +100,74 @@ struct blk_mq_hw_ctx {
 	struct srcu_struct	srcu[0];
 };
 
+/*
+ * xen-blkfront tag_set的例子:
+ *
+ * info->tag_set.ops = &blkfront_mq_ops;
+ * info->tag_set.nr_hw_queues = info->nr_rings;
+ * if (HAS_EXTRA_REQ && info->max_indirect_segments == 0) {
+ *         info->tag_set.queue_depth =  BLK_RING_SIZE(info) / 2;
+ * } else
+ *         info->tag_set.queue_depth = BLK_RING_SIZE(info);
+ * info->tag_set.numa_node = NUMA_NO_NODE;
+ * info->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;
+ * info->tag_set.cmd_size = sizeof(struct blkif_req);
+ * info->tag_set.driver_data = info
+ *
+ *
+ * virblk tag_set的例子:
+ * vblk->tag_set.ops = &virtio_mq_ops;
+ * vblk->tag_set.queue_depth = virtblk_queue_depth;
+ * vblk->tag_set.numa_node = NUMA_NO_NODE;
+ * vblk->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+ * vblk->tag_set.cmd_size =
+ *         sizeof(struct virtblk_req) +
+ *         sizeof(struct scatterlist) * sg_elems;
+ * vblk->tag_set.driver_data = vblk;
+ * vblk->tag_set.nr_hw_queues = vblk->num_vqs;
+ */
+/*
+ * 多个request_queue可能共用一个struct blk_mq_tag_set,
+ * 比如一个nvme的controller (struct nvme_ctrl) 可以有多个nvme的namespace (struct nvme_ns)
+ *     -----> list_add_tail(&ns->list, &ctrl->namespaces);
+ */
 struct blk_mq_tag_set {
-	unsigned int		*mq_map;
+	/*
+	 * 在blk_mq_update_queue_map()分配元素
+	 * map在blk_mq_update_queue_map()初始化的
+	 *
+	 * 在blk_mq_init_allocated_queue()把request_queue的mq_map指向set的mq_map
+	 */
+	unsigned int		*mq_map; /* 分配cpu个元素 */
 	const struct blk_mq_ops	*ops;
-	unsigned int		nr_hw_queues;
+	/*
+	 * 在blk_mq_alloc_tag_set()会检查nr_hw_queues不可以大于cpu
+	 */
+	unsigned int		nr_hw_queues;   /* 应该都是由block driver设置的 */
 	unsigned int		queue_depth;	/* max hw supported */
-	unsigned int		reserved_tags;
-	unsigned int		cmd_size;	/* per-request extra data */
+	unsigned int		reserved_tags;  /* 除了nvme的某些(非pci)的情况 没见过用的 */
+	unsigned int		cmd_size;	/* per-request extra data */ /* 附属在request后面的部分 */
 	int			numa_node;
 	unsigned int		timeout;
 	unsigned int		flags;		/* BLK_MQ_F_* */
 	void			*driver_data;
 
-	struct blk_mq_tags	**tags;
+	/*
+	 * 在blk_mq_alloc_tag_set()分配第一维数组指针
+	 *
+	 * 分配实例在: blk_mq_alloc_tag_set()-->blk_mq_alloc_rq_maps()-->__blk_mq_alloc_rq_maps()-->__blk_mq_alloc_rq_map()
+	 */
+	struct blk_mq_tags	**tags; /* 数组有cpu个(struct blk_mq_tags *) 用的时候是nr_hw_queues个*/
 
 	struct mutex		tag_list_lock;
-	struct list_head	tag_list;
+	/*
+	 * 多个request_queue可能共用一个struct blk_mq_tag_set,
+	 * 比如一个nvme的controller (struct nvme_ctrl) 可以有多个nvme的namespace (struct nvme_ns)
+	 *       -----> list_add_tail(&ns->list, &ctrl->namespaces);
+	 *
+	 * BLK_MQ_F_TAG_SHARED在include/linux/blk-mq.h
+	 */
+	struct list_head	tag_list; /* 在blk_mq_add_queue_tag_set() 链接着所有的request_queue->tag_set_list */
 };
 
 struct blk_mq_queue_data {
@@ -176,15 +256,39 @@ struct blk_mq_ops {
 
 enum {
 	BLK_MQ_F_SHOULD_MERGE	= 1 << 0,
+	/*
+	 * 设置BLK_MQ_F_TAG_SHARED的地方:
+	 *   - block/blk-mq.c|3048| <<queue_set_hctx_shared>> hctx->flags |= BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|3112| <<blk_mq_add_queue_tag_set>> set->flags |= BLK_MQ_F_TAG_SHARED;
+	 *
+	 * 清除BLK_MQ_F_TAG_SHARED的地方:
+	 *   - block/blk-mq.c|2766| <<blk_mq_init_hctx>> hctx->flags = set->flags & ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|3052| <<queue_set_hctx_shared>> hctx->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 *   - block/blk-mq.c|3087| <<blk_mq_del_queue_tag_set>> set->flags &= ~BLK_MQ_F_TAG_SHARED;
+	 */
 	BLK_MQ_F_TAG_SHARED	= 1 << 1,
 	BLK_MQ_F_SG_MERGE	= 1 << 2,
 	BLK_MQ_F_BLOCKING	= 1 << 5,
+	/*
+	 * 以下几个明确设置了不用SCHED:
+	 *   - nvme的fc的admin: <<nvme_fc_init_ctrl>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 *   - nvme的pci的admin: <<nvme_alloc_admin_tags>> dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+	 *   - nvme的rdma: <<nvme_rdma_alloc_tagset>> set->flags = BLK_MQ_F_NO_SCHED;
+	 *   - nvme的target的loop: <<nvme_loop_configure_admin_queue>> ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+	 */
 	BLK_MQ_F_NO_SCHED	= 1 << 6,
 	BLK_MQ_F_ALLOC_POLICY_START_BIT = 8,
 	BLK_MQ_F_ALLOC_POLICY_BITS = 1,
 
 	BLK_MQ_S_STOPPED	= 0,
 	BLK_MQ_S_TAG_ACTIVE	= 1,
+	/*
+	 * 在以下地方set或者clear:
+	 *   - block/blk-mq-sched.c|78| <<blk_mq_sched_mark_restart_hctx>> set_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 *   - block/blk-mq.c|1216| <<blk_mq_mark_tag_wait>> set_bit(BLK_MQ_S_SCHED_RESTART, &this_hctx->state);
+	 *   
+	 *   - block/blk-mq-sched.c|95| <<blk_mq_sched_restart_hctx>> clear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);
+	 */
 	BLK_MQ_S_SCHED_RESTART	= 2,
 
 	BLK_MQ_MAX_DEPTH	= 10240,
@@ -218,7 +322,7 @@ enum {
 	/* allocate from reserved pool */
 	BLK_MQ_REQ_RESERVED	= (__force blk_mq_req_flags_t)(1 << 1),
 	/* allocate internal/sched tag */
-	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),
+	BLK_MQ_REQ_INTERNAL	= (__force blk_mq_req_flags_t)(1 << 2),  // 在blk_mq_get_request()当request_queue有elevator的时候会在data->flags上mark
 	/* set RQF_PREEMPT */
 	BLK_MQ_REQ_PREEMPT	= (__force blk_mq_req_flags_t)(1 << 3),
 };
@@ -301,10 +405,12 @@ static inline void *blk_mq_rq_to_pdu(struct request *rq)
 	return rq + 1;
 }
 
+/* 对于每一个hw dispatch queue */
 #define queue_for_each_hw_ctx(q, hctx, i)				\
 	for ((i) = 0; (i) < (q)->nr_hw_queues &&			\
 	     ({ hctx = (q)->queue_hw_ctx[i]; 1; }); (i)++)
 
+/* 对于每一个sw dispatch queue */
 #define hctx_for_each_ctx(hctx, ctx, i)					\
 	for ((i) = 0; (i) < (hctx)->nr_ctx &&				\
 	     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index 1602bf4..816ab57 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -388,6 +388,7 @@ static inline bool blk_qc_t_is_internal(blk_qc_t cookie)
 	return (cookie & BLK_QC_T_INTERNAL) != 0;
 }
 
+/* 每个cpu一个 在blk_stat_callback */
 struct blk_rq_stat {
 	u64 mean;
 	u64 min;
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 7d04746..5b3b0f5 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -117,7 +117,7 @@ typedef __u32 __bitwise req_flags_t;
 /* runtime pm request */
 #define RQF_PM			((__force req_flags_t)(1 << 15))
 /* on IO scheduler merge hash */
-#define RQF_HASHED		((__force req_flags_t)(1 << 16))
+#define RQF_HASHED		((__force req_flags_t)(1 << 16)) // 只在elv_rqhash_add()和__elv_rqhash_del()修改
 /* IO stats tracking on */
 #define RQF_STATS		((__force req_flags_t)(1 << 17))
 /* Look at ->special_vec for the actual data payload instead of the
@@ -126,6 +126,13 @@ typedef __u32 __bitwise req_flags_t;
 /* The per-zone write lock is held for this request */
 #define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
 /* timeout is expired */
+/*
+ * 设置已经过期的标志:
+ *   - block/blk-mq.c|1075| <<blk_mq_rq_timed_out>> req->rq_flags |= RQF_MQ_TIMEOUT_EXPIRED;
+ *
+ * 清除已经过期的标志:
+ *   - block/blk-timeout.c|244| <<blk_add_timer>> req->rq_flags &= ~RQF_MQ_TIMEOUT_EXPIRED;
+ */
 #define RQF_MQ_TIMEOUT_EXPIRED	((__force req_flags_t)(1 << 20))
 /* already slept for hybrid poll */
 #define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 21))
@@ -151,9 +158,9 @@ struct request {
 	int internal_tag;
 
 	/* the following two fields are internal, NEVER access directly */
-	unsigned int __data_len;	/* total data len */
+	unsigned int __data_len;	/* total data len */ // bytes left in the entire request
 	int tag;
-	sector_t __sector;		/* sector cursor */
+	sector_t __sector;		/* sector cursor */ // the current sector
 
 	struct bio *bio;
 	struct bio *biotail;
@@ -178,6 +185,9 @@ struct request {
 	 * completion_data share space with the rb_node.
 	 */
 	union {
+		/*
+		 * 在elv_rb_add()中添加元素到这里
+		 */
 		struct rb_node rb_node;	/* sort/lookup */
 		struct bio_vec special_vec;
 		void *completion_data;
@@ -244,6 +254,9 @@ struct request {
 	 * recycle instance of this request.  See blk_mq_timeout_work().
 	 */
 	struct u64_stats_sync aborted_gstate_sync;
+	/*
+	 * 只在blk_mq_rq_update_aborted_gstate()被修改
+	 */
 	u64 aborted_gstate;
 
 	/* access through blk_rq_set_deadline, blk_rq_deadline */
@@ -277,6 +290,7 @@ static inline bool blk_op_is_scsi(unsigned int op)
 	return op == REQ_OP_SCSI_IN || op == REQ_OP_SCSI_OUT;
 }
 
+/* Driver private requests */
 static inline bool blk_op_is_private(unsigned int op)
 {
 	return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;
@@ -287,13 +301,16 @@ static inline bool blk_rq_is_scsi(struct request *rq)
 	return blk_op_is_scsi(req_op(rq));
 }
 
+/* Driver private requests */
 static inline bool blk_rq_is_private(struct request *rq)
 {
 	return blk_op_is_private(req_op(rq));
 }
 
+/* scsi passthrough或者Driver private requests */
 static inline bool blk_rq_is_passthrough(struct request *rq)
 {
+	/* scsi passthrough或者Driver private requests */
 	return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
 }
 
@@ -437,8 +454,19 @@ struct request_queue {
 	/*
 	 * Together with queue_head for cacheline sharing
 	 */
+	/*
+	 * 猜测queue_head只用在single queue
+	 *
+	 * 在如下添加新元素:
+	 *   block/blk-flush.c|142| <<blk_flush_queue_rq>> list_add(&rq->queuelist, &rq->q->queue_head);
+	 *   block/blk-flush.c|144| <<blk_flush_queue_rq>> list_add_tail(&rq->queuelist, &rq->q->queue_head);
+	 *   block/blk-flush.c|485| <<blk_insert_flush>> list_add_tail(&rq->queuelist, &q->queue_head);
+	 *   block/elevator.c|638| <<elv_dispatch_add_tail>> list_add_tail(&rq->queuelist, &q->queue_head);
+	 *   block/elevator.c|930| <<__elv_add_request>> list_add(&rq->queuelist, &q->queue_head);
+	 *   block/elevator.c|936| <<__elv_add_request>> list_add_tail(&rq->queuelist, &q->queue_head);
+	 */
 	struct list_head	queue_head;
-	struct request		*last_merge;
+	struct request		*last_merge;    /* 保存上一次合并的请求指针 */
 	struct elevator_queue	*elevator;
 	int			nr_rqs[2];	/* # allocated [a]sync rqs */
 	int			nr_rqs_elvpriv;	/* # allocated rqs w/ elvpriv */
@@ -458,10 +486,37 @@ struct request_queue {
 
 	request_fn_proc		*request_fn;
 	make_request_fn		*make_request_fn;
+	/*
+	 * 设置的地方:
+	 *   - block/blk-mq.c|3178| <<blk_mq_init_allocated_queue>> q->poll_fn = blk_mq_poll;
+	 *   - drivers/nvme/host/multipath.c|192| <<nvme_mpath_alloc_disk>> q->poll_fn = nvme_ns_head_poll;
+	 *
+	 * 调用的地方:
+	 *   - block/blk-core.c|2644| <<blk_poll>> return q->poll_fn(q, cookie);
+	 *   - drivers/nvme/host/multipath.c|142| <<nvme_ns_head_poll>> found = ns->queue->poll_fn(q, qc);
+	 */
 	poll_q_fn		*poll_fn;
 	prep_rq_fn		*prep_rq_fn;
 	unprep_rq_fn		*unprep_rq_fn;
-	softirq_done_fn		*softirq_done_fn;
+	/*
+	 * 在blk_queue_softirq_done()配置
+	 *   - block/blk-mq.c|3119| <<blk_mq_init_allocated_queue>> blk_queue_softirq_done(q, set->ops->complete);
+	 *   - block/bsg-lib.c|331| <<bsg_setup_queue>> blk_queue_softirq_done(q, bsg_softirq_done);
+	 *   - drivers/block/null_blk.c|1791| <<null_add_dev>> blk_queue_softirq_done(nullb->q, null_softirq_done_fn);
+	 *   - drivers/md/dm-rq.c|720| <<dm_old_init_request_queue>> blk_queue_softirq_done(md->queue, dm_softirq_done);
+	 *   - drivers/scsi/scsi_lib.c|2277| <<scsi_old_alloc_queue>> blk_queue_softirq_done(q, scsi_softirq_done);
+	 *
+	 * 关于xen和kvm, set.ops.complete被设置为:
+	 *   - xen-blkfront : blkif_complete_rq()     ---> blk_mq_end_request()
+	 *   - virtio-blk   : virtblk_request_done()  ---> blk_mq_end_request()
+	 *
+	 * 被以下调用:
+	 *   - block/blk-mq.c|612| <<__blk_mq_complete_request_remote>> rq->q->softirq_done_fn(rq);                                                                                                     
+	 *   - block/blk-mq.c|654| <<__blk_mq_complete_request>> rq->q->softirq_done_fn(rq);
+	 *   - block/blk-mq.c|685| <<__blk_mq_complete_request>> rq->q->softirq_done_fn(rq);
+	 *   - block/blk-softirq.c|37| <<blk_done_softirq>> rq->q->softirq_done_fn(rq);
+	 */
+	softirq_done_fn		*softirq_done_fn; // 在blk_queue_softirq_done()配置
 	rq_timed_out_fn		*rq_timed_out_fn;
 	dma_drain_needed_fn	*dma_drain_needed;
 	lld_busy_fn		*lld_busy_fn;
@@ -472,19 +527,20 @@ struct request_queue {
 	/* Called from inside blk_get_request() */
 	void (*initialize_rq_fn)(struct request *rq);
 
-	const struct blk_mq_ops	*mq_ops;
+	const struct blk_mq_ops	*mq_ops; /* 来自blk_mq_tag_set的ops 由driver配置 */
 
-	unsigned int		*mq_map;
+	/* 在blk_mq_init_allocated_queue()把request_queue的mq_map指向set的mq_map */
+	unsigned int		*mq_map; /* 指向struct blk_mq_tag_set 的mq_map */
 
 	/* sw queues */
-	struct blk_mq_ctx __percpu	*queue_ctx;
-	unsigned int		nr_queues;
+	struct blk_mq_ctx __percpu	*queue_ctx;  // 在blk_mq_init_allocated_queue()分配
+	unsigned int		nr_queues;  // 在blk_mq_init_allocated_queue()设置的是cpu的数量
 
 	unsigned int		queue_depth;
 
 	/* hw dispatch queues */
-	struct blk_mq_hw_ctx	**queue_hw_ctx;
-	unsigned int		nr_hw_queues;
+	struct blk_mq_hw_ctx	**queue_hw_ctx;  // 实例是在blk_mq_realloc_hw_ctxs()分配的
+	unsigned int		nr_hw_queues;  // 在blk_mq_realloc_hw_ctxs()初始化hctx的时候获得
 
 	/*
 	 * Dispatch queue sorting
@@ -552,11 +608,17 @@ struct request_queue {
 	/*
 	 * queue settings
 	 */
-	unsigned long		nr_requests;	/* Max # of requests */
+	unsigned long		nr_requests;	/* Max # of requests */ /* set->queue_depth */
 	unsigned int		nr_congestion_on;
 	unsigned int		nr_congestion_off;
 	unsigned int		nr_batching;
 
+	/*
+	 * 在blk_queue_dma_drain()设置
+	 *
+	 * 只有一处设置了dma_drain_size:
+	 *   - drivers/ata/libata-scsi.c|1292| <<ata_scsi_dev_config>> blk_queue_dma_drain(q, atapi_drain_needed, buf, ATAPI_MAX_DRAIN);
+	 */
 	unsigned int		dma_drain_size;
 	void			*dma_drain_buffer;
 	unsigned int		dma_pad_mask;
@@ -565,6 +627,15 @@ struct request_queue {
 	struct blk_queue_tag	*queue_tags;
 	struct list_head	tag_busy_list;
 
+	/*
+	 * 增加的地方:
+	 *   - __elv_add_request()
+	 *
+	 * 减少的地方:
+	 *   - elv_dispatch_sort()
+	 *   - elv_dispatch_add_tail()
+	 *   - elv_merge_requests()
+	 */
 	unsigned int		nr_sorted;
 	unsigned int		in_flight[2];
 
@@ -581,8 +652,17 @@ struct request_queue {
 	struct blk_stat_callback	*poll_cb;
 	struct blk_rq_stat	poll_stat[BLK_MQ_POLL_STATS_BKTS];
 
-	struct timer_list	timeout;
-	struct work_struct	timeout_work;
+	struct timer_list	timeout;  // 注册的是blk_rq_timed_out_timer()
+	/*
+	 * - block/blk-core.c|1184| <<blk_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_timeout_work);
+	 * - block/blk-mq.c|3322| <<blk_mq_init_allocated_queue>> INIT_WORK(&q->timeout_work, blk_mq_timeout_work);
+	 *
+	 * 在以下情况使用:
+	 *   - block/blk-core.c|416| <<blk_sync_queue>> cancel_work_sync(&q->timeout_work);
+	 *   - block/blk-core.c|980| <<blk_rq_timed_out_timer>> kblockd_schedule_work(&q->timeout_work);
+	 *   - block/blk-timeout.c|180| <<blk_abort_request>> kblockd_schedule_work(&req->q->timeout_work);
+	 */
+	struct work_struct	timeout_work; // 根据种类, blk_timeout_work()或者blk_mq_timeout_work()
 	struct list_head	timeout_list;
 
 	struct list_head	icq_list;
@@ -630,13 +710,28 @@ struct request_queue {
 	 */
 	struct blk_flush_queue	*fq;
 
+	/*
+	 * 以下会添加元素:
+	 *   - block/blk-mq.c|1073| <<blk_mq_add_to_requeue_list>> list_add(&rq->queuelist, &q->requeue_list);
+	 *   - block/blk-mq.c|1075| <<blk_mq_add_to_requeue_list>> list_add_tail(&rq->queuelist, &q->requeue_list);
+	 */
 	struct list_head	requeue_list;
 	spinlock_t		requeue_lock;
+	/*
+	 * 被设置为blk_mq_requeue_work()
+	 *     block/blk-mq.c|3352| <<blk_mq_init_allocated_queue>> INIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);
+	 */
 	struct delayed_work	requeue_work;
 
 	struct mutex		sysfs_lock;
 
 	int			bypass_depth;
+	/*
+	 * 只在blk_freeze_queue_start() 增加
+	 * 只在blk_mq_unfreeze_queue()  减少
+	 *
+	 * 默认开始应该是0
+	 */
 	atomic_t		mq_freeze_depth;
 
 #if defined(CONFIG_BLK_DEV_BSG)
@@ -649,12 +744,24 @@ struct request_queue {
 	struct throtl_data *td;
 #endif
 	struct rcu_head		rcu_head;
+	/*
+	 * 在以下等待:
+	 *   - block/blk-core.c|953| <<blk_queue_enter>> wait_event(q->mq_freeze_wq,
+	 *   - block/blk-mq.c|230| <<blk_mq_freeze_queue_wait>> wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
+	 *   - block/blk-mq.c|241| <<blk_mq_freeze_queue_wait_timeout>> return wait_event_timeout(q->mq_freeze_wq,
+	 *
+	 * 在以下唤醒:
+	 *   - block/blk-core.c|447| <<blk_clear_preempt_only>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|722| <<blk_set_queue_dying>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-core.c|972| <<blk_queue_usage_counter_release>> wake_up_all(&q->mq_freeze_wq);
+	 *   - block/blk-mq.c|293| <<blk_mq_unfreeze_queue>> wake_up_all(&q->mq_freeze_wq);
+	 */
 	wait_queue_head_t	mq_freeze_wq;
 	struct percpu_ref	q_usage_counter;
 	struct list_head	all_q_node;
 
 	struct blk_mq_tag_set	*tag_set;
-	struct list_head	tag_set_list;
+	struct list_head	tag_set_list; /* 在blk_mq_add_queue_tag_set() 作为元素链接在blk_mq_tag_set->tag_list */
 	struct bio_set		*bio_split;
 
 #ifdef CONFIG_BLK_DEBUG_FS
@@ -678,7 +785,32 @@ struct request_queue {
 #define QUEUE_FLAG_DYING	2	/* queue being torn down */
 #define QUEUE_FLAG_BYPASS	3	/* act as dumb FIFO queue */
 #define QUEUE_FLAG_BIDI		4	/* queue supports bidi requests */
+/*
+ * 在如下地方设置:
+ *   - blk_cleanup_queue()
+ *   - queue_nomerges_store()
+ *   - __loop_update_dio()
+ *   - loop_add()
+ *   - megasas_set_nvme_device_properties()
+ *   - scsih_slave_configure()
+ *
+ * 在如下地方清除:
+ *   - queue_nomerges_store()
+ *   - __loop_update_dio()
+ */
 #define QUEUE_FLAG_NOMERGES     5	/* disable merge attempts */
+/*
+ * sq和mq的default flag都包含了:
+ *   - include/linux/blkdev.h|775| <<QUEUE_FLAG_DEFAULT>> (1 << QUEUE_FLAG_SAME_COMP) | \
+ *   - include/linux/blkdev.h|779| <<QUEUE_FLAG_MQ_DEFAULT>> (1 << QUEUE_FLAG_SAME_COMP) | \
+ *
+ * 设置QUEUE_FLAG_SAME_COMP的地方:
+ *   - block/blk-sysfs.c|351| <<queue_rq_affinity_store>> queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+ *   - block/blk-sysfs.c|354| <<queue_rq_affinity_store>> queue_flag_set(QUEUE_FLAG_SAME_COMP, q);
+ *
+ * 清空QUEUE_FLAG_SAME_COMP的地方:
+ *   - block/blk-sysfs.c|357| <<queue_rq_affinity_store>> queue_flag_clear(QUEUE_FLAG_SAME_COMP, q);
+ */
 #define QUEUE_FLAG_SAME_COMP	6	/* complete on same CPU-group */
 #define QUEUE_FLAG_FAIL_IO	7	/* fake timeout */
 #define QUEUE_FLAG_NONROT	9	/* non-rotational device (SSD) */
@@ -688,6 +820,18 @@ struct request_queue {
 #define QUEUE_FLAG_NOXMERGES   12	/* No extended merges */
 #define QUEUE_FLAG_ADD_RANDOM  13	/* Contributes to random pool */
 #define QUEUE_FLAG_SECERASE    14	/* supports secure erase */
+/*
+ * sq和mq的default flag并不包含QUEUE_FLAG_SAME_FORCE:
+ *   QUEUE_FLAG_DEFAULT
+ *   QUEUE_FLAG_MQ_DEFAULT
+ *
+ * 设置QUEUE_FLAG_SAME_FORCE的地方:
+ *   - block/blk-sysfs.c|352| <<queue_rq_affinity_store>> queue_flag_set(QUEUE_FLAG_SAME_FORCE, q);
+ *
+ * 清空QUEUE_FLAG_SAME_FORCE的地方:
+ *   - block/blk-sysfs.c|355| <<queue_rq_affinity_store>> queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+ *   - block/blk-sysfs.c|358| <<queue_rq_affinity_store>> queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);
+ */
 #define QUEUE_FLAG_SAME_FORCE  15	/* force complete on same CPU */
 #define QUEUE_FLAG_DEAD        16	/* queue tear-down finished */
 #define QUEUE_FLAG_INIT_DONE   17	/* queue is initialized */
@@ -697,10 +841,25 @@ struct request_queue {
 #define QUEUE_FLAG_FUA	       21	/* device supports FUA writes */
 #define QUEUE_FLAG_FLUSH_NQ    22	/* flush not queueuable */
 #define QUEUE_FLAG_DAX         23	/* device supports DAX */
+/*
+ * 设置QUEUE_FLAG_STATS:
+ *   - block/blk-stat.c|197| <<blk_stat_add_callback>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *   - block/blk-stat.c|259| <<blk_stat_enable_accounting>> blk_queue_flag_set(QUEUE_FLAG_STATS, q);
+ *
+ * 清空QUEUE_FLAG_STATS:
+ *   - block/blk-stat.c|216| <<blk_stat_remove_callback>> blk_queue_flag_clear(QUEUE_FLAG_STATS, q);
+ */
 #define QUEUE_FLAG_STATS       24	/* track rq completion times */
 #define QUEUE_FLAG_POLL_STATS  25	/* collecting stats for hybrid polling */
 #define QUEUE_FLAG_REGISTERED  26	/* queue has been registered to a disk */
 #define QUEUE_FLAG_SCSI_PASSTHROUGH 27	/* queue supports SCSI commands */
+/*
+ * 设置QUEUE_FLAG_QUIESCED:
+ *   - block/blk-mq.c|318| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *
+ * 清除QUEUE_FLAG_QUIESCED:
+ *   - block/blk-mq.c|359| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ */
 #define QUEUE_FLAG_QUIESCED    28	/* queue has been quiesced */
 #define QUEUE_FLAG_PREEMPT_ONLY	29	/* only process REQ_PREEMPT requests */
 
@@ -723,6 +882,19 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 #define blk_queue_dead(q)	test_bit(QUEUE_FLAG_DEAD, &(q)->queue_flags)
 #define blk_queue_bypass(q)	test_bit(QUEUE_FLAG_BYPASS, &(q)->queue_flags)
 #define blk_queue_init_done(q)	test_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)
+/*
+ * 在如下地方设置:
+ *   - blk_cleanup_queue()
+ *   - queue_nomerges_store()
+ *   - __loop_update_dio()
+ *   - loop_add()
+ *   - megasas_set_nvme_device_properties()
+ *   - scsih_slave_configure()
+ *
+ * 在如下地方清除:
+ *   - queue_nomerges_store()
+ *   - __loop_update_dio()
+ */
 #define blk_queue_nomerges(q)	test_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)
 #define blk_queue_noxmerges(q)	\
 	test_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)
@@ -739,6 +911,13 @@ bool blk_queue_flag_test_and_clear(unsigned int flag, struct request_queue *q);
 #define blk_noretry_request(rq) \
 	((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \
 			     REQ_FAILFAST_DRIVER))
+/*
+ * 设置QUEUE_FLAG_QUIESCED:
+ *   - block/blk-mq.c|318| <<blk_mq_quiesce_queue_nowait>> blk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);
+ *
+ * 清除QUEUE_FLAG_QUIESCED:
+ *   - block/blk-mq.c|359| <<blk_mq_unquiesce_queue>> blk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
+ */
 #define blk_queue_quiesced(q)	test_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)
 #define blk_queue_preempt_only(q)				\
 	test_bit(QUEUE_FLAG_PREEMPT_ONLY, &(q)->queue_flags)
@@ -849,14 +1028,21 @@ static inline void blk_clear_rl_full(struct request_list *rl, bool sync)
 	rl->flags &= ~flag;
 }
 
+/*
+ * 判断这个request是否允许merge,
+ * 比如scsi passthrough或者Driver private requests不能被merge
+ */
 static inline bool rq_mergeable(struct request *rq)
 {
+	/* scsi passthrough或者Driver private requests */
 	if (blk_rq_is_passthrough(rq))
 		return false;
 
+	/* flush the volatile write cache */
 	if (req_op(rq) == REQ_OP_FLUSH)
 		return false;
 
+	/* write the zero filled sector many times */
 	if (req_op(rq) == REQ_OP_WRITE_ZEROES)
 		return false;
 
@@ -1053,6 +1239,9 @@ static inline sector_t blk_rq_pos(const struct request *rq)
 	return rq->__sector;
 }
 
+/*
+ * bytes left in the entire request
+ */
 static inline unsigned int blk_rq_bytes(const struct request *rq)
 {
 	return rq->__data_len;
@@ -1323,10 +1512,13 @@ static inline void blk_set_runtime_active(struct request_queue *q) {}
  * the plug list when the task sleeps by itself. For details, please see
  * schedule() where blk_schedule_flush_plug() is called.
  */
+/*
+ * 如果q->mq_ops存在, 使用mq_list, 否则使用list
+ */
 struct blk_plug {
-	struct list_head list; /* requests */
+	struct list_head list; /* requests */ // 放的struct request
 	struct list_head mq_list; /* blk-mq requests */
-	struct list_head cb_list; /* md requires an unplug callback */
+	struct list_head cb_list; /* md requires an unplug callback */ // 放的struct blk_plug_cb
 };
 #define BLK_MAX_REQUEST_COUNT 16
 #define BLK_PLUG_FLUSH_SIZE (128 * 1024)
diff --git a/include/linux/elevator.h b/include/linux/elevator.h
index 6d9e230..9078262 100644
--- a/include/linux/elevator.h
+++ b/include/linux/elevator.h
@@ -176,6 +176,11 @@ struct elevator_queue
 	struct mutex sysfs_lock;
 	unsigned int registered:1;
 	unsigned int uses_mq:1;
+	/*
+	 * 实际是struct hlist_head name[1 << (ELV_HASH_BITS)]
+	 *
+	 * 是不是放的都是可以merge的???????
+	 */
 	DECLARE_HASHTABLE(hash, ELV_HASH_BITS);
 };
 
diff --git a/include/linux/sbitmap.h b/include/linux/sbitmap.h
index 841585f..f79a7c5 100644
--- a/include/linux/sbitmap.h
+++ b/include/linux/sbitmap.h
@@ -26,6 +26,9 @@
 /**
  * struct sbitmap_word - Word in a &struct sbitmap.
  */
+/*
+ * struct sbitmap会包含一组struct sbitmap_word
+ */
 struct sbitmap_word {
 	/**
 	 * @word: The bitmap word itself.
@@ -35,7 +38,7 @@ struct sbitmap_word {
 	/**
 	 * @depth: Number of bits being used in @word.
 	 */
-	unsigned long depth;
+	unsigned long depth;  /* sbitmap_init_node()和sbitmap_resize()的时候会设置这个值 */
 } ____cacheline_aligned_in_smp;
 
 /**
@@ -44,43 +47,64 @@ struct sbitmap_word {
  * A &struct sbitmap is spread over multiple cachelines to avoid ping-pong. This
  * trades off higher memory usage for better scalability.
  */
+/*
+ * 可以被包含在struct sbitmap_queue中
+ */
 struct sbitmap {
 	/**
 	 * @depth: Number of bits used in the whole bitmap.
 	 */
-	unsigned int depth;
+	unsigned int depth;  // 这个sbitmap (包含sbitmap_word)一共多少bit
 
 	/**
 	 * @shift: log2(number of bits used per word)
 	 */
-	unsigned int shift;
+	unsigned int shift;  // 每一个sbitmap_word的bit的log
 
 	/**
 	 * @map_nr: Number of words (cachelines) being used for the bitmap.
 	 */
-	unsigned int map_nr;
+	unsigned int map_nr;  // 一共多少sbitmap_word
 
 	/**
 	 * @map: Allocated bitmap.
 	 */
+	/*
+	 * struct sbitmap_word声明有____cacheline_aligned_in_smp
+	 * 可以保证在不同的cache line
+	 */
 	struct sbitmap_word *map;
 };
 
-#define SBQ_WAIT_QUEUES 8
-#define SBQ_WAKE_BATCH 8
+#define SBQ_WAIT_QUEUES 8   // struct sbq_wait_state的数目 (在struct sbitmap_queue中)
+#define SBQ_WAKE_BATCH 8    // 唤醒每个struct sbq_wait_state需要cache的wait entry的数目
 
 /**
  * struct sbq_wait_state - Wait queue in a &struct sbitmap_queue.
  */
+/*
+ * 在struct sbitmap_queue中
+ */
 struct sbq_wait_state {
 	/**
 	 * @wait_cnt: Number of frees remaining before we wake up.
 	 */
-	atomic_t wait_cnt;
+	/*
+	 * 只有在sbq_wake_up()中真正被使用过
+	 *
+	 * wait_cnt不小于等于0就不会wake_up下面的wait
+	 */
+	atomic_t wait_cnt;   // 初始化的值来自sbq->wake_batch, 在sbitmap_queue_init_node()初始化的值小于或等于SBQ_WAKE_BATCH
 
 	/**
 	 * @wait: Wait queue.
 	 */
+	/*
+	 * wait的部分不再sbitmap实现中
+	 *
+	 * 比如在blk_mq_get_tag()中会调用:
+	 *     prepare_to_wait_exclusive(&ws->wait, &wait, TASK_UNINTERRUPTIBLE);
+	 */
 	wait_queue_head_t wait;
 } ____cacheline_aligned_in_smp;
 
@@ -105,13 +129,17 @@ struct sbitmap_queue {
 	 * This is per-cpu, which allows multiple users to stick to different
 	 * cachelines until the map is exhausted.
 	 */
-	unsigned int __percpu *alloc_hint;
+	/* 每个cpu的hint不一样 */
+	unsigned int __percpu *alloc_hint;  // 上面的注释说的是bit 所以应该是全局bit的index
 
 	/**
 	 * @wake_batch: Number of bits which must be freed before we wake up any
 	 * waiters.
 	 */
-	unsigned int wake_batch;
+	/*
+	 * 在sbitmap_queue_init_node()初始化的值小于或等于SBQ_WAKE_BATCH
+	 */
+	unsigned int wake_batch;  // 似乎在sbitmap_queue_init_node()初始化后除了sbitmap_queue_resize()不会改变
 
 	/**
 	 * @wake_index: Next wait queue in @ws to wake up.
@@ -121,12 +149,15 @@ struct sbitmap_queue {
 	/**
 	 * @ws: Wait queues.
 	 */
-	struct sbq_wait_state *ws;
+	/*
+	 * ws在sbitmap_queue_init_node()分配实体
+	 */
+	struct sbq_wait_state *ws; /* 数组, indexed by wake_index 最多SBQ_WAIT_QUEUES个元素 */
 
 	/**
 	 * @round_robin: Allocate bits in strict round-robin order.
 	 */
-	bool round_robin;
+	bool round_robin; // 在sbitmap_queue_init_node()会指定是否激活 激活了就不用hint了
 };
 
 /**
@@ -213,7 +244,15 @@ bool sbitmap_any_bit_set(const struct sbitmap *sb);
  */
 bool sbitmap_any_bit_clear(const struct sbitmap *sb);
 
+/*
+ * 返回的index用来索引struct sbitmap的map (struct sbitmap_word)
+ *
+ * 似乎是给一个全局的bit然后转成对应word的index
+ */
 #define SB_NR_TO_INDEX(sb, bitnr) ((bitnr) >> (sb)->shift)
+/*
+ * 似乎是给一个全局的bit然后转成对应的在某一个word内部的index
+ */
 #define SB_NR_TO_BIT(sb, bitnr) ((bitnr) & ((1U << (sb)->shift) - 1U))
 
 typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);
@@ -228,6 +267,14 @@ typedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);
  * This is inline even though it's non-trivial so that the function calls to the
  * callback will hopefully get optimized away.
  */
+/*
+ * called only by:
+ *   - blk_mq_dequeue_from_ctx()
+ *   - sbitmap_for_each_set()
+ *
+ * 对一sbitmap中每一个不为1的bit, 调用fn()
+ * 如果fn返回false (0), 则退出
+ */
 static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 					  unsigned int start,
 					  sb_for_each_fn fn, void *data)
@@ -236,17 +283,37 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 	unsigned int nr;
 	unsigned int scanned = 0;
 
+	/*
+	 * @depth: Number of bits used in the whole bitmap.
+	 *
+	 * start是bitmap全局的 不是per-word的
+	 */
 	if (start >= sb->depth)
 		start = 0;
+	/* 似乎是给一个全局的bit然后转成对应word的index */
 	index = SB_NR_TO_INDEX(sb, start);
+	/* 似乎是给一个全局的bit然后转成对应的在某一个word内部的index */
 	nr = SB_NR_TO_BIT(sb, start);
 
+	/*
+	 * 假设depth是64, word->depth是8. start是21
+	 * index = 2
+	 * nr    = 5
+	 * index开始的一轮还能扫描3个, 5, 6, 7
+	 */
+
+	/* 必须scan Number of bits used in the whole bitmap. */
 	while (scanned < sb->depth) {
 		struct sbitmap_word *word = &sb->map[index];
+		/* 这一轮准备扫描depth个 ??? */
 		unsigned int depth = min_t(unsigned int, word->depth - nr,
 					   sb->depth - scanned);
 
+		/*
+		 * 在这个函数scanned只在这里增加
+		 */
 		scanned += depth;
+		/* 如果这个word是空的就不用扫描了 go to next的第一件事就是把nr置0 */
 		if (!word->word)
 			goto next;
 
@@ -255,16 +322,29 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
 		 * bit offset back to the size of the word for find_next_bit().
 		 * On all other iterations, nr is zero, so this is a noop.
 		 */
-		depth += nr;
+		depth += nr;  // "next:" 那里才会改变一轮中nr的值
 		while (1) {
+			/* 只有第一轮nr不是0 */
+			/* Find the next set bit in a memory region. */
 			nr = find_next_bit(&word->word, depth, nr);
 			if (nr >= depth)
 				break;
+			/*
+			 * dispatch_rq_from_ctx(), dispatch_rq_data
+			 * bt_for_each(), bt_iter_data
+			 * bt_tags_for_each(), bt_tags_iter_data
+			 * blk_mq_flush_busy_ctxs(), flush_busy_ctx_data
+			 *
+			 * sb->shift: 每一个sbitmap_word的bit的log
+			 *
+			 * 如果fn返回false (0), 则退出
+			 */
 			if (!fn(sb, (index << sb->shift) + nr, data))
 				return;
 
 			nr++;
 		}
+/* 在next这里会重置nr */
 next:
 		nr = 0;
 		if (++index >= sb->map_nr)
@@ -278,15 +358,30 @@ static inline void __sbitmap_for_each_set(struct sbitmap *sb,
  * @fn: Callback. Should return true to continue or false to break early.
  * @data: Pointer to pass to callback.
  */
+/*
+ * called by:
+ *   - bt_for_each()             -- block/blk-mq-tag.c
+ *   - bt_tags_for_each()        -- block/blk-mq-tag.c
+ *   - blk_mq_flush_busy_ctxs()  -- block/blk-mq.c
+ */
+/*
+ * 对一sbitmap中每一个不为1的bit, 调用fn(), 从0开始
+ */
 static inline void sbitmap_for_each_set(struct sbitmap *sb, sb_for_each_fn fn,
 					void *data)
 {
+	/* 对一sbitmap中每一个不为1的bit, 调用fn() */
 	__sbitmap_for_each_set(sb, 0, fn, data);
 }
 
 static inline unsigned long *__sbitmap_word(struct sbitmap *sb,
 					    unsigned int bitnr)
 {
+	/*
+	 * 返回的index用来索引struct sbitmap的map (struct sbitmap_word)
+	 *
+	 * 似乎是给一个全局的bit然后转成对应word的index
+	 */
 	return &sb->map[SB_NR_TO_INDEX(sb, bitnr)].word;
 }
 
@@ -294,22 +389,34 @@ static inline unsigned long *__sbitmap_word(struct sbitmap *sb,
 
 static inline void sbitmap_set_bit(struct sbitmap *sb, unsigned int bitnr)
 {
+	/*
+	 * SB_NR_TO_BIT(): 似乎是给一个全局的bit然后转成对应的在某一个word内部的index
+	 */
 	set_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
 }
 
 static inline void sbitmap_clear_bit(struct sbitmap *sb, unsigned int bitnr)
 {
+	/*
+	 * SB_NR_TO_BIT(): 似乎是给一个全局的bit然后转成对应的在某一个word内部的index
+	 */
 	clear_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
 }
 
 static inline void sbitmap_clear_bit_unlock(struct sbitmap *sb,
 					    unsigned int bitnr)
 {
+	/*
+	 * SB_NR_TO_BIT(): 似乎是给一个全局的bit然后转成对应的在某一个word内部的index
+	 */
 	clear_bit_unlock(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
 }
 
 static inline int sbitmap_test_bit(struct sbitmap *sb, unsigned int bitnr)
 {
+	/*
+	 * SB_NR_TO_BIT(): 似乎是给一个全局的bit然后转成对应的在某一个word内部的index
+	 */
 	return test_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));
 }
 
@@ -448,6 +555,12 @@ static inline int sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu);
 
+/*
+ * called by:
+ *   - sbq_index_atomic_inc()
+ *   - sbq_wake_ptr()
+ *   - sbitmap_queue_wake_all()
+ */
 static inline int sbq_index_inc(int index)
 {
 	return (index + 1) & (SBQ_WAIT_QUEUES - 1);
@@ -466,9 +579,19 @@ static inline void sbq_index_atomic_inc(atomic_t *index)
  * @sbq: Bitmap queue to wait on.
  * @wait_index: A counter per "user" of @sbq.
  */
+/*
+ * called by:
+ *   - bt_wait_ptr()
+ *   - kyber_get_domain_token()
+ *
+ * 返回wait_index索引的下一个sbq->ws(struct sbq_wait_state类型) 增加wait_index
+ */
 static inline struct sbq_wait_state *sbq_wait_ptr(struct sbitmap_queue *sbq,
 						  atomic_t *wait_index)
 {
+	/*
+	 * struct sbitmap_queue.ws是一个struct struct sbq_wait_state的数组
+	 */
 	struct sbq_wait_state *ws;
 
 	ws = &sbq->ws[atomic_read(wait_index)];
diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index e6a9c06..b94b3a3 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -20,12 +20,22 @@
 #include <linux/sbitmap.h>
 #include <linux/seq_file.h>
 
+/*
+ * called by:
+ *   - blk_mq_init_hctx()
+ *   - sbitmap_queue_init_node()
+ *
+ *  @depth: Number of bits used in the whole bitmap
+ *  @shift: log2(number of bits used per word)
+ *  @map_nr: Number of words (cachelines) being used for the bitmap.
+ */
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 		      gfp_t flags, int node)
 {
 	unsigned int bits_per_word;
 	unsigned int i;
 
+	/* 比如shift可能是 -1 */
 	if (shift < 0) {
 		shift = ilog2(BITS_PER_LONG);
 		/*
@@ -64,6 +74,17 @@ int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 }
 EXPORT_SYMBOL_GPL(sbitmap_init_node);
 
+/*
+ * called by:
+ *   - blk_mq_map_swqueue()
+ *   - sbitmap_queue_resize()
+ *
+ *  @depth: Number of bits used in the whole bitmap
+ *  @shift: log2(number of bits used per word)
+ *  @map_nr: Number of words (cachelines) being used for the bitmap.
+ *
+ *  根据depth重新规划map_nr (map_nr可能会增加或者减少)
+ */
 void sbitmap_resize(struct sbitmap *sb, unsigned int depth)
 {
 	unsigned int bits_per_word = 1U << sb->shift;
@@ -79,6 +100,17 @@ void sbitmap_resize(struct sbitmap *sb, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_resize);
 
+/*
+ * called by:
+ *   - sbitmap_get()
+ *   - sbitmap_get_shallow()
+ *
+ * @depth: Number of bits used in the whole bitmap
+ * @shift: log2(number of bits used per word)
+ * @map_nr: Number of words (cachelines) being used for the bitmap.
+ *
+ * 除掉corner case, 大意是返回word的一个不为0的bit, 置为0
+ */
 static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 			      unsigned int hint, bool wrap)
 {
@@ -86,6 +118,9 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 	int nr;
 
 	while (1) {
+		/*
+		 * hint是offset, 指的是从word的哪一位开始??
+		 */
 		nr = find_next_zero_bit(word, depth, hint);
 		if (unlikely(nr >= depth)) {
 			/*
@@ -100,6 +135,7 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 			return -1;
 		}
 
+		/* Set a bit and return its old value for lock */
 		if (!test_and_set_bit_lock(nr, word))
 			break;
 
@@ -111,14 +147,34 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 	return nr;
 }
 
+/*
+ * called only by __sbitmap_queue_get()
+ *
+ *
+ * Try to allocate a free bit from a &struct sbitmap.
+ *
+ * @alloc_hint: Hint for where to start searching for a free bit.
+ * @round_robin: If true, be stricter about allocation order; always allocate
+ *               starting from the last allocated bit. This is less efficient
+ *               than the default behavior (false).
+ *
+ * 返回的nr是全局的
+ */
 int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 {
 	unsigned int i, index;
 	int nr = -1;
 
+	/* 给一个全局的alloc_hint然后转成对应word的index */
 	index = SB_NR_TO_INDEX(sb, alloc_hint);
 
+	/* 遍历所有的map, 从index开始 */
 	for (i = 0; i < sb->map_nr; i++) {
+		/*
+		 * SB_NR_TO_BIT():给一个全局的bit然后转成对应的在某一个word内部的index
+		 *
+		 * 除掉corner case, 大意是返回word的一个不为0的bit, 置为0
+		 */
 		nr = __sbitmap_get_word(&sb->map[index].word,
 					sb->map[index].depth,
 					SB_NR_TO_BIT(sb, alloc_hint),
@@ -130,6 +186,7 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 
 		/* Jump to next index. */
 		index++;
+		/* @shift: log2(number of bits used per word) */
 		alloc_hint = index << sb->shift;
 
 		if (index >= sb->map_nr) {
@@ -142,6 +199,13 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 }
 EXPORT_SYMBOL_GPL(sbitmap_get);
 
+/*
+ * called by only __sbitmap_queue_get_shallow()
+ *
+ * Try to allocate a free bit from a &struct sbitmap, limiting the depth used from each word.
+ *
+ * @shallow_depth: The maximum number of bits to allocate from a single word.
+ */
 int sbitmap_get_shallow(struct sbitmap *sb, unsigned int alloc_hint,
 			unsigned long shallow_depth)
 {
@@ -173,11 +237,20 @@ int sbitmap_get_shallow(struct sbitmap *sb, unsigned int alloc_hint,
 }
 EXPORT_SYMBOL_GPL(sbitmap_get_shallow);
 
+/*
+ * called by:
+ *   - blk_mq_do_dispatch_ctx()
+ *   - blk_mq_hctx_has_pending()
+ *   - kyber_has_work()
+ *
+ * Return: true if any bit in the bitmap is set, false otherwise
+ */
 bool sbitmap_any_bit_set(const struct sbitmap *sb)
 {
 	unsigned int i;
 
 	for (i = 0; i < sb->map_nr; i++) {
+		/* map是struct sbitmap_word类型 */
 		if (sb->map[i].word)
 			return true;
 	}
@@ -185,14 +258,22 @@ bool sbitmap_any_bit_set(const struct sbitmap *sb)
 }
 EXPORT_SYMBOL_GPL(sbitmap_any_bit_set);
 
+/*
+ * called by:
+ *   - blk_mq_has_free_tags()
+ *
+ * Return: true if any bit in the bitmap is clear, false otherwise
+ */
 bool sbitmap_any_bit_clear(const struct sbitmap *sb)
 {
 	unsigned int i;
 
 	for (i = 0; i < sb->map_nr; i++) {
+		/* map是struct sbitmap_word类型 */
 		const struct sbitmap_word *word = &sb->map[i];
 		unsigned long ret;
 
+		/* @depth: Number of bits being used in @word */
 		ret = find_first_zero_bit(&word->word, word->depth);
 		if (ret < word->depth)
 			return true;
@@ -201,6 +282,11 @@ bool sbitmap_any_bit_clear(const struct sbitmap *sb)
 }
 EXPORT_SYMBOL_GPL(sbitmap_any_bit_clear);
 
+/*
+ * 只被sbitmap_show()调用
+ *
+ * bitmap中1的个数?
+ */
 unsigned int sbitmap_weight(const struct sbitmap *sb)
 {
 	unsigned int i, weight = 0;
@@ -208,12 +294,16 @@ unsigned int sbitmap_weight(const struct sbitmap *sb)
 	for (i = 0; i < sb->map_nr; i++) {
 		const struct sbitmap_word *word = &sb->map[i];
 
+		/* bitmap中1的个数? */
 		weight += bitmap_weight(&word->word, word->depth);
 	}
 	return weight;
 }
 EXPORT_SYMBOL_GPL(sbitmap_weight);
 
+/*
+ * called only by sbitmap_queue_show()
+ */
 void sbitmap_show(struct sbitmap *sb, struct seq_file *m)
 {
 	seq_printf(m, "depth=%u\n", sb->depth);
@@ -223,6 +313,9 @@ void sbitmap_show(struct sbitmap *sb, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_show);
 
+/*
+ * called only by sbitmap_bitmap_show()
+ */
 static inline void emit_byte(struct seq_file *m, unsigned int offset, u8 byte)
 {
 	if ((offset & 0xf) == 0) {
@@ -270,6 +363,15 @@ void sbitmap_bitmap_show(struct sbitmap *sb, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_bitmap_show);
 
+/*
+ * called by:
+ *   - sbitmap_queue_init_node()
+ *   - sbitmap_queue_resize()
+ *
+ * @depth: Number of bits used in the whole bitmap
+ *
+ * 返回的值小于或等于SBQ_WAKE_BATCH
+ */
 static unsigned int sbq_calc_wake_batch(unsigned int depth)
 {
 	unsigned int wake_batch;
@@ -280,18 +382,25 @@ static unsigned int sbq_calc_wake_batch(unsigned int depth)
 	 * enough to wake up all of the queues.
 	 */
 	wake_batch = SBQ_WAKE_BATCH;
+	/* 如果depth < 64 满足条件 */
 	if (wake_batch > depth / SBQ_WAIT_QUEUES)
 		wake_batch = max(1U, depth / SBQ_WAIT_QUEUES);
 
 	return wake_batch;
 }
 
+/*
+ * called by:
+ *   - bt_alloc()
+ *   - kyber_queue_data_alloc()
+ */
 int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 			    int shift, bool round_robin, gfp_t flags, int node)
 {
 	int ret;
 	int i;
 
+	/* 初始化sbq->sb (struct sbitmap类型) */
 	ret = sbitmap_init_node(&sbq->sb, depth, shift, flags, node);
 	if (ret)
 		return ret;
@@ -307,7 +416,12 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 			*per_cpu_ptr(sbq->alloc_hint, i) = prandom_u32() % depth;
 	}
 
+	/*
+	 * @depth: Number of bits used in the whole bitmap
+	 * 返回的值小于或等于SBQ_WAKE_BATCH
+	 */
 	sbq->wake_batch = sbq_calc_wake_batch(depth);
+	/* Next wait queue in @ws (struct sbq_wait_state数组) to wake up */
 	atomic_set(&sbq->wake_index, 0);
 
 	sbq->ws = kzalloc_node(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags, node);
@@ -319,6 +433,11 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 
 	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
 		init_waitqueue_head(&sbq->ws[i].wait);
+		/*
+		 * sbq->wake_batch在上面初始化的值小于或等于SBQ_WAKE_BATCH
+		 *
+		 * wait_cnt只在sbq_wake_up()减少和恢复
+		 */
 		atomic_set(&sbq->ws[i].wait_cnt, sbq->wake_batch);
 	}
 
@@ -327,6 +446,13 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
 
+/*
+ * called by:
+ *   - blk_mq_tag_update_depth()
+ *   - kyber_adjust_rw_depth()
+ *   - kyber_adjust_other_depth()
+ *   - kyber_queue_data_alloc()
+ */
 void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 {
 	unsigned int wake_batch = sbq_calc_wake_batch(depth);
@@ -339,24 +465,38 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 		 * the batch size is updated before the wait counts.
 		 */
 		smp_mb__before_atomic();
+		/* 下面的wait_cnt为什么是 1 */
 		for (i = 0; i < SBQ_WAIT_QUEUES; i++)
-			atomic_set(&sbq->ws[i].wait_cnt, 1);
+			atomic_set(&sbq->ws[i].wait_cnt, 1); // wait_cnt只在sbq_wake_up()减少和恢复
 	}
 	sbitmap_resize(&sbq->sb, depth);
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
+/*
+ * called by:
+ *   - __blk_mq_get_tag()
+ *   - kyber_get_domain_token() 两次
+ *   - sbitmap_queue_get()
+ *
+ * 从sbitmap_queue(的sb)中分配一个bit (bit是全局的)
+ */
 int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 {
 	unsigned int hint, depth;
 	int nr;
 
 	hint = this_cpu_read(*sbq->alloc_hint);
+	/* @depth: Number of bits used in the whole bitmap. */
 	depth = READ_ONCE(sbq->sb.depth);
 	if (unlikely(hint >= depth)) {
 		hint = depth ? prandom_u32() % depth : 0;
 		this_cpu_write(*sbq->alloc_hint, hint);
 	}
+	/*
+	 * Try to allocate a free bit from a &struct sbitmap
+	 * 返回的nr是全局的
+	 */
 	nr = sbitmap_get(&sbq->sb, hint, sbq->round_robin);
 
 	if (nr == -1) {
@@ -374,6 +514,15 @@ int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 }
 EXPORT_SYMBOL_GPL(__sbitmap_queue_get);
 
+/*
+ * called by:
+ *   - __blk_mq_get_tag()
+ *   - sbitmap_queue_get_shallow()
+ *
+ * Try to allocate a free bit from a &struc sbitmap_queue, limiting the depth used from each word.
+ *
+ * @shallow_depth: The maximum number of bits to allocate from a single word.
+ */
 int __sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
 				unsigned int shallow_depth)
 {
@@ -403,14 +552,23 @@ int __sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
 }
 EXPORT_SYMBOL_GPL(__sbitmap_queue_get_shallow);
 
+/*
+ * called only by sbq_wake_up()
+ *
+ * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的
+ * 更新sbq->wake_index (sbq->wake_index就是当前active的那个)
+ * 否则返回NULL (就返回一个)
+ */
 static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
 
+	/* 下一个struct sbq_wait_state的index */
 	wake_index = atomic_read(&sbq->wake_index);
 	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
 		struct sbq_wait_state *ws = &sbq->ws[wake_index];
 
+		/* 只要找到一个active的 下面的return就会退出 */
 		if (waitqueue_active(&ws->wait)) {
 			int o = atomic_read(&sbq->wake_index);
 
@@ -425,6 +583,14 @@ static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 	return NULL;
 }
 
+/*
+ * called only by sbitmap_queue_clear()
+ *
+ * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的ws
+ * 更新sbq->wake_index (sbq->wake_index就是当前active的那个ws)
+ * 把ws->wait_cnt减1 如果ws->wait_cnt小于等于0了则唤醒ws上的wait queue的wake_batch个entry
+ * 增加sbq->wake_index, 把dec的wake_batch恢复
+ */
 static void sbq_wake_up(struct sbitmap_queue *sbq)
 {
 	struct sbq_wait_state *ws;
@@ -440,6 +606,11 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 	 */
 	smp_mb__after_atomic();
 
+	/*
+	 * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的
+	 * 更新sbq->wake_index (sbq->wake_index就是当前active的那个)
+	 * 否则返回NULL (就返回一个)
+	 */
 	ws = sbq_wake_ptr(sbq);
 	if (!ws)
 		return;
@@ -460,22 +631,50 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 		 * either cause the cmpxchg to fail or overwrite after the
 		 * cmpxchg.
 		 */
+		/*
+		 * 再加回去 比如最初的值
+		 */
 		atomic_cmpxchg(&ws->wait_cnt, wait_cnt, wait_cnt + wake_batch);
 		sbq_index_atomic_inc(&sbq->wake_index);
 		wake_up_nr(&ws->wait, wake_batch);
 	}
 }
 
+/*
+ * called by:
+ *   - blk_mq_put_tag() 两次  -----> 还是要靠sbitmap的使用者外部唤醒
+ *   - rq_clear_domain_token() -- block/kyber-iosched.c
+ *
+ * Free an allocated bit and wake up waiters on a struct sbitmap_queue.
+ *
+ * 清除sbitmap_queue的sbitmap对应的bit nr
+ * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的ws
+ * 更新sbq->wake_index (sbq->wake_index就是当前active的那个ws)
+ * 把ws->wait_cnt减1 如果ws->wait_cnt小于等于0了则"唤醒"ws上的wait queue的wake_batch个entry
+ * 增加sbq->wake_index, 把dec的wake_batch恢复
+ */
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
+	/* 清除sbitmap_queue的sbitmap对应的bit nr */
 	sbitmap_clear_bit_unlock(&sbq->sb, nr);
+	/*
+	 * 从sbq->wake_index开始遍历所有的struct sbq_wait_state 返回active的ws
+	 * 更新sbq->wake_index (sbq->wake_index就是当前active的那个ws)
+	 * 把ws->wait_cnt减1 如果ws->wait_cnt小于等于0了则"唤醒"ws上的wait queue的wake_batch个entry
+	 * 增加sbq->wake_index, 把dec的wake_batch恢复
+	 */
 	sbq_wake_up(sbq);
 	if (likely(!sbq->round_robin && nr < sbq->sb.depth))
 		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 
+/*
+ * called by only blk_mq_tag_wakeup_all()
+ *
+ * 遍历所有的struct sbq_wait_state并wakeup有等待事件的wait queue
+ */
 void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -485,10 +684,12 @@ void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 	 * sbq_wake_up().
 	 */
 	smp_mb();
+	/* 用来索引struct sbq_wait_state的ws数组 下一个要用的*/
 	wake_index = atomic_read(&sbq->wake_index);
 	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
 		struct sbq_wait_state *ws = &sbq->ws[wake_index];
 
+		/* returns true if the wait list is not empty */
 		if (waitqueue_active(&ws->wait))
 			wake_up(&ws->wait);
 
@@ -497,6 +698,11 @@ void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_wake_all);
 
+/*
+ * called by:
+ *   - blk_mq_debugfs_tags_show() 两次
+ *   - KYBER_DEBUGFS_DOMAIN_ATTRS()
+ */
 void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 {
 	bool first;
-- 
2.7.4

