From 90685777c3084801ddd1dde97c24dc772fa3aa0a Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 20 Jan 2020 22:28:13 -0800
Subject: [PATCH 1/1] linux uek4 mm

v4.1.12-124.24.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 mm/slab.h |   5 ++
 mm/slub.c | 244 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 249 insertions(+)

diff --git a/mm/slab.h b/mm/slab.h
index 4c3ac12..74ac22a 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -299,6 +299,11 @@ static inline void slab_init_memcg_params(struct kmem_cache *s)
 }
 #endif /* CONFIG_MEMCG_KMEM */
 
+/*
+ * called by:
+ *   - mm/slab.c|3554| <<kmem_cache_free>> cachep = cache_from_obj(cachep, objp);
+ *   - mm/slub.c|2955| <<kmem_cache_free>> s = cache_from_obj(s, x);
+ */
 static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
 {
 	struct kmem_cache *cachep;
diff --git a/mm/slub.c b/mm/slub.c
index 3d7932c..82fec07 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -115,15 +115,55 @@
  * 			the fast path and disables lockless freelists.
  */
 
+ /*
+  * called by:
+  *   - mm/slub.c|130| <<kmem_cache_has_cpu_partial>> return !kmem_cache_debug(s);
+  *   - mm/slub.c|1549| <<__free_slab>> if (kmem_cache_debug(s)) {
+  *   - mm/slub.c|2004| <<deactivate_slab>> if (kmem_cache_debug(s) && !lock) {
+  *   - mm/slub.c|2507| <<__slab_alloc>> if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
+  *   - mm/slub.c|2511| <<__slab_alloc>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2697| <<__slab_free>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2772| <<__slab_free>> if (kmem_cache_debug(s))
+  *
+  * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+  *   - SLAB_RED_ZONE
+  *   - SLAB_POISON
+  *   - SLAB_STORE_USER
+  *   - SLAB_TRACE
+  *   - SLAB_DEBUG_FREE
+  */
 static inline int kmem_cache_debug(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
+	/*
+	 * SLAB_DEBUG_FLAGS包含以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 */
 	return unlikely(s->flags & SLAB_DEBUG_FLAGS);
 #else
 	return 0;
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1742| <<get_partial_node>> if (!kmem_cache_has_cpu_partial(s)
+ *   - mm/slub.c|2714| <<__slab_free>> if (kmem_cache_has_cpu_partial(s) && !prior) {
+ *   - mm/slub.c|2771| <<__slab_free>> if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
+ *   - mm/slub.c|3279| <<kmem_cache_open>> if (!kmem_cache_has_cpu_partial(s))
+ *   - mm/slub.c|4575| <<cpu_partial_store>> if (objects && !kmem_cache_has_cpu_partial(s))
+ *
+ * 如果kmem_cache->flags设置了SLAB_DEBUG_FLAGS中的以下任何则返回false:
+ *   - SLAB_RED_ZONE
+ *   - SLAB_POISON
+ *   - SLAB_STORE_USER
+ *   - SLAB_TRACE
+ *   - SLAB_DEBUG_FREE
+ */
 static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
@@ -225,6 +265,13 @@ static inline void stat(const struct kmem_cache *s, enum stat_item si)
  *******************************************************************/
 
 /* Verify that a pointer has an address that is valid within a slab page */
+/*
+ * called by:
+ *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|905| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+ *   - mm/slub.c|1037| <<alloc_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+ *   - mm/slub.c|1078| <<free_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+ */
 static inline int check_valid_pointer(struct kmem_cache *s,
 				struct page *page, const void *object)
 {
@@ -242,6 +289,18 @@ static inline int check_valid_pointer(struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|262| <<get_freepointer_safe>> p = get_freepointer(s, object);
+ *   - mm/slub.c|455| <<get_map>> for (p = page->freelist; p; p = get_freepointer(s, p))
+ *   - mm/slub.c|627| <<print_trailer>> p, p - addr, get_freepointer(s, p));
+ *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|920| <<on_freelist>> fp = get_freepointer(s, object);
+ *   - mm/slub.c|1843| <<deactivate_slab>> while (freelist && (nextfree = get_freepointer(s, freelist))) {
+ *   - mm/slub.c|2377| <<__slab_alloc>> c->freelist = get_freepointer(s, freelist);
+ *   - mm/slub.c|2409| <<__slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist));
+ *   - mm/slub.c|2942| <<early_kmem_cache_node_alloc>> page->freelist = get_freepointer(kmem_cache_node, n);
+ */
 static inline void *get_freepointer(struct kmem_cache *s, void *object)
 {
 	return *(void **)(object + s->offset);
@@ -252,6 +311,10 @@ static void prefetch_freepointer(const struct kmem_cache *s, void *object)
 	prefetch(object + s->offset);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2477| <<slab_alloc_node>> void *next_object = get_freepointer_safe(s, object);
+ */
 static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 {
 	void *p;
@@ -264,6 +327,17 @@ static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 	return p;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|857| <<check_object>> set_freepointer(s, p, NULL);
+ *   - mm/slub.c|909| <<on_freelist>> set_freepointer(s, object, NULL);
+ *   - mm/slub.c|1444| <<new_slab>> set_freepointer(s, p, p + s->size);
+ *   - mm/slub.c|1446| <<new_slab>> set_freepointer(s, p, NULL); 
+ *   - mm/slub.c|1850| <<deactivate_slab>> set_freepointer(s, freelist, prior);
+ *   - mm/slub.c|1887| <<deactivate_slab>> set_freepointer(s, freelist, old.freelist);
+ *   - mm/slub.c|2602| <<__slab_free>> set_freepointer(s, object, prior);
+ *   - mm/slub.c|2728| <<slab_free>> set_freepointer(s, object, c->freelist);
+ */
 static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 {
 	*(void **)(object + s->offset) = fp;
@@ -279,11 +353,24 @@ static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 			__p += (__s)->size, __idx++)
 
 /* Determine object index from a given position */
+/*
+ * called by:
+ *   - mm/slub.c|456| <<get_map>> set_bit(slab_index(p, s, addr), map);
+ *   - mm/slub.c|3219| <<list_slab_objects>> if (!test_bit(slab_index(p, s, addr), map)) {
+ *   - mm/slub.c|3889| <<validate_slab>> if (test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|3895| <<validate_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4096| <<process_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ */
 static inline int slab_index(void *p, struct kmem_cache *s, void *addr)
 {
 	return (p - addr) / s->size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1320| <<slab_post_alloc_hook>> kmemcheck_slab_alloc(s, flags, object, slab_ksize(s));
+ *   - mm/slub.c|3431| <<__ksize>> return slab_ksize(page->slab_cache);
+ */
 static inline size_t slab_ksize(const struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
@@ -308,11 +395,24 @@ static inline size_t slab_ksize(const struct kmem_cache *s)
 	return s->size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|362| <<oo_make>> (order << OO_SHIFT) + order_objects(order, size, reserved)
+ *   - mm/slub.c|916| <<check_slab>> maxobj = order_objects(compound_order(page), s->size, s->reserved);
+ *   - mm/slub.c|966| <<on_freelist>> max_objects = order_objects(compound_order(page), s->size, s->reserved);
+ *   - mm/slub.c|2851| <<slab_order>> if (order_objects(min_order, size, reserved) > MAX_OBJS_PER_PAGE)
+ *   - mm/slub.c|2891| <<calculate_order>> max_objects = order_objects(slub_max_order, size, reserved);
+ */
 static inline int order_objects(int order, unsigned long size, int reserved)
 {
 	return ((PAGE_SIZE << order) - reserved) / size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3154| <<calculate_sizes>> s->oo = oo_make(order, size, s->reserved);
+ *   - mm/slub.c|3155| <<calculate_sizes>> s->min = oo_make(get_order(size), size, s->reserved);
+ */
 static inline struct kmem_cache_order_objects oo_make(int order,
 		unsigned long size, int reserved)
 {
@@ -323,11 +423,34 @@ static inline struct kmem_cache_order_objects oo_make(int order,
 	return x;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1358| <<alloc_slab_page>> int order = oo_order(oo);
+ *   - mm/slub.c|1412| <<allocate_slab>> int pages = 1 << oo_order(oo);
+ *   - mm/slub.c|1414| <<allocate_slab>> kmemcheck_alloc_shadow(page, oo_order(oo), alloc_gfp, node);
+ *   - mm/slub.c|1435| <<allocate_slab>> 1 << oo_order(oo));
+ *   - mm/slub.c|2234| <<slab_out_of_memory>> s->name, s->object_size, s->size, oo_order(s->oo),
+ *   - mm/slub.c|2235| <<slab_out_of_memory>> oo_order(s->min));
+ *   - mm/slub.c|2237| <<slab_out_of_memory>> if (oo_order(s->min) > get_order(s->object_size))
+ *   - mm/slub.c|3241| <<kmem_cache_open>> oo_order(s->oo), s->offset, flags);
+ *   - mm/slub.c|4473| <<order_show>> return sprintf(buf, "%d\n", oo_order(s->oo));
+ *   - mm/slub.c|5375| <<get_slabinfo>> sinfo->cache_order = oo_order(s->oo);
+ */
 static inline int oo_order(struct kmem_cache_order_objects x)
 {
 	return x.x >> OO_SHIFT;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1431| <<allocate_slab>> page->objects = oo_objects(oo);
+ *   - mm/slub.c|3156| <<calculate_sizes>> if (oo_objects(s->oo) > oo_objects(s->max))
+ *   - mm/slub.c|3159| <<calculate_sizes>> return !!oo_objects(s->oo);
+ *   - mm/slub.c|3988| <<validate_slab_cache>> unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
+ *   - mm/slub.c|4149| <<list_locations>> unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
+ *   - mm/slub.c|4450| <<objs_per_slab_show>> return sprintf(buf, "%d\n", oo_objects(s->oo));
+ *   - mm/slub.c|5374| <<get_slabinfo>> sinfo->objects_per_slab = oo_objects(s->oo);
+ */
 static inline int oo_objects(struct kmem_cache_order_objects x)
 {
 	return x.x & OO_MASK;
@@ -346,6 +469,11 @@ static __always_inline void slab_unlock(struct page *page)
 	__bit_spin_unlock(PG_locked, &page->flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|427| <<__cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ *   - mm/slub.c|466| <<cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ */
 static inline void set_page_slub_counters(struct page *page, unsigned long counters_new)
 {
 	struct page tmp;
@@ -1027,6 +1155,10 @@ static void setup_object_debug(struct kmem_cache *s, struct page *page,
 	init_tracking(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2564| <<__slab_alloc>> !alloc_debug_processing(s, page, freelist, addr))
+ */
 static noinline int alloc_debug_processing(struct kmem_cache *s,
 					struct page *page,
 					void *object, unsigned long addr)
@@ -1063,6 +1195,10 @@ bad:
 	return 0;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2801| <<__slab_free>> !(n = free_debug_processing(s, page, x, addr, &flags)))
+ */
 static noinline struct kmem_cache_node *free_debug_processing(
 	struct kmem_cache *s, struct page *page, void *object,
 	unsigned long addr, unsigned long *flags)
@@ -1258,6 +1394,10 @@ static inline void kfree_hook(const void *x)
 	kasan_kfree_large(x);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2540| <<slab_alloc_node>> s = slab_pre_alloc_hook(s, gfpflags);
+ */
 static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 						     gfp_t flags)
 {
@@ -1271,6 +1411,10 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 	return memcg_kmem_get_cache(s, flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2614| <<slab_alloc_node>> slab_post_alloc_hook(s, gfpflags, object);
+ */
 static inline void slab_post_alloc_hook(struct kmem_cache *s,
 					gfp_t flags, void *object)
 {
@@ -1281,6 +1425,10 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s,
 	kasan_slab_alloc(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2918| <<slab_free>> slab_free_hook(s, x);
+ */
 static inline void slab_free_hook(struct kmem_cache *s, void *x)
 {
 	kmemleak_free_recursive(x, s->flags);
@@ -1815,6 +1963,13 @@ static void init_kmem_cache_cpus(struct kmem_cache *s)
 /*
  * Remove the cpu slab
  */
+/*
+ * called by:
+ *   - mm/slub.c|2180| <<flush_slab>> deactivate_slab(s, c->page, c->freelist);
+ *   - mm/slub.c|2424| <<__slab_alloc>> deactivate_slab(s, page, c->freelist);
+ *   - mm/slub.c|2437| <<__slab_alloc>> deactivate_slab(s, page, c->freelist);
+ *   - mm/slub.c|2497| <<__slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist));
+ */
 static void deactivate_slab(struct kmem_cache *s, struct page *page,
 				void *freelist)
 {
@@ -1964,6 +2119,12 @@ redo:
  * for the cpu using c (or some other guarantee must be there
  * to guarantee no concurrent accesses).
  */
+/*
+ * called by:
+ *   - mm/slub.c|2203| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|2225| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|2266| <<__flush_cpu_slab>> unfreeze_partials(s, c);
+ */
 static void unfreeze_partials(struct kmem_cache *s,
 		struct kmem_cache_cpu *c)
 {
@@ -2086,6 +2247,11 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2198| <<__flush_cpu_slab>> flush_slab(s, c);
+ *   - mm/slub.c|2318| <<new_slab_objects>> flush_slab(s, c);
+ */
 static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
 {
 	stat(s, CPUSLAB_FLUSH);
@@ -2101,6 +2267,12 @@ static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
  *
  * Called from IPI handler with interrupts disabled.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2208| <<flush_cpu_slab>> __flush_cpu_slab(s, smp_processor_id());
+ *   - mm/slub.c|3741| <<bootstrap>> __flush_cpu_slab(s, smp_processor_id());
+ *   - mm/slub.c|3880| <<slab_cpuup_callback>> __flush_cpu_slab(s, cpu);
+ */
 static inline void __flush_cpu_slab(struct kmem_cache *s, int cpu)
 {
 	struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);
@@ -2303,6 +2475,10 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
  * we need to allocate a new slab. This is the slowest path since it involves
  * a call to the page allocator and the setup of a new slab.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2580| <<slab_alloc_node>> object = __slab_alloc(s, gfpflags, node, addr, c);
+ */
 static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
@@ -2398,10 +2574,28 @@ new_slab:
 	}
 
 	page = c->page;
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 这里如果不包含就goto到load_freelist不走debug路线了
+	 */
 	if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
 		goto load_freelist;
 
 	/* Only entered in the debug case */
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 如果包含, 则调用alloc_debug_processing()
+	 */
 	if (kmem_cache_debug(s) &&
 			!alloc_debug_processing(s, page, freelist, addr))
 		goto new_slab;	/* Slab failed checks. Next slab needed */
@@ -2423,6 +2617,14 @@ new_slab:
  *
  * Otherwise we can simply pick the next object from the lockless free list.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2622| <<slab_alloc>> return slab_alloc_node(s, gfpflags, NUMA_NO_NODE, addr);
+ *   - mm/slub.c|2650| <<kmem_cache_alloc_node>> void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+ *   - mm/slub.c|2664| <<kmem_cache_alloc_node_trace>> void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+ *   - mm/slub.c|3470| <<__kmalloc_node>> ret = slab_alloc_node(s, flags, node, _RET_IP_);
+ *   - mm/slub.c|3958| <<__kmalloc_node_track_caller>> ret = slab_alloc_node(s, gfpflags, node, caller);
+ */
 static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 		gfp_t gfpflags, int node, unsigned long addr)
 {
@@ -2472,6 +2674,12 @@ redo:
 	page = c->page;
 	if (unlikely(!object || !node_match(page, node))) {
 		object = __slab_alloc(s, gfpflags, node, addr, c);
+		/*
+		 * ALLOC_SLOWPATH除了在这里只在一处使用:
+		 *   - mm/slub.c|4962| <<global>> STAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);
+		 *
+		 * 似乎只有在CONFIG_SLUB_STATS的时候才能用sysfs
+		 */
 		stat(s, ALLOC_SLOWPATH);
 	} else {
 		void *next_object = get_freepointer_safe(s, object);
@@ -2499,6 +2707,13 @@ redo:
 			goto redo;
 		}
 		prefetch_freepointer(s, next_object);
+		/*
+		 * 除了这里只在一处使用:
+		 *   - mm/slub.c|4961| <<global>> STAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);
+		 *
+		 * # cat /sys/kernel/slab/kmalloc-128/alloc_slowpath 
+		 * 7128 C0=2995 C1=4133
+		 */
 		stat(s, ALLOC_FASTPATH);
 	}
 
@@ -2510,6 +2725,13 @@ redo:
 	return object;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2627| <<kmem_cache_alloc>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|2639| <<kmem_cache_alloc_trace>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|3425| <<__kmalloc>> ret = slab_alloc(s, flags, _RET_IP_);
+ *   - mm/slub.c|3928| <<__kmalloc_track_caller>> ret = slab_alloc(s, gfpflags, caller);
+ */
 static __always_inline void *slab_alloc(struct kmem_cache *s,
 		gfp_t gfpflags, unsigned long addr)
 {
@@ -2575,6 +2797,10 @@ EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
  * lock and free the item. If there is no additional partial page
  * handling required then we can return immediately.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2949| <<slab_free>> __slab_free(s, page, x, addr);
+ */
 static void __slab_free(struct kmem_cache *s, struct page *page,
 			void *x, unsigned long addr)
 {
@@ -2586,8 +2812,21 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 	struct kmem_cache_node *n = NULL;
 	unsigned long uninitialized_var(flags);
 
+	/*
+	 * 除了这里FREE_SLOWPATH只在下面使用:
+	 *   - mm/slub.c|5067| <<global>> STAT_ATTR(FREE_SLOWPATH, free_slowpath);
+	 */
 	stat(s, FREE_SLOWPATH);
 
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 如果有则调用free_debug_processing()
+	 */
 	if (kmem_cache_debug(s) &&
 		!(n = free_debug_processing(s, page, x, addr, &flags)))
 		return;
@@ -2699,6 +2938,11 @@ slab_empty:
  * If fastpath is not possible then fall back to __slab_free where we deal
  * with all sorts of special processing.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2958| <<kmem_cache_free>> slab_free(s, virt_to_head_page(x), x, _RET_IP_);
+ *   - mm/slub.c|3628| <<kfree>> slab_free(page->slab_cache, page, object, _RET_IP_);
+ */
 static __always_inline void slab_free(struct kmem_cache *s,
 			struct page *page, void *x, unsigned long addr)
 {
-- 
2.7.4

