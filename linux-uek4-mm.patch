From af0ab2d922beac827bc9ba06a9f67fccf0b13fa8 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Tue, 28 Jan 2020 16:23:07 -0800
Subject: [PATCH 1/1] linux uek4 mm

v4.1.12-124.24.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 include/linux/poison.h |   2 +
 mm/slab.h              |   5 +
 mm/slub.c              | 682 +++++++++++++++++++++++++++++++++++++++++
 3 files changed, 689 insertions(+)

diff --git a/include/linux/poison.h b/include/linux/poison.h
index 253c9b4198ef..e338a5c4ccec 100644
--- a/include/linux/poison.h
+++ b/include/linux/poison.h
@@ -44,7 +44,9 @@
 #define SLUB_RED_ACTIVE		0xcc
 
 /* ...and for poisoning */
+/* Z */
 #define	POISON_INUSE	0x5a	/* for use-uninitialised poisoning */
+/* k */
 #define POISON_FREE	0x6b	/* for use-after-free poisoning */
 #define	POISON_END	0xa5	/* end-byte of poisoning */
 
diff --git a/mm/slab.h b/mm/slab.h
index 4c3ac12dd644..74ac22a49c7c 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -299,6 +299,11 @@ static inline void slab_init_memcg_params(struct kmem_cache *s)
 }
 #endif /* CONFIG_MEMCG_KMEM */
 
+/*
+ * called by:
+ *   - mm/slab.c|3554| <<kmem_cache_free>> cachep = cache_from_obj(cachep, objp);
+ *   - mm/slub.c|2955| <<kmem_cache_free>> s = cache_from_obj(s, x);
+ */
 static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
 {
 	struct kmem_cache *cachep;
diff --git a/mm/slub.c b/mm/slub.c
index 3d7932cc7084..b9f2142aaa1e 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -115,15 +115,55 @@
  * 			the fast path and disables lockless freelists.
  */
 
+ /*
+  * called by:
+  *   - mm/slub.c|130| <<kmem_cache_has_cpu_partial>> return !kmem_cache_debug(s);
+  *   - mm/slub.c|1549| <<__free_slab>> if (kmem_cache_debug(s)) {
+  *   - mm/slub.c|2004| <<deactivate_slab>> if (kmem_cache_debug(s) && !lock) {
+  *   - mm/slub.c|2507| <<__slab_alloc>> if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
+  *   - mm/slub.c|2511| <<__slab_alloc>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2697| <<__slab_free>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2772| <<__slab_free>> if (kmem_cache_debug(s))
+  *
+  * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+  *   - SLAB_RED_ZONE
+  *   - SLAB_POISON
+  *   - SLAB_STORE_USER
+  *   - SLAB_TRACE
+  *   - SLAB_DEBUG_FREE
+  */
 static inline int kmem_cache_debug(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
+	/*
+	 * SLAB_DEBUG_FLAGS包含以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 */
 	return unlikely(s->flags & SLAB_DEBUG_FLAGS);
 #else
 	return 0;
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1742| <<get_partial_node>> if (!kmem_cache_has_cpu_partial(s)
+ *   - mm/slub.c|2714| <<__slab_free>> if (kmem_cache_has_cpu_partial(s) && !prior) {
+ *   - mm/slub.c|2771| <<__slab_free>> if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
+ *   - mm/slub.c|3279| <<kmem_cache_open>> if (!kmem_cache_has_cpu_partial(s))
+ *   - mm/slub.c|4575| <<cpu_partial_store>> if (objects && !kmem_cache_has_cpu_partial(s))
+ *
+ * 如果kmem_cache->flags设置了SLAB_DEBUG_FLAGS中的以下任何则返回false:
+ *   - SLAB_RED_ZONE
+ *   - SLAB_POISON
+ *   - SLAB_STORE_USER
+ *   - SLAB_TRACE
+ *   - SLAB_DEBUG_FREE
+ */
 static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
@@ -225,6 +265,16 @@ static inline void stat(const struct kmem_cache *s, enum stat_item si)
  *******************************************************************/
 
 /* Verify that a pointer has an address that is valid within a slab page */
+/*
+ * called by:
+ *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|905| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+ *   - mm/slub.c|1037| <<alloc_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+ *   - mm/slub.c|1078| <<free_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+ *
+ * 查看object的地址是否在page的slab的范围内
+ * 如果在返回1, 否则返回0
+ */
 static inline int check_valid_pointer(struct kmem_cache *s,
 				struct page *page, const void *object)
 {
@@ -242,6 +292,18 @@ static inline int check_valid_pointer(struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|262| <<get_freepointer_safe>> p = get_freepointer(s, object);
+ *   - mm/slub.c|455| <<get_map>> for (p = page->freelist; p; p = get_freepointer(s, p))
+ *   - mm/slub.c|627| <<print_trailer>> p, p - addr, get_freepointer(s, p));
+ *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|920| <<on_freelist>> fp = get_freepointer(s, object);
+ *   - mm/slub.c|1843| <<deactivate_slab>> while (freelist && (nextfree = get_freepointer(s, freelist))) {
+ *   - mm/slub.c|2377| <<__slab_alloc>> c->freelist = get_freepointer(s, freelist);
+ *   - mm/slub.c|2409| <<__slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist));
+ *   - mm/slub.c|2942| <<early_kmem_cache_node_alloc>> page->freelist = get_freepointer(kmem_cache_node, n);
+ */
 static inline void *get_freepointer(struct kmem_cache *s, void *object)
 {
 	return *(void **)(object + s->offset);
@@ -252,6 +314,10 @@ static void prefetch_freepointer(const struct kmem_cache *s, void *object)
 	prefetch(object + s->offset);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2477| <<slab_alloc_node>> void *next_object = get_freepointer_safe(s, object);
+ */
 static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 {
 	void *p;
@@ -264,6 +330,17 @@ static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 	return p;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|857| <<check_object>> set_freepointer(s, p, NULL);
+ *   - mm/slub.c|909| <<on_freelist>> set_freepointer(s, object, NULL);
+ *   - mm/slub.c|1444| <<new_slab>> set_freepointer(s, p, p + s->size);
+ *   - mm/slub.c|1446| <<new_slab>> set_freepointer(s, p, NULL); 
+ *   - mm/slub.c|1850| <<deactivate_slab>> set_freepointer(s, freelist, prior);
+ *   - mm/slub.c|1887| <<deactivate_slab>> set_freepointer(s, freelist, old.freelist);
+ *   - mm/slub.c|2602| <<__slab_free>> set_freepointer(s, object, prior);
+ *   - mm/slub.c|2728| <<slab_free>> set_freepointer(s, object, c->freelist);
+ */
 static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 {
 	*(void **)(object + s->offset) = fp;
@@ -279,11 +356,24 @@ static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 			__p += (__s)->size, __idx++)
 
 /* Determine object index from a given position */
+/*
+ * called by:
+ *   - mm/slub.c|456| <<get_map>> set_bit(slab_index(p, s, addr), map);
+ *   - mm/slub.c|3219| <<list_slab_objects>> if (!test_bit(slab_index(p, s, addr), map)) {
+ *   - mm/slub.c|3889| <<validate_slab>> if (test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|3895| <<validate_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4096| <<process_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ */
 static inline int slab_index(void *p, struct kmem_cache *s, void *addr)
 {
 	return (p - addr) / s->size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1320| <<slab_post_alloc_hook>> kmemcheck_slab_alloc(s, flags, object, slab_ksize(s));
+ *   - mm/slub.c|3431| <<__ksize>> return slab_ksize(page->slab_cache);
+ */
 static inline size_t slab_ksize(const struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
@@ -308,11 +398,24 @@ static inline size_t slab_ksize(const struct kmem_cache *s)
 	return s->size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|362| <<oo_make>> (order << OO_SHIFT) + order_objects(order, size, reserved)
+ *   - mm/slub.c|916| <<check_slab>> maxobj = order_objects(compound_order(page), s->size, s->reserved);
+ *   - mm/slub.c|966| <<on_freelist>> max_objects = order_objects(compound_order(page), s->size, s->reserved);
+ *   - mm/slub.c|2851| <<slab_order>> if (order_objects(min_order, size, reserved) > MAX_OBJS_PER_PAGE)
+ *   - mm/slub.c|2891| <<calculate_order>> max_objects = order_objects(slub_max_order, size, reserved);
+ */
 static inline int order_objects(int order, unsigned long size, int reserved)
 {
 	return ((PAGE_SIZE << order) - reserved) / size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3154| <<calculate_sizes>> s->oo = oo_make(order, size, s->reserved);
+ *   - mm/slub.c|3155| <<calculate_sizes>> s->min = oo_make(get_order(size), size, s->reserved);
+ */
 static inline struct kmem_cache_order_objects oo_make(int order,
 		unsigned long size, int reserved)
 {
@@ -323,11 +426,34 @@ static inline struct kmem_cache_order_objects oo_make(int order,
 	return x;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1358| <<alloc_slab_page>> int order = oo_order(oo);
+ *   - mm/slub.c|1412| <<allocate_slab>> int pages = 1 << oo_order(oo);
+ *   - mm/slub.c|1414| <<allocate_slab>> kmemcheck_alloc_shadow(page, oo_order(oo), alloc_gfp, node);
+ *   - mm/slub.c|1435| <<allocate_slab>> 1 << oo_order(oo));
+ *   - mm/slub.c|2234| <<slab_out_of_memory>> s->name, s->object_size, s->size, oo_order(s->oo),
+ *   - mm/slub.c|2235| <<slab_out_of_memory>> oo_order(s->min));
+ *   - mm/slub.c|2237| <<slab_out_of_memory>> if (oo_order(s->min) > get_order(s->object_size))
+ *   - mm/slub.c|3241| <<kmem_cache_open>> oo_order(s->oo), s->offset, flags);
+ *   - mm/slub.c|4473| <<order_show>> return sprintf(buf, "%d\n", oo_order(s->oo));
+ *   - mm/slub.c|5375| <<get_slabinfo>> sinfo->cache_order = oo_order(s->oo);
+ */
 static inline int oo_order(struct kmem_cache_order_objects x)
 {
 	return x.x >> OO_SHIFT;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1431| <<allocate_slab>> page->objects = oo_objects(oo);
+ *   - mm/slub.c|3156| <<calculate_sizes>> if (oo_objects(s->oo) > oo_objects(s->max))
+ *   - mm/slub.c|3159| <<calculate_sizes>> return !!oo_objects(s->oo);
+ *   - mm/slub.c|3988| <<validate_slab_cache>> unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
+ *   - mm/slub.c|4149| <<list_locations>> unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
+ *   - mm/slub.c|4450| <<objs_per_slab_show>> return sprintf(buf, "%d\n", oo_objects(s->oo));
+ *   - mm/slub.c|5374| <<get_slabinfo>> sinfo->objects_per_slab = oo_objects(s->oo);
+ */
 static inline int oo_objects(struct kmem_cache_order_objects x)
 {
 	return x.x & OO_MASK;
@@ -346,6 +472,15 @@ static __always_inline void slab_unlock(struct page *page)
 	__bit_spin_unlock(PG_locked, &page->flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|427| <<__cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ *   - mm/slub.c|466| <<cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ *
+ * page->counters和{page->frozen, page->inuse, page->objects}属于一个union
+ * 设置page->counters=counters_new...
+ * 等价于设置page->frozen,page->inuse,page->projects
+ */
 static inline void set_page_slub_counters(struct page *page, unsigned long counters_new)
 {
 	struct page tmp;
@@ -362,6 +497,18 @@ static inline void set_page_slub_counters(struct page *page, unsigned long count
 }
 
 /* Interrupts must be disabled (for the fallback code to work right) */
+/*
+ * called by:
+ *   - mm/slub.c|2036| <<acquire_slab>> if (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2298| <<deactivate_slab>> } while (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2387| <<deactivate_slab>> if (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2449| <<unfreeze_partials>> } while (!__cmpxchg_double_slab(s, page,
+ *   - mm/slub.c|2742| <<get_freelist>> } while (!__cmpxchg_double_slab(s, page,
+ *
+ * 我们期待__cmpxchg_double_slab()返回true, 不希望false
+ * 核心思想是判断page->freelist和page->counters是否和old的相等
+ * 如果相等,则把page->freelist和page->counters都更新成新的
+ */
 static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -399,6 +546,14 @@ static inline bool __cmpxchg_double_slab(struct kmem_cache *s, struct page *page
 	return false;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3165| <<__slab_free>> } while (!cmpxchg_double_slab(s, page,
+ *
+ * 我们期待cmpxchg_double_slab()返回true, 不希望false
+ * 核心思想是判断page->freelist和page->counters是否和old的相等
+ * 如果相等,则把page->freelist和page->counters都更新成新的
+ */
 static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
 		void *freelist_old, unsigned long counters_old,
 		void *freelist_new, unsigned long counters_new,
@@ -447,6 +602,12 @@ static inline bool cmpxchg_double_slab(struct kmem_cache *s, struct page *page,
  * Node listlock must be held to guarantee that the page does
  * not vanish from under us.
  */
+/*
+ * called by:
+ *   - mm/slub.c|3756| <<list_slab_objects>> get_map(s, page, map);
+ *   - mm/slub.c|4431| <<validate_slab>> get_map(s, page, map);
+ *   - mm/slub.c|4656| <<process_slab>> get_map(s, page, map);
+ */
 static void get_map(struct kmem_cache *s, struct page *page, unsigned long *map)
 {
 	void *p;
@@ -474,11 +635,25 @@ static int disable_higher_order_debug;
  * be reported by kasan as a bounds error.  metadata_access_enable() is used
  * to tell kasan that these accesses are OK.
  */
+/*
+ * called by:
+ *   - mm/slub.c|622| <<print_section>> metadata_access_enable();
+ *   - mm/slub.c|655| <<set_track>> metadata_access_enable();
+ *   - mm/slub.c|879| <<check_bytes_and_report>> metadata_access_enable();
+ *   - mm/slub.c|987| <<slab_pad_check>> metadata_access_enable();
+ */
 static inline void metadata_access_enable(void)
 {
 	kasan_disable_current();
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|625| <<print_section>> metadata_access_disable();
+ *   - mm/slub.c|657| <<set_track>> metadata_access_disable();
+ *   - mm/slub.c|881| <<check_bytes_and_report>> metadata_access_disable();
+ *   - mm/slub.c|989| <<slab_pad_check>> metadata_access_disable();
+ */
 static inline void metadata_access_disable(void)
 {
 	kasan_enable_current();
@@ -487,6 +662,17 @@ static inline void metadata_access_disable(void)
 /*
  * Object debugging
  */
+/*
+ * called by:
+ *   - mm/slub.c|786| <<print_trailer>> print_section("Bytes b4 ", p - 16, 16);
+ *   - mm/slub.c|788| <<print_trailer>> print_section("Object ", p, min_t(unsigned long , s->object_size,
+ *   - mm/slub.c|791| <<print_trailer>> print_section("Redzone ", p + s->object_size,
+ *   - mm/slub.c|804| <<print_trailer>> print_section("Padding ", p + off, s->size - off);
+ *   - mm/slub.c|996| <<slab_pad_check>> print_section("Padding ", end - remainder, remainder);
+ *   - mm/slub.c|1179| <<trace>> print_section("Object ", (void *)object,
+ *
+ * 就是打印一些hex的数据,没有修改
+ */
 static void print_section(char *text, u8 *addr, unsigned int length)
 {
 	metadata_access_enable();
@@ -495,6 +681,13 @@ static void print_section(char *text, u8 *addr, unsigned int length)
 	metadata_access_disable();
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|644| <<set_track>> struct track *p = get_track(s, object, alloc);
+ *   - mm/slub.c|718| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|719| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ *   - mm/slub.c|4465| <<process_slab>> add_location(t, s, get_track(s, p, alloc));
+ */
 static struct track *get_track(struct kmem_cache *s, void *object,
 	enum track_item alloc)
 {
@@ -508,6 +701,13 @@ static struct track *get_track(struct kmem_cache *s, void *object,
 	return p + alloc;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|680| <<init_tracking>> set_track(s, object, TRACK_FREE, 0UL);
+ *   - mm/slub.c|681| <<init_tracking>> set_track(s, object, TRACK_ALLOC, 0UL);
+ *   - mm/slub.c|1284| <<alloc_debug_processing>> set_track(s, object, TRACK_ALLOC, addr);
+ *   - mm/slub.c|1358| <<free_debug_processing>> set_track(s, object, TRACK_FREE, addr);
+ */
 static void set_track(struct kmem_cache *s, void *object,
 			enum track_item alloc, unsigned long addr)
 {
@@ -542,6 +742,11 @@ static void set_track(struct kmem_cache *s, void *object,
 		memset(p, 0, sizeof(struct track));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1252| <<setup_object_debug>> init_tracking(s, object);
+ *   - mm/slub.c|3308| <<early_kmem_cache_node_alloc>> init_tracking(kmem_cache_node, n);
+ */
 static void init_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -551,6 +756,11 @@ static void init_tracking(struct kmem_cache *s, void *object)
 	set_track(s, object, TRACK_ALLOC, 0UL);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|706| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|707| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ */
 static void print_track(const char *s, struct track *t)
 {
 	if (!t->addr)
@@ -570,6 +780,11 @@ static void print_track(const char *s, struct track *t)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|750| <<print_trailer>> print_tracking(s, p);
+ *   - mm/slub.c|3465| <<list_slab_objects>> print_tracking(s, p);
+ */
 static void print_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -579,6 +794,13 @@ static void print_tracking(struct kmem_cache *s, void *object)
 	print_track("Freed", get_track(s, object, TRACK_FREE));
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|780| <<print_trailer>> print_page_info(page);
+ *   - mm/slub.c|826| <<slab_err>> print_page_info(page);
+ *
+ * 就是用pr_err()打印log, 没有修改
+ */
 static void print_page_info(struct page *page)
 {
 	pr_err("INFO: Slab 0x%p objects=%u used=%u fp=0x%p flags=0x%04lx\n",
@@ -586,6 +808,14 @@ static void print_page_info(struct page *page)
 
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|864| <<object_err>> slab_bug(s, "%s", reason);
+ *   - mm/slub.c|877| <<slab_err>> slab_bug(s, "%s", buf);
+ *   - mm/slub.c|941| <<check_bytes_and_report>> slab_bug(s, "%s overwritten", what);
+ *
+ * 就是打印log, 没有修改
+ */
 static void slab_bug(struct kmem_cache *s, char *fmt, ...)
 {
 	struct va_format vaf;
@@ -602,6 +832,17 @@ static void slab_bug(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|838| <<restore_bytes>> slab_fix(s, "Restoring 0x%p-0x%p=0x%x\n", from, to - 1, data);
+ *   - mm/slub.c|1062| <<on_freelist>> slab_fix(s, "Freelist cleared");
+ *   - mm/slub.c|1080| <<on_freelist>> slab_fix(s, "Number of objects adjusted.");
+ *   - mm/slub.c|1086| <<on_freelist>> slab_fix(s, "Object count adjusted.");
+ *   - mm/slub.c|1211| <<alloc_debug_processing>> slab_fix(s, "Marking all objects used");
+ *   - mm/slub.c|1276| <<free_debug_processing>> slab_fix(s, "Object at 0x%p not freed", object);
+ *
+ * slab_fix()就是打印几句错误
+ */
 static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 {
 	struct va_format vaf;
@@ -614,6 +855,13 @@ static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|785| <<object_err>> print_trailer(s, page, object);
+ *   - mm/slub.c|842| <<check_bytes_and_report>> print_trailer(s, page, object);
+ *
+ * 就是打印log, 没有修改
+ */
 static void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)
 {
 	unsigned int off;	/* Offset of last byte */
@@ -621,11 +869,15 @@ static void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)
 
 	print_tracking(s, p);
 
+	/* 就是用pr_err()打印log, 没有修改 */
 	print_page_info(page);
 
 	pr_err("INFO: Object 0x%p @offset=%tu fp=0x%p\n\n",
 	       p, p - addr, get_freepointer(s, p));
 
+	/*
+	 * print_section()就是打印一些hex的数据,没有修改
+	 */
 	if (p > addr + 16)
 		print_section("Bytes b4 ", p - 16, 16);
 
@@ -650,13 +902,41 @@ static void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)
 	dump_stack();
 }
 
+/*
+ * called by:
+ *   - mm/kasan/report.c|120| <<print_address_description>> object_err(cache, page, object,
+ *   - mm/slub.c|1103| <<check_object>> object_err(s, page, p, "Freepointer corrupt");
+ *   - mm/slub.c|1184| <<on_freelist>> object_err(s, page, object,
+ *   - mm/slub.c|1327| <<alloc_debug_processing>> object_err(s, page, object, "Freelist Pointer check fails");
+ *   - mm/slub.c|1387| <<free_debug_processing>> object_err(s, page, object, "Object already free");
+ *   - mm/slub.c|1404| <<free_debug_processing>> object_err(s, page, object,
+ *
+ * 就是打印log, 没有修改
+ */
 void object_err(struct kmem_cache *s, struct page *page,
 			u8 *object, char *reason)
 {
+	/* 就是打印log, 没有修改 */
 	slab_bug(s, "%s", reason);
+	/* 就是打印log, 没有修改 */
 	print_trailer(s, page, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1047| <<slab_pad_check>> slab_err(s, page, "Padding overwritten. 0x%p-0x%p", fault, end - 1);
+ *   - mm/slub.c|1135| <<check_slab>> slab_err(s, page, "Not a valid slab page");
+ *   - mm/slub.c|1142| <<check_slab>> slab_err(s, page, "objects %u > max %u",
+ *   - mm/slub.c|1148| <<check_slab>> slab_err(s, page, "inuse %u > max %u",
+ *   - mm/slub.c|1188| <<on_freelist>> slab_err(s, page, "Freepointer corrupt");
+ *   - mm/slub.c|1206| <<on_freelist>> slab_err(s, page, "Wrong number of objects. Found %d but "
+ *   - mm/slub.c|1212| <<on_freelist>> slab_err(s, page, "Wrong object count. Counter is %d but "
+ *   - mm/slub.c|1381| <<free_debug_processing>> slab_err(s, page, "Invalid object pointer 0x%p", object);
+ *   - mm/slub.c|1397| <<free_debug_processing>> slab_err(s, page, "Attempt to free object(0x%p) "
+ *   - mm/slub.c|3629| <<list_slab_objects>> slab_err(s, page, text, s->name);
+ *
+ * 就是打印log, 没有修改
+ */
 static void slab_err(struct kmem_cache *s, struct page *page,
 			const char *fmt, ...)
 {
@@ -666,11 +946,19 @@ static void slab_err(struct kmem_cache *s, struct page *page,
 	va_start(args, fmt);
 	vsnprintf(buf, sizeof(buf), fmt, args);
 	va_end(args);
+	/* 就是打印log, 没有修改 */
 	slab_bug(s, "%s", buf);
 	print_page_info(page);
 	dump_stack();
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1187| <<setup_object_debug>> init_object(s, object, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|1214| <<alloc_debug_processing>> init_object(s, object, SLUB_RED_ACTIVE);
+ *   - mm/slub.c|1277| <<free_debug_processing>> init_object(s, object, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|3224| <<early_kmem_cache_node_alloc>> init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);
+ */
 static void init_object(struct kmem_cache *s, void *object, u8 val)
 {
 	u8 *p = object;
@@ -684,13 +972,32 @@ static void init_object(struct kmem_cache *s, void *object, u8 val)
 		memset(p + s->object_size, val, s->inuse - s->object_size);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|844| <<check_bytes_and_report>> restore_bytes(s, what, value, fault, end);
+ *   - mm/slub.c|935| <<slab_pad_check>> restore_bytes(s, "slab padding", POISON_INUSE, end - remainder, end);
+ *
+ * 在用slab_fix()打印log后会用memset()恢复数据!!!!!
+ */
 static void restore_bytes(struct kmem_cache *s, char *message, u8 data,
 						void *from, void *to)
 {
+	/* slab_fix()就是打印几句错 */
 	slab_fix(s, "Restoring 0x%p-0x%p=0x%x\n", from, to - 1, data);
+	/* 这句才是真正restore用的 */
 	memset(from, data, to - from);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|943| <<check_pad_bytes>> return check_bytes_and_report(s, page, p, "Object padding",
+ *   - mm/slub.c|1006| <<check_object>> if (!check_bytes_and_report(s, page, object, "Redzone",
+ *   - mm/slub.c|1011| <<check_object>> check_bytes_and_report(s, page, p, "Alignment padding",
+ *   - mm/slub.c|1019| <<check_object>> (!check_bytes_and_report(s, page, p, "Poison", p,
+ *   - mm/slub.c|1021| <<check_object>> !check_bytes_and_report(s, page, p, "Poison",
+ *
+ * 检查之后会恢复数据---> FIX FIX FIX !!!
+ */
 static int check_bytes_and_report(struct kmem_cache *s, struct page *page,
 			u8 *object, char *what,
 			u8 *start, unsigned int value, unsigned int bytes)
@@ -699,6 +1006,11 @@ static int check_bytes_and_report(struct kmem_cache *s, struct page *page,
 	u8 *end;
 
 	metadata_access_enable();
+	/*
+	 * Find an unmatching character in an area of memory.
+	 * returns the address of the first character other than @c, or %NULL
+	 * if the whole buffer contains just @c.
+	 */
 	fault = memchr_inv(start, value, bytes);
 	metadata_access_disable();
 	if (!fault)
@@ -708,11 +1020,14 @@ static int check_bytes_and_report(struct kmem_cache *s, struct page *page,
 	while (end > fault && end[-1] == value)
 		end--;
 
+	/* 就是打印log, 没有修改 */
 	slab_bug(s, "%s overwritten", what);
 	pr_err("INFO: 0x%p-0x%p. First byte 0x%x instead of 0x%x\n",
 					fault, end - 1, fault[0], value);
+	/* 就是打印log, 没有修改 */
 	print_trailer(s, page, object);
 
+	/* 在用slab_fix()打印log后会用memset()恢复数据!!!!! */
 	restore_bytes(s, what, value, fault, end);
 	return 0;
 }
@@ -755,6 +1070,13 @@ static int check_bytes_and_report(struct kmem_cache *s, struct page *page,
  * may be used with merged slabcaches.
  */
 
+/*
+ * called by:
+ *   - mm/slub.c|1027| <<check_object>> check_pad_bytes(s, page, p);
+ *
+ * 这里是检查object最后(包括debug info)到最后之间的数据是否是POISON_INUSE
+ * 如果不是,恢复修改这些padding!!! ----> FIX FIX FIX!!!!
+ */
 static int check_pad_bytes(struct kmem_cache *s, struct page *page, u8 *p)
 {
 	unsigned long off = s->inuse;	/* The end of info */
@@ -770,11 +1092,20 @@ static int check_pad_bytes(struct kmem_cache *s, struct page *page, u8 *p)
 	if (s->size == off)
 		return 1;
 
+	/* 检查之后会恢复数据---> FIX FIX FIX !!! */
 	return check_bytes_and_report(s, page, p, "Object padding",
 				p + off, POISON_INUSE, s->size - off);
 }
 
 /* Check the pad bytes at the end of a slab page */
+/*
+ * called by:
+ *   - mm/slub.c|1047| <<check_slab>> slab_pad_check(s, page);
+ *   - mm/slub.c|1645| <<__free_slab>> slab_pad_check(s, page);
+ *
+ * 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+ * 不是的话修改过来
+ */
 static int slab_pad_check(struct kmem_cache *s, struct page *page)
 {
 	u8 *start;
@@ -783,6 +1114,7 @@ static int slab_pad_check(struct kmem_cache *s, struct page *page)
 	int length;
 	int remainder;
 
+	/* 如果没有poison直接返回 */
 	if (!(s->flags & SLAB_POISON))
 		return 1;
 
@@ -808,18 +1140,40 @@ static int slab_pad_check(struct kmem_cache *s, struct page *page)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1207| <<alloc_debug_processing>> if (!check_object(s, page, object, SLUB_RED_INACTIVE))
+ *   - mm/slub.c|1257| <<free_debug_processing>> if (!check_object(s, page, object, SLUB_RED_ACTIVE))
+ *   - mm/slub.c|1648| <<__free_slab>> check_object(s, page, p, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|4171| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_INACTIVE))
+ *   - mm/slub.c|4177| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_ACTIVE))
+ *
+ * 我们不希望check_object()返回0
+ */
 static int check_object(struct kmem_cache *s, struct page *page,
 					void *object, u8 val)
 {
 	u8 *p = object;
+	/*
+	 * endobject是object之后, 可能还有一些padding数据
+	 */
 	u8 *endobject = object + s->object_size;
 
 	if (s->flags & SLAB_RED_ZONE) {
+		/*
+		 * 检查之后会恢复数据---> FIX FIX FIX !!!
+		 */
 		if (!check_bytes_and_report(s, page, object, "Redzone",
 			endobject, val, s->inuse - s->object_size))
 			return 0;
 	} else {
 		if ((s->flags & SLAB_POISON) && s->object_size < s->inuse) {
+			/*
+			 * endobject是object之后, 可能还有一些padding数据
+			 * 检查object和fp之间的padding是否是POISON_INUSE(初始化设置的)
+			 *
+			 * 检查之后会恢复数据---> FIX FIX FIX !!!
+			 */
 			check_bytes_and_report(s, page, p, "Alignment padding",
 				endobject, POISON_INUSE,
 				s->inuse - s->object_size);
@@ -827,6 +1181,11 @@ static int check_object(struct kmem_cache *s, struct page *page,
 	}
 
 	if (s->flags & SLAB_POISON) {
+		/*
+		 * 如果是alloc的时候, 检查object是否是POISON_FREE和POISON_END (最后一位)
+		 *
+		 * 检查之后会恢复数据---> FIX FIX FIX !!!
+		 */
 		if (val != SLUB_RED_ACTIVE && (s->flags & __OBJECT_POISON) &&
 			(!check_bytes_and_report(s, page, p, "Poison", p,
 					POISON_FREE, s->object_size - 1) ||
@@ -836,9 +1195,16 @@ static int check_object(struct kmem_cache *s, struct page *page,
 		/*
 		 * check_pad_bytes cleans up on its own.
 		 */
+		/*
+		 * 这里是检查object最后(包括debug info)到最后之间的数据是否是POISON_INUSE
+		 * 如果不是,恢复修改这些padding!!! ----> FIX FIX FIX!!!!
+		 */
 		check_pad_bytes(s, page, p);
 	}
 
+	/*
+	 * 这里是free的情况, 如果fp没单拿出来也没法检查
+	 */
 	if (!s->offset && val == SLUB_RED_ACTIVE)
 		/*
 		 * Object and freepointer overlap. Cannot check
@@ -847,42 +1213,72 @@ static int check_object(struct kmem_cache *s, struct page *page,
 		return 1;
 
 	/* Check free pointer validity */
+	/*
+	 * 查看object的地址是否在page的slab的范围内
+	 * 如果在返回1, 否则返回0
+	 */
 	if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+		/* 就是打印log, 没有修改 */
 		object_err(s, page, p, "Freepointer corrupt");
 		/*
 		 * No choice but to zap it and thus lose the remainder
 		 * of the free objects in this slab. May cause
 		 * another error because the object count is now wrong.
 		 */
+		/*
+		 * 这里会FIX FIX FIX!!!!!!!!!!!
+		 */
 		set_freepointer(s, p, NULL);
 		return 0;
 	}
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1199| <<alloc_debug_processing>> if (!check_slab(s, page))
+ *   - mm/slub.c|1244| <<free_debug_processing>> if (!check_slab(s, page))
+ *   - mm/slub.c|4161| <<validate_slab>> if (!check_slab(s, page) ||
+ *
+ * 1. 检查page是否是个slab page (通过PageSlab)
+ * 2. 检查page中所有object的数目是否超过最大可能的值
+ * 3. 检查哦page中正在使用的数目是否超出了支持的数目
+ * 4. 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+ * 不是的话修改过来 ---> FIX FIX FIX !!!!
+ */
 static int check_slab(struct kmem_cache *s, struct page *page)
 {
 	int maxobj;
 
 	VM_BUG_ON(!irqs_disabled());
 
+	/* 检查page是否是个slab page (通过PageSlab) */
 	if (!PageSlab(page)) {
+		/* 就是打印log, 没有修改 */
 		slab_err(s, page, "Not a valid slab page");
 		return 0;
 	}
 
+	/* 检查page中所有object的数目是否超过最大可能的值 */
 	maxobj = order_objects(compound_order(page), s->size, s->reserved);
 	if (page->objects > maxobj) {
+		/* 就是打印log, 没有修改 */
 		slab_err(s, page, "objects %u > max %u",
 			page->objects, maxobj);
 		return 0;
 	}
+	/* 检查哦page中正在使用的数目是否超出了支持的数目 */
 	if (page->inuse > page->objects) {
+		/* 就是打印log, 没有修改 */
 		slab_err(s, page, "inuse %u > max %u",
 			page->inuse, page->objects);
 		return 0;
 	}
 	/* Slab_pad_check fixes things up after itself */
+	/*
+	 * 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+	 * 不是的话修改过来 --> FIX FIX FIX!!!
+	 */
 	slab_pad_check(s, page);
 	return 1;
 }
@@ -891,6 +1287,18 @@ static int check_slab(struct kmem_cache *s, struct page *page)
  * Determine if a certain object on a page is on the freelist. Must hold the
  * slab lock to guarantee that the chains are in a consistent state.
  */
+/*
+ * called by:
+ *   - mm/slub.c|1316| <<free_debug_processing>> if (on_freelist(s, page, object)) {
+ *   - mm/slub.c|4228| <<validate_slab>> !on_freelist(s, page, NULL))
+ *
+ * 1. 首先遍历page中所有的object, 如果发现要释放的, 则返回1报错
+ * 2. 遍历的过程中如果发现有freelist有问题, 把list直接截断,
+ *    或者把page->freelist直接设置成NULL ---> FIX FIX FIX !!!
+ * 3. 检查这组page的object的数目对不对, 不对修改过来 ---> FIX FIX FIX!
+ *
+ * 期待返回0, 不期待返回1
+ */
 static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 {
 	int nr = 0;
@@ -900,14 +1308,31 @@ static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 
 	fp = page->freelist;
 	while (fp && nr <= page->objects) {
+		/*
+		 * 如果发现要释放的object已经是free的了, 直接返回1报错
+		 */
 		if (fp == search)
 			return 1;
+		/*
+		 * called by:
+		 *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+		 *   - mm/slub.c|905| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+		 *   - mm/slub.c|1037| <<alloc_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+		 *   - mm/slub.c|1078| <<free_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+		 *
+		 * 查看object的地址是否在page的范围内
+		 * 如果在返回1, 否则返回0
+		 */
 		if (!check_valid_pointer(s, page, fp)) {
 			if (object) {
 				object_err(s, page, object,
 					"Freechain corrupt");
+				/* 让上一个object的freelist指向NULL */
 				set_freepointer(s, object, NULL);
 			} else {
+				/*
+				 * page->freelist有问题, 把这个page踢出去
+				 */
 				slab_err(s, page, "Freepointer corrupt");
 				page->freelist = NULL;
 				page->inuse = page->objects;
@@ -916,11 +1341,17 @@ static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 			}
 			break;
 		}
+		/*
+		 * object一开始是NULL, 用来记录上一个object
+		 */
 		object = fp;
 		fp = get_freepointer(s, object);
 		nr++;
 	}
 
+	/*
+	 * 下面检查这组page的object的数目对不对, 不对修改过来
+	 */
 	max_objects = order_objects(compound_order(page), s->size, s->reserved);
 	if (max_objects > MAX_OBJS_PER_PAGE)
 		max_objects = MAX_OBJS_PER_PAGE;
@@ -1027,18 +1458,52 @@ static void setup_object_debug(struct kmem_cache *s, struct page *page,
 	init_tracking(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2564| <<__slab_alloc>> !alloc_debug_processing(s, page, freelist, addr))
+ */
 static noinline int alloc_debug_processing(struct kmem_cache *s,
 					struct page *page,
 					void *object, unsigned long addr)
 {
+	/*
+	 * check_slab()在以下调用:
+	 *   - mm/slub.c|1199| <<alloc_debug_processing>> if (!check_slab(s, page))
+	 *   - mm/slub.c|1244| <<free_debug_processing>> if (!check_slab(s, page))
+	 *   - mm/slub.c|4161| <<validate_slab>> if (!check_slab(s, page) ||
+	 *
+	 * 1. 检查page是否是个slab page (通过PageSlab)
+	 * 2. 检查page中所有object的数目是否超过最大可能的值
+	 * 3. 检查哦page中正在使用的数目是否超出了支持的数目
+	 * 4. 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+	 * 不是的话修改过来 ---> FIX FIX FIX !!!!
+	 */
 	if (!check_slab(s, page))
 		goto bad;
 
+	/*
+	 * check_valid_pointer()在以下调用:
+	 *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+	 *   - mm/slub.c|905| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+	 *   - mm/slub.c|1037| <<alloc_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+	 *   - mm/slub.c|1078| <<free_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+	 *
+	 * 查看object的地址是否在page的范围内
+	 */
 	if (!check_valid_pointer(s, page, object)) {
+		/* 就是打印log, 没有修改 */
 		object_err(s, page, object, "Freelist Pointer check fails");
 		goto bad;
 	}
 
+	/*
+	 * check_object()在以下调用:
+	 *   - mm/slub.c|1207| <<alloc_debug_processing>> if (!check_object(s, page, object, SLUB_RED_INACTIVE))
+	 *   - mm/slub.c|1257| <<free_debug_processing>> if (!check_object(s, page, object, SLUB_RED_ACTIVE))
+	 *   - mm/slub.c|1648| <<__free_slab>> check_object(s, page, p, SLUB_RED_INACTIVE);
+	 *   - mm/slub.c|4171| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_INACTIVE))
+	 *   - mm/slub.c|4177| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_ACTIVE))
+	 */
 	if (!check_object(s, page, object, SLUB_RED_INACTIVE))
 		goto bad;
 
@@ -1056,6 +1521,7 @@ bad:
 		 * to avoid issues in the future. Marking all objects
 		 * as used avoids touching the remaining objects.
 		 */
+		/* slab_fix()就是打印几句错误 */
 		slab_fix(s, "Marking all objects used");
 		page->inuse = page->objects;
 		page->freelist = NULL;
@@ -1063,6 +1529,17 @@ bad:
 	return 0;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2801| <<__slab_free>> !(n = free_debug_processing(s, page, x, addr, &flags)))
+ *
+ * kmem_cache_free() or kfree()
+ *  -> slab_free()
+ *      -> __slab_free()
+ *          -> free_debug_processing()
+ *
+ * 被free的时候object还不在page->freelist上
+ */
 static noinline struct kmem_cache_node *free_debug_processing(
 	struct kmem_cache *s, struct page *page, void *object,
 	unsigned long addr, unsigned long *flags)
@@ -1072,24 +1549,71 @@ static noinline struct kmem_cache_node *free_debug_processing(
 	spin_lock_irqsave(&n->list_lock, *flags);
 	slab_lock(page);
 
+	/*
+	 * 调用check_slab()的地方:
+	 *   - mm/slub.c|1199| <<alloc_debug_processing>> if (!check_slab(s, page))
+	 *   - mm/slub.c|1244| <<free_debug_processing>> if (!check_slab(s, page))
+	 *   - mm/slub.c|4161| <<validate_slab>> if (!check_slab(s, page) ||
+	 *
+	 * 1. 检查page是否是个slab page (通过PageSlab)
+	 * 2. 检查page中所有object的数目是否超过最大可能的值
+	 * 3. 检查哦page中正在使用的数目是否超出了支持的数目
+	 * 4. 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+	 * 不是的话修改过来 ---> FIX FIX FIX !!!!
+	 */
 	if (!check_slab(s, page))
 		goto fail;
 
+	/*
+	 * 调用check_valid_pointer()的地方:
+	 *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+	 *   - mm/slub.c|905| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+	 *   - mm/slub.c|1037| <<alloc_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+	 *   - mm/slub.c|1078| <<free_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+	 *
+	 * 查看object的地址是否在page的范围内
+	 * 如果在返回1, 否则返回0
+	 */
 	if (!check_valid_pointer(s, page, object)) {
+		/* 打印log */
 		slab_err(s, page, "Invalid object pointer 0x%p", object);
 		goto fail;
 	}
 
+	/*
+	 * 调用on_freelist()的地方:
+	 *   - mm/slub.c|1316| <<free_debug_processing>> if (on_freelist(s, page, object)) {
+	 *   - mm/slub.c|4228| <<validate_slab>> !on_freelist(s, page, NULL))
+	 *
+	 * 1. 首先遍历page中所有的object, 如果发现要释放的, 则返回1报错
+	 * 2. 遍历的过程中如果发现有freelist有问题, 把list直接截断,
+	 *    或者把page->freelist直接设置成NULL ---> FIX FIX FIX !!!
+	 * 3. 检查这组page的object的数目对不对, 不对修改过来 ---> FIX FIX FIX!
+	 */
 	if (on_freelist(s, page, object)) {
+		/* 打印log */
 		object_err(s, page, object, "Object already free");
 		goto fail;
 	}
 
+	/*
+	 * 调用check_object的地方:
+	 *   - mm/slub.c|1207| <<alloc_debug_processing>> if (!check_object(s, page, object, SLUB_RED_INACTIVE))
+	 *   - mm/slub.c|1257| <<free_debug_processing>> if (!check_object(s, page, object, SLUB_RED_ACTIVE))
+	 *   - mm/slub.c|1648| <<__free_slab>> check_object(s, page, p, SLUB_RED_INACTIVE);
+	 *   - mm/slub.c|4171| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_INACTIVE))
+	 *   - mm/slub.c|4177| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_ACTIVE))
+	 */
 	if (!check_object(s, page, object, SLUB_RED_ACTIVE))
 		goto out;
 
+	/*
+	 * 检查page的slab是不是要回收的slab!
+	 */
 	if (unlikely(s != page->slab_cache)) {
+		/* object_err()和slab_err()就是打印log, 没有修改 */
 		if (!PageSlab(page)) {
+			/* 就是打印log, 没有修改 */
 			slab_err(s, page, "Attempt to free object(0x%p) "
 				"outside of slab", object);
 		} else if (!page->slab_cache) {
@@ -1117,6 +1641,7 @@ out:
 fail:
 	slab_unlock(page);
 	spin_unlock_irqrestore(&n->list_lock, *flags);
+	/* slab_fix()就是打印几句错误 */
 	slab_fix(s, "Object at 0x%p not freed", object);
 	return NULL;
 }
@@ -1258,6 +1783,10 @@ static inline void kfree_hook(const void *x)
 	kasan_kfree_large(x);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2540| <<slab_alloc_node>> s = slab_pre_alloc_hook(s, gfpflags);
+ */
 static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 						     gfp_t flags)
 {
@@ -1271,6 +1800,10 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 	return memcg_kmem_get_cache(s, flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2614| <<slab_alloc_node>> slab_post_alloc_hook(s, gfpflags, object);
+ */
 static inline void slab_post_alloc_hook(struct kmem_cache *s,
 					gfp_t flags, void *object)
 {
@@ -1281,6 +1814,10 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s,
 	kasan_slab_alloc(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2918| <<slab_free>> slab_free_hook(s, x);
+ */
 static inline void slab_free_hook(struct kmem_cache *s, void *x)
 {
 	kmemleak_free_recursive(x, s->flags);
@@ -1453,6 +1990,11 @@ out:
 	return page;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1868| <<rcu_free_slab>> __free_slab(page->slab_cache, page);
+ *   - mm/slub.c|1891| <<free_slab>> __free_slab(s, page);
+ */
 static void __free_slab(struct kmem_cache *s, struct page *page)
 {
 	int order = compound_order(page);
@@ -1461,6 +2003,10 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
 	if (kmem_cache_debug(s)) {
 		void *p;
 
+		/*
+		 * 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+		 * 不是的话修改过来
+		 */
 		slab_pad_check(s, page);
 		for_each_object(p, s, page_address(page),
 						page->objects)
@@ -1815,6 +2361,13 @@ static void init_kmem_cache_cpus(struct kmem_cache *s)
 /*
  * Remove the cpu slab
  */
+/*
+ * called by:
+ *   - mm/slub.c|2180| <<flush_slab>> deactivate_slab(s, c->page, c->freelist);
+ *   - mm/slub.c|2424| <<__slab_alloc>> deactivate_slab(s, page, c->freelist);
+ *   - mm/slub.c|2437| <<__slab_alloc>> deactivate_slab(s, page, c->freelist);
+ *   - mm/slub.c|2497| <<__slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist));
+ */
 static void deactivate_slab(struct kmem_cache *s, struct page *page,
 				void *freelist)
 {
@@ -1964,6 +2517,12 @@ redo:
  * for the cpu using c (or some other guarantee must be there
  * to guarantee no concurrent accesses).
  */
+/*
+ * called by:
+ *   - mm/slub.c|2203| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|2225| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|2266| <<__flush_cpu_slab>> unfreeze_partials(s, c);
+ */
 static void unfreeze_partials(struct kmem_cache *s,
 		struct kmem_cache_cpu *c)
 {
@@ -2086,6 +2645,11 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2198| <<__flush_cpu_slab>> flush_slab(s, c);
+ *   - mm/slub.c|2318| <<new_slab_objects>> flush_slab(s, c);
+ */
 static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
 {
 	stat(s, CPUSLAB_FLUSH);
@@ -2101,6 +2665,12 @@ static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
  *
  * Called from IPI handler with interrupts disabled.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2208| <<flush_cpu_slab>> __flush_cpu_slab(s, smp_processor_id());
+ *   - mm/slub.c|3741| <<bootstrap>> __flush_cpu_slab(s, smp_processor_id());
+ *   - mm/slub.c|3880| <<slab_cpuup_callback>> __flush_cpu_slab(s, cpu);
+ */
 static inline void __flush_cpu_slab(struct kmem_cache *s, int cpu)
 {
 	struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);
@@ -2303,6 +2873,10 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
  * we need to allocate a new slab. This is the slowest path since it involves
  * a call to the page allocator and the setup of a new slab.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2580| <<slab_alloc_node>> object = __slab_alloc(s, gfpflags, node, addr, c);
+ */
 static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
@@ -2398,10 +2972,28 @@ new_slab:
 	}
 
 	page = c->page;
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 这里如果不包含就goto到load_freelist不走debug路线了
+	 */
 	if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
 		goto load_freelist;
 
 	/* Only entered in the debug case */
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 如果包含, 则调用alloc_debug_processing()
+	 */
 	if (kmem_cache_debug(s) &&
 			!alloc_debug_processing(s, page, freelist, addr))
 		goto new_slab;	/* Slab failed checks. Next slab needed */
@@ -2423,6 +3015,14 @@ new_slab:
  *
  * Otherwise we can simply pick the next object from the lockless free list.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2622| <<slab_alloc>> return slab_alloc_node(s, gfpflags, NUMA_NO_NODE, addr);
+ *   - mm/slub.c|2650| <<kmem_cache_alloc_node>> void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+ *   - mm/slub.c|2664| <<kmem_cache_alloc_node_trace>> void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+ *   - mm/slub.c|3470| <<__kmalloc_node>> ret = slab_alloc_node(s, flags, node, _RET_IP_);
+ *   - mm/slub.c|3958| <<__kmalloc_node_track_caller>> ret = slab_alloc_node(s, gfpflags, node, caller);
+ */
 static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 		gfp_t gfpflags, int node, unsigned long addr)
 {
@@ -2472,6 +3072,12 @@ redo:
 	page = c->page;
 	if (unlikely(!object || !node_match(page, node))) {
 		object = __slab_alloc(s, gfpflags, node, addr, c);
+		/*
+		 * ALLOC_SLOWPATH除了在这里只在一处使用:
+		 *   - mm/slub.c|4962| <<global>> STAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);
+		 *
+		 * 似乎只有在CONFIG_SLUB_STATS的时候才能用sysfs
+		 */
 		stat(s, ALLOC_SLOWPATH);
 	} else {
 		void *next_object = get_freepointer_safe(s, object);
@@ -2499,6 +3105,13 @@ redo:
 			goto redo;
 		}
 		prefetch_freepointer(s, next_object);
+		/*
+		 * 除了这里只在一处使用:
+		 *   - mm/slub.c|4961| <<global>> STAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);
+		 *
+		 * # cat /sys/kernel/slab/kmalloc-128/alloc_slowpath 
+		 * 7128 C0=2995 C1=4133
+		 */
 		stat(s, ALLOC_FASTPATH);
 	}
 
@@ -2510,6 +3123,13 @@ redo:
 	return object;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2627| <<kmem_cache_alloc>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|2639| <<kmem_cache_alloc_trace>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|3425| <<__kmalloc>> ret = slab_alloc(s, flags, _RET_IP_);
+ *   - mm/slub.c|3928| <<__kmalloc_track_caller>> ret = slab_alloc(s, gfpflags, caller);
+ */
 static __always_inline void *slab_alloc(struct kmem_cache *s,
 		gfp_t gfpflags, unsigned long addr)
 {
@@ -2575,6 +3195,14 @@ EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
  * lock and free the item. If there is no additional partial page
  * handling required then we can return immediately.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2949| <<slab_free>> __slab_free(s, page, x, addr);
+ *
+ * kmem_cache_free() or kfree()
+ *  -> slab_free()
+ *      -> __slab_free()
+ */
 static void __slab_free(struct kmem_cache *s, struct page *page,
 			void *x, unsigned long addr)
 {
@@ -2586,12 +3214,31 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 	struct kmem_cache_node *n = NULL;
 	unsigned long uninitialized_var(flags);
 
+	/*
+	 * 除了这里FREE_SLOWPATH只在下面使用:
+	 *   - mm/slub.c|5067| <<global>> STAT_ATTR(FREE_SLOWPATH, free_slowpath);
+	 */
 	stat(s, FREE_SLOWPATH);
 
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 如果有则调用free_debug_processing()
+	 *
+	 * 在这里如果开启了debug但是free_debug_processing()失败了(返回NULL),
+	 * 就不会真正回收这个page了
+	 */
 	if (kmem_cache_debug(s) &&
 		!(n = free_debug_processing(s, page, x, addr, &flags)))
 		return;
 
+	/*
+	 * 下面最主要的一步是把要回收的object的fp设置成page->freelist
+	 */
 	do {
 		if (unlikely(n)) {
 			spin_unlock_irqrestore(&n->list_lock, flags);
@@ -2599,6 +3246,9 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 		}
 		prior = page->freelist;
 		counters = page->counters;
+		/*
+		 * 把要回收的object的fp设置成page->freelist
+		 */
 		set_freepointer(s, object, prior);
 		new.counters = counters;
 		was_frozen = new.frozen;
@@ -2699,6 +3349,11 @@ slab_empty:
  * If fastpath is not possible then fall back to __slab_free where we deal
  * with all sorts of special processing.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2958| <<kmem_cache_free>> slab_free(s, virt_to_head_page(x), x, _RET_IP_);
+ *   - mm/slub.c|3628| <<kfree>> slab_free(page->slab_cache, page, object, _RET_IP_);
+ */
 static __always_inline void slab_free(struct kmem_cache *s,
 			struct page *page, void *x, unsigned long addr)
 {
@@ -3200,6 +3855,10 @@ error:
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3487| <<free_partial>> list_slab_objects(s, page,
+ */
 static void list_slab_objects(struct kmem_cache *s, struct page *page,
 							const char *text)
 {
@@ -3871,6 +4530,10 @@ static int count_total(struct page *page)
 #endif
 
 #ifdef CONFIG_SLUB_DEBUG
+/*
+ * called by:
+ *   - mm/slub.c|4252| <<validate_slab_slab>> validate_slab(s, page, map);
+ */
 static int validate_slab(struct kmem_cache *s, struct page *page,
 						unsigned long *map)
 {
@@ -3898,6 +4561,11 @@ static int validate_slab(struct kmem_cache *s, struct page *page,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4389| <<validate_slab_node>> validate_slab_slab(s, page, map);
+ *   - mm/slub.c|4400| <<validate_slab_node>> validate_slab_slab(s, page, map);
+ */
 static void validate_slab_slab(struct kmem_cache *s, struct page *page,
 						unsigned long *map)
 {
@@ -3906,6 +4574,10 @@ static void validate_slab_slab(struct kmem_cache *s, struct page *page,
 	slab_unlock(page);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4425| <<validate_slab_cache>> count += validate_slab_node(s, n, map);
+ */
 static int validate_slab_node(struct kmem_cache *s,
 		struct kmem_cache_node *n, unsigned long *map)
 {
@@ -3939,6 +4611,16 @@ out:
 	return count;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|4672| <<resiliency_test>> validate_slab_cache(kmalloc_caches[4]);
+ *   - mm/slub.c|4681| <<resiliency_test>> validate_slab_cache(kmalloc_caches[5]);
+ *   - mm/slub.c|4688| <<resiliency_test>> validate_slab_cache(kmalloc_caches[6]);
+ *   - mm/slub.c|4695| <<resiliency_test>> validate_slab_cache(kmalloc_caches[7]);
+ *   - mm/slub.c|4701| <<resiliency_test>> validate_slab_cache(kmalloc_caches[8]);
+ *   - mm/slub.c|4707| <<resiliency_test>> validate_slab_cache(kmalloc_caches[9]);
+ *   - mm/slub.c|5187| <<validate_store>> ret = validate_slab_cache(s);
+ */
 static long validate_slab_cache(struct kmem_cache *s)
 {
 	int node;
-- 
2.17.1

