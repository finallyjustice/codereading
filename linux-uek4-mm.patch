From 5b003ce67a3bab3bfd8ac135d2d07bef402a9af4 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 27 Jan 2020 16:27:23 -0800
Subject: [PATCH 1/1] linux uek4 mm

v4.1.12-124.24.3

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 mm/slab.h |   5 +
 mm/slub.c | 347 ++++++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 352 insertions(+)

diff --git a/mm/slab.h b/mm/slab.h
index 4c3ac12dd644..74ac22a49c7c 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -299,6 +299,11 @@ static inline void slab_init_memcg_params(struct kmem_cache *s)
 }
 #endif /* CONFIG_MEMCG_KMEM */
 
+/*
+ * called by:
+ *   - mm/slab.c|3554| <<kmem_cache_free>> cachep = cache_from_obj(cachep, objp);
+ *   - mm/slub.c|2955| <<kmem_cache_free>> s = cache_from_obj(s, x);
+ */
 static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
 {
 	struct kmem_cache *cachep;
diff --git a/mm/slub.c b/mm/slub.c
index 3d7932cc7084..52fe801ed5d8 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -115,15 +115,55 @@
  * 			the fast path and disables lockless freelists.
  */
 
+ /*
+  * called by:
+  *   - mm/slub.c|130| <<kmem_cache_has_cpu_partial>> return !kmem_cache_debug(s);
+  *   - mm/slub.c|1549| <<__free_slab>> if (kmem_cache_debug(s)) {
+  *   - mm/slub.c|2004| <<deactivate_slab>> if (kmem_cache_debug(s) && !lock) {
+  *   - mm/slub.c|2507| <<__slab_alloc>> if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
+  *   - mm/slub.c|2511| <<__slab_alloc>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2697| <<__slab_free>> if (kmem_cache_debug(s) &&
+  *   - mm/slub.c|2772| <<__slab_free>> if (kmem_cache_debug(s))
+  *
+  * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+  *   - SLAB_RED_ZONE
+  *   - SLAB_POISON
+  *   - SLAB_STORE_USER
+  *   - SLAB_TRACE
+  *   - SLAB_DEBUG_FREE
+  */
 static inline int kmem_cache_debug(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
+	/*
+	 * SLAB_DEBUG_FLAGS包含以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 */
 	return unlikely(s->flags & SLAB_DEBUG_FLAGS);
 #else
 	return 0;
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1742| <<get_partial_node>> if (!kmem_cache_has_cpu_partial(s)
+ *   - mm/slub.c|2714| <<__slab_free>> if (kmem_cache_has_cpu_partial(s) && !prior) {
+ *   - mm/slub.c|2771| <<__slab_free>> if (!kmem_cache_has_cpu_partial(s) && unlikely(!prior)) {
+ *   - mm/slub.c|3279| <<kmem_cache_open>> if (!kmem_cache_has_cpu_partial(s))
+ *   - mm/slub.c|4575| <<cpu_partial_store>> if (objects && !kmem_cache_has_cpu_partial(s))
+ *
+ * 如果kmem_cache->flags设置了SLAB_DEBUG_FLAGS中的以下任何则返回false:
+ *   - SLAB_RED_ZONE
+ *   - SLAB_POISON
+ *   - SLAB_STORE_USER
+ *   - SLAB_TRACE
+ *   - SLAB_DEBUG_FREE
+ */
 static inline bool kmem_cache_has_cpu_partial(struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_CPU_PARTIAL
@@ -225,6 +265,15 @@ static inline void stat(const struct kmem_cache *s, enum stat_item si)
  *******************************************************************/
 
 /* Verify that a pointer has an address that is valid within a slab page */
+/*
+ * called by:
+ *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|905| <<on_freelist>> if (!check_valid_pointer(s, page, fp)) {
+ *   - mm/slub.c|1037| <<alloc_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+ *   - mm/slub.c|1078| <<free_debug_processing>> if (!check_valid_pointer(s, page, object)) {
+ *
+ * 查看object的地址是否在page的范围内
+ */
 static inline int check_valid_pointer(struct kmem_cache *s,
 				struct page *page, const void *object)
 {
@@ -242,6 +291,18 @@ static inline int check_valid_pointer(struct kmem_cache *s,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|262| <<get_freepointer_safe>> p = get_freepointer(s, object);
+ *   - mm/slub.c|455| <<get_map>> for (p = page->freelist; p; p = get_freepointer(s, p))
+ *   - mm/slub.c|627| <<print_trailer>> p, p - addr, get_freepointer(s, p));
+ *   - mm/slub.c|850| <<check_object>> if (!check_valid_pointer(s, page, get_freepointer(s, p))) {
+ *   - mm/slub.c|920| <<on_freelist>> fp = get_freepointer(s, object);
+ *   - mm/slub.c|1843| <<deactivate_slab>> while (freelist && (nextfree = get_freepointer(s, freelist))) {
+ *   - mm/slub.c|2377| <<__slab_alloc>> c->freelist = get_freepointer(s, freelist);
+ *   - mm/slub.c|2409| <<__slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist));
+ *   - mm/slub.c|2942| <<early_kmem_cache_node_alloc>> page->freelist = get_freepointer(kmem_cache_node, n);
+ */
 static inline void *get_freepointer(struct kmem_cache *s, void *object)
 {
 	return *(void **)(object + s->offset);
@@ -252,6 +313,10 @@ static void prefetch_freepointer(const struct kmem_cache *s, void *object)
 	prefetch(object + s->offset);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2477| <<slab_alloc_node>> void *next_object = get_freepointer_safe(s, object);
+ */
 static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 {
 	void *p;
@@ -264,6 +329,17 @@ static inline void *get_freepointer_safe(struct kmem_cache *s, void *object)
 	return p;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|857| <<check_object>> set_freepointer(s, p, NULL);
+ *   - mm/slub.c|909| <<on_freelist>> set_freepointer(s, object, NULL);
+ *   - mm/slub.c|1444| <<new_slab>> set_freepointer(s, p, p + s->size);
+ *   - mm/slub.c|1446| <<new_slab>> set_freepointer(s, p, NULL); 
+ *   - mm/slub.c|1850| <<deactivate_slab>> set_freepointer(s, freelist, prior);
+ *   - mm/slub.c|1887| <<deactivate_slab>> set_freepointer(s, freelist, old.freelist);
+ *   - mm/slub.c|2602| <<__slab_free>> set_freepointer(s, object, prior);
+ *   - mm/slub.c|2728| <<slab_free>> set_freepointer(s, object, c->freelist);
+ */
 static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 {
 	*(void **)(object + s->offset) = fp;
@@ -279,11 +355,24 @@ static inline void set_freepointer(struct kmem_cache *s, void *object, void *fp)
 			__p += (__s)->size, __idx++)
 
 /* Determine object index from a given position */
+/*
+ * called by:
+ *   - mm/slub.c|456| <<get_map>> set_bit(slab_index(p, s, addr), map);
+ *   - mm/slub.c|3219| <<list_slab_objects>> if (!test_bit(slab_index(p, s, addr), map)) {
+ *   - mm/slub.c|3889| <<validate_slab>> if (test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|3895| <<validate_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ *   - mm/slub.c|4096| <<process_slab>> if (!test_bit(slab_index(p, s, addr), map))
+ */
 static inline int slab_index(void *p, struct kmem_cache *s, void *addr)
 {
 	return (p - addr) / s->size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1320| <<slab_post_alloc_hook>> kmemcheck_slab_alloc(s, flags, object, slab_ksize(s));
+ *   - mm/slub.c|3431| <<__ksize>> return slab_ksize(page->slab_cache);
+ */
 static inline size_t slab_ksize(const struct kmem_cache *s)
 {
 #ifdef CONFIG_SLUB_DEBUG
@@ -308,11 +397,24 @@ static inline size_t slab_ksize(const struct kmem_cache *s)
 	return s->size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|362| <<oo_make>> (order << OO_SHIFT) + order_objects(order, size, reserved)
+ *   - mm/slub.c|916| <<check_slab>> maxobj = order_objects(compound_order(page), s->size, s->reserved);
+ *   - mm/slub.c|966| <<on_freelist>> max_objects = order_objects(compound_order(page), s->size, s->reserved);
+ *   - mm/slub.c|2851| <<slab_order>> if (order_objects(min_order, size, reserved) > MAX_OBJS_PER_PAGE)
+ *   - mm/slub.c|2891| <<calculate_order>> max_objects = order_objects(slub_max_order, size, reserved);
+ */
 static inline int order_objects(int order, unsigned long size, int reserved)
 {
 	return ((PAGE_SIZE << order) - reserved) / size;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3154| <<calculate_sizes>> s->oo = oo_make(order, size, s->reserved);
+ *   - mm/slub.c|3155| <<calculate_sizes>> s->min = oo_make(get_order(size), size, s->reserved);
+ */
 static inline struct kmem_cache_order_objects oo_make(int order,
 		unsigned long size, int reserved)
 {
@@ -323,11 +425,34 @@ static inline struct kmem_cache_order_objects oo_make(int order,
 	return x;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1358| <<alloc_slab_page>> int order = oo_order(oo);
+ *   - mm/slub.c|1412| <<allocate_slab>> int pages = 1 << oo_order(oo);
+ *   - mm/slub.c|1414| <<allocate_slab>> kmemcheck_alloc_shadow(page, oo_order(oo), alloc_gfp, node);
+ *   - mm/slub.c|1435| <<allocate_slab>> 1 << oo_order(oo));
+ *   - mm/slub.c|2234| <<slab_out_of_memory>> s->name, s->object_size, s->size, oo_order(s->oo),
+ *   - mm/slub.c|2235| <<slab_out_of_memory>> oo_order(s->min));
+ *   - mm/slub.c|2237| <<slab_out_of_memory>> if (oo_order(s->min) > get_order(s->object_size))
+ *   - mm/slub.c|3241| <<kmem_cache_open>> oo_order(s->oo), s->offset, flags);
+ *   - mm/slub.c|4473| <<order_show>> return sprintf(buf, "%d\n", oo_order(s->oo));
+ *   - mm/slub.c|5375| <<get_slabinfo>> sinfo->cache_order = oo_order(s->oo);
+ */
 static inline int oo_order(struct kmem_cache_order_objects x)
 {
 	return x.x >> OO_SHIFT;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1431| <<allocate_slab>> page->objects = oo_objects(oo);
+ *   - mm/slub.c|3156| <<calculate_sizes>> if (oo_objects(s->oo) > oo_objects(s->max))
+ *   - mm/slub.c|3159| <<calculate_sizes>> return !!oo_objects(s->oo);
+ *   - mm/slub.c|3988| <<validate_slab_cache>> unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
+ *   - mm/slub.c|4149| <<list_locations>> unsigned long *map = kmalloc(BITS_TO_LONGS(oo_objects(s->max)) *
+ *   - mm/slub.c|4450| <<objs_per_slab_show>> return sprintf(buf, "%d\n", oo_objects(s->oo));
+ *   - mm/slub.c|5374| <<get_slabinfo>> sinfo->objects_per_slab = oo_objects(s->oo);
+ */
 static inline int oo_objects(struct kmem_cache_order_objects x)
 {
 	return x.x & OO_MASK;
@@ -346,6 +471,11 @@ static __always_inline void slab_unlock(struct page *page)
 	__bit_spin_unlock(PG_locked, &page->flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|427| <<__cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ *   - mm/slub.c|466| <<cmpxchg_double_slab>> set_page_slub_counters(page, counters_new);
+ */
 static inline void set_page_slub_counters(struct page *page, unsigned long counters_new)
 {
 	struct page tmp;
@@ -551,6 +681,11 @@ static void init_tracking(struct kmem_cache *s, void *object)
 	set_track(s, object, TRACK_ALLOC, 0UL);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|706| <<print_tracking>> print_track("Allocated", get_track(s, object, TRACK_ALLOC));
+ *   - mm/slub.c|707| <<print_tracking>> print_track("Freed", get_track(s, object, TRACK_FREE));
+ */
 static void print_track(const char *s, struct track *t)
 {
 	if (!t->addr)
@@ -570,6 +705,11 @@ static void print_track(const char *s, struct track *t)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|750| <<print_trailer>> print_tracking(s, p);
+ *   - mm/slub.c|3465| <<list_slab_objects>> print_tracking(s, p);
+ */
 static void print_tracking(struct kmem_cache *s, void *object)
 {
 	if (!(s->flags & SLAB_STORE_USER))
@@ -602,6 +742,17 @@ static void slab_bug(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|838| <<restore_bytes>> slab_fix(s, "Restoring 0x%p-0x%p=0x%x\n", from, to - 1, data);
+ *   - mm/slub.c|1062| <<on_freelist>> slab_fix(s, "Freelist cleared");
+ *   - mm/slub.c|1080| <<on_freelist>> slab_fix(s, "Number of objects adjusted.");
+ *   - mm/slub.c|1086| <<on_freelist>> slab_fix(s, "Object count adjusted.");
+ *   - mm/slub.c|1211| <<alloc_debug_processing>> slab_fix(s, "Marking all objects used");
+ *   - mm/slub.c|1276| <<free_debug_processing>> slab_fix(s, "Object at 0x%p not freed", object);
+ *
+ * slab_fix()就是打印几句错误
+ */
 static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 {
 	struct va_format vaf;
@@ -614,6 +765,11 @@ static void slab_fix(struct kmem_cache *s, char *fmt, ...)
 	va_end(args);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|785| <<object_err>> print_trailer(s, page, object);
+ *   - mm/slub.c|842| <<check_bytes_and_report>> print_trailer(s, page, object);
+ */
 static void print_trailer(struct kmem_cache *s, struct page *page, u8 *p)
 {
 	unsigned int off;	/* Offset of last byte */
@@ -671,6 +827,13 @@ static void slab_err(struct kmem_cache *s, struct page *page,
 	dump_stack();
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1187| <<setup_object_debug>> init_object(s, object, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|1214| <<alloc_debug_processing>> init_object(s, object, SLUB_RED_ACTIVE);
+ *   - mm/slub.c|1277| <<free_debug_processing>> init_object(s, object, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|3224| <<early_kmem_cache_node_alloc>> init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);
+ */
 static void init_object(struct kmem_cache *s, void *object, u8 val)
 {
 	u8 *p = object;
@@ -684,10 +847,17 @@ static void init_object(struct kmem_cache *s, void *object, u8 val)
 		memset(p + s->object_size, val, s->inuse - s->object_size);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|844| <<check_bytes_and_report>> restore_bytes(s, what, value, fault, end);
+ *   - mm/slub.c|935| <<slab_pad_check>> restore_bytes(s, "slab padding", POISON_INUSE, end - remainder, end);
+ */
 static void restore_bytes(struct kmem_cache *s, char *message, u8 data,
 						void *from, void *to)
 {
+	/* slab_fix()就是打印几句错 */
 	slab_fix(s, "Restoring 0x%p-0x%p=0x%x\n", from, to - 1, data);
+	/* 这句才是真正restore用的 */
 	memset(from, data, to - from);
 }
 
@@ -775,6 +945,14 @@ static int check_pad_bytes(struct kmem_cache *s, struct page *page, u8 *p)
 }
 
 /* Check the pad bytes at the end of a slab page */
+/*
+ * called by:
+ *   - mm/slub.c|1047| <<check_slab>> slab_pad_check(s, page);
+ *   - mm/slub.c|1645| <<__free_slab>> slab_pad_check(s, page);
+ *
+ * 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+ * 不是的话修改过来
+ */
 static int slab_pad_check(struct kmem_cache *s, struct page *page)
 {
 	u8 *start;
@@ -783,6 +961,7 @@ static int slab_pad_check(struct kmem_cache *s, struct page *page)
 	int length;
 	int remainder;
 
+	/* 如果没有poison直接返回 */
 	if (!(s->flags & SLAB_POISON))
 		return 1;
 
@@ -808,6 +987,15 @@ static int slab_pad_check(struct kmem_cache *s, struct page *page)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1207| <<alloc_debug_processing>> if (!check_object(s, page, object, SLUB_RED_INACTIVE))
+ *   - mm/slub.c|1257| <<free_debug_processing>> if (!check_object(s, page, object, SLUB_RED_ACTIVE))
+ *   - mm/slub.c|1387| <<check_object>> static inline int check_object(struct kmem_cache *s, struct page *page,
+ *   - mm/slub.c|1648| <<__free_slab>> check_object(s, page, p, SLUB_RED_INACTIVE);
+ *   - mm/slub.c|4171| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_INACTIVE))
+ *   - mm/slub.c|4177| <<validate_slab>> if (!check_object(s, page, p, SLUB_RED_ACTIVE))
+ */
 static int check_object(struct kmem_cache *s, struct page *page,
 					void *object, u8 val)
 {
@@ -860,29 +1048,48 @@ static int check_object(struct kmem_cache *s, struct page *page,
 	return 1;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|1199| <<alloc_debug_processing>> if (!check_slab(s, page))
+ *   - mm/slub.c|1244| <<free_debug_processing>> if (!check_slab(s, page))
+ *   - mm/slub.c|4161| <<validate_slab>> if (!check_slab(s, page) ||
+ *
+ * 1. 检查page是否是个slab page (通过PageSlab)
+ * 2. 检查page中所有object的数目是否超过最大可能的值
+ * 3. 检查哦page中正在使用的数目是否超出了支持的数目
+ * 4. 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+ * 不是的话修改过来
+ */
 static int check_slab(struct kmem_cache *s, struct page *page)
 {
 	int maxobj;
 
 	VM_BUG_ON(!irqs_disabled());
 
+	/* 检查page是否是个slab page (通过PageSlab) */
 	if (!PageSlab(page)) {
 		slab_err(s, page, "Not a valid slab page");
 		return 0;
 	}
 
+	/* 检查page中所有object的数目是否超过最大可能的值 */
 	maxobj = order_objects(compound_order(page), s->size, s->reserved);
 	if (page->objects > maxobj) {
 		slab_err(s, page, "objects %u > max %u",
 			page->objects, maxobj);
 		return 0;
 	}
+	/* 检查哦page中正在使用的数目是否超出了支持的数目 */
 	if (page->inuse > page->objects) {
 		slab_err(s, page, "inuse %u > max %u",
 			page->inuse, page->objects);
 		return 0;
 	}
 	/* Slab_pad_check fixes things up after itself */
+	/*
+	 * 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+	 * 不是的话修改过来
+	 */
 	slab_pad_check(s, page);
 	return 1;
 }
@@ -902,6 +1109,7 @@ static int on_freelist(struct kmem_cache *s, struct page *page, void *search)
 	while (fp && nr <= page->objects) {
 		if (fp == search)
 			return 1;
+		/* 查看object的地址是否在page的范围内 */
 		if (!check_valid_pointer(s, page, fp)) {
 			if (object) {
 				object_err(s, page, object,
@@ -1027,13 +1235,25 @@ static void setup_object_debug(struct kmem_cache *s, struct page *page,
 	init_tracking(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2564| <<__slab_alloc>> !alloc_debug_processing(s, page, freelist, addr))
+ */
 static noinline int alloc_debug_processing(struct kmem_cache *s,
 					struct page *page,
 					void *object, unsigned long addr)
 {
+	/*
+	 * 1. 检查page是否是个slab page (通过PageSlab)
+	 * 2. 检查page中所有object的数目是否超过最大可能的值
+	 * 3. 检查哦page中正在使用的数目是否超出了支持的数目
+	 * 4. 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+	 * 不是的话修改过来
+	 */
 	if (!check_slab(s, page))
 		goto bad;
 
+	/* 查看object的地址是否在page的范围内 */
 	if (!check_valid_pointer(s, page, object)) {
 		object_err(s, page, object, "Freelist Pointer check fails");
 		goto bad;
@@ -1063,6 +1283,10 @@ bad:
 	return 0;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2801| <<__slab_free>> !(n = free_debug_processing(s, page, x, addr, &flags)))
+ */
 static noinline struct kmem_cache_node *free_debug_processing(
 	struct kmem_cache *s, struct page *page, void *object,
 	unsigned long addr, unsigned long *flags)
@@ -1072,15 +1296,25 @@ static noinline struct kmem_cache_node *free_debug_processing(
 	spin_lock_irqsave(&n->list_lock, *flags);
 	slab_lock(page);
 
+	/*
+	 * 1. 检查page是否是个slab page (通过PageSlab)
+	 * 2. 检查page中所有object的数目是否超过最大可能的值
+	 * 3. 检查哦page中正在使用的数目是否超出了支持的数目
+	 * 4. 检测page所属的那一组page们的结尾部分是不是都是POISON_INUSE
+	 * 不是的话修改过来
+	 */
 	if (!check_slab(s, page))
 		goto fail;
 
+	/* 查看object的地址是否在page的范围内 */
 	if (!check_valid_pointer(s, page, object)) {
+		/* 打印log */
 		slab_err(s, page, "Invalid object pointer 0x%p", object);
 		goto fail;
 	}
 
 	if (on_freelist(s, page, object)) {
+		/* 打印log */
 		object_err(s, page, object, "Object already free");
 		goto fail;
 	}
@@ -1090,6 +1324,7 @@ static noinline struct kmem_cache_node *free_debug_processing(
 
 	if (unlikely(s != page->slab_cache)) {
 		if (!PageSlab(page)) {
+			/* 打印log */
 			slab_err(s, page, "Attempt to free object(0x%p) "
 				"outside of slab", object);
 		} else if (!page->slab_cache) {
@@ -1258,6 +1493,10 @@ static inline void kfree_hook(const void *x)
 	kasan_kfree_large(x);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2540| <<slab_alloc_node>> s = slab_pre_alloc_hook(s, gfpflags);
+ */
 static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 						     gfp_t flags)
 {
@@ -1271,6 +1510,10 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,
 	return memcg_kmem_get_cache(s, flags);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2614| <<slab_alloc_node>> slab_post_alloc_hook(s, gfpflags, object);
+ */
 static inline void slab_post_alloc_hook(struct kmem_cache *s,
 					gfp_t flags, void *object)
 {
@@ -1281,6 +1524,10 @@ static inline void slab_post_alloc_hook(struct kmem_cache *s,
 	kasan_slab_alloc(s, object);
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2918| <<slab_free>> slab_free_hook(s, x);
+ */
 static inline void slab_free_hook(struct kmem_cache *s, void *x)
 {
 	kmemleak_free_recursive(x, s->flags);
@@ -1815,6 +2062,13 @@ static void init_kmem_cache_cpus(struct kmem_cache *s)
 /*
  * Remove the cpu slab
  */
+/*
+ * called by:
+ *   - mm/slub.c|2180| <<flush_slab>> deactivate_slab(s, c->page, c->freelist);
+ *   - mm/slub.c|2424| <<__slab_alloc>> deactivate_slab(s, page, c->freelist);
+ *   - mm/slub.c|2437| <<__slab_alloc>> deactivate_slab(s, page, c->freelist);
+ *   - mm/slub.c|2497| <<__slab_alloc>> deactivate_slab(s, page, get_freepointer(s, freelist));
+ */
 static void deactivate_slab(struct kmem_cache *s, struct page *page,
 				void *freelist)
 {
@@ -1964,6 +2218,12 @@ redo:
  * for the cpu using c (or some other guarantee must be there
  * to guarantee no concurrent accesses).
  */
+/*
+ * called by:
+ *   - mm/slub.c|2203| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|2225| <<put_cpu_partial>> unfreeze_partials(s, this_cpu_ptr(s->cpu_slab));
+ *   - mm/slub.c|2266| <<__flush_cpu_slab>> unfreeze_partials(s, c);
+ */
 static void unfreeze_partials(struct kmem_cache *s,
 		struct kmem_cache_cpu *c)
 {
@@ -2086,6 +2346,11 @@ static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
 #endif
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2198| <<__flush_cpu_slab>> flush_slab(s, c);
+ *   - mm/slub.c|2318| <<new_slab_objects>> flush_slab(s, c);
+ */
 static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
 {
 	stat(s, CPUSLAB_FLUSH);
@@ -2101,6 +2366,12 @@ static inline void flush_slab(struct kmem_cache *s, struct kmem_cache_cpu *c)
  *
  * Called from IPI handler with interrupts disabled.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2208| <<flush_cpu_slab>> __flush_cpu_slab(s, smp_processor_id());
+ *   - mm/slub.c|3741| <<bootstrap>> __flush_cpu_slab(s, smp_processor_id());
+ *   - mm/slub.c|3880| <<slab_cpuup_callback>> __flush_cpu_slab(s, cpu);
+ */
 static inline void __flush_cpu_slab(struct kmem_cache *s, int cpu)
 {
 	struct kmem_cache_cpu *c = per_cpu_ptr(s->cpu_slab, cpu);
@@ -2303,6 +2574,10 @@ static inline void *get_freelist(struct kmem_cache *s, struct page *page)
  * we need to allocate a new slab. This is the slowest path since it involves
  * a call to the page allocator and the setup of a new slab.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2580| <<slab_alloc_node>> object = __slab_alloc(s, gfpflags, node, addr, c);
+ */
 static void *__slab_alloc(struct kmem_cache *s, gfp_t gfpflags, int node,
 			  unsigned long addr, struct kmem_cache_cpu *c)
 {
@@ -2398,10 +2673,28 @@ new_slab:
 	}
 
 	page = c->page;
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 这里如果不包含就goto到load_freelist不走debug路线了
+	 */
 	if (likely(!kmem_cache_debug(s) && pfmemalloc_match(page, gfpflags)))
 		goto load_freelist;
 
 	/* Only entered in the debug case */
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 如果包含, 则调用alloc_debug_processing()
+	 */
 	if (kmem_cache_debug(s) &&
 			!alloc_debug_processing(s, page, freelist, addr))
 		goto new_slab;	/* Slab failed checks. Next slab needed */
@@ -2423,6 +2716,14 @@ new_slab:
  *
  * Otherwise we can simply pick the next object from the lockless free list.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2622| <<slab_alloc>> return slab_alloc_node(s, gfpflags, NUMA_NO_NODE, addr);
+ *   - mm/slub.c|2650| <<kmem_cache_alloc_node>> void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+ *   - mm/slub.c|2664| <<kmem_cache_alloc_node_trace>> void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
+ *   - mm/slub.c|3470| <<__kmalloc_node>> ret = slab_alloc_node(s, flags, node, _RET_IP_);
+ *   - mm/slub.c|3958| <<__kmalloc_node_track_caller>> ret = slab_alloc_node(s, gfpflags, node, caller);
+ */
 static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 		gfp_t gfpflags, int node, unsigned long addr)
 {
@@ -2472,6 +2773,12 @@ redo:
 	page = c->page;
 	if (unlikely(!object || !node_match(page, node))) {
 		object = __slab_alloc(s, gfpflags, node, addr, c);
+		/*
+		 * ALLOC_SLOWPATH除了在这里只在一处使用:
+		 *   - mm/slub.c|4962| <<global>> STAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);
+		 *
+		 * 似乎只有在CONFIG_SLUB_STATS的时候才能用sysfs
+		 */
 		stat(s, ALLOC_SLOWPATH);
 	} else {
 		void *next_object = get_freepointer_safe(s, object);
@@ -2499,6 +2806,13 @@ redo:
 			goto redo;
 		}
 		prefetch_freepointer(s, next_object);
+		/*
+		 * 除了这里只在一处使用:
+		 *   - mm/slub.c|4961| <<global>> STAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);
+		 *
+		 * # cat /sys/kernel/slab/kmalloc-128/alloc_slowpath 
+		 * 7128 C0=2995 C1=4133
+		 */
 		stat(s, ALLOC_FASTPATH);
 	}
 
@@ -2510,6 +2824,13 @@ redo:
 	return object;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|2627| <<kmem_cache_alloc>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|2639| <<kmem_cache_alloc_trace>> void *ret = slab_alloc(s, gfpflags, _RET_IP_);
+ *   - mm/slub.c|3425| <<__kmalloc>> ret = slab_alloc(s, flags, _RET_IP_);
+ *   - mm/slub.c|3928| <<__kmalloc_track_caller>> ret = slab_alloc(s, gfpflags, caller);
+ */
 static __always_inline void *slab_alloc(struct kmem_cache *s,
 		gfp_t gfpflags, unsigned long addr)
 {
@@ -2575,6 +2896,10 @@ EXPORT_SYMBOL(kmem_cache_alloc_node_trace);
  * lock and free the item. If there is no additional partial page
  * handling required then we can return immediately.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2949| <<slab_free>> __slab_free(s, page, x, addr);
+ */
 static void __slab_free(struct kmem_cache *s, struct page *page,
 			void *x, unsigned long addr)
 {
@@ -2586,8 +2911,21 @@ static void __slab_free(struct kmem_cache *s, struct page *page,
 	struct kmem_cache_node *n = NULL;
 	unsigned long uninitialized_var(flags);
 
+	/*
+	 * 除了这里FREE_SLOWPATH只在下面使用:
+	 *   - mm/slub.c|5067| <<global>> STAT_ATTR(FREE_SLOWPATH, free_slowpath);
+	 */
 	stat(s, FREE_SLOWPATH);
 
+	/*
+	 * 判断kmem_cache->flags是否有SLAB_DEBUG_FLAGS包含的以下:
+	 *   - SLAB_RED_ZONE
+	 *   - SLAB_POISON
+	 *   - SLAB_STORE_USER
+	 *   - SLAB_TRACE
+	 *   - SLAB_DEBUG_FREE
+	 * 如果有则调用free_debug_processing()
+	 */
 	if (kmem_cache_debug(s) &&
 		!(n = free_debug_processing(s, page, x, addr, &flags)))
 		return;
@@ -2699,6 +3037,11 @@ slab_empty:
  * If fastpath is not possible then fall back to __slab_free where we deal
  * with all sorts of special processing.
  */
+/*
+ * called by:
+ *   - mm/slub.c|2958| <<kmem_cache_free>> slab_free(s, virt_to_head_page(x), x, _RET_IP_);
+ *   - mm/slub.c|3628| <<kfree>> slab_free(page->slab_cache, page, object, _RET_IP_);
+ */
 static __always_inline void slab_free(struct kmem_cache *s,
 			struct page *page, void *x, unsigned long addr)
 {
@@ -3200,6 +3543,10 @@ error:
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - mm/slub.c|3487| <<free_partial>> list_slab_objects(s, page,
+ */
 static void list_slab_objects(struct kmem_cache *s, struct page *page,
 							const char *text)
 {
-- 
2.17.1

