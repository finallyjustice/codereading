From 75e6afce5c79aac0b8e9a254fd3e8038a4e6a79d Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Fri, 31 Aug 2018 13:33:28 +0800
Subject: [PATCH 1/1] xen-swiotlb: uek4 source code comment

based on v4.1.12-112.16.7

Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
---
 arch/x86/include/asm/dma-mapping.h |   4 +
 arch/x86/xen/pci-swiotlb-xen.c     |   4 +
 drivers/xen/swiotlb-xen.c          | 268 +++++++++++++++++++++++++++-
 include/linux/device.h             |   2 +
 include/linux/dma-direction.h      |   8 +
 lib/swiotlb.c                      | 348 ++++++++++++++++++++++++++++++++++++-
 6 files changed, 631 insertions(+), 3 deletions(-)

diff --git a/arch/x86/include/asm/dma-mapping.h b/arch/x86/include/asm/dma-mapping.h
index 808dae6..95535ca 100644
--- a/arch/x86/include/asm/dma-mapping.h
+++ b/arch/x86/include/asm/dma-mapping.h
@@ -100,6 +100,10 @@ dma_cache_sync(struct device *dev, void *vaddr, size_t size,
 	flush_write_buffers();
 }
 
+/*
+ * 如果dev->coherent_dma_mask已经有了就返回dev->coherent_dma_mask
+ * 否则根据gfp决定是DMA_BIT_MASK(24)还是DMA_BIT_MASK(32)
+ */
 static inline unsigned long dma_alloc_coherent_mask(struct device *dev,
 						    gfp_t gfp)
 {
diff --git a/arch/x86/xen/pci-swiotlb-xen.c b/arch/x86/xen/pci-swiotlb-xen.c
index 0e98e5d..6e6fbc1 100644
--- a/arch/x86/xen/pci-swiotlb-xen.c
+++ b/arch/x86/xen/pci-swiotlb-xen.c
@@ -16,6 +16,9 @@
 #endif
 #include <linux/export.h>
 
+/*
+ * xen_swiotlb 在测试机器是 1
+ */
 int xen_swiotlb __read_mostly;
 
 static struct dma_map_ops xen_swiotlb_dma_ops = {
@@ -55,6 +58,7 @@ int __init pci_xen_swiotlb_detect(void)
 	/* If we are running under Xen, we MUST disable the native SWIOTLB.
 	 * Don't worry about swiotlb_force flag activating the native, as
 	 * the 'swiotlb' flag is the only one turning it on. */
+	/* 在xen上是 0 */
 	swiotlb = 0;
 
 #ifdef CONFIG_X86_64
diff --git a/drivers/xen/swiotlb-xen.c b/drivers/xen/swiotlb-xen.c
index 63ddeb6..7c33acd 100644
--- a/drivers/xen/swiotlb-xen.c
+++ b/drivers/xen/swiotlb-xen.c
@@ -33,6 +33,14 @@
  *
  */
 
+/*
+ * iommu (iommu_setup) is configured at arch/x86/kernel/pci-dma.c
+ *
+ * iommu=soft 似乎在所有操作系统上是default (swiotlb = 1)
+ *
+ * 在xen上swiotlb = 0
+ */
+
 #define pr_fmt(fmt) "xen:" KBUILD_MODNAME ": " fmt
 
 #include <linux/bootmem.h>
@@ -53,6 +61,12 @@
  * API.
  */
 
+/* 
+ * 忽略吧 和x86无关
+ *
+ * 对于x86 如果dev->coherent_dma_mask已经有了就返回dev->coherent_dma_mask
+ * 否则根据gfp决定是DMA_BIT_MASK(24)还是DMA_BIT_MASK(32)
+ */
 #ifndef CONFIG_X86
 static unsigned long dma_alloc_coherent_mask(struct device *dev,
 					    gfp_t gfp)
@@ -67,14 +81,28 @@ static unsigned long dma_alloc_coherent_mask(struct device *dev,
 }
 #endif
 
+/*
+ * xen_io_tlb_start: swiotlb预留map用的起始虚拟地址
+ * xen_io_tlb_end: swiotlb预留map用的终止虚拟地址
+ *
+ * 在测试机器上 (start和end之间是64MB):
+ * xen_io_tlb_start  = 0xffff8804a2a00000
+ * xen_io_tlb_end    = 0xffff8804a6a00000
+ * xen_io_tlb_nslabs = 32768
+ */
 static char *xen_io_tlb_start, *xen_io_tlb_end;
 static unsigned long xen_io_tlb_nslabs;
 /*
  * Quick lookup value of the bus address of the IOTLB.
  */
 
+/*
+ * 在测试机上是0x1c0000
+ *
+ * 在xen_swiotlb_init()被配置成xen_virt_to_bus(xen_io_tlb_start)
+ */
 static u64 start_dma_addr;
-
+ 
 /*
  * Both of these functions should avoid XEN_PFN_PHYS because phys_addr_t
  * can be 32bit when dma_addr_t is 64bit leading to a loss in
@@ -106,6 +134,11 @@ static inline dma_addr_t xen_virt_to_bus(void *address)
 	return xen_phys_to_bus(virt_to_phys(address));
 }
 
+/*
+ * 检查xen_pfn们对应的mfn是否机器连续
+ *
+ * 返回 1 说明连续
+ */
 static int check_pages_physically_contiguous(unsigned long xen_pfn,
 					     unsigned int offset,
 					     size_t length)
@@ -124,6 +157,7 @@ static int check_pages_physically_contiguous(unsigned long xen_pfn,
 	return 1;
 }
 
+/* 返回 0 说明连续 */
 static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 {
 	unsigned long xen_pfn = XEN_PFN_DOWN(p);
@@ -131,11 +165,16 @@ static inline int range_straddles_page_boundary(phys_addr_t p, size_t size)
 
 	if (offset + size <= XEN_PAGE_SIZE)
 		return 0;
+	/*
+	 * 检查xen_pfn们对应的mfn是否机器连续
+	 * 返回1说明连续
+	 */
 	if (check_pages_physically_contiguous(xen_pfn, offset, size))
 		return 0;
 	return 1;
 }
 
+/* 判断地址是否在swiotlb预留范围内 */
 static int is_xen_swiotlb_buffer(dma_addr_t dma_addr)
 {
 	unsigned long bfn = XEN_PFN_DOWN(dma_addr);
@@ -155,14 +194,33 @@ static int is_xen_swiotlb_buffer(dma_addr_t dma_addr)
 
 static int max_dma_bits = 32;
 
+/*
+ * called only by xen_swiotlb_init()
+ *
+ * 把预分配的页面换成连续的
+ *
+ * 在测试机上,
+ *
+ * buf    = 0xffff8804a2a00000 
+ * size   = 64MB
+ * nslabs = 32768
+ */
 static int
 xen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)
 {
 	int i, rc;
 	int dma_bits;
 	dma_addr_t dma_handle;
+	/* 物理地址应该是0x4a2a00000 */
 	phys_addr_t p = virt_to_phys(buf);
 
+	/*
+	 * IO_TLB_SEGSIZE: Maximum allowable number of contiguous slabs to map
+	 * IO_TLB_SHIFT: log of the size of each IO TLB slab
+	 *
+	 * IO_TLB_SEGSIZE = 128, 左移11位是262144=0x40000 (256KB)
+	 * order是6, 加上PAGE_SHIFT后, dma_bits = 18 
+	 */
 	dma_bits = get_order(IO_TLB_SEGSIZE << IO_TLB_SHIFT) + PAGE_SHIFT;
 
 	i = 0;
@@ -170,6 +228,9 @@ xen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)
 		int slabs = min(nslabs - i, (unsigned long)IO_TLB_SEGSIZE);
 
 		do {
+			/*
+			 * 我们期待rc返回0
+			 */
 			rc = xen_create_contiguous_region(
 				p + (i << IO_TLB_SHIFT),
 				get_order(slabs << IO_TLB_SHIFT),
@@ -182,6 +243,12 @@ xen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)
 	} while (i < nslabs);
 	return 0;
 }
+
+/*
+ * called only by xen_swiotlb_init()
+ *
+ * nr_tlb在测试机是32768不为空, 左移11位返回67108864=0x4000000 (64MB)
+ */
 static unsigned long xen_set_nslabs(unsigned long nr_tbl)
 {
 	if (!nr_tbl) {
@@ -190,6 +257,7 @@ static unsigned long xen_set_nslabs(unsigned long nr_tbl)
 	} else
 		xen_io_tlb_nslabs = nr_tbl;
 
+	/* xen_io_tlb_nslabs在测试机是32768, 左移11位是67108864=0x4000000 (64MB) */
 	return xen_io_tlb_nslabs << IO_TLB_SHIFT;
 }
 
@@ -199,6 +267,9 @@ enum xen_swiotlb_err {
 	XEN_SWIOTLB_EFIXUP
 };
 
+/*
+ * 被xen_swiotlb_init()两次调用
+ */
 static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 {
 	switch (err) {
@@ -214,6 +285,21 @@ static const char *xen_swiotlb_error(enum xen_swiotlb_err err)
 	}
 	return "";
 }
+
+/*
+ * 可能被如下两个调用:
+ *   - pci_xen_swiotlb_init()
+ *   - pci_xen_swiotlb_init_late()
+ *
+ * 在virtualbox上测试upstream linux, xen_swiotlb_init()只调用一次, early=true!
+ *
+ * [    0.275532]  xen_swiotlb_init+0x47/0x4e0
+ * [    0.275538]  pci_xen_swiotlb_init+0x18/0x29
+ * [    0.275541]  pci_iommu_alloc+0x5f/0x67
+ * [    0.275543]  mem_init+0xb/0x98
+ * [    0.275546]  start_kernel+0x23b/0x4da
+ * [    0.275548]  xen_start_kernel+0x561/0x56b
+ */
 int __ref xen_swiotlb_init(int verbose, bool early)
 {
 	unsigned long bytes, order;
@@ -221,13 +307,35 @@ int __ref xen_swiotlb_init(int verbose, bool early)
 	enum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;
 	unsigned int repeat = 3;
 
+	/*
+	 * 返回io_tlb_nslabs, 在测试机是32768
+	 *
+	 * xen_io_tlb_nslabs最终在测试机也是32768 (不知道后面逻辑有没有影响)
+	 *
+	 * 另外 在测试机上 (start和end之间是64MB):
+	 *   io_tlb_start: 0x4a2a00000
+	 *   io_tlb_end  : 0x4a6a00000
+	 */
 	xen_io_tlb_nslabs = swiotlb_nr_tbl();
 retry:
+	/*
+	 * nr_tlb在测试机是32768不为空, 左移11位返回67108864=0x4000000 (64MB)
+	 *
+	 * 因此bytes=64MB
+	 */
 	bytes = xen_set_nslabs(xen_io_tlb_nslabs);
+	/*
+	 * xen_io_tlb_nslabs左移11位是67108864=0x4000000 (64MB)
+	 *
+	 * order是14, 2^14是16384个4k的page, 总数是67108864
+	 */
 	order = get_order(xen_io_tlb_nslabs << IO_TLB_SHIFT);
 	/*
 	 * Get IO TLB memory from any location.
 	 */
+	/*
+	 * 因为在upstream linux上early=true, 在bootmem分配64MB内存
+	 */
 	if (early)
 		xen_io_tlb_start = alloc_bootmem_pages(PAGE_ALIGN(bytes));
 	else {
@@ -250,14 +358,24 @@ retry:
 		m_ret = XEN_SWIOTLB_ENOMEM;
 		goto error;
 	}
+	/*
+	 * 在测试机 上面bytes在xen_set_nslabs()推算的是64MB
+	 *
+	 * 于是, 我们在测试机得到如下结果 (start和end之间是64MB):
+	 *   xen_io_tlb_start  = 0xffff8804a2a00000
+	 *   xen_io_tlb_end    = 0xffff8804a6a00000
+	 *   xen_io_tlb_nslabs = 32768
+	 */
 	xen_io_tlb_end = xen_io_tlb_start + bytes;
 	/*
 	 * And replace that memory with pages under 4GB.
 	 */
+	/* 把预分配的页面换成连续的 */
 	rc = xen_swiotlb_fixup(xen_io_tlb_start,
 			       bytes,
 			       xen_io_tlb_nslabs);
 	if (rc) {
+		/* 下面是错误的时候 */
 		if (early)
 			free_bootmem(__pa(xen_io_tlb_start), PAGE_ALIGN(bytes));
 		else {
@@ -267,6 +385,7 @@ retry:
 		m_ret = XEN_SWIOTLB_EFIXUP;
 		goto error;
 	}
+	/* start_dma_addr在测试机是0x1c0000 */
 	start_dma_addr = xen_virt_to_bus(xen_io_tlb_start);
 	if (early) {
 		if (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs,
@@ -275,6 +394,12 @@ retry:
 		rc = 0;
 	} else
 		rc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);
+	/*
+	 * 在测试机器上 (start和end之间是64MB):
+	 *    xen_io_tlb_start  = 0xffff8804a2a00000
+	 *    xen_io_tlb_end    = 0xffff8804a6a00000
+	 *    xen_io_tlb_nslabs = 32768
+	 */
 	return rc;
 error:
 	if (repeat--) {
@@ -291,6 +416,8 @@ error:
 		free_pages((unsigned long)xen_io_tlb_start, order);
 	return rc;
 }
+
+/* struct dma_map_ops xen_swiotlb_dma_ops.alloc = xen_swiotlb_alloc_coherent */
 void *
 xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 			   dma_addr_t *dma_handle, gfp_t flags,
@@ -310,6 +437,9 @@ xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 	*/
 	flags &= ~(__GFP_DMA | __GFP_HIGHMEM);
 
+	/*
+	 * 根据config, 这里应该是不用的 需要验证!
+	 */
 	if (dma_alloc_from_coherent(hwdev, size, dma_handle, &ret))
 		return ret;
 
@@ -318,11 +448,24 @@ xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 	 * address. In fact on ARM virt_to_phys only works for kernel direct
 	 * mapped RAM memory. Also see comment below.
 	 */
+	/* 就是调用__get_free_pages(), 然后转成物理地址存在dma_handle, 返回虚拟地址 */
 	ret = xen_alloc_coherent_pages(hwdev, size, dma_handle, flags, attrs);
 
+	/*
+	 * 到了这里:
+	 *   ret 是虚拟地址
+	 *   dma_handle是物理地址
+	 */
 	if (!ret)
 		return ret;
 
+	/*
+	 * 如果dev->coherent_dma_mask已经有了就返回dev->coherent_dma_mask
+	 * 否则根据gfp决定是DMA_BIT_MASK(24)还是DMA_BIT_MASK(32)
+	 *
+	 * 在这里, 如果dev->coherent_dma_mask已经有了就返回dev->coherent_dma_mask
+	 * 否则用开始的32位
+	 */
 	if (hwdev && hwdev->coherent_dma_mask)
 		dma_mask = dma_alloc_coherent_mask(hwdev, flags);
 
@@ -332,6 +475,10 @@ xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 	 * to *dma_handle. */
 	phys = *dma_handle;
 	dev_addr = xen_phys_to_bus(phys);
+	/*
+	 * 如果地址在dma_mask范围内而且机器连续就直接使用
+	 * 否则exchange成机器连续的
+	 */
 	if (((dev_addr + size - 1 <= dma_mask)) &&
 	    !range_straddles_page_boundary(phys, size))
 		*dma_handle = dev_addr;
@@ -347,6 +494,7 @@ xen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_alloc_coherent);
 
+/* struct dma_map_ops xen_swiotlb_dma_ops.free = xen_swiotlb_free_coherent */
 void
 xen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
 			  dma_addr_t dev_addr, struct dma_attrs *attrs)
@@ -355,6 +503,9 @@ xen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
 	phys_addr_t phys;
 	u64 dma_mask = DMA_BIT_MASK(32);
 
+	/*
+	 * ol6上CONFIG_HAVE_GENERIC_DMA_COHERENT没有配置
+	 */
 	if (dma_release_from_coherent(hwdev, order, vaddr))
 		return;
 
@@ -369,6 +520,7 @@ xen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
 	    range_straddles_page_boundary(phys, size))
 		xen_destroy_contiguous_region(phys, order);
 
+	/* 就是用free_pages()释放pages */
 	xen_free_coherent_pages(hwdev, size, vaddr, (dma_addr_t)phys, attrs);
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_free_coherent);
@@ -381,12 +533,34 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_free_coherent);
  * Once the device is given the dma address, the device owns this memory until
  * either xen_swiotlb_unmap_page or xen_swiotlb_dma_sync_single is performed.
  */
+/*
+ * 来自LDD
+ *
+ * Once a buffer has been mapped, it belongs to the device, not the processor. Until
+ * the buffer has been unmapped, the driver should not touch its contents in any
+ * way. Only after dma_unmap_single has been called is it safe for the driver to
+ * access the contents of the buffer.
+ *
+ * Occasionally a driver needs to access the contents of a streaming DMA buffer
+ * without numapping it.
+ *
+ * dma_sync_single_for_cpu() should be called before the processor accesses a streaming
+ * DMA buffer. Once the call has been made, the CPU "owns" the DMA buffer and can work
+ * with it as needed.
+ *
+ * Before the device accesses the buffer, however, ownership should be transferred to 
+ * the device with dma_sync_single_for_device(). The processor, once again, should not
+ * access the DMA buffer after this call has been made.
+ */
+/* struct dma_map_ops xen_swiotlb_dma_ops.map_page = xen_swiotlb_map_page */
 dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 				unsigned long offset, size_t size,
 				enum dma_data_direction dir,
 				struct dma_attrs *attrs)
 {
+	/* phys是page中offset的物理地址 */
 	phys_addr_t map, phys = page_to_phys(page) + offset;
+	/* 物理地址转换成机器地址 */
 	dma_addr_t dev_addr = xen_phys_to_bus(phys);
 
 	BUG_ON(dir == DMA_NONE);
@@ -395,6 +569,13 @@ dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 	 * we can safely return the device addr and not worry about bounce
 	 * buffering it.
 	 */
+	/*
+	 * range_straddles_page_boundary()返回0说明连续
+	 *
+	 * xen_arch_need_swiotlb()对于x86永远是false
+	 *
+	 * 满足这些条件直接返回机器地址就可以了
+	 */
 	if (dma_capable(dev, dev_addr, size) &&
 	    !range_straddles_page_boundary(phys, size) &&
 		!xen_arch_need_swiotlb(dev, phys, dev_addr) &&
@@ -402,6 +583,7 @@ dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 		/* we are not interested in the dma_addr returned by
 		 * xen_dma_map_page, only in the potential cache flushes executed
 		 * by the function. */
+		/* xen_dma_map_page()对于x86什么也不做 */
 		xen_dma_map_page(dev, page, dev_addr, offset, size, dir, attrs);
 		return dev_addr;
 	}
@@ -411,12 +593,19 @@ dma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,
 	 */
 	trace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);
 
+	/*
+	 * map是物理地址
+	 *
+	 * 主要就是这个函数
+	 */
 	map = swiotlb_tbl_map_single(dev, start_dma_addr, phys, size, dir);
 	if (map == SWIOTLB_MAP_ERROR)
 		return DMA_ERROR_CODE;
 
+	/* xen_dma_map_page()对于x86什么也不做 */
 	xen_dma_map_page(dev, pfn_to_page(map >> PAGE_SHIFT),
 					dev_addr, map & ~PAGE_MASK, size, dir, attrs);
+	/* map是物理地址 转换为机器地址 */
 	dev_addr = xen_phys_to_bus(map);
 
 	/*
@@ -438,6 +627,11 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_map_page);
  * After this call, reads by the cpu to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
+/*
+ * called by:
+ *   - xen_swiotlb_unmap_page()
+ *   - xen_swiotlb_unmap_sg_attrs()
+ */
 static void xen_unmap_single(struct device *hwdev, dma_addr_t dev_addr,
 			     size_t size, enum dma_data_direction dir,
 				 struct dma_attrs *attrs)
@@ -446,9 +640,11 @@ static void xen_unmap_single(struct device *hwdev, dma_addr_t dev_addr,
 
 	BUG_ON(dir == DMA_NONE);
 
+	/* xen_dma_unmap_page()对于x86什么也不做 */
 	xen_dma_unmap_page(hwdev, dev_addr, size, dir, attrs);
 
 	/* NOTE: We use dev_addr here, not paddr! */
+	/* 判断地址是否在swiotlb预留范围内 */
 	if (is_xen_swiotlb_buffer(dev_addr)) {
 		swiotlb_tbl_unmap_single(hwdev, paddr, size, dir);
 		return;
@@ -463,9 +659,13 @@ static void xen_unmap_single(struct device *hwdev, dma_addr_t dev_addr,
 	 * are fine since dma_mark_clean() is null on POWERPC. We can
 	 * make dma_mark_clean() take a physical address if necessary.
 	 */
+	/* 对于x86什么也不做 */
 	dma_mark_clean(phys_to_virt(paddr), size);
 }
 
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.unmap_page = xen_swiotlb_unmap_page
+ */
 void xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 			    size_t size, enum dma_data_direction dir,
 			    struct dma_attrs *attrs)
@@ -484,6 +684,33 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_unmap_page);
  * address back to the card, you must first perform a
  * xen_swiotlb_dma_sync_for_device, and then the device again owns the buffer
  */
+
+/*
+ * 来自LDD
+ *
+ * Once a buffer has been mapped, it belongs to the device, not the processor. Until
+ * the buffer has been unmapped, the driver should not touch its contents in any
+ * way. Only after dma_unmap_single has been called is it safe for the driver to
+ * access the contents of the buffer.
+ *
+ * Occasionally a driver needs to access the contents of a streaming DMA buffer
+ * without numapping it.
+ *
+ * dma_sync_single_for_cpu() should be called before the processor accesses a streaming
+ * DMA buffer. Once the call has been made, the CPU "owns" the DMA buffer and can work
+ * with it as needed.
+ *
+ * Before the device accesses the buffer, however, ownership should be transferred to 
+ * the device with dma_sync_single_for_device(). The processor, once again, should not
+ * access the DMA buffer after this call has been made.
+ */
+
+/*
+ * called by:
+ *   - xen_swiotlb_sync_single_for_cpu()   ----> struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_cpu
+ *   - xen_swiotlb_sync_single_for_device() ----> struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_device
+ *   - xen_swiotlb_sync_sg()
+ */
 static void
 xen_swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,
 			size_t size, enum dma_data_direction dir,
@@ -493,22 +720,31 @@ xen_swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,
 
 	BUG_ON(dir == DMA_NONE);
 
+	/* xen_dma_sync_single_for_cpu()在x86下什么也不做 */
 	if (target == SYNC_FOR_CPU)
 		xen_dma_sync_single_for_cpu(hwdev, dev_addr, size, dir);
 
 	/* NOTE: We use dev_addr here, not paddr! */
+	/*
+	 * 判断地址是否在swiotlb预留范围内
+	 *
+	 * dev_addr是machine address
+	 */
 	if (is_xen_swiotlb_buffer(dev_addr))
-		swiotlb_tbl_sync_single(hwdev, paddr, size, dir, target);
+		swiotlb_tbl_sync_single(hwdev, paddr, size, dir, target); // -->参数paddr是物理地址
 
+	/* xen_dma_sync_single_for_device()在x86下什么也不做 */
 	if (target == SYNC_FOR_DEVICE)
 		xen_dma_sync_single_for_device(hwdev, dev_addr, size, dir);
 
 	if (dir != DMA_FROM_DEVICE)
 		return;
 
+	/* dma_mark_clean()在x86下什么也不做 */
 	dma_mark_clean(phys_to_virt(paddr), size);
 }
 
+/* struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu */
 void
 xen_swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,
 				size_t size, enum dma_data_direction dir)
@@ -517,6 +753,7 @@ xen_swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_sync_single_for_cpu);
 
+/* struct dma_map_ops xen_swiotlb_dma_ops.sync_single_for_device = xen_swiotlb_sync_single_for_device */
 void
 xen_swiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,
 				   size_t size, enum dma_data_direction dir)
@@ -541,6 +778,9 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_sync_single_for_device);
  * Device ownership issues as mentioned above for xen_swiotlb_map_page are the
  * same here.
  */
+/*
+ * struct dma_map_ops xen_swiotlb_dma_ops.map_sg = xen_swiotlb_map_sg_attrs 
+ */
 int
 xen_swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
 			 int nelems, enum dma_data_direction dir,
@@ -602,6 +842,11 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_map_sg_attrs);
  * Unmap a set of streaming mode DMA translations.  Again, cpu read rules
  * concerning calls here are the same as for swiotlb_unmap_page() above.
  */
+/*
+ * called by:
+ *   - struct dma_map_ops xen_swiotlb_dma_ops.unmap_sg = xen_swiotlb_unmap_sg_attrs
+ *   - xen_swiotlb_map_sg_attrs() 失败用到
+ */
 void
 xen_swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
 			   int nelems, enum dma_data_direction dir,
@@ -625,6 +870,11 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_unmap_sg_attrs);
  * The same as swiotlb_sync_single_* but for a scatter-gather list, same rules
  * and usage.
  */
+/*
+ * called by (就在下面):
+ *   - xen_swiotlb_sync_sg_for_cpu()
+ *   - xen_swiotlb_sync_sg_for_device()
+ */
 static void
 xen_swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,
 		    int nelems, enum dma_data_direction dir,
@@ -638,6 +888,7 @@ xen_swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,
 					sg_dma_len(sg), dir, target);
 }
 
+/* struct dma_map_ops xen_swiotlb_dma_ops.sync_sg_for_cpu = xen_swiotlb_sync_sg_for_cpu */
 void
 xen_swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
 			    int nelems, enum dma_data_direction dir)
@@ -646,6 +897,7 @@ xen_swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_sync_sg_for_cpu);
 
+/* struct dma_map_ops xen_swiotlb_dma_ops.sync_sg_for_device = xen_swiotlb_sync_sg_for_device */
 void
 xen_swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
 			       int nelems, enum dma_data_direction dir)
@@ -654,6 +906,7 @@ xen_swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_sync_sg_for_device);
 
+/* struct dma_map_ops xen_swiotlb_dma_ops.mapping_error = xen_swiotlb_dma_mapping_error */
 int
 xen_swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)
 {
@@ -667,6 +920,11 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_dma_mapping_error);
  * during bus mastering, then you would pass 0x00ffffff as the mask to
  * this function.
  */
+/*
+ * 两处用到:
+ *   - xen_swiotlb_set_dma_mask() ---> 只和arm相关
+ *   - struct dma_map_ops xen_swiotlb_dma_ops.dma_supported = xen_swiotlb_dma_supported()
+ */
 int
 xen_swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
@@ -674,6 +932,9 @@ xen_swiotlb_dma_supported(struct device *hwdev, u64 mask)
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_dma_supported);
 
+/*
+ * 这个函数只和arm相关
+ */
 int
 xen_swiotlb_set_dma_mask(struct device *dev, u64 dma_mask)
 {
@@ -691,6 +952,7 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_set_dma_mask);
  * This function should be called with the pages from the current domain only,
  * passing pages mapped from other domains would lead to memory corruption.
  */
+/* 这个函数只和arm相关 */
 int
 xen_swiotlb_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 		     void *cpu_addr, dma_addr_t dma_addr, size_t size,
@@ -709,6 +971,7 @@ EXPORT_SYMBOL_GPL(xen_swiotlb_dma_mmap);
  * This function should be called with the pages from the current domain only,
  * passing pages mapped from other domains would lead to memory corruption.
  */
+/* 这个函数只和arm相关 */
 int
 xen_swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,
 			void *cpu_addr, dma_addr_t handle, size_t size,
@@ -729,6 +992,7 @@ xen_swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,
 							   handle, size, attrs);
 	}
 #endif
+	/* x86直接忽略上面看这里 */
 	return dma_common_get_sgtable(dev, sgt, cpu_addr, handle, size);
 }
 EXPORT_SYMBOL_GPL(xen_swiotlb_get_sgtable);
diff --git a/include/linux/device.h b/include/linux/device.h
index bd1d844..677f20d 100644
--- a/include/linux/device.h
+++ b/include/linux/device.h
@@ -758,7 +758,9 @@ struct device {
 #ifdef CONFIG_NUMA
 	int		numa_node;	/* NUMA node this device is close to */
 #endif
+	/* 是设备DMA可以寻址的范围 */
 	u64		*dma_mask;	/* dma mask (if dma'able device) */
+	/* 用于申请一致性的内存区域 */
 	u64		coherent_dma_mask;/* Like dma_mask, but for
 					     alloc_coherent mappings as
 					     not all hardware supports
diff --git a/include/linux/dma-direction.h b/include/linux/dma-direction.h
index 95b6a82..9b74576 100644
--- a/include/linux/dma-direction.h
+++ b/include/linux/dma-direction.h
@@ -4,6 +4,14 @@
  * These definitions mirror those in pci.h, so they can be used
  * interchangeably with their PCI_ counterparts.
  */
+/*
+ * DMA_TO_DEVICE:
+ * If data is being sent to the device (in response, perhaps, to
+ * a write system call)
+ *
+ * DMA_FROM_DEVICE:
+ * data going to the CPU
+ */
 enum dma_data_direction {
 	DMA_BIDIRECTIONAL = 0,
 	DMA_TO_DEVICE = 1,
diff --git a/lib/swiotlb.c b/lib/swiotlb.c
index 3c365ab..451af2a 100644
--- a/lib/swiotlb.c
+++ b/lib/swiotlb.c
@@ -53,6 +53,7 @@
  */
 #define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)
 
+/* 在测试机上是 0 */
 int swiotlb_force;
 
 /*
@@ -60,12 +61,20 @@ int swiotlb_force;
  * swiotlb_tbl_sync_single_*, to see if the memory was in fact allocated by this
  * API.
  */
+/*
+ * 在测试机上 (start和end之间是64MB):
+ * io_tlb_start: 0x4a2a00000
+ * io_tlb_end  : 0x4a6a00000
+ */
 static phys_addr_t io_tlb_start, io_tlb_end;
 
 /*
  * The number of IO TLB blocks (in groups of 64) between io_tlb_start and
  * io_tlb_end.  This is command line adjustable via setup_io_tlb_npages.
  */
+/*
+ * io_tlb_nslabs在测试机是32768
+ */
 static unsigned long io_tlb_nslabs;
 
 /*
@@ -73,13 +82,20 @@ static unsigned long io_tlb_nslabs;
  */
 static unsigned long io_tlb_overflow = 32*1024;
 
+/*
+ * 测试机是0x7bff8000
+ */
 static phys_addr_t io_tlb_overflow_buffer;
 
 /*
  * This is a free list describing the number of free entries available from
  * each index
  */
+/* io_tlb_list[i]存的好像是从当前开始连续的free的slot的数量 */
 static unsigned int *io_tlb_list;
+/*
+ * io_tlb_index除了初始化 只在swiotlb_tbl_map_single()中修改过
+ */
 static unsigned int io_tlb_index;
 
 /*
@@ -114,6 +130,13 @@ setup_io_tlb_npages(char *str)
 early_param("swiotlb", setup_io_tlb_npages);
 /* make io_tlb_overflow tunable too? */
 
+/*
+ * 返回io_tlb_nslabs, 在测试机是32768
+ *
+ * 另外 在测试机上 (start和end之间是64MB):
+ *   io_tlb_start: 0x4a2a00000
+ *   io_tlb_end  : 0x4a6a00000
+ */
 unsigned long swiotlb_nr_tbl(void)
 {
 	return io_tlb_nslabs;
@@ -135,11 +158,19 @@ unsigned long swiotlb_size_or_default(void)
 static dma_addr_t swiotlb_virt_to_bus(struct device *hwdev,
 				      volatile void *address)
 {
+	/* x86直接返回virt_to_phys(address) */
 	return phys_to_dma(hwdev, virt_to_phys(address));
 }
 
+/* 测试机上是false */
 static bool no_iotlb_memory;
 
+/*
+ * called by:
+ *   - swiotlb_init_with_tbl()
+ *   - swiotlb_late_init_with_tbl()
+ *   - pci_swiotlb_late_init()
+ */
 void swiotlb_print_info(void)
 {
 	unsigned long bytes = io_tlb_nslabs << IO_TLB_SHIFT;
@@ -159,6 +190,21 @@ void swiotlb_print_info(void)
 	       bytes >> 20, vstart, vend - 1);
 }
 
+/*
+ * called by:
+ *   - swiotlb_init_with_tbl() -- drivers/xen/swiotlb-xen.c
+ *   - swiotlb_init -- lib/swiotlb.c ----> 在xen上用不到
+ *
+ * upstream linux在virtualbox xen上
+ * [    0.294544]  dump_stack
+ * [    0.294547]  swiotlb_init_with_tbl
+ * [    0.294551]  xen_swiotlb_init
+ * [    0.294554]  pci_xen_swiotlb_init
+ * [    0.294556]  pci_iommu_alloc
+ * [    0.294559]  mem_init
+ * [    0.294561]  start_kernel
+ * [    0.294563]  xen_start_kernel
+ */
 int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 {
 	void *v_overflow_buffer;
@@ -193,6 +239,10 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
 				PAGE_ALIGN(io_tlb_nslabs * sizeof(phys_addr_t)),
 				PAGE_SIZE);
 	for (i = 0; i < io_tlb_nslabs; i++) {
+		/*
+		 * 初始化后, 大概如下pattern:
+		 * 128, 127, ..., 3, 2, 1, 128, 127, ... , 3, 2, 1..
+		 */
 		io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
 		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 	}
@@ -208,6 +258,9 @@ int __init swiotlb_init_with_tbl(char *tlb, unsigned long nslabs, int verbose)
  * Statically reserve bounce buffer space and initialize bounce buffer data
  * structures for the software IO TLB used to implement the DMA API.
  */
+/*
+ * 似乎在xen上用不到
+ */
 void  __init
 swiotlb_init(int verbose)
 {
@@ -239,6 +292,9 @@ swiotlb_init(int verbose)
  * initialize the swiotlb later using the slab allocator if needed.
  * This should be just like above, but with some error catching.
  */
+/*
+ * 猜测xen上用不到
+ */
 int
 swiotlb_late_init_with_default_size(size_t default_size)
 {
@@ -282,6 +338,16 @@ swiotlb_late_init_with_default_size(size_t default_size)
 	return rc;
 }
 
+/*
+ * called by:
+ *   - xen_swiotlb_init() --  drivers/xen/swiotlb-xen.c
+ *   - swiotlb_late_init_with_default_size() 在上面 猜测xen上用不到
+ *
+ * 对于xen来说, 在测试机器上 (start和end之间是64MB):
+ *    tlb是     ---> xen_io_tlb_start  = 0xffff8804a2a00000
+ *                   xen_io_tlb_end    = 0xffff8804a6a00000
+ *    nslabs是  ---> xen_io_tlb_nslabs = 32768
+ */
 int
 swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 {
@@ -290,6 +356,11 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 
 	bytes = nslabs << IO_TLB_SHIFT;
 
+	/*
+	 * 在测试机上 (start和end之间是64MB):
+	 *    io_tlb_start: 0x4a2a00000
+	 *    io_tlb_end  : 0x4a6a00000
+	 */
 	io_tlb_nslabs = nslabs;
 	io_tlb_start = virt_to_phys(tlb);
 	io_tlb_end = io_tlb_start + bytes;
@@ -311,11 +382,21 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 	 * to find contiguous free memory regions of size up to IO_TLB_SEGSIZE
 	 * between io_tlb_start and io_tlb_end.
 	 */
+	/*
+	 * 分配一些page用来存储io_tlb_list数组
+	 * 这些page一共占用 io_tlb_nslabs=32768个int
+	 * 每个io_tlb_list[32768]是一个int
+	 */
 	io_tlb_list = (unsigned int *)__get_free_pages(GFP_KERNEL,
 	                              get_order(io_tlb_nslabs * sizeof(int)));
 	if (!io_tlb_list)
 		goto cleanup3;
 
+	/*
+	 * 分配一些page用来存储io_tlb_orig_addr
+	 * 这些page一共占用 io_tlb_nslabs=32768个phys_addr_t
+	 * 每个io_tlb_orig_addr[32768]是一个phys_addr_t
+	 */
 	io_tlb_orig_addr = (phys_addr_t *)
 		__get_free_pages(GFP_KERNEL,
 				 get_order(io_tlb_nslabs *
@@ -323,7 +404,9 @@ swiotlb_late_init_with_tbl(char *tlb, unsigned long nslabs)
 	if (!io_tlb_orig_addr)
 		goto cleanup4;
 
+	/* for 32768次 */
 	for (i = 0; i < io_tlb_nslabs; i++) {
+		/* 似乎io_tlb_list[]就是从0-127不停轮循 */
 		io_tlb_list[i] = IO_TLB_SEGSIZE - OFFSET(i, IO_TLB_SEGSIZE);
 		io_tlb_orig_addr[i] = INVALID_PHYS_ADDR;
 	}
@@ -350,6 +433,9 @@ cleanup2:
 	return -ENOMEM;
 }
 
+/*
+ * called by pci_swiotlb_late_init()
+ */
 void __init swiotlb_free(void)
 {
 	if (!io_tlb_orig_addr)
@@ -377,6 +463,13 @@ void __init swiotlb_free(void)
 	io_tlb_nslabs = 0;
 }
 
+/*
+ * called by:
+ *   - x86_swiotlb_free_coherent() -- arch/x86/kernel/pci-swiotlb.c
+ *   - swiotlb_free_coherent() -- lib/swiotlb.c
+ *   - unmap_single() -- lib/swiotlb.c
+ *   - swiotlb_sync_single() -- lib/swiotlb.c
+ */
 int is_swiotlb_buffer(phys_addr_t paddr)
 {
 	return paddr >= io_tlb_start && paddr < io_tlb_end;
@@ -385,12 +478,29 @@ int is_swiotlb_buffer(phys_addr_t paddr)
 /*
  * Bounce: copy the swiotlb buffer back to the original dma location
  */
+/*
+ * called by:
+ *   - swiotlb_tbl_map_single()
+ *   - swiotlb_tbl_unmap_single()
+ *   - swiotlb_tbl_sync_single()
+ *   - swiotlb_tbl_sync_single()
+ *
+ *   如果是DMA_TO_DEVICE就要从原始内存拷贝到bounce buffer
+ *   如果是其他 (比如DMA_FROM_DEVICE) 就要从bounce buffer拷贝到原始内存
+ *
+ *   vaddr来自函数参数的tlb_addr, 说明tlb_addr是bounce buffer (预分配的)
+ *
+ *   tlb_addr是物理地址
+ */
 static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 			   size_t size, enum dma_data_direction dir)
 {
 	unsigned long pfn = PFN_DOWN(orig_addr);
 	unsigned char *vaddr = phys_to_virt(tlb_addr);
 
+	/*
+	 * 这里pfn是orig_addr的
+	 */
 	if (PageHighMem(pfn_to_page(pfn))) {
 		/* The buffer does not have a mapping.  Map it in and copy */
 		unsigned int offset = orig_addr & ~PAGE_MASK;
@@ -416,12 +526,104 @@ static void swiotlb_bounce(phys_addr_t orig_addr, phys_addr_t tlb_addr,
 			offset = 0;
 		}
 	} else if (dir == DMA_TO_DEVICE) {
+		/*
+		 * 如果是DMA_TO_DEVICE就要从原始内存拷贝到bounce buffer
+		 * 如果是其他就要从bounce buffer拷贝到原始内存
+		 *
+		 * vaddr来自函数参数的tlb_addr, 说明tlb_addr是bounce buffer (预分配的)
+		 */
 		memcpy(vaddr, phys_to_virt(orig_addr), size);
 	} else {
 		memcpy(phys_to_virt(orig_addr), vaddr, size);
 	}
 }
 
+/*
+ * called by:
+ *   - xen_swiotlb_map_page()  -- drivers/xen/swiotlb-xen.c
+ *   - xen_swiotlb_map_sg_attrs()  -- drivers/xen/swiotlb-xen.c
+ *   - map_single()  -- lib/swiotlb.c
+ *
+ * 返回值是物理地址
+ */
+/*io_tlb_list[]存着从当前开始有几个连续的slot
+ *
+ * 假设开始状态 index = 5, nslots = 4:
+ *
+ * io_tlb_list[]  OFFSET(i, IO_TLB_SEGSIZE)
+ *      .                 .
+ *      .                 .
+ *      .                 .
+ *      2                126
+ *      1                127
+ *     128 (index = 0)    0 <------ index = 0
+ *     127                1
+ *     126                2
+ *     125                3
+ *     124                4
+ *     123 (index = 5)    5 <------ index = 5
+ *     122                6
+ *     121                7
+ *     122                8
+ *     123                9
+ *      .                 .
+ *      .                 .
+ *      .                 .
+ *
+ *
+ * 第一步:
+ * for (i = index; i < (int) (index + nslots); i++)
+ *	io_tlb_list[i] = 0;
+ *
+ * 把io_tlb_list[5-8]全设置成0
+ *
+ * io_tlb_list[]  OFFSET(i, IO_TLB_SEGSIZE)
+ *     .                 .
+ *     .                 .
+ *     .                 .
+ *     2                126
+ *     1                127
+ *    128 (index = 0)    0 <------ index = 0
+ *    127                1
+ *    126                2
+ *    125                3
+ *    124                4
+ *     0  (index = 5)    5 <------ index = 5
+ *     0                 6
+ *     0                 7
+ *     0                 8
+ *    123                9
+ *     .                 .
+ *     .                 .
+ *     .                 .
+ *
+ *
+ * 第二步:
+ * for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)
+ *	io_tlb_list[i] = ++count;
+ *
+ * 把io_tlb_list[4-0]设置成1-5 (++count)
+ * 
+ * io_tlb_list[]  OFFSET(i, IO_TLB_SEGSIZE)
+ *     .                 .
+ *     .                 .
+ *     .                 .
+ *     2                126
+ *     1                127
+ *     5  (index = 0)    0 <------ index = 0
+ *     4                 1
+ *     3                 2
+ *     2                 3
+ *     1                 4
+ *     0  (index = 5)    5 <------ index = 5
+ *     0                 6
+ *     0                 7
+ *     0                 8
+ *    123                9
+ *     .                 .
+ *     .                 .
+ *     .                 .
+ */
 phys_addr_t swiotlb_tbl_map_single(struct device *hwdev,
 				   dma_addr_t tbl_dma_addr,
 				   phys_addr_t orig_addr, size_t size,
@@ -442,6 +644,7 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev,
 
 	tbl_dma_addr &= mask;
 
+	/* slots的index ? */
 	offset_slots = ALIGN(tbl_dma_addr, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
 
 	/*
@@ -455,6 +658,11 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev,
 	 * For mappings greater than a page, we limit the stride (and
 	 * hence alignment) to a page size.
 	 */
+	/*
+	 * 一个slot是2K??
+	 *
+	 * 看看这些size占用几个slot
+	 */
 	nslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
 	if (size > PAGE_SIZE)
 		stride = (1 << (PAGE_SHIFT - IO_TLB_SHIFT));
@@ -469,6 +677,7 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev,
 	 */
 	spin_lock_irqsave(&io_tlb_lock, flags);
 	index = ALIGN(io_tlb_index, stride);
+	/* io_tlb_nslabs在测试机是32768 */
 	if (index >= io_tlb_nslabs)
 		index = 0;
 	wrap = index;
@@ -493,23 +702,33 @@ phys_addr_t swiotlb_tbl_map_single(struct device *hwdev,
 
 			for (i = index; i < (int) (index + nslots); i++)
 				io_tlb_list[i] = 0;
+			/*
+			 * (OFFSET(i, IO_TLB_SEGSIZE)是不能改变的
+			 */
 			for (i = index - 1; (OFFSET(i, IO_TLB_SEGSIZE) != IO_TLB_SEGSIZE - 1) && io_tlb_list[i]; i--)
 				io_tlb_list[i] = ++count;
+			/*
+			 * 在测试机上 (start和end之间是64MB):
+			 *   io_tlb_start: 0x4a2a00000
+			 *   io_tlb_end  : 0x4a6a00000
+			 */
 			tlb_addr = io_tlb_start + (index << IO_TLB_SHIFT);
 
 			/*
 			 * Update the indices to avoid searching in the next
 			 * round.
 			 */
+			/* io_tlb_index除了初始化 只在swiotlb_tbl_map_single()中修改过 */
 			io_tlb_index = ((index + nslots) < io_tlb_nslabs
 					? (index + nslots) : 0);
 
 			goto found;
 		}
+		/* 增加index 查看下一个是否符合条件 */
 		index += stride;
 		if (index >= io_tlb_nslabs)
 			index = 0;
-	} while (index != wrap);
+	} while (index != wrap); // wrap是最初的index, 如果转了一圈还没找到合适的就退出
 
 not_found:
 	spin_unlock_irqrestore(&io_tlb_lock, flags);
@@ -524,8 +743,14 @@ found:
 	 * This is needed when we sync the memory.  Then we sync the buffer if
 	 * needed.
 	 */
+	/* 每个slot是2k??? */
 	for (i = 0; i < nslots; i++)
 		io_tlb_orig_addr[index+i] = orig_addr + (i << IO_TLB_SHIFT);
+	/* Bounce: copy the swiotlb buffer from the original dma location */
+	/*
+	 * 如果是DMA_TO_DEVICE就要从原始内存拷贝到bounce buffer
+	 * 如果是其他 (比如DMA_FROM_DEVICE) 就要从bounce buffer拷贝到原始内存
+	 */
 	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)
 		swiotlb_bounce(orig_addr, tlb_addr, size, DMA_TO_DEVICE);
 
@@ -537,10 +762,23 @@ EXPORT_SYMBOL_GPL(swiotlb_tbl_map_single);
  * Allocates bounce buffer and returns its kernel virtual address.
  */
 
+/*
+ * called by:
+ *   - swiotlb_alloc_coherent() -- lib/swiotlb.c
+ *   - swiotlb_map_page()  -- lib/swiotlb.c
+ *   - swiotlb_map_sg_attrs()  -- lib/swiotlb.c
+ */
 static phys_addr_t
 map_single(struct device *hwdev, phys_addr_t phys, size_t size,
 	   enum dma_data_direction dir)
 {
+	/*
+	 * phys_to_dma()直接返回io_tlb_start
+	 *
+	 * 在测试机上 (start和end之间是64MB):
+	 *    io_tlb_start: 0x4a2a00000
+	 *    io_tlb_end  : 0x4a6a00000
+	 */
 	dma_addr_t start_dma_addr = phys_to_dma(hwdev, io_tlb_start);
 
 	return swiotlb_tbl_map_single(hwdev, start_dma_addr, phys, size, dir);
@@ -549,17 +787,31 @@ map_single(struct device *hwdev, phys_addr_t phys, size_t size,
 /*
  * dma_addr is the kernel virtual address of the bounce buffer to unmap.
  */
+/*
+ * called by:
+ *   - xen_swiotlb_map_page() -- drivers/xen/swiotlb-xen.c
+ *   - xen_unmap_single() -- drivers/xen/swiotlb-xen.c
+ *   - swiotlb_alloc_coherent() -- lib/swiotlb.c
+ *   - swiotlb_free_coherent() -- lib/swiotlb.c
+ *   - swiotlb_map_page()  -- lib/swiotlb.c
+ *   - unmap_single()  -- lib/swiotlb.c
+ *
+ *   tlb_addr是要unmap的物理地址
+ */
 void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 			      size_t size, enum dma_data_direction dir)
 {
 	unsigned long flags;
 	int i, count, nslots = ALIGN(size, 1 << IO_TLB_SHIFT) >> IO_TLB_SHIFT;
+	/* tlb_addr是要unmap的物理地址 */
 	int index = (tlb_addr - io_tlb_start) >> IO_TLB_SHIFT;
 	phys_addr_t orig_addr = io_tlb_orig_addr[index];
 
 	/*
 	 * First, sync the memory before unmapping the entry
 	 */
+	/* Bounce: copy the swiotlb buffer back to the original dma location */
+	/* 从tlb拷贝到orig的地址 */
 	if (orig_addr != INVALID_PHYS_ADDR &&
 	    ((dir == DMA_FROM_DEVICE) || (dir == DMA_BIDIRECTIONAL)))
 		swiotlb_bounce(orig_addr, tlb_addr, size, DMA_FROM_DEVICE);
@@ -593,6 +845,15 @@ void swiotlb_tbl_unmap_single(struct device *hwdev, phys_addr_t tlb_addr,
 }
 EXPORT_SYMBOL_GPL(swiotlb_tbl_unmap_single);
 
+/*
+ * called by:
+ *   - xen_swiotlb_sync_single() -- drivers/xen/swiotlb-xen.c
+ *   - swiotlb_sync_single() -- lib/swiotlb.c -- caller都不是xen的swiotlb
+ *
+ * tlb_addr是物理地址
+ *
+ * 会调用到swiotlb_bounce()
+ */
 void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 			     size_t size, enum dma_data_direction dir,
 			     enum dma_sync_target target)
@@ -607,6 +868,10 @@ void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 	switch (target) {
 	case SYNC_FOR_CPU:
 		if (likely(dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))
+			/*
+			 * 如果是DMA_TO_DEVICE就要从原始内存拷贝到bounce buffer
+			 * 如果是其他 (比如DMA_FROM_DEVICE) 就要从bounce buffer拷贝到原始内存
+			 */
 			swiotlb_bounce(orig_addr, tlb_addr,
 				       size, DMA_FROM_DEVICE);
 		else
@@ -614,6 +879,10 @@ void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 		break;
 	case SYNC_FOR_DEVICE:
 		if (likely(dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL))
+			/*
+			 * 如果是DMA_TO_DEVICE就要从原始内存拷贝到bounce buffer
+			 * 如果是其他 (比如DMA_FROM_DEVICE) 就要从bounce buffer拷贝到原始内存
+			 */
 			swiotlb_bounce(orig_addr, tlb_addr,
 				       size, DMA_TO_DEVICE);
 		else
@@ -625,6 +894,9 @@ void swiotlb_tbl_sync_single(struct device *hwdev, phys_addr_t tlb_addr,
 }
 EXPORT_SYMBOL_GPL(swiotlb_tbl_sync_single);
 
+/*
+ * called by x86_swiotlb_alloc_coherent() -- arch/x86/kernel/pci-swiotlb.c
+ */
 void *
 swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 		       dma_addr_t *dma_handle, gfp_t flags)
@@ -681,6 +953,9 @@ swiotlb_alloc_coherent(struct device *hwdev, size_t size,
 }
 EXPORT_SYMBOL(swiotlb_alloc_coherent);
 
+/*
+ * called by x86_swiotlb_free_coherent() -- arch/x86/kernel/pci-swiotlb.c
+ */
 void
 swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
 		      dma_addr_t dev_addr)
@@ -696,6 +971,11 @@ swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,
 }
 EXPORT_SYMBOL(swiotlb_free_coherent);
 
+/*
+ * called by (似乎可以忽略):
+ *   - swiotlb_map_page() -- 不是xen的swiotlb
+ *   - swiotlb_map_sg_attrs() -- caller不是没有caller, 就是不是xen的swiotlb
+ */
 static void
 swiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,
 	     int do_panic)
@@ -728,6 +1008,10 @@ swiotlb_full(struct device *dev, size_t size, enum dma_data_direction dir,
  * Once the device is given the dma address, the device owns this memory until
  * either swiotlb_unmap_page or swiotlb_dma_sync_single is performed.
  */
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_ops.map_page = swiotlb_map_page
+ */
 dma_addr_t swiotlb_map_page(struct device *dev, struct page *page,
 			    unsigned long offset, size_t size,
 			    enum dma_data_direction dir,
@@ -774,6 +1058,11 @@ EXPORT_SYMBOL_GPL(swiotlb_map_page);
  * After this call, reads by the cpu to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
+/*
+ * called by (似乎可以忽略):
+ *   - swiotlb_unmap_page()  -- 不是xen的swiotlb
+ *   - swiotlb_unmap_sg_attrs() -- 似乎可以忽略
+ */
 static void unmap_single(struct device *hwdev, dma_addr_t dev_addr,
 			 size_t size, enum dma_data_direction dir)
 {
@@ -798,6 +1087,10 @@ static void unmap_single(struct device *hwdev, dma_addr_t dev_addr,
 	dma_mark_clean(phys_to_virt(paddr), size);
 }
 
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_ops.unmap_page = swiotlb_unmap_page
+ */
 void swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,
 			size_t size, enum dma_data_direction dir,
 			struct dma_attrs *attrs)
@@ -816,6 +1109,12 @@ EXPORT_SYMBOL_GPL(swiotlb_unmap_page);
  * address back to the card, you must first perform a
  * swiotlb_dma_sync_for_device, and then the device again owns the buffer
  */
+/*
+ * called by (caller都不是xen的swiotlb):
+ *   - swiotlb_sync_single_for_cpu()  --> 不是xen的swiotlb
+ *   - swiotlb_sync_single_for_device()  --> 不是xen的swiotlb
+ *   - swiotlb_sync_sg() --> caller都不是xen的swiotlb
+ */
 static void
 swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,
 		    size_t size, enum dma_data_direction dir,
@@ -836,6 +1135,10 @@ swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,
 	dma_mark_clean(phys_to_virt(paddr), size);
 }
 
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_opssync_single_for_cpu = swiotlb_sync_single_for_cpu
+ */
 void
 swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,
 			    size_t size, enum dma_data_direction dir)
@@ -844,6 +1147,10 @@ swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,
 }
 EXPORT_SYMBOL(swiotlb_sync_single_for_cpu);
 
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_ops.sync_single_for_device = swiotlb_sync_single_for_device
+ */
 void
 swiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,
 			       size_t size, enum dma_data_direction dir)
@@ -868,6 +1175,13 @@ EXPORT_SYMBOL(swiotlb_sync_single_for_device);
  * Device ownership issues as mentioned above for swiotlb_map_page are the
  * same here.
  */
+/*
+ * caller不是没有caller, 就是不是xen的swiotlb
+ *
+ * called by:
+ *   - swiotlb_map_sg() -- 没有任何caller
+ *   - struct dma_map_ops swiotlb_dma_ops.map_sg = swiotlb_map_sg_attrs() -- 不是xen的swiotlb
+ */
 int
 swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,
 		     enum dma_data_direction dir, struct dma_attrs *attrs)
@@ -903,6 +1217,7 @@ swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl, int nelems,
 }
 EXPORT_SYMBOL(swiotlb_map_sg_attrs);
 
+/* 没有任何caller */
 int
 swiotlb_map_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 	       enum dma_data_direction dir)
@@ -915,6 +1230,14 @@ EXPORT_SYMBOL(swiotlb_map_sg);
  * Unmap a set of streaming mode DMA translations.  Again, cpu read rules
  * concerning calls here are the same as for swiotlb_unmap_page() above.
  */
+/*
+ * 似乎可以忽略
+ *
+ * called by:
+ *   - swiotlb_map_sg_attrs() -- caller不是没有caller, 就是不是xen的swiotlb
+ *   - swiotlb_unmap_sg() 没有任何caller
+ *   - struct dma_map_ops swiotlb_dma_ops.unmap_sg = swiotlb_unmap_sg_attrs -- 不是xen的swiotlb
+ */
 void
 swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
 		       int nelems, enum dma_data_direction dir, struct dma_attrs *attrs)
@@ -930,6 +1253,7 @@ swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
 }
 EXPORT_SYMBOL(swiotlb_unmap_sg_attrs);
 
+/* 没有任何caller */
 void
 swiotlb_unmap_sg(struct device *hwdev, struct scatterlist *sgl, int nelems,
 		 enum dma_data_direction dir)
@@ -945,6 +1269,11 @@ EXPORT_SYMBOL(swiotlb_unmap_sg);
  * The same as swiotlb_sync_single_* but for a scatter-gather list, same rules
  * and usage.
  */
+/*
+ * called by (都不是xen的swiotlb):
+ *   - swiotlb_sync_sg_for_cpu() -- 不是xen的swiotlb
+ *   - swiotlb_sync_sg_for_device() -- 不是xen的swiotlb
+ */
 static void
 swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,
 		int nelems, enum dma_data_direction dir,
@@ -958,6 +1287,10 @@ swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,
 				    sg_dma_len(sg), dir, target);
 }
 
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_ops.sync_sg_for_cpu = swiotlb_sync_sg_for_cpu
+ */
 void
 swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
 			int nelems, enum dma_data_direction dir)
@@ -966,6 +1299,10 @@ swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
 }
 EXPORT_SYMBOL(swiotlb_sync_sg_for_cpu);
 
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_ops.sync_sg_for_device = swiotlb_sync_sg_for_device
+ */
 void
 swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
 			   int nelems, enum dma_data_direction dir)
@@ -974,6 +1311,10 @@ swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
 }
 EXPORT_SYMBOL(swiotlb_sync_sg_for_device);
 
+/*
+ * 不是xen的swiotlb
+ * struct dma_map_ops swiotlb_dma_ops.mapping_error = swiotlb_dma_mapping_error
+ */
 int
 swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)
 {
@@ -987,6 +1328,11 @@ EXPORT_SYMBOL(swiotlb_dma_mapping_error);
  * during bus mastering, then you would pass 0x00ffffff as the mask to
  * this function.
  */
+/*
+ * 对于x86, 判断io_tlb_end是不是小于设备的mask
+ *
+ * 但在x86上没看到调用者
+ */
 int
 swiotlb_dma_supported(struct device *hwdev, u64 mask)
 {
-- 
2.7.4

