From f1205fc45d2d158b4f0562c979a778e96f0670b0 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Thu, 9 Jan 2025 08:31:37 -0800
Subject: [PATCH 1/1] linux-v5.4.17-2136.333.5

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/arm64/kvm/pmu-emul.c        |    5 +
 arch/x86/kvm/pmu.c               |   16 +
 arch/x86/kvm/pmu.h               |   16 +
 arch/x86/kvm/svm/pmu.c           |   11 +
 arch/x86/kvm/x86.c               |   69 ++
 drivers/gpu/drm/i915/gvt/kvmgt.c |    9 +
 drivers/iommu/iommu.c            |   45 +
 drivers/vfio/vfio.c              |   26 +
 drivers/vfio/vfio_iommu_type1.c  | 1540 ++++++++++++++++++++++++++++++
 include/linux/mm_types.h         |   18 +
 include/linux/padata.h           |   21 +
 include/linux/vfio.h             |    8 +
 kernel/events/core.c             |  129 +++
 kernel/padata.c                  |  368 +++++++
 mm/gup.c                         |  285 ++++++
 mm/util.c                        |   54 ++
 16 files changed, 2620 insertions(+)

diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
index 68409559aabd..f8db3d75c598 100644
--- a/arch/arm64/kvm/pmu-emul.c
+++ b/arch/arm64/kvm/pmu-emul.c
@@ -82,6 +82,11 @@ u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 	 * The real counter value is equal to the value of counter register plus
 	 * the value perf event counts.
 	 */
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|86| <<kvm_pmu_get_counter_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|56| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 */
 	if (pmc->perf_event)
 		counter += perf_event_read_value(pmc->perf_event, &enabled,
 						 &running);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index b8eba33c0cf7..5c863fd1cd03 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -384,6 +384,13 @@ void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 		kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3073| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|3134| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+ *   - arch/x86/kvm/x86.c|3220| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ *   - arch/x86/kvm/x86.c|3403| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+ */
 bool kvm_pmu_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)
 {
 	return kvm_x86_ops.pmu_ops->msr_idx_to_pmc(vcpu, msr) ||
@@ -404,9 +411,18 @@ int kvm_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return kvm_x86_ops.pmu_ops->get_msr(vcpu, msr_info);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3074| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ *   - arch/x86/kvm/x86.c|3135| <<kvm_set_msr_common>> return kvm_pmu_set_msr(vcpu, msr_info);
+ */
 int kvm_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	kvm_pmu_mark_pmc_in_use(vcpu, msr_info->index);
+	/*
+	 * intel_pmu_set_msr()
+	 * amd_pmu_set_msr()
+	 */
 	return kvm_x86_ops.pmu_ops->set_msr(vcpu, msr_info);
 }
 
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index f4fde6bcc8cc..4d63bd0c73de 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -47,11 +47,27 @@ static inline u64 pmc_bitmask(struct kvm_pmc *pmc)
 	return pmu->counter_bitmask[pmc->type];
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|377| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+ *   - arch/x86/kvm/pmu.h|75| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|249| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/svm/pmu.c|272| <<amd_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|228| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|233| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|297| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+ */
 static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
 
 	counter = pmc->counter;
+	/*
+	 * called by:
+	 *   - arch/arm64/kvm/pmu-emul.c|86| <<kvm_pmu_get_counter_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 *   - arch/x86/kvm/pmu.h|56| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+	 */
 	if (pmc->perf_event && !pmc->is_paused)
 		counter += perf_event_read_value(pmc->perf_event,
 						 &enabled, &running);
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index 663d943f85db..c2e35782cabd 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -269,6 +269,17 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	/* MSR_PERFCTRn */
 	pmc = get_gp_pmc_amd(pmu, msr, PMU_TYPE_COUNTER);
 	if (pmc) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/pmu.c|377| <<kvm_pmu_rdpmc>> *data = pmc_read_counter(pmc) & mask;
+		 *   - arch/x86/kvm/pmu.h|75| <<pmc_stop_counter>> pmc->counter = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/svm/pmu.c|249| <<amd_pmu_get_msr>> msr_info->data = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/svm/pmu.c|272| <<amd_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|228| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|233| <<intel_pmu_get_msr>> u64 val = pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|293| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|297| <<intel_pmu_set_msr>> pmc->counter += data - pmc_read_counter(pmc);
+		 */
 		pmc->counter += data - pmc_read_counter(pmc);
 		pmc_update_sample_period(pmc);
 		return 0;
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 45da303ced50..d5d638d9f4ee 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -278,6 +278,12 @@ EXPORT_SYMBOL_GPL(x86_fpu_cache);
  * Return 0 if we want to ignore/silent this failed msr access, or 1 if we want
  * to fail the caller.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1464| <<do_get_msr_feature>> r = kvm_msr_ignored_check(vcpu, index, 0, false);
+ *   - arch/x86/kvm/x86.c|1592| <<kvm_set_msr_ignored_check>> ret = kvm_msr_ignored_check(vcpu, index, data, true);
+ *   - arch/x86/kvm/x86.c|1626| <<kvm_get_msr_ignored_check>> ret = kvm_msr_ignored_check(vcpu, index, 0, false);
+ */
 static int kvm_msr_ignored_check(struct kvm_vcpu *vcpu, u32 msr,
 				 u64 data, bool write)
 {
@@ -1545,6 +1551,10 @@ EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
  * Returns 0 on success, non-0 otherwise.
  * Assumes vcpu_load() was already called.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1589| <<kvm_set_msr_ignored_check>> int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
+ */
 static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 			 bool host_initiated)
 {
@@ -1580,14 +1590,29 @@ static int __kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data,
 	msr.index = index;
 	msr.host_initiated = host_initiated;
 
+	/*
+	 * vmx_set_msr()
+	 * svm_set_msr()
+	 */
 	return kvm_x86_ops.set_msr(vcpu, &msr);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|1640| <<kvm_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, data, false);
+ *   - arch/x86/kvm/x86.c|1758| <<do_set_msr>> return kvm_set_msr_ignored_check(vcpu, index, *data, true);
+ */
 static int kvm_set_msr_ignored_check(struct kvm_vcpu *vcpu,
 				     u32 index, u64 data, bool host_initiated)
 {
 	int ret = __kvm_set_msr(vcpu, index, data, host_initiated);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/x86.c|1464| <<do_get_msr_feature>> r = kvm_msr_ignored_check(vcpu, index, 0, false);
+	 *   - arch/x86/kvm/x86.c|1592| <<kvm_set_msr_ignored_check>> ret = kvm_msr_ignored_check(vcpu, index, data, true);
+	 *   - arch/x86/kvm/x86.c|1626| <<kvm_get_msr_ignored_check>> ret = kvm_msr_ignored_check(vcpu, index, 0, false);
+	 */
 	if (ret == KVM_MSR_RET_INVALID)
 		ret = kvm_msr_ignored_check(vcpu, index, data, true);
 
@@ -1753,6 +1778,10 @@ static int do_get_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 	return kvm_get_msr_ignored_check(vcpu, index, data, true);
 }
 
+/*
+ * 处理KVM_SET_MSRS:
+ *   - arch/x86/kvm/x86.c|4550| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+ */
 static int do_set_msr(struct kvm_vcpu *vcpu, unsigned index, u64 *data)
 {
 	return kvm_set_msr_ignored_check(vcpu, index, *data, true);
@@ -2857,6 +2886,15 @@ static void record_steal_time(struct kvm_vcpu *vcpu)
 	kvm_unmap_gfn(vcpu, &map, &vcpu->arch.st.cache, true, false);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|2700| <<svm_set_msr>> return kvm_set_msr_common(vcpu, msr);
+ *   - arch/x86/kvm/vmx/vmx.c|2069| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2104| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2202| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2205| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ *   - arch/x86/kvm/vmx/vmx.c|2305| <<vmx_set_msr>> ret = kvm_set_msr_common(vcpu, msr_info);
+ */
 int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 {
 	bool pr = false;
@@ -3131,6 +3169,13 @@ int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	default:
 		if (msr && (msr == vcpu->kvm->arch.xen_hvm_config.msr))
 			return xen_hvm_config(vcpu, data);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|3073| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+		 *   - arch/x86/kvm/x86.c|3134| <<kvm_set_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr))
+		 *   - arch/x86/kvm/x86.c|3220| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+		 *   - arch/x86/kvm/x86.c|3403| <<kvm_get_msr_common>> if (kvm_pmu_is_valid_msr(vcpu, msr_info->index))
+		 */
 		if (kvm_pmu_is_valid_msr(vcpu, msr))
 			return kvm_pmu_set_msr(vcpu, msr_info);
 		return KVM_MSR_RET_INVALID;
@@ -3432,6 +3477,12 @@ static int __msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs *msrs,
  *
  * @return number of msrs set successfully.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3697| <<kvm_arch_dev_ioctl(KVM_GET_MSRS)>> r = msr_io(NULL, argp, do_get_msr_feature, 1);
+ *   - arch/x86/kvm/x86.c|4544| <<kvm_arch_vcpu_ioctl(KVM_GET_MSRS)>> r = msr_io(vcpu, argp, do_get_msr, 1);
+ *   - arch/x86/kvm/x86.c|4550| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+ */
 static int msr_io(struct kvm_vcpu *vcpu, struct kvm_msrs __user *user_msrs,
 		  int (*do_msr)(struct kvm_vcpu *vcpu,
 				unsigned index, u64 *data),
@@ -3694,6 +3745,12 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		break;
 	}
 	case KVM_GET_MSRS:
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|3697| <<kvm_arch_dev_ioctl(KVM_GET_MSRS)>> r = msr_io(NULL, argp, do_get_msr_feature, 1);
+		 *   - arch/x86/kvm/x86.c|4544| <<kvm_arch_vcpu_ioctl(KVM_GET_MSRS)>> r = msr_io(vcpu, argp, do_get_msr, 1);
+		 *   - arch/x86/kvm/x86.c|4550| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+		 */
 		r = msr_io(NULL, argp, do_get_msr_feature, 1);
 		break;
 	default:
@@ -4541,12 +4598,24 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 	case KVM_GET_MSRS: {
 		int idx = srcu_read_lock(&vcpu->kvm->srcu);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|3697| <<kvm_arch_dev_ioctl(KVM_GET_MSRS)>> r = msr_io(NULL, argp, do_get_msr_feature, 1);
+		 *   - arch/x86/kvm/x86.c|4544| <<kvm_arch_vcpu_ioctl(KVM_GET_MSRS)>> r = msr_io(vcpu, argp, do_get_msr, 1);
+		 *   - arch/x86/kvm/x86.c|4550| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+		 */
 		r = msr_io(vcpu, argp, do_get_msr, 1);
 		srcu_read_unlock(&vcpu->kvm->srcu, idx);
 		break;
 	}
 	case KVM_SET_MSRS: {
 		int idx = srcu_read_lock(&vcpu->kvm->srcu);
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/x86.c|3697| <<kvm_arch_dev_ioctl(KVM_GET_MSRS)>> r = msr_io(NULL, argp, do_get_msr_feature, 1);
+		 *   - arch/x86/kvm/x86.c|4544| <<kvm_arch_vcpu_ioctl(KVM_GET_MSRS)>> r = msr_io(vcpu, argp, do_get_msr, 1);
+		 *   - arch/x86/kvm/x86.c|4550| <<kvm_arch_vcpu_ioctl(KVM_SET_MSRS)>> r = msr_io(vcpu, argp, do_set_msr, 0);
+		 */
 		r = msr_io(vcpu, argp, do_set_msr, 0);
 		srcu_read_unlock(&vcpu->kvm->srcu, idx);
 		break;
diff --git a/drivers/gpu/drm/i915/gvt/kvmgt.c b/drivers/gpu/drm/i915/gvt/kvmgt.c
index e98aa33dd58f..a2f32f8296a2 100644
--- a/drivers/gpu/drm/i915/gvt/kvmgt.c
+++ b/drivers/gpu/drm/i915/gvt/kvmgt.c
@@ -134,6 +134,10 @@ static void gvt_unpin_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|194| <<gvt_dma_map_page>> ret = gvt_pin_guest_page(vgpu, gfn, size, &page);
+ */
 /* Pin a normal or compound guest page for dma. */
 static int gvt_pin_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,
 		unsigned long size, struct page **page)
@@ -184,6 +188,11 @@ static int gvt_pin_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1902| <<kvmgt_dma_map_guest_page>> ret = gvt_dma_map_page(vgpu, gfn, dma_addr, size);
+ *   - drivers/gpu/drm/i915/gvt/kvmgt.c|1914| <<kvmgt_dma_map_guest_page>> ret = gvt_dma_map_page(vgpu, gfn, dma_addr, size);
+ */
 static int gvt_dma_map_page(struct intel_vgpu *vgpu, unsigned long gfn,
 		dma_addr_t *dma_addr, unsigned long size)
 {
diff --git a/drivers/iommu/iommu.c b/drivers/iommu/iommu.c
index c5758fb696cc..c21d72a82755 100644
--- a/drivers/iommu/iommu.c
+++ b/drivers/iommu/iommu.c
@@ -1612,6 +1612,27 @@ static struct iommu_domain *__iommu_domain_alloc(struct bus_type *bus,
 	return domain;
 }
 
+/*
+ * called by:
+ *   - arch/arm/mm/dma-mapping.c|2113| <<arm_iommu_create_mapping>> mapping->domain = iommu_domain_alloc(bus);
+ *   - drivers/gpu/drm/msm/adreno/a6xx_gmu.c|998| <<a6xx_gmu_memory_probe>> gmu->domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/msm/disp/dpu1/dpu_kms.c|741| <<_dpu_kms_mmu_init>> domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/msm/disp/mdp4/mdp4_kms.c|566| <<mdp4_get_config>> config.iommu = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/msm/disp/mdp5/mdp5_cfg.c|845| <<mdp5_get_config>> config.iommu = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/msm/msm_gpu.c|819| <<msm_gpu_create_address_space>> struct iommu_domain *iommu = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/nouveau/nvkm/engine/device/tegra.c|127| <<nvkm_device_tegra_probe_iommu>> tdev->iommu.domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/rockchip/rockchip_drm_drv.c|84| <<rockchip_drm_init_iommu>> private->domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/drm/tegra/drm.c|100| <<tegra_drm_load>> tegra->domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/gpu/host1x/dev.c|286| <<host1x_probe>> host->domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/infiniband/hw/usnic/usnic_uiom.c|451| <<usnic_uiom_alloc_pd>> pd->domain = domain = iommu_domain_alloc(&pci_bus_type);
+ *   - drivers/iommu/amd_iommu_v2.c|781| <<amd_iommu_init_device>> dev_state->domain = iommu_domain_alloc(&pci_bus_type);
+ *   - drivers/media/platform/qcom/venus/firmware.c|255| <<venus_firmware_init>> iommu_dom = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/remoteproc/remoteproc_core.c|109| <<rproc_enable_iommu>> domain = iommu_domain_alloc(dev->bus);
+ *   - drivers/soc/fsl/qbman/qman_portal.c|54| <<portal_set_cpu>> pcfg->iommu_domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/staging/media/tegra-vde/iommu.c|81| <<tegra_vde_iommu_init>> vde->domain = iommu_domain_alloc(&platform_bus_type);
+ *   - drivers/vfio/vfio_iommu_type1.c|3261| <<vfio_iommu_type1_attach_group>> domain->domain = iommu_domain_alloc(bus);
+ *   - drivers/vhost/vdpa.c|1159| <<vhost_vdpa_alloc_domain>> v->domain = iommu_domain_alloc(bus);
+ */
 struct iommu_domain *iommu_domain_alloc(struct bus_type *bus)
 {
 	return __iommu_domain_alloc(bus, IOMMU_DOMAIN_UNMANAGED);
@@ -1858,6 +1879,30 @@ static size_t iommu_pgsize(struct iommu_domain *domain,
 	return pgsize;
 }
 
+/*
+ * called by:
+ *   - arch/arm/mm/dma-mapping.c|1374| <<__iommu_create_mapping>> ret = iommu_map(mapping->domain, iova, phys, len, __dma_info_to_prot(DMA_BIDIRECTIONAL, attrs));
+ *   - arch/arm/mm/dma-mapping.c|1643| <<__map_sg_chunk>> ret = iommu_map(mapping->domain, iova, phys, len, prot);
+ *   - arch/arm/mm/dma-mapping.c|1851| <<arm_coherent_iommu_map_page>> ret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, prot);
+ *   - arch/arm/mm/dma-mapping.c|1957| <<arm_iommu_map_resource>> ret = iommu_map(mapping->domain, dma_addr, addr, len, prot);
+ *   - drivers/gpu/drm/msm/adreno/a6xx_gmu.c|947| <<a6xx_gmu_memory_alloc>> ret = iommu_map(gmu->domain, bo->iova + (PAGE_SIZE * i), page_to_phys(bo->pages[i]), PAGE_SIZE, IOMMU_READ | IOMMU_WRITE);
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/instmem/gk20a.c|477| <<gk20a_instobj_ctor_iommu>> ret = iommu_map(imem->domain, offset, node->dma_addrs[i], PAGE_SIZE, IOMMU_READ | IOMMU_WRITE);
+ *   - drivers/gpu/drm/tegra/drm.c|1168| <<tegra_drm_alloc>> err = iommu_map(tegra->domain, *dma, virt_to_phys(virt), size, IOMMU_READ | IOMMU_WRITE);
+ *   - drivers/gpu/host1x/cdma.c|107| <<host1x_pushbuffer_init>> err = iommu_map(host1x->domain, pb->dma, pb->phys, size, IOMMU_READ);
+ *   - drivers/infiniband/hw/usnic/usnic_uiom.c|284| <<usnic_uiom_map_sorted_intervals>> err = iommu_map(pd->domain, va_start, pa_start,
+ *   - drivers/infiniband/hw/usnic/usnic_uiom.c|301| <<usnic_uiom_map_sorted_intervals>> err = iommu_map(pd->domain, va_start, pa_start,
+ *   - drivers/iommu/dma-iommu.c|479| <<__iommu_dma_map>> if (iommu_map(domain, iova, phys - iova_off, size, prot)) {
+ *   - drivers/iommu/dma-iommu.c|1163| <<iommu_dma_get_msi_page>> if (iommu_map(domain, iova, msi_addr, size, prot))
+ *   - drivers/iommu/iommu.c|663| <<iommu_group_create_direct_mappings>> ret = iommu_map(domain, addr, addr, pg_size, entry->prot);
+ *   - drivers/iommu/iommu.c|2010| <<iommu_map_sg>> ret = iommu_map(domain, iova + mapped, start, len, prot);
+ *   - drivers/media/platform/qcom/venus/firmware.c|144| <<venus_boot_no_tz>> ret = iommu_map(iommu, VENUS_FW_START_ADDR, mem_phys, mem_size,
+ *   - drivers/remoteproc/remoteproc_core.c|703| <<rproc_handle_devmem>> ret = iommu_map(rproc->domain, rsc->da, rsc->pa, rsc->len, rsc->flags);
+ *   - drivers/remoteproc/remoteproc_core.c|793| <<rproc_alloc_carveout>> ret = iommu_map(rproc->domain, mem->da, dma, mem->len,
+ *   - drivers/vfio/vfio_iommu_type1.c|1245| <<vfio_iommu_map>> ret = iommu_map(d->domain, iova, (phys_addr_t)pfn << PAGE_SHIFT,
+ *   - drivers/vfio/vfio_iommu_type1.c|1675| <<vfio_iommu_replay>> ret = iommu_map(domain->domain, iova, phys,
+ *   - drivers/vfio/vfio_iommu_type1.c|1759| <<vfio_test_domain_fgsp>> ret = iommu_map(domain->domain, 0, page_to_phys(pages), PAGE_SIZE * 2,
+ *   - drivers/vhost/vdpa.c|825| <<vhost_vdpa_map>> r = iommu_map(v->domain, iova, pa, size,
+ */
 int iommu_map(struct iommu_domain *domain, unsigned long iova,
 	      phys_addr_t paddr, size_t size, int prot)
 {
diff --git a/drivers/vfio/vfio.c b/drivers/vfio/vfio.c
index e3016e1f4041..2f974d93c9ca 100644
--- a/drivers/vfio/vfio.c
+++ b/drivers/vfio/vfio.c
@@ -1074,6 +1074,14 @@ static int __vfio_container_attach_groups(struct vfio_container *container,
 	int ret = -ENODEV;
 
 	list_for_each_entry(group, &container->group_list, container_next) {
+		/*
+		 * 在以下使用vfio_iommu_driver_ops->attach_group:
+		 *   - drivers/vfio/vfio.c|213| <<global>> .attach_group = vfio_noiommu_attach_group,
+		 *   - drivers/vfio/vfio_iommu_spapr_tce.c|1368| <<global>> .attach_group = tce_iommu_attach_group,
+		 *   - drivers/vfio/vfio_iommu_type1.c|3129| <<global>> .attach_group = vfio_iommu_type1_attach_group,
+		 *   - drivers/vfio/vfio.c|1077| <<__vfio_container_attach_groups>> ret = driver->ops->attach_group(data, group->iommu_group);
+		 *   - drivers/vfio/vfio.c|1402| <<vfio_group_set_container>> ret = driver->ops->attach_group(container->iommu_data,
+		 */
 		ret = driver->ops->attach_group(data, group->iommu_group);
 		if (ret)
 			goto unwind;
@@ -1362,6 +1370,10 @@ static void vfio_group_try_dissolve_container(struct vfio_group *group)
 		__vfio_group_unset_container(group);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio.c|1569| <<vfio_group_fops_unl_ioctl(VFIO_GROUP_SET_CONTAINER)>> ret = vfio_group_set_container(group, fd);
+ */
 static int vfio_group_set_container(struct vfio_group *group, int container_fd)
 {
 	struct fd f;
@@ -1399,6 +1411,14 @@ static int vfio_group_set_container(struct vfio_group *group, int container_fd)
 
 	driver = container->iommu_driver;
 	if (driver) {
+		/*
+		 * 在以下使用vfio_iommu_driver_ops->attach_group:
+		 *   - drivers/vfio/vfio.c|213| <<global>> .attach_group = vfio_noiommu_attach_group,
+		 *   - drivers/vfio/vfio_iommu_spapr_tce.c|1368| <<global>> .attach_group = tce_iommu_attach_group,
+		 *   - drivers/vfio/vfio_iommu_type1.c|3129| <<global>> .attach_group = vfio_iommu_type1_attach_group,
+		 *   - drivers/vfio/vfio.c|1077| <<__vfio_container_attach_groups>> ret = driver->ops->attach_group(data, group->iommu_group);
+		 *   - drivers/vfio/vfio.c|1402| <<vfio_group_set_container>> ret = driver->ops->attach_group(container->iommu_data,
+		 */
 		ret = driver->ops->attach_group(container->iommu_data,
 						group->iommu_group);
 		if (ret)
@@ -1770,6 +1790,12 @@ struct vfio_group *vfio_group_get_external_user(struct file *filep)
 }
 EXPORT_SYMBOL_GPL(vfio_group_get_external_user);
 
+/*
+ * 在以下使用vfio_group_put_external_user():
+ *   - drivers/vfio/pci/vfio_pci.c|1235| <<vfio_pci_ioctl>> vfio_group_put_external_user(groups[group_idx].group);
+ *   - virt/kvm/vfio.c|71| <<kvm_vfio_group_put_external_user>> fn = symbol_get(vfio_group_put_external_user);
+ *   - virt/kvm/vfio.c|77| <<kvm_vfio_group_put_external_user>> symbol_put(vfio_group_put_external_user);
+ */
 void vfio_group_put_external_user(struct vfio_group *group)
 {
 	vfio_group_try_dissolve_container(group);
diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c
index 0befe9185b16..4921109294de 100644
--- a/drivers/vfio/vfio_iommu_type1.c
+++ b/drivers/vfio/vfio_iommu_type1.c
@@ -43,6 +43,80 @@
 #include <linux/irqdomain.h>
 #include <linux/padata.h>
 
+/*
+ * QEMU通过ioctl(VFIO_IOMMU_MAP_DMA)要求KVM去map一段内存.
+ *
+ * struct vfio_iommu_type1_dma_map {
+ *	__u32   argsz;
+ *	__u32   flags;
+ * #define VFIO_DMA_MAP_FLAG_READ (1 << 0)         // readable from device
+ * #define VFIO_DMA_MAP_FLAG_WRITE (1 << 1)        // writable from device
+ * #define VFIO_DMA_MAP_FLAG_VADDR (1 << 2)
+ *	__u64   vaddr;                          // Process virtual address
+ *	__u64   iova;                           // IO virtual address
+ *	__u64   size;                           // Size of mapping (bytes)
+ * };
+ *
+ * 下面是KVM的callstack.
+ *
+ * vfio_iommu_type1_ioctl
+ * vfio_fops_unl_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 最终在KVM生成vfio_dma, 插入到vfio_iommu->dma_list.
+ *
+ * struct vfio_dma {
+ *	struct rb_node          node;
+ *	dma_addr_t              iova;           // Device address
+ *	unsigned long           vaddr;          // Process virtual addr
+ *	size_t                  size;           // Map size (bytes)
+ *	int                     prot;           // IOMMU_READ/WRITE
+ *	bool                    iommu_mapped;
+ *	bool                    lock_cap;       // capable(CAP_IPC_LOCK)
+ *	bool                    vaddr_invalid;
+ *	struct task_struct      *task;
+ *	struct rb_root          pfn_list;       // Ex-user pinned pfn list
+ * };
+ *
+ * 所以如果ioctl完成的时候, 这段内存要么全被pin, 要么全部unpin.
+ * 并且, 要么vfio_dma在vfio_iommu->dma_list, 要么被移除被free.
+ *
+ *
+ * PADATA会把这个range分割成好几个部分并行pin. 只有全部成功了才会设置/增加dma->size.
+ * 有一个失败了也不会设置dma->size.
+ *
+ * 2225         ret = padata_do_multithreaded(&job);
+ * 2226
+ * 2227         dma->iommu_mapped = true;
+ * 2228
+ * 2229         if (ret)
+ * 2230                 vfio_remove_dma(iommu, dma);
+ * 2231         else
+ * 2232                 dma->size += map_size;
+ * 2233
+ * 2234         return ret;
+ *
+ *
+ * 1. 把pin的range分成好几个parallel的helper.
+ *
+ * 2. 每个helper执行vfio_pin_map_dma_chunk().
+ * 如果失败了在vfio_pin_map_dma_chunk()内部调用.
+ *
+ * 3. 对于那些成功了的, 会在__padata_do_multithreaded()->padata_undo()统一unpin.
+ *
+ * 4. 关于每一个helper的vfio_pin_map_dma_chunk()
+ *
+ * vfio_pin_map_dma_chunk()
+ * -> while (unmapped_size)
+ *    -> vfio_pin_pages_remote()
+ *       unpin_out: unpin当前while iteration的page
+ *    -> vfio_pin_map_dma_undo() unpin之前成功的iteration.
+ */
+
 #define DRIVER_VERSION  "0.2"
 #define DRIVER_AUTHOR   "Alex Williamson <alex.williamson@redhat.com>"
 #define DRIVER_DESC     "Type1 IOMMU driver for VFIO"
@@ -65,17 +139,102 @@ MODULE_PARM_DESC(dma_entry_limit,
 		 "Maximum number of user DMA mappings per container (65535).");
 
 struct vfio_iommu {
+	/*
+	 * 在以下使用vfio_iommu->domain_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|182| <<IS_IOMMU_CAP_DOMAIN_IN_CONTAINER>> (!list_empty(&iommu->domain_list))
+	 *   - drivers/vfio/vfio_iommu_type1.c|1454| <<vfio_unmap_unpin>> domain = d = list_first_entry(&iommu->domain_list,
+	 *   - drivers/vfio/vfio_iommu_type1.c|1457| <<vfio_unmap_unpin>> list_for_each_entry_continue(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1575| <<vfio_pgsize_bitmap>> list_for_each_entry(domain, &iommu->domain_list, next)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1754| <<vfio_iommu_map>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1766| <<vfio_iommu_map>> list_for_each_entry_continue_reverse(d, &iommu->domain_list, next)
+	 *   - drivers/vfio/vfio_iommu_type1.c|2249| <<vfio_iommu_replay>> if (!list_empty(&iommu->domain_list))
+	 *   - drivers/vfio/vfio_iommu_type1.c|2250| <<vfio_iommu_replay>> d = list_first_entry(&iommu->domain_list,
+	 *   - drivers/vfio/vfio_iommu_type1.c|2845| <<vfio_iommu_type1_attach_group>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2979| <<vfio_iommu_type1_attach_group>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3009| <<vfio_iommu_type1_attach_group>> list_add(&domain->next, &iommu->domain_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3113| <<vfio_iommu_aper_expand>> list_for_each_entry(domain, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3148| <<vfio_iommu_resv_refresh>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3230| <<vfio_iommu_type1_detach_group>> list_for_each_entry(domain, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3246| <<vfio_iommu_type1_detach_group>> if (list_is_singular(&iommu->domain_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3311| <<vfio_iommu_type1_open>> INIT_LIST_HEAD(&iommu->domain_list);
+	 *   -  drivers/vfio/vfio_iommu_type1.c|3379| <<vfio_iommu_type1_release>> list_for_each_entry_safe(domain, domain_tmp, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3396| <<vfio_domains_have_iommu_cache>> list_for_each_entry(domain, &iommu->domain_list, next) {
+	 */
 	struct list_head	domain_list;
+	/*
+	 * 在以下使用vfio_iommu->iova_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|2047| <<vfio_iommu_iova_dma_valid>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2664| <<vfio_iommu_aper_conflict>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2826| <<vfio_iommu_iova_get_copy>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2846| <<vfio_iommu_iova_insert_copy>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3360| <<vfio_iommu_type1_open>> INIT_LIST_HEAD(&iommu->iova_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3433| <<vfio_iommu_type1_release>> vfio_iommu_iova_free(&iommu->iova_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3486| <<vfio_iommu_iova_build_caps>> list_for_each_entry(iova, &iommu->iova_list, list)
+	 *   - drivers/vfio/vfio_iommu_type1.c|3508| <<vfio_iommu_iova_build_caps>> list_for_each_entry(iova, &iommu->iova_list, list) {
+	 */
 	struct list_head	iova_list;
+	/*
+	 * 在以下使用vfio_iommu->external_domain:
+	 *   - drivers/vfio/vfio_iommu_type1.c|3166| <<vfio_iommu_type1_attach_group>> if (iommu->external_domain) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3167| <<vfio_iommu_type1_attach_group>> if (find_iommu_group(iommu->external_domain, iommu_group)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3196| <<vfio_iommu_type1_attach_group>> if (!iommu->external_domain) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3198| <<vfio_iommu_type1_attach_group>> iommu->external_domain = domain;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3204| <<vfio_iommu_type1_attach_group>> list_add(&group->next, &iommu->external_domain->group_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3557| <<vfio_iommu_type1_detach_group>> if (iommu->external_domain) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3558| <<vfio_iommu_type1_detach_group>> group = find_iommu_group(iommu->external_domain, iommu_group);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3563| <<vfio_iommu_type1_detach_group>> if (list_empty(&iommu->external_domain->group_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3569| <<vfio_iommu_type1_detach_group>> kfree(iommu->external_domain);
+	 *   -  11 drivers/vfio/vfio_iommu_type1.c|3570| <<vfio_iommu_type1_detach_group>> iommu->external_domain = NULL;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3600| <<vfio_iommu_type1_detach_group>> if (!iommu->external_domain) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3732| <<vfio_iommu_type1_release>> if (iommu->external_domain) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3733| <<vfio_iommu_type1_release>> vfio_release_domain(iommu->external_domain, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3734| <<vfio_iommu_type1_release>> kfree(iommu->external_domain);
+	 */
 	struct vfio_domain	*external_domain; /* domain for external user */
 	struct mutex		lock;
+	/*
+	 * 在以下使用vfio_iommu->dma_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|214| <<vfio_find_dma>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|238| <<vfio_find_dma_first_node>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|270| <<vfio_link_dma>> struct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;
+	 *   - drivers/vfio/vfio_iommu_type1.c|284| <<vfio_link_dma>> rb_insert_color(&new->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|293| <<vfio_unlink_dma>> rb_erase(&old->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2227| <<vfio_iommu_replay>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2326| <<vfio_iommu_replay>> for (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3015| <<vfio_iommu_unmap_unpin_all>> while ((node = rb_first(&iommu->dma_list)))
+	 *   - drivers/vfio/vfio_iommu_type1.c|3027| <<vfio_iommu_unmap_unpin_reaccount>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3274| <<vfio_iommu_type1_open>> iommu->dma_list = RB_ROOT;
+	 */
 	struct rb_root		dma_list;
 	struct blocking_notifier_head notifier;
+	/*
+	 * 在以下使用vfio_iommu->dma_avail:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1636| <<vfio_remove_dma>> iommu->dma_avail++;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2255| <<vfio_dma_do_map>> if (!iommu->dma_avail) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2274| <<vfio_dma_do_map>> iommu->dma_avail--;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3448| <<vfio_iommu_type1_open>> iommu->dma_avail = dma_entry_limit;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3618| <<vfio_iommu_dma_avail_build_caps>> cap_dma_avail.avail = iommu->dma_avail;
+	 */
 	unsigned int		dma_avail;
 	unsigned int		vaddr_invalid_count;
+	/*
+	 * 在以下使用vfio_iommu->vaddr_wait:
+	 *   - drivers/vfio/vfio_iommu_type1.c|634| <<vfio_wait>> prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|638| <<vfio_wait>> finish_wait(&iommu->vaddr_wait, &wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1296| <<vfio_remove_dma>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1853| <<vfio_dma_do_map>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2982| <<vfio_iommu_type1_open>> init_waitqueue_head(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3353| <<vfio_iommu_type1_notify>> wake_up_all(&iommu->vaddr_wait);
+	 */
 	wait_queue_head_t	vaddr_wait;
 	bool			v2;
 	bool			nesting;
+	/*
+	 * 在以下使用vfio_iommu->container_open:
+	 *   - drivers/vfio/vfio_iommu_type1.c|792| <<vfio_wait>> if (kthread_should_stop() || !iommu->container_open ||
+	 *   - drivers/vfio/vfio_iommu_type1.c|3374| <<vfio_iommu_type1_open>> iommu->container_open = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3808| <<vfio_iommu_type1_notify>> iommu->container_open = false;
+	 */
 	bool			container_open;
 };
 
@@ -93,10 +252,37 @@ struct vfio_dma {
 	unsigned long		vaddr;		/* Process virtual addr */
 	size_t			size;		/* Map size (bytes) */
 	int			prot;		/* IOMMU_READ/WRITE */
+	/*
+	 * 在以下使用vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1125| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1556| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1809| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1859| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1875| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1885| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2532| <<vfio_iommu_unmap_unpin_reaccount>> dma->iommu_mapped = false;
+	 */
 	bool			iommu_mapped;
 	bool			lock_cap;	/* capable(CAP_IPC_LOCK) */
 	bool			vaddr_invalid;
 	struct task_struct	*task;
+	/*
+	 * 在以下使用vfio_dma->pfn_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|266| <<vfio_find_vpfn>> struct rb_node *node = dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|287| <<vfio_link_pfn>> link = &dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|299| <<vfio_link_pfn>> rb_insert_color(&new->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|304| <<vfio_unlink_pfn>> rb_erase(&old->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1178| <<vfio_remove_dma>> WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));
+	 *   - drivers/vfio/vfio_iommu_type1.c|1325| <<vfio_dma_do_unmap>> if (!RB_EMPTY_ROOT(&dma->pfn_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1789| <<vfio_dma_do_map>> dma->pfn_list = RB_ROOT;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2633| <<vfio_iommu_unmap_unpin_reaccount>> p = rb_first(&dma->pfn_list);
+	 *
+	 * 一般没有调用
+	 * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.pin_pages = vfio_iommu_type1_pin_pages()
+	 * -> vfio_add_to_pfn_list()
+	 *    -> vfio_link_pfn()
+	 *       -> rb_insert_color(&new->node, &dma->pfn_list);
+	 */
 	struct rb_root		pfn_list;	/* Ex-user pinned pfn list */
 };
 
@@ -129,6 +315,27 @@ struct vfio_regions {
 	size_t len;
 };
 
+/*
+ * 在以下使用vfio_iommu->domain_list:
+ *   - drivers/vfio/vfio_iommu_type1.c|182| <<IS_IOMMU_CAP_DOMAIN_IN_CONTAINER>> (!list_empty(&iommu->domain_list))
+ *   - drivers/vfio/vfio_iommu_type1.c|1454| <<vfio_unmap_unpin>> domain = d = list_first_entry(&iommu->domain_list,
+ *   - drivers/vfio/vfio_iommu_type1.c|1457| <<vfio_unmap_unpin>> list_for_each_entry_continue(d, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|1575| <<vfio_pgsize_bitmap>> list_for_each_entry(domain, &iommu->domain_list, next)
+ *   - drivers/vfio/vfio_iommu_type1.c|1754| <<vfio_iommu_map>> list_for_each_entry(d, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|1766| <<vfio_iommu_map>> list_for_each_entry_continue_reverse(d, &iommu->domain_list, next)
+ *   - drivers/vfio/vfio_iommu_type1.c|2249| <<vfio_iommu_replay>> if (!list_empty(&iommu->domain_list))
+ *   - drivers/vfio/vfio_iommu_type1.c|2250| <<vfio_iommu_replay>> d = list_first_entry(&iommu->domain_list,
+ *   - drivers/vfio/vfio_iommu_type1.c|2845| <<vfio_iommu_type1_attach_group>> list_for_each_entry(d, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|2979| <<vfio_iommu_type1_attach_group>> list_for_each_entry(d, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3009| <<vfio_iommu_type1_attach_group>> list_add(&domain->next, &iommu->domain_list);
+ *   - drivers/vfio/vfio_iommu_type1.c|3113| <<vfio_iommu_aper_expand>> list_for_each_entry(domain, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3148| <<vfio_iommu_resv_refresh>> list_for_each_entry(d, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3230| <<vfio_iommu_type1_detach_group>> list_for_each_entry(domain, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3246| <<vfio_iommu_type1_detach_group>> if (list_is_singular(&iommu->domain_list)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3311| <<vfio_iommu_type1_open>> INIT_LIST_HEAD(&iommu->domain_list);
+ *   -  drivers/vfio/vfio_iommu_type1.c|3379| <<vfio_iommu_type1_release>> list_for_each_entry_safe(domain, domain_tmp, &iommu->domain_list, next) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3396| <<vfio_domains_have_iommu_cache>> list_for_each_entry(domain, &iommu->domain_list, next) {
+ */
 #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\
 					(!list_empty(&iommu->domain_list))
 
@@ -141,9 +348,40 @@ static int put_pfn(unsigned long pfn, int prot);
  * into DMA'ble space using the IOMMU
  */
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|513| <<vfio_find_dma_valid>> *dma_p = vfio_find_dma(iommu, start, size);
+ *   - drivers/vfio/vfio_iommu_type1.c|757| <<vfio_iommu_type1_pin_pages>> dma = vfio_find_dma(iommu, iova, PAGE_SIZE);
+ *   - drivers/vfio/vfio_iommu_type1.c|797| <<vfio_iommu_type1_pin_pages>> dma = vfio_find_dma(iommu, iova, PAGE_SIZE);
+ *   - drivers/vfio/vfio_iommu_type1.c|829| <<vfio_iommu_type1_unpin_pages>> dma = vfio_find_dma(iommu, iova, PAGE_SIZE);
+ *   - drivers/vfio/vfio_iommu_type1.c|1117| <<vfio_dma_do_unmap>> dma = vfio_find_dma(iommu, iova, 1);
+ *   - drivers/vfio/vfio_iommu_type1.c|1121| <<vfio_dma_do_unmap>> dma = vfio_find_dma(iommu, iova + size - 1, 0);
+ *   - drivers/vfio/vfio_iommu_type1.c|1410| <<vfio_dma_do_map>> dma = vfio_find_dma(iommu, iova, size);
+ *   - drivers/vfio/vfio_iommu_type1.c|1865| <<vfio_iommu_aper_conflict>> if (vfio_find_dma(iommu, first->start, start - first->start))
+ *   - drivers/vfio/vfio_iommu_type1.c|1871| <<vfio_iommu_aper_conflict>> if (vfio_find_dma(iommu, end + 1, last->end - end))
+ *   - drivers/vfio/vfio_iommu_type1.c|1932| <<vfio_iommu_resv_conflict>> if (vfio_find_dma(iommu, region->start, region->length))
+ *
+ *  90 struct vfio_dma {
+ *  91         struct rb_node          node;
+ *  92         dma_addr_t              iova;           // Device address
+ *  93         unsigned long           vaddr;          // Process virtual addr
+ *  94         size_t                  size;           // Map size (bytes)
+ *  95         int                     prot;           // IOMMU_READ/WRITE
+ *  96         bool                    iommu_mapped;
+ *  97         bool                    lock_cap;       // capable(CAP_IPC_LOCK)
+ *  98         bool                    vaddr_invalid;
+ *  99         struct task_struct      *task;
+ * 100         struct rb_root          pfn_list;       // Ex-user pinned pfn list
+ * 101 };
+ */
 static struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,
 				      dma_addr_t start, size_t size)
 {
+	/*
+	 * struct vfio_iommu *iommu:
+	 * -> struct rb_root dma_list;
+	 *    -> struct rb_node *rb_node;
+	 */
 	struct rb_node *node = iommu->dma_list.rb_node;
 
 	while (node) {
@@ -160,6 +398,10 @@ static struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1288| <<vfio_dma_do_unmap>> n = first_n = vfio_find_dma_first_node(iommu, iova, size);
+ */
 static struct rb_node *vfio_find_dma_first_node(struct vfio_iommu *iommu,
 						dma_addr_t start, u64 size)
 {
@@ -185,6 +427,15 @@ static struct rb_node *vfio_find_dma_first_node(struct vfio_iommu *iommu,
 	return res;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1482| <<vfio_dma_do_map>> vfio_link_dma(iommu, dma);
+ *
+ * 把vfio_dma插入到vfio_mmu
+ * struct vfio_iommu *iommu:
+ * -> struct rb_root dma_list;
+ *    -> struct rb_node *rb_node;
+ */
 static void vfio_link_dma(struct vfio_iommu *iommu, struct vfio_dma *new)
 {
 	struct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;
@@ -200,10 +451,27 @@ static void vfio_link_dma(struct vfio_iommu *iommu, struct vfio_dma *new)
 			link = &(*link)->rb_right;
 	}
 
+	/*
+	 * 在以下使用vfio_iommu->dma_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|214| <<vfio_find_dma>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|238| <<vfio_find_dma_first_node>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|270| <<vfio_link_dma>> struct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;
+	 *   - drivers/vfio/vfio_iommu_type1.c|284| <<vfio_link_dma>> rb_insert_color(&new->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|293| <<vfio_unlink_dma>> rb_erase(&old->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2227| <<vfio_iommu_replay>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2326| <<vfio_iommu_replay>> for (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3015| <<vfio_iommu_unmap_unpin_all>> while ((node = rb_first(&iommu->dma_list)))
+	 *   - drivers/vfio/vfio_iommu_type1.c|3027| <<vfio_iommu_unmap_unpin_reaccount>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3274| <<vfio_iommu_type1_open>> iommu->dma_list = RB_ROOT;
+	 */
 	rb_link_node(&new->node, parent, link);
 	rb_insert_color(&new->node, &iommu->dma_list);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1180| <<vfio_remove_dma>> vfio_unlink_dma(iommu, dma);
+ */
 static void vfio_unlink_dma(struct vfio_iommu *iommu, struct vfio_dma *old)
 {
 	rb_erase(&old->node, &iommu->dma_list);
@@ -212,9 +480,30 @@ static void vfio_unlink_dma(struct vfio_iommu *iommu, struct vfio_dma *old)
 /*
  * Helper Functions for host iova-pfn list
  */
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|315| <<vfio_iova_get_vfio_pfn>> struct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);
+ *   - drivers/vfio/vfio_iommu_type1.c|616| <<vfio_pin_pages_remote>> if (!rsvd && !vfio_find_vpfn(dma, iova)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|647| <<vfio_pin_pages_remote>> if (!rsvd && !vfio_find_vpfn(dma, iova)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|703| <<vfio_unpin_pages_remote>> if (vfio_find_vpfn(dma, iova))
+ *   - drivers/vfio/vfio_iommu_type1.c|745| <<vfio_unpin_page_external>> struct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);
+ */
 static struct vfio_pfn *vfio_find_vpfn(struct vfio_dma *dma, dma_addr_t iova)
 {
 	struct vfio_pfn *vpfn;
+	/*
+	 * 在以下使用vfio_dma->pfn_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|266| <<vfio_find_vpfn>> struct rb_node *node = dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|287| <<vfio_link_pfn>> link = &dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|299| <<vfio_link_pfn>> rb_insert_color(&new->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|304| <<vfio_unlink_pfn>> rb_erase(&old->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1178| <<vfio_remove_dma>> WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));
+	 *   - drivers/vfio/vfio_iommu_type1.c|1325| <<vfio_dma_do_unmap>> if (!RB_EMPTY_ROOT(&dma->pfn_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1789| <<vfio_dma_do_map>> dma->pfn_list = RB_ROOT;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2633| <<vfio_iommu_unmap_unpin_reaccount>> p = rb_first(&dma->pfn_list);
+	 *
+	 * Ex-user pinned pfn list
+	 */
 	struct rb_node *node = dma->pfn_list.rb_node;
 
 	while (node) {
@@ -230,12 +519,36 @@ static struct vfio_pfn *vfio_find_vpfn(struct vfio_dma *dma, dma_addr_t iova)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|381| <<vfio_add_to_pfn_list>> vfio_link_pfn(dma, vpfn);
+ *
+ * struct vfio_pfn {
+ *     struct rb_node          node;
+ *     dma_addr_t              iova;           // Device address
+ *     unsigned long           pfn;            // Host pfn
+ *     atomic_t                ref_count;
+ * };
+ */
 static void vfio_link_pfn(struct vfio_dma *dma,
 			  struct vfio_pfn *new)
 {
 	struct rb_node **link, *parent = NULL;
 	struct vfio_pfn *vpfn;
 
+	/*
+	 * 在以下使用vfio_dma->pfn_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|266| <<vfio_find_vpfn>> struct rb_node *node = dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|287| <<vfio_link_pfn>> link = &dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|299| <<vfio_link_pfn>> rb_insert_color(&new->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|304| <<vfio_unlink_pfn>> rb_erase(&old->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1178| <<vfio_remove_dma>> WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));
+	 *   - drivers/vfio/vfio_iommu_type1.c|1325| <<vfio_dma_do_unmap>> if (!RB_EMPTY_ROOT(&dma->pfn_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1789| <<vfio_dma_do_map>> dma->pfn_list = RB_ROOT;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2633| <<vfio_iommu_unmap_unpin_reaccount>> p = rb_first(&dma->pfn_list);
+	 *
+	 * Ex-user pinned pfn list
+	 */
 	link = &dma->pfn_list.rb_node;
 	while (*link) {
 		parent = *link;
@@ -248,14 +561,43 @@ static void vfio_link_pfn(struct vfio_dma *dma,
 	}
 
 	rb_link_node(&new->node, parent, link);
+	/*
+	 * 一般没有调用
+	 * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.pin_pages = vfio_iommu_type1_pin_pages()
+	 * -> vfio_add_to_pfn_list()
+	 *    -> vfio_link_pfn()
+	 *       -> rb_insert_color(&new->node, &dma->pfn_list);
+	 */
 	rb_insert_color(&new->node, &dma->pfn_list);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|388| <<vfio_remove_from_pfn_list>> vfio_unlink_pfn(dma, vpfn);
+ */
 static void vfio_unlink_pfn(struct vfio_dma *dma, struct vfio_pfn *old)
 {
+	/*
+	 * 在以下使用vfio_dma->pfn_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|266| <<vfio_find_vpfn>> struct rb_node *node = dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|287| <<vfio_link_pfn>> link = &dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|299| <<vfio_link_pfn>> rb_insert_color(&new->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|304| <<vfio_unlink_pfn>> rb_erase(&old->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1178| <<vfio_remove_dma>> WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));
+	 *   - drivers/vfio/vfio_iommu_type1.c|1325| <<vfio_dma_do_unmap>> if (!RB_EMPTY_ROOT(&dma->pfn_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1789| <<vfio_dma_do_map>> dma->pfn_list = RB_ROOT;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2633| <<vfio_iommu_unmap_unpin_reaccount>> p = rb_first(&dma->pfn_list);
+	 */
 	rb_erase(&old->node, &dma->pfn_list);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1000| <<vfio_iommu_type1_pin_pages>> ret = vfio_add_to_pfn_list(dma, iova, phys_pfn[i]);
+ *
+ * vfio_add_to_pfn_list()只被一般没有调用的vfio_iommu_type1_pin_pages()调用
+ * 所以vfio_add_to_pfn_list()一般不被调用
+ */
 static int vfio_add_to_pfn_list(struct vfio_dma *dma, dma_addr_t iova,
 				unsigned long pfn)
 {
@@ -268,10 +610,17 @@ static int vfio_add_to_pfn_list(struct vfio_dma *dma, dma_addr_t iova,
 	vpfn->iova = iova;
 	vpfn->pfn = pfn;
 	atomic_set(&vpfn->ref_count, 1);
+	/*
+	 * 只在此处调用
+	 */
 	vfio_link_pfn(dma, vpfn);
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|408| <<vfio_iova_put_vfio_pfn>> vfio_remove_from_pfn_list(dma, vpfn);
+ */
 static void vfio_remove_from_pfn_list(struct vfio_dma *dma,
 				      struct vfio_pfn *vpfn)
 {
@@ -279,6 +628,10 @@ static void vfio_remove_from_pfn_list(struct vfio_dma *dma,
 	kfree(vpfn);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|988| <<vfio_iommu_type1_pin_pages>> vpfn = vfio_iova_get_vfio_pfn(dma, iova);
+ */
 static struct vfio_pfn *vfio_iova_get_vfio_pfn(struct vfio_dma *dma,
 					       unsigned long iova)
 {
@@ -289,6 +642,10 @@ static struct vfio_pfn *vfio_iova_get_vfio_pfn(struct vfio_dma *dma,
 	return vpfn;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|910| <<vfio_unpin_page_external>> unlocked = vfio_iova_put_vfio_pfn(dma, vpfn);
+ */
 static int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)
 {
 	int ret = 0;
@@ -300,6 +657,18 @@ static int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|611| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|659| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|686| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|997| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1255| <<vfio_pin_map_dma_chunk>> ret = vfio_lock_acct(dma, cache_size, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|1285| <<vfio_pin_map_dma_chunk>> vfio_lock_acct(dma, -lock_cache, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|2246| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+ */
 static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 {
 	struct mm_struct *mm;
@@ -313,7 +682,16 @@ static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
 		return -ESRCH; /* process exited */
 
 	ret = down_write_killable(&mm->mmap_sem);
+	/*
+	 * 返回0说明lock成功了
+	 */
 	if (!ret) {
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|359| <<vfio_lock_acct>> ret = __account_locked_vm(mm, abs(npage), npage > 0,
+		 *                 dma->task, dma->lock_cap);
+		 *   - mm/util.c|507| <<account_locked_vm>> ret = __account_locked_vm(mm, pages, inc, current, capable(CAP_IPC_LOCK));
+		 */
 		ret = __account_locked_vm(mm, abs(npage), npage > 0, dma->task,
 					  dma->lock_cap);
 		if (!ret)
@@ -332,6 +710,20 @@ static int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)
  * MMIO range for our own or another device.  These use a different
  * pfn conversion and shouldn't be tracked as locked pages.
  */
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|413| <<put_pfn>> if (!is_invalid_reserved_pfn(pfn)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|512| <<vaddr_get_pfn>> if (!ret && !is_invalid_reserved_pfn(*pfn))
+ *   - drivers/vfio/vfio_iommu_type1.c|610| <<vfio_pin_pages_remote>> rsvd = is_invalid_reserved_pfn(*pfn_base);
+ *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_pin_pages_remote>> rsvd != is_invalid_reserved_pfn(pfn)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|725| <<vfio_pin_page_external>> if (!ret && do_accounting && !is_invalid_reserved_pfn(*pfn_base)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|2404| <<vfio_iommu_unmap_unpin_reaccount>> if (!is_invalid_reserved_pfn(vpfn->pfn))
+ *
+ * 注释:
+ * Some mappings aren't backed by a struct page, for example an mmap'd
+ * MMIO range for our own or another device.  These use a different
+ * pfn conversion and shouldn't be tracked as locked pages.
+ */
 static bool is_invalid_reserved_pfn(unsigned long pfn)
 {
 	if (pfn_valid(pfn)) {
@@ -360,6 +752,17 @@ static bool is_invalid_reserved_pfn(unsigned long pfn)
 	return true;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|345| <<vfio_iova_put_vfio_pfn>> ret = put_pfn(vpfn->pfn, dma->prot);
+ *   - drivers/vfio/vfio_iommu_type1.c|672| <<vfio_pin_pages_remote>> put_pfn(*pfn_base, dma->prot);
+ *   - drivers/vfio/vfio_iommu_type1.c|705| <<vfio_pin_pages_remote>> put_pfn(pfn, dma->prot);
+ *   - drivers/vfio/vfio_iommu_type1.c|720| <<vfio_pin_pages_remote>> put_pfn(pfn, dma->prot);
+ *   - drivers/vfio/vfio_iommu_type1.c|754| <<vfio_pin_pages_remote>> put_pfn(pfn, dma->prot);
+ *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_unpin_pages_remote>> if (put_pfn(pfn++, dma->prot)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|802| <<vfio_pin_page_external>> put_pfn(*pfn_base, dma->prot);
+ *   - drivers/vfio/vfio_iommu_type1.c|916| <<vfio_iommu_type1_pin_pages>> if (put_pfn(phys_pfn[i], dma->prot) && do_accounting)
+ */
 static int put_pfn(unsigned long pfn, int prot)
 {
 	if (!is_invalid_reserved_pfn(pfn)) {
@@ -372,6 +775,10 @@ static int put_pfn(unsigned long pfn, int prot)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|617| <<vaddr_get_pfn>> ret = follow_fault_pfn(vma, mm, vaddr, pfn, prot & IOMMU_WRITE);
+ */
 static int follow_fault_pfn(struct vm_area_struct *vma, struct mm_struct *mm,
 			    unsigned long vaddr, unsigned long *pfn,
 			    bool write_fault)
@@ -408,6 +815,12 @@ static int follow_fault_pfn(struct vm_area_struct *vma, struct mm_struct *mm,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|623| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|674| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, &pfn, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|782| <<vfio_pin_page_external>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, true);
+ */
 static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
 			 int prot, unsigned long *pfn, bool handle_mmap_sem)
 {
@@ -423,9 +836,47 @@ static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
 	if (handle_mmap_sem)
 		down_read(&mm->mmap_sem);
 	if (mm == current->mm) {
+		/*
+		 * 可能返回-512 (-ERESTARTSYS).
+		 *
+		 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+		 * 886                 unsigned long start, unsigned long nr_pages,
+		 * 887                 unsigned int gup_flags, struct page **pages,
+		 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+		 * ... ...
+		 * 933                 //
+		 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+		 * 935                 // potentially allocating memory.
+		 * 936                  //
+		 * 937                 if (fatal_signal_pending(current)) {
+		 * 938                         ret = -ERESTARTSYS;
+		 * 939                         goto out;
+		 * 940                 }
+		 *
+		 * nr_pages是1
+		 */
 		ret = get_user_pages(vaddr, 1, flags | FOLL_LONGTERM, page,
 				     vmas);
 	} else {
+		/*
+		 * 可能返回-512 (-ERESTARTSYS).
+		 *
+		 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+		 * 886                 unsigned long start, unsigned long nr_pages,
+		 * 887                 unsigned int gup_flags, struct page **pages,
+		 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+		 * ... ...
+		 * 933                 //
+		 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+		 * 935                 // potentially allocating memory.
+		 * 936                  //
+		 * 937                 if (fatal_signal_pending(current)) {
+		 * 938                         ret = -ERESTARTSYS;
+		 * 939                         goto out;
+		 * 940                 }
+		 *
+		 * nr_pages是1
+		 */
 		ret = get_user_pages_remote(NULL, mm, vaddr, 1, flags, page,
 					    vmas, NULL);
 		/*
@@ -457,6 +908,9 @@ static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
 	vma = find_vma_intersection(mm, vaddr, vaddr + 1);
 
 	if (vma && vma->vm_flags & VM_PFNMAP) {
+		/*
+		 * 只在此处调用
+		 */
 		ret = follow_fault_pfn(vma, mm, vaddr, pfn, prot & IOMMU_WRITE);
 		if (ret == -EAGAIN)
 			goto retry;
@@ -470,10 +924,24 @@ static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|664| <<vfio_find_dma_valid>> ret = vfio_wait(iommu);
+ *   - drivers/vfio/vfio_iommu_type1.c|680| <<vfio_wait_all_valid>> ret = vfio_wait(iommu);
+ */
 static int vfio_wait(struct vfio_iommu *iommu)
 {
 	DEFINE_WAIT(wait);
 
+	/*
+	 * 在以下使用vfio_iommu->vaddr_wait:
+	 *   - drivers/vfio/vfio_iommu_type1.c|634| <<vfio_wait>> prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|638| <<vfio_wait>> finish_wait(&iommu->vaddr_wait, &wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1296| <<vfio_remove_dma>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1853| <<vfio_dma_do_map>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2982| <<vfio_iommu_type1_open>> init_waitqueue_head(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3353| <<vfio_iommu_type1_notify>> wake_up_all(&iommu->vaddr_wait);
+	 */
 	prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
 	mutex_unlock(&iommu->lock);
 	schedule();
@@ -492,6 +960,10 @@ static int vfio_wait(struct vfio_iommu *iommu)
  * Return 0 on success with no waiting, WAITED on success if waited, and -errno
  * on error.
  */
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|952| <<vfio_iommu_type1_pin_pages>> ret = vfio_find_dma_valid(iommu, iova, PAGE_SIZE, &dma);
+ */
 static int vfio_find_dma_valid(struct vfio_iommu *iommu, dma_addr_t start,
 			       size_t size, struct vfio_dma **dma_p)
 {
@@ -530,6 +1002,13 @@ static int vfio_wait_all_valid(struct vfio_iommu *iommu)
  * the iommu can only map chunks of consecutive pfns anyway, so get the
  * first page and all consecutive pages with the same locking.
  */
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1278| <<vfio_pin_map_dma_chunk>> npage = vfio_pin_pages_remote(dma, start_vaddr + mapped_size,
+ *                                                unmapped_size >> PAGE_SHIFT, &pfn, args->limit, args->mm, &lock_cache);
+ *   - drivers/vfio/vfio_iommu_type1.c|1570| <<vfio_iommu_replay>> npage = vfio_pin_pages_remote(dma, vaddr, n >> PAGE_SHIFT,
+ *                                                &pfn, limit, current->mm, &lock_cache);
+ */
 static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 				  long npage, unsigned long *pfn_base,
 				  unsigned long limit, struct mm_struct *mm,
@@ -545,6 +1024,12 @@ static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 		return -ENODEV;
 
 	down_read(&mm->mmap_sem);
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|623| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|674| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, &pfn, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|782| <<vfio_pin_page_external>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, true);
+	 */
 	ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, false);
 	if (ret) {
 		up_read(&mm->mmap_sem);
@@ -552,12 +1037,33 @@ static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 	}
 
 	pinned++;
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|413| <<put_pfn>> if (!is_invalid_reserved_pfn(pfn)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|512| <<vaddr_get_pfn>> if (!ret && !is_invalid_reserved_pfn(*pfn))
+	 *   - drivers/vfio/vfio_iommu_type1.c|610| <<vfio_pin_pages_remote>> rsvd = is_invalid_reserved_pfn(*pfn_base);
+	 *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_pin_pages_remote>> rsvd != is_invalid_reserved_pfn(pfn)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|725| <<vfio_pin_page_external>> if (!ret && do_accounting && !is_invalid_reserved_pfn(*pfn_base)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2404| <<vfio_iommu_unmap_unpin_reaccount>> if (!is_invalid_reserved_pfn(vpfn->pfn))
+	 */
 	rsvd = is_invalid_reserved_pfn(*pfn_base);
 
 	/*
 	 * Reserved pages aren't counted against the user, externally pinned
 	 * pages are already counted against the user.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|315| <<vfio_iova_get_vfio_pfn>> struct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);
+	 *   - drivers/vfio/vfio_iommu_type1.c|616| <<vfio_pin_pages_remote>> if (!rsvd && !vfio_find_vpfn(dma, iova)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|647| <<vfio_pin_pages_remote>> if (!rsvd && !vfio_find_vpfn(dma, iova)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|703| <<vfio_unpin_pages_remote>> if (vfio_find_vpfn(dma, iova))
+	 *   - drivers/vfio/vfio_iommu_type1.c|745| <<vfio_unpin_page_external>> struct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);
+	 *
+	 * 要满足两个条件:
+	 * 1. !rsvd --> 不是reserved
+	 * 2. !vfio_find_vpfn(dma, iova) --> 不是externally pinned (Ex-user pinned pfn list)
+	 */
 	if (!rsvd && !vfio_find_vpfn(dma, iova)) {
 		if (!dma->lock_cap && *lock_cache == 0 &&
 		    mm->locked_vm + 1 > limit) {
@@ -579,16 +1085,44 @@ static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 	/* Lock all the consecutive pages from pfn_base */
 	for (vaddr += PAGE_SIZE, iova += PAGE_SIZE; pinned < npage;
 	     pinned++, vaddr += PAGE_SIZE, iova += PAGE_SIZE) {
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|623| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, false);
+		 *   - drivers/vfio/vfio_iommu_type1.c|674| <<vfio_pin_pages_remote>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, &pfn, false);
+		 *   - drivers/vfio/vfio_iommu_type1.c|782| <<vfio_pin_page_external>> ret = vaddr_get_pfn(mm, vaddr, dma->prot, pfn_base, true);
+		 */
 		ret = vaddr_get_pfn(mm, vaddr, dma->prot, &pfn, false);
 		if (ret)
 			break;
 
+		/*
+		 * 在以下调用is_invalid_reserved_pfn():
+		 *   - drivers/vfio/vfio_iommu_type1.c|413| <<put_pfn>> if (!is_invalid_reserved_pfn(pfn)) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|512| <<vaddr_get_pfn>> if (!ret && !is_invalid_reserved_pfn(*pfn))
+		 *   - drivers/vfio/vfio_iommu_type1.c|610| <<vfio_pin_pages_remote>> rsvd = is_invalid_reserved_pfn(*pfn_base);
+		 *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_pin_pages_remote>> rsvd != is_invalid_reserved_pfn(pfn)) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|725| <<vfio_pin_page_external>> if (!ret && do_accounting && !is_invalid_reserved_pfn(*pfn_base)) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|2404| <<vfio_iommu_unmap_unpin_reaccount>> if (!is_invalid_reserved_pfn(vpfn->pfn))
+		 *
+		 * 注释:
+		 * Some mappings aren't backed by a struct page, for example an mmap'd
+		 * MMIO range for our own or another device.  These use a different
+		 * pfn conversion and shouldn't be tracked as locked pages.
+		 */
 		if (pfn != *pfn_base + pinned ||
 		    rsvd != is_invalid_reserved_pfn(pfn)) {
 			put_pfn(pfn, dma->prot);
 			break;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|315| <<vfio_iova_get_vfio_pfn>> struct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);
+		 *   - drivers/vfio/vfio_iommu_type1.c|616| <<vfio_pin_pages_remote>> if (!rsvd && !vfio_find_vpfn(dma, iova)) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|647| <<vfio_pin_pages_remote>> if (!rsvd && !vfio_find_vpfn(dma, iova)) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|703| <<vfio_unpin_pages_remote>> if (vfio_find_vpfn(dma, iova))
+		 *   - drivers/vfio/vfio_iommu_type1.c|745| <<vfio_unpin_page_external>> struct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);
+		 */
 		if (!rsvd && !vfio_find_vpfn(dma, iova)) {
 			if (!dma->lock_cap && *lock_cache == 0 &&
 			    mm->locked_vm + lock_acct + 1 > limit) {
@@ -608,6 +1142,18 @@ static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 
 out:
 	up_read(&mm->mmap_sem);
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|611| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|659| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|686| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|997| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1255| <<vfio_pin_map_dma_chunk>> ret = vfio_lock_acct(dma, cache_size, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1285| <<vfio_pin_map_dma_chunk>> vfio_lock_acct(dma, -lock_cache, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2246| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+	 */
 	ret = vfio_lock_acct(dma, lock_acct, false);
 
 unpin_out:
@@ -623,6 +1169,19 @@ static long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,
 	return pinned;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|989| <<vfio_sync_unpin>> unlocked += vfio_unpin_pages_remote(dma, entry->iova,
+ *                                                entry->phys >> PAGE_SHIFT, entry->len >> PAGE_SHIFT, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|1060| <<unmap_unpin_slow>> *unlocked += vfio_unpin_pages_remote(dma, *iova,
+ *                                                phys >> PAGE_SHIFT, unmapped >> PAGE_SHIFT, false);
+ *   - drivers/vfio/vfio_iommu_type1.c|1533| <<vfio_pin_map_dma_chunk>> vfio_unpin_pages_remote(dma, iova + mapped_size,
+ *                                                pfn, npage, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1921| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+ *                                                size >> PAGE_SHIFT, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1974| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+ *                                                size >> PAGE_SHIFT, true);
+ */
 static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,
 				    unsigned long pfn, long npage,
 				    bool do_accounting)
@@ -631,6 +1190,17 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,
 	long i;
 
 	for (i = 0; i < npage; i++, iova += PAGE_SIZE) {
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|345| <<vfio_iova_put_vfio_pfn>> ret = put_pfn(vpfn->pfn, dma->prot);
+		 *   - drivers/vfio/vfio_iommu_type1.c|672| <<vfio_pin_pages_remote>> put_pfn(*pfn_base, dma->prot);
+		 *   - drivers/vfio/vfio_iommu_type1.c|705| <<vfio_pin_pages_remote>> put_pfn(pfn, dma->prot);
+		 *   - drivers/vfio/vfio_iommu_type1.c|720| <<vfio_pin_pages_remote>> put_pfn(pfn, dma->prot);
+		 *   - drivers/vfio/vfio_iommu_type1.c|754| <<vfio_pin_pages_remote>> put_pfn(pfn, dma->prot);
+		 *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_unpin_pages_remote>> if (put_pfn(pfn++, dma->prot)) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|802| <<vfio_pin_page_external>> put_pfn(*pfn_base, dma->prot);
+		 *   - drivers/vfio/vfio_iommu_type1.c|916| <<vfio_iommu_type1_pin_pages>> if (put_pfn(phys_pfn[i], dma->prot) && do_accounting)
+		 */
 		if (put_pfn(pfn++, dma->prot)) {
 			unlocked++;
 			if (vfio_find_vpfn(dma, iova))
@@ -638,12 +1208,28 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,
 		}
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|611| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|659| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|686| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|997| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1255| <<vfio_pin_map_dma_chunk>> ret = vfio_lock_acct(dma, cache_size, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1285| <<vfio_pin_map_dma_chunk>> vfio_lock_acct(dma, -lock_cache, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2246| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+	 */
 	if (do_accounting)
 		vfio_lock_acct(dma, locked - unlocked, true);
 
 	return unlocked;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|881| <<vfio_iommu_type1_pin_pages>> ret = vfio_pin_page_external(dma, remote_vaddr, &phys_pfn[i], do_accounting);
+ */
 static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,
 				  unsigned long *pfn_base, bool do_accounting)
 {
@@ -671,6 +1257,11 @@ static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1115| <<vfio_iommu_type1_pin_pages>> vfio_unpin_page_external(dma, iova, do_accounting);
+ *   - drivers/vfio/vfio_iommu_type1.c|1165| <<vfio_iommu_type1_unpin_pages>> vfio_unpin_page_external(dma, iova, do_accounting);
+ */
 static int vfio_unpin_page_external(struct vfio_dma *dma, dma_addr_t iova,
 				    bool do_accounting)
 {
@@ -688,6 +1279,26 @@ static int vfio_unpin_page_external(struct vfio_dma *dma, dma_addr_t iova,
 	return unlocked;
 }
 
+/*
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * 一般没有调用
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.pin_pages = vfio_iommu_type1_pin_pages()
+ */
 static int vfio_iommu_type1_pin_pages(void *iommu_data,
 				      unsigned long *user_pfn,
 				      int npage, int prot,
@@ -760,11 +1371,18 @@ static int vfio_iommu_type1_pin_pages(void *iommu_data,
 		}
 
 		remote_vaddr = dma->vaddr + (iova - dma->iova);
+		/*
+		 * 只在此处调用
+		 */
 		ret = vfio_pin_page_external(dma, remote_vaddr, &phys_pfn[i],
 					     do_accounting);
 		if (ret)
 			goto pin_unwind;
 
+		/*
+		 * vfio_add_to_pfn_list()只被一般没有调用的vfio_iommu_type1_pin_pages()调用
+		 * 所以vfio_add_to_pfn_list()一般不被调用
+		 */
 		ret = vfio_add_to_pfn_list(dma, iova, phys_pfn[i]);
 		if (ret) {
 			if (put_pfn(phys_pfn[i], dma->prot) && do_accounting)
@@ -791,6 +1409,26 @@ static int vfio_iommu_type1_pin_pages(void *iommu_data,
 	return ret;
 }
 
+/*
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * 简单的测试没见到调用
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.unpin_pages = vfio_iommu_type1_unpin_pages()
+ */
 static int vfio_iommu_type1_unpin_pages(void *iommu_data,
 					unsigned long *user_pfn,
 					int npage)
@@ -825,6 +1463,11 @@ static int vfio_iommu_type1_unpin_pages(void *iommu_data,
 	return i > npage ? npage : (i > 0 ? i : -EINVAL);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1039| <<unmap_unpin_fast>> *unlocked += vfio_sync_unpin(dma, domain, unmapped_list, iotlb_gather);
+ *   - drivers/vfio/vfio_iommu_type1.c|1154| <<vfio_unmap_unpin>> unlocked += vfio_sync_unpin(dma, domain, &unmapped_region_list, &iotlb_gather);
+ */
 static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,
 			    struct list_head *regions,
 			    struct iommu_iotlb_gather *iotlb_gather)
@@ -834,7 +1477,28 @@ static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,
 
 	iommu_tlb_sync(domain->domain, iotlb_gather);
 
+	/*
+	 * struct vfio_regions {
+	 *     struct list_head list;
+	 *     dma_addr_t iova;
+	 *     phys_addr_t phys;
+	 *     size_t len;
+	 * };
+	 */
 	list_for_each_entry_safe(entry, next, regions, list) {
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|989| <<vfio_sync_unpin>> unlocked += vfio_unpin_pages_remote(dma, entry->iova,
+		 *                                                entry->phys >> PAGE_SHIFT, entry->len >> PAGE_SHIFT, false);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1060| <<unmap_unpin_slow>> *unlocked += vfio_unpin_pages_remote(dma, *iova,
+		 *                                                phys >> PAGE_SHIFT, unmapped >> PAGE_SHIFT, false);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1533| <<vfio_pin_map_dma_chunk>> vfio_unpin_pages_remote(dma, iova + mapped_size,
+		 *                                                pfn, npage, true);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1921| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+		 *                                                size >> PAGE_SHIFT, true);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1974| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+		 *                                                size >> PAGE_SHIFT, true);
+		 */
 		unlocked += vfio_unpin_pages_remote(dma,
 						    entry->iova,
 						    entry->phys >> PAGE_SHIFT,
@@ -858,6 +1522,12 @@ static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,
  */
 #define VFIO_IOMMU_TLB_SYNC_MAX		512
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1415| <<vfio_unmap_unpin>> unmapped = unmap_unpin_fast(domain, dma, &iova,
+ *							len, phys, &unlocked, &unmapped_region_list,
+ *							&unmapped_region_cnt, &iotlb_gather);
+ */
 static size_t unmap_unpin_fast(struct vfio_domain *domain,
 			       struct vfio_dma *dma, dma_addr_t *iova,
 			       size_t len, phys_addr_t phys, long *unlocked,
@@ -866,9 +1536,20 @@ static size_t unmap_unpin_fast(struct vfio_domain *domain,
 			       struct iommu_iotlb_gather *iotlb_gather)
 {
 	size_t unmapped = 0;
+	/*
+	 * struct vfio_regions {
+	 *     struct list_head list;
+	 *     dma_addr_t iova;
+	 *     phys_addr_t phys;
+	 *     size_t len;
+	 * };
+	 */
 	struct vfio_regions *entry = kzalloc(sizeof(*entry), GFP_KERNEL);
 
 	if (entry) {
+		/*
+		 * 返回unmap的page的数量
+		 */
 		unmapped = iommu_unmap_fast(domain->domain, *iova, len,
 					    iotlb_gather);
 
@@ -898,6 +1579,10 @@ static size_t unmap_unpin_fast(struct vfio_domain *domain,
 	return unmapped;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1251| <<vfio_unmap_unpin>> unmapped = unmap_unpin_slow(domain, dma, &iova, len, phys, &unlocked);
+ */
 static size_t unmap_unpin_slow(struct vfio_domain *domain,
 			       struct vfio_dma *dma, dma_addr_t *iova,
 			       size_t len, phys_addr_t phys,
@@ -906,6 +1591,19 @@ static size_t unmap_unpin_slow(struct vfio_domain *domain,
 	size_t unmapped = iommu_unmap(domain->domain, *iova, len);
 
 	if (unmapped) {
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|989| <<vfio_sync_unpin>> unlocked += vfio_unpin_pages_remote(dma, entry->iova,
+		 *                                                entry->phys >> PAGE_SHIFT, entry->len >> PAGE_SHIFT, false);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1060| <<unmap_unpin_slow>> *unlocked += vfio_unpin_pages_remote(dma, *iova,
+		 *                                                phys >> PAGE_SHIFT, unmapped >> PAGE_SHIFT, false);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1533| <<vfio_pin_map_dma_chunk>> vfio_unpin_pages_remote(dma, iova + mapped_size,
+		 *                                                pfn, npage, true);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1921| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+		 *                                                size >> PAGE_SHIFT, true);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1974| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+		 *                                                size >> PAGE_SHIFT, true);
+		 */
 		*unlocked += vfio_unpin_pages_remote(dma, *iova,
 						     phys >> PAGE_SHIFT,
 						     unmapped >> PAGE_SHIFT,
@@ -916,6 +1614,46 @@ static size_t unmap_unpin_slow(struct vfio_domain *domain,
 	return unmapped;
 }
 
+/*
+ * 例子
+ *
+ * put_pfn
+ * vfio_sync_unpin.isra.19
+ * vfio_unmap_unpin
+ * vfio_pin_map_dma_undo
+ * vfio_pin_map_dma_chunk
+ * padata_mt_helper
+ * __padata_do_multithreaded
+ * vfio_dma_do_map
+ * vfio_iommu_type1_ioctl
+ * vfio_fops_unl_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * put_pfn
+ * vfio_sync_unpin.isra.19
+ * vfio_unmap_unpin
+ * vfio_pin_map_dma_undo
+ * __padata_do_multithreaded
+ * vfio_dma_do_map
+ * vfio_iommu_type1_ioctl
+ * vfio_fops_unl_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1142| <<vfio_remove_dma>> vfio_unmap_unpin(iommu, dma, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|1368| <<vfio_pin_map_dma_undo>> vfio_unmap_unpin(args->iommu, args->dma, true);
+ *   - drivers/vfio/vfio_iommu_type1.c|2531| <<vfio_iommu_unmap_unpin_reaccount>> unlocked += vfio_unmap_unpin(iommu, dma, false);
+ *
+ * 只有dma->size不是0的时候才会真正生效.
+ */
 static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			     bool do_accounting)
 {
@@ -974,11 +1712,17 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 		 * First, try to use fast unmap/unpin. In case of failure,
 		 * switch to slow unmap/unpin path.
 		 */
+		/*
+		 * 只在此处调用
+		 */
 		unmapped = unmap_unpin_fast(domain, dma, &iova, len, phys,
 					    &unlocked, &unmapped_region_list,
 					    &unmapped_region_cnt,
 					    &iotlb_gather);
 		if (!unmapped) {
+			/*
+			 * 只在此处调用
+			 */
 			unmapped = unmap_unpin_slow(domain, dma, &iova, len,
 						    phys, &unlocked);
 			if (WARN_ON(!unmapped))
@@ -986,8 +1730,21 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 		}
 	}
 
+	/*
+	 * 在以下使用vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1125| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1556| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1809| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1859| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1875| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1885| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2532| <<vfio_iommu_unmap_unpin_reaccount>> dma->iommu_mapped = false;
+	 */
 	dma->iommu_mapped = false;
 
+	/*
+	 * 如果还有unpin的
+	 */
 	if (unmapped_region_cnt) {
 		unlocked += vfio_sync_unpin(dma, domain, &unmapped_region_list,
 					    &iotlb_gather);
@@ -1000,14 +1757,37 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 	return unlocked;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1332| <<vfio_dma_do_unmap>> vfio_remove_dma(iommu, dma);
+ *   - drivers/vfio/vfio_iommu_type1.c|1575| <<vfio_pin_map_dma>> vfio_remove_dma(iommu, dma);
+ *   - drivers/vfio/vfio_iommu_type1.c|2544| <<vfio_iommu_unmap_unpin_all>> vfio_remove_dma(iommu, rb_entry(node, struct vfio_dma, node));
+ */
 static void vfio_remove_dma(struct vfio_iommu *iommu, struct vfio_dma *dma)
 {
 	WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1142| <<vfio_remove_dma>> vfio_unmap_unpin(iommu, dma, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1368| <<vfio_pin_map_dma_undo>> vfio_unmap_unpin(args->iommu, args->dma, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2531| <<vfio_iommu_unmap_unpin_reaccount>> unlocked += vfio_unmap_unpin(iommu, dma, false);
+	 *
+	 * 只有dma->size不是0的时候才会真正生效.
+	 */
 	vfio_unmap_unpin(iommu, dma, true);
 	vfio_unlink_dma(iommu, dma);
 	put_task_struct(dma->task);
 	if (dma->vaddr_invalid) {
 		iommu->vaddr_invalid_count--;
+		/*
+		 * 在以下使用vfio_iommu->vaddr_wait:
+		 *   - drivers/vfio/vfio_iommu_type1.c|634| <<vfio_wait>> prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
+		 *   - drivers/vfio/vfio_iommu_type1.c|638| <<vfio_wait>> finish_wait(&iommu->vaddr_wait, &wait);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1296| <<vfio_remove_dma>> wake_up_all(&iommu->vaddr_wait);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1853| <<vfio_dma_do_map>> wake_up_all(&iommu->vaddr_wait);
+		 *   - drivers/vfio/vfio_iommu_type1.c|2982| <<vfio_iommu_type1_open>> init_waitqueue_head(&iommu->vaddr_wait);
+		 *   - drivers/vfio/vfio_iommu_type1.c|3353| <<vfio_iommu_type1_notify>> wake_up_all(&iommu->vaddr_wait);
+		 */
 		wake_up_all(&iommu->vaddr_wait);
 	}
 	kfree(dma);
@@ -1189,6 +1969,10 @@ static int vfio_dma_do_unmap(struct vfio_iommu *iommu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1323| <<vfio_pin_map_dma_chunk>> ret = vfio_iommu_map(args->iommu, iova + mapped_size, pfn, npage, dma->prot);
+ */
 static int vfio_iommu_map(struct vfio_iommu *iommu, dma_addr_t iova,
 			  unsigned long pfn, long npage, int prot)
 {
@@ -1220,14 +2004,78 @@ struct vfio_pin_args {
 	struct mm_struct *mm;
 };
 
+/*
+ * put_pfn
+ * vfio_sync_unpin.isra.19
+ * vfio_unmap_unpin
+ * vfio_pin_map_dma_undo
+ * vfio_pin_map_dma_chunk
+ * padata_mt_helper
+ * __padata_do_multithreaded
+ * vfio_dma_do_map
+ * vfio_iommu_type1_ioctl
+ * vfio_fops_unl_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * put_pfn
+ * vfio_sync_unpin.isra.19
+ * vfio_unmap_unpin
+ * vfio_pin_map_dma_undo
+ * __padata_do_multithreaded
+ * vfio_dma_do_map
+ * vfio_iommu_type1_ioctl
+ * vfio_fops_unl_ioctl
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用vfio_pin_map_dma_undo():
+ *   - drivers/vfio/vfio_iommu_type1.c|1891| <<vfio_pin_map_dma_chunk>> vfio_pin_map_dma_undo(start_vaddr, start_vaddr + mapped_size,
+ *   - drivers/vfio/vfio_iommu_type1.c|1915| <<vfio_pin_map_dma>> .undo_fn = vfio_pin_map_dma_undo,
+ */
 static void vfio_pin_map_dma_undo(unsigned long start_vaddr,
 				  unsigned long end_vaddr, void *arg)
 {
 	struct vfio_pin_args *args = arg;
 
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1142| <<vfio_remove_dma>> vfio_unmap_unpin(iommu, dma, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1368| <<vfio_pin_map_dma_undo>> vfio_unmap_unpin(args->iommu, args->dma, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2531| <<vfio_iommu_unmap_unpin_reaccount>> unlocked += vfio_unmap_unpin(iommu, dma, false);
+	 *
+	 * 只有dma->size不是0的时候才会真正生效.
+	 */
 	vfio_unmap_unpin(args->iommu, args->dma, true);
 }
 
+/*
+ * commit bf5042499e6b62291438daa65ee844e25f594aeb
+ * Author: Daniel Jordan <daniel.m.jordan@oracle.com>
+ * Date:   Tue Oct 23 16:44:14 2018 -0400
+ *
+ * vfio: ease mmap_sem writer contention by caching locked_vm
+ *
+ * ktask threads hold mmap_sem as reader for the majority of their runtime
+ * but also periodically take mmap_sem as writer for short periods, hurting
+ * parallelism.
+ *
+ * Alleviate the write-side contention with a per-thread cache of locked_vm.
+ *
+ * Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
+ * Reviewed-by: Khalid Aziz <khalid.aziz@oracle.com>
+ * Reviewed-by: Prasad Singamsetty <prasad.singamsetty@oracle.com>
+ *
+ * Orabug: 29708084
+ * Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
+ * Reviewed-by: Khalid Aziz <khalid.aziz@oracle.com> 
+ */
 /*
  * Relieve mmap_sem contention when multithreading page pinning by caching
  * locked_vm locally.  Bound the locked_vm that a thread will cache but not use
@@ -1236,11 +2084,48 @@ static void vfio_pin_map_dma_undo(unsigned long start_vaddr,
  */
 #define LOCK_CACHE_MAX	16384
 
+/*
+ * 1471 static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
+ * 1472                             size_t map_size)
+ * 1473 {       
+ * 1474         unsigned long limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+ * 1475         int ret = 0;
+ * 1476         struct vfio_pin_args args = { iommu, dma, limit, current->mm };
+ * 1477         // Stay on PMD boundary in case THP is being used.
+ * 1478         struct padata_mt_job job = {
+ * 1479                 .thread_fn   = vfio_pin_map_dma_chunk,
+ * 1480                 .fn_arg      = &args,
+ * 1481                 .start       = dma->vaddr,
+ * 1482                 .size        = map_size,
+ * 1483                 .align       = PMD_SIZE,
+ * 1484                 .min_chunk   = (1ul << 27),
+ * 1485                 .undo_fn     = vfio_pin_map_dma_undo,
+ * 1486                 .max_threads = 16,
+ * 1487         };
+ *
+ * 在以下使用vfio_pin_map_dma_chunk():
+ *   - drivers/vfio/vfio_iommu_type1.c|1306| <<vfio_pin_map_dma>> .thread_fn = vfio_pin_map_dma_chunk,
+ *
+ * 这里的start_vaddr和end_vaddr是每一个padata的worker分到的部分
+ */
 static int vfio_pin_map_dma_chunk(unsigned long start_vaddr,
 				  unsigned long end_vaddr, void *arg)
 {
+	/*
+	 * 1347 struct vfio_pin_args {
+	 * 1348         struct vfio_iommu *iommu;
+	 * 1349         struct vfio_dma *dma;
+	 * 1350         unsigned long limit;
+	 * 1351         struct mm_struct *mm;
+	 * 1352 };
+	 */
 	struct vfio_pin_args *args = arg;
 	struct vfio_dma *dma = args->dma;
+	/*
+	 * 参数的start_vaddr和end_vaddr是每一个padata的worker分到的部分
+	 *
+	 * 要map从iova到unmapped_size!
+	 */
 	dma_addr_t iova = dma->iova + (start_vaddr - dma->vaddr);
 	unsigned long unmapped_size = end_vaddr - start_vaddr;
 	unsigned long pfn, mapped_size = 0;
@@ -1248,16 +2133,46 @@ static int vfio_pin_map_dma_chunk(unsigned long start_vaddr,
 	int ret = 0;
 	long cache_size, lock_cache = 0;
 
+	/*
+	 * 这是一个循环,
+	 * 也许每次只map一部分
+	 */
 	while (unmapped_size) {
 		if (lock_cache == 0) {
+			/*
+			 * 注释:
+			 * Relieve mmap_sem contention when multithreading page pinning by caching
+			 * locked_vm locally.  Bound the locked_vm that a thread will cache but not use
+			 * with this constant, which is the smallest value that worked well in testing.
+			 * NOTE - LOCK_CACHE_MAX is in pages.
+			 */
 			cache_size = min_t(long, unmapped_size >> PAGE_SHIFT,
 					   LOCK_CACHE_MAX);
+			/*
+			 * called by:
+			 *   - drivers/vfio/vfio_iommu_type1.c|611| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|659| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|686| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|997| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1255| <<vfio_pin_map_dma_chunk>> ret = vfio_lock_acct(dma, cache_size, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1285| <<vfio_pin_map_dma_chunk>> vfio_lock_acct(dma, -lock_cache, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|2246| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+			 */
 			ret = vfio_lock_acct(dma, cache_size, false);
 			if (ret)
 				break;
 			lock_cache = cache_size;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|1278| <<vfio_pin_map_dma_chunk>> npage = vfio_pin_pages_remote(dma, start_vaddr + mapped_size,
+		 *                                                unmapped_size >> PAGE_SHIFT, &pfn, args->limit, args->mm, &lock_cache);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1570| <<vfio_iommu_replay>> npage = vfio_pin_pages_remote(dma, vaddr, n >> PAGE_SHIFT,
+		 *                                                &pfn, limit, current->mm, &lock_cache);
+		 */
 		/* Pin a contiguous chunk of memory */
 		npage = vfio_pin_pages_remote(dma, start_vaddr + mapped_size,
 					      unmapped_size >> PAGE_SHIFT,
@@ -1269,10 +2184,26 @@ static int vfio_pin_map_dma_chunk(unsigned long start_vaddr,
 			break;
 		}
 
+		/*
+		 * 只在此处调用
+		 */
 		/* Map it! */
 		ret = vfio_iommu_map(args->iommu, iova + mapped_size, pfn,
 				     npage, dma->prot);
 		if (ret) {
+			/*
+			 * called by:
+			 *   - drivers/vfio/vfio_iommu_type1.c|989| <<vfio_sync_unpin>> unlocked += vfio_unpin_pages_remote(dma, entry->iova,
+			 *                                                entry->phys >> PAGE_SHIFT, entry->len >> PAGE_SHIFT, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1060| <<unmap_unpin_slow>> *unlocked += vfio_unpin_pages_remote(dma, *iova,
+			 *                                                phys >> PAGE_SHIFT, unmapped >> PAGE_SHIFT, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1533| <<vfio_pin_map_dma_chunk>> vfio_unpin_pages_remote(dma, iova + mapped_size,
+			 *                                                pfn, npage, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1921| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+			 *                                                size >> PAGE_SHIFT, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1974| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+			 *                                                size >> PAGE_SHIFT, true);
+			 */
 			vfio_unpin_pages_remote(dma, iova + mapped_size, pfn,
 						npage, true);
 			break;
@@ -1282,12 +2213,31 @@ static int vfio_pin_map_dma_chunk(unsigned long start_vaddr,
 		mapped_size   += npage << PAGE_SHIFT;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|611| <<vfio_pin_pages_remote>> ret = vfio_lock_acct(dma, lock_acct, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|642| <<vfio_unpin_pages_remote>> vfio_lock_acct(dma, locked - unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|659| <<vfio_pin_page_external>> ret = vfio_lock_acct(dma, 1, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|686| <<vfio_unpin_page_external>> vfio_lock_acct(dma, -unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|771| <<vfio_iommu_type1_pin_pages>> vfio_lock_acct(dma, -1, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|997| <<vfio_unmap_unpin>> vfio_lock_acct(dma, -unlocked, true);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1255| <<vfio_pin_map_dma_chunk>> ret = vfio_lock_acct(dma, cache_size, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1285| <<vfio_pin_map_dma_chunk>> vfio_lock_acct(dma, -lock_cache, false);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2246| <<vfio_iommu_unmap_unpin_reaccount>> vfio_lock_acct(dma, locked - unlocked, true);
+	 *
+	 * 这个是在while循环外面
+	 */
 	vfio_lock_acct(dma, -lock_cache, false);
 
 	/*
 	 * Undo the successfully completed part of this chunk now.  padata will
 	 * undo previously completed chunks internally at the end of the task.
 	 */
+	/*
+	 * 在以下使用vfio_pin_map_dma_undo():
+	 *   - drivers/vfio/vfio_iommu_type1.c|1891| <<vfio_pin_map_dma_chunk>> vfio_pin_map_dma_undo(start_vaddr, start_vaddr + mapped_size,
+	 *   - drivers/vfio/vfio_iommu_type1.c|1915| <<vfio_pin_map_dma>> .undo_fn = vfio_pin_map_dma_undo,
+	 */
 	if (ret)
 		vfio_pin_map_dma_undo(start_vaddr, start_vaddr + mapped_size,
 				      args);
@@ -1295,12 +2245,32 @@ static int vfio_pin_map_dma_chunk(unsigned long start_vaddr,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1464| <<vfio_dma_do_map>> ret = vfio_pin_map_dma(iommu, dma, size);
+ */
 static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			    size_t map_size)
 {
 	unsigned long limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 	int ret = 0;
 	struct vfio_pin_args args = { iommu, dma, limit, current->mm };
+	/*
+	 * 158 struct padata_mt_job {
+	 * 159         int (*thread_fn)(unsigned long start, unsigned long end, void *arg);
+	 * 160         void                    *fn_arg;
+	 * 161         unsigned long           start;
+	 * 162         unsigned long           size;
+	 * 163         unsigned long           align;
+	 * 164         unsigned long           min_chunk;
+	 * 165
+	 * 166         // Have default values.
+	 * 167         void (*undo_fn)(unsigned long start, unsigned long end, void *arg);
+	 * 168         int                     max_threads;
+	 * 169 };
+	 *
+	 * 1 << 27 是 134217728, 也就是128MB
+	 */
 	/* Stay on PMD boundary in case THP is being used. */
 	struct padata_mt_job job = {
 		.thread_fn   = vfio_pin_map_dma_chunk,
@@ -1313,8 +2283,25 @@ static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 		.max_threads = 16,
 	};
 
+	/*
+	 * 在以下使用padata_do_multithreaded():
+	 *   - drivers/vfio/vfio_iommu_type1.c|2094| <<vfio_pin_map_dma>> ret = padata_do_multithreaded(&job);
+	 *   - mm/memory.c|1070| <<copy_page_range_mt>> return padata_do_multithreaded(&job);
+	 *   - mm/memory.c|1425| <<unmap_page_range_mt>> padata_do_multithreaded(&job);
+	 *   - mm/page_alloc.c|1931| <<deferred_init_memmap>> padata_do_multithreaded(&job);
+	 */
 	ret = padata_do_multithreaded(&job);
 
+	/*
+	 * 在以下使用vfio_dma->iommu_mapped:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1125| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1556| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1809| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1859| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1875| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+	 *   - drivers/vfio/vfio_iommu_type1.c|1885| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2532| <<vfio_iommu_unmap_unpin_reaccount>> dma->iommu_mapped = false;
+	 */
 	dma->iommu_mapped = true;
 
 	if (ret)
@@ -1328,9 +2315,24 @@ static int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,
 /*
  * Check dma map request is within a valid iova range
  */
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|1625| <<vfio_dma_do_map>> if (!vfio_iommu_iova_dma_valid(iommu, iova, iova + size - 1)) {
+ */
 static bool vfio_iommu_iova_dma_valid(struct vfio_iommu *iommu,
 				      dma_addr_t start, dma_addr_t end)
 {
+	/*
+	 * 在以下使用iova_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|2047| <<vfio_iommu_iova_dma_valid>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2664| <<vfio_iommu_aper_conflict>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2826| <<vfio_iommu_iova_get_copy>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2846| <<vfio_iommu_iova_insert_copy>> struct list_head *iova = &iommu->iova_list;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3360| <<vfio_iommu_type1_open>> INIT_LIST_HEAD(&iommu->iova_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3433| <<vfio_iommu_type1_release>> vfio_iommu_iova_free(&iommu->iova_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3486| <<vfio_iommu_iova_build_caps>> list_for_each_entry(iova, &iommu->iova_list, list)
+	 *   - drivers/vfio/vfio_iommu_type1.c|3508| <<vfio_iommu_iova_build_caps>> list_for_each_entry(iova, &iommu->iova_list, list) {
+	 */
 	struct list_head *iova = &iommu->iova_list;
 	struct vfio_iova *node;
 
@@ -1346,9 +2348,47 @@ static bool vfio_iommu_iova_dma_valid(struct vfio_iommu *iommu,
 	return list_empty(iova);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|2675| <<vfio_iommu_type1_ioctl(VFIO_IOMMU_MAP_DMA)>> return vfio_dma_do_map(iommu, &map);
+ */
 static int vfio_dma_do_map(struct vfio_iommu *iommu,
 			   struct vfio_iommu_type1_dma_map *map)
 {
+	/*
+	 * 2829 struct vfio_iommu_type1_dma_map {
+	 * 2691                  * 830         __u32   argsz;
+	 * 2692                  * 831         __u32   flags;
+	 * 2693                  * 832 #define VFIO_DMA_MAP_FLAG_READ (1 << 0)         // readable from device
+	 * 2694                  * 833 #define VFIO_DMA_MAP_FLAG_WRITE (1 << 1)        // writable from device
+	 * 2695                  * 834 #define VFIO_DMA_MAP_FLAG_VADDR (1 << 2)
+	 * 2696                  * 835         __u64   vaddr;                          // Process virtual address
+	 * 2697                  * 836         __u64   iova;                           // IO virtual address
+	 * 2698                  * 837         __u64   size;                           // Size of mapping (bytes)
+	 * 2699                  * 838 };
+	 *
+	 * 至少没见到QEMU-9.2使用VFIO_DMA_MAP_FLAG_VADDR.
+	 *
+	 * 613 // Do the DMA mapping with VFIO.
+	 * 614 static int qemu_vfio_do_mapping(QEMUVFIOState *s, void *host, size_t size,
+	 * 615                                 uint64_t iova, Error **errp)
+	 * 616 {
+	 * 617     struct vfio_iommu_type1_dma_map dma_map = {
+	 * 618         .argsz = sizeof(dma_map),
+	 * 619         .flags = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE,
+	 * 620         .iova = iova,
+	 * 621         .vaddr = (uintptr_t)host,
+	 * 622         .size = size,
+	 * 623     };
+	 * 624     trace_qemu_vfio_do_mapping(s, host, iova, size);
+	 * 625
+	 * 626     if (ioctl(s->container, VFIO_IOMMU_MAP_DMA, &dma_map)) {
+	 * 627         error_setg_errno(errp, errno, "VFIO_MAP_DMA failed");
+	 * 628         return -errno;
+	 * 629     }
+	 * 630     return 0;
+	 * 631 }
+	 */
 	bool set_vaddr = map->flags & VFIO_DMA_MAP_FLAG_VADDR;
 	dma_addr_t iova = map->iova;
 	unsigned long vaddr = map->vaddr;
@@ -1383,6 +2423,34 @@ static int vfio_dma_do_map(struct vfio_iommu *iommu,
 
 	mutex_lock(&iommu->lock);
 
+	/*
+	 * called by:
+	 *   - drivers/vfio/vfio_iommu_type1.c|513| <<vfio_find_dma_valid>> *dma_p = vfio_find_dma(iommu, start, size);
+	 *   - drivers/vfio/vfio_iommu_type1.c|757| <<vfio_iommu_type1_pin_pages>> dma = vfio_find_dma(iommu, iova, PAGE_SIZE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|797| <<vfio_iommu_type1_pin_pages>> dma = vfio_find_dma(iommu, iova, PAGE_SIZE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|829| <<vfio_iommu_type1_unpin_pages>> dma = vfio_find_dma(iommu, iova, PAGE_SIZE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1117| <<vfio_dma_do_unmap>> dma = vfio_find_dma(iommu, iova, 1);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1121| <<vfio_dma_do_unmap>> dma = vfio_find_dma(iommu, iova + size - 1, 0);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1410| <<vfio_dma_do_map>> dma = vfio_find_dma(iommu, iova, size);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1865| <<vfio_iommu_aper_conflict>> if (vfio_find_dma(iommu, first->start, start - first->start))
+	 *   - drivers/vfio/vfio_iommu_type1.c|1871| <<vfio_iommu_aper_conflict>> if (vfio_find_dma(iommu, end + 1, last->end - end))
+	 *   - drivers/vfio/vfio_iommu_type1.c|1932| <<vfio_iommu_resv_conflict>> if (vfio_find_dma(iommu, region->start, region->length))
+	 *
+	 *  90 struct vfio_dma {
+	 *  91         struct rb_node          node;
+	 *  92         dma_addr_t              iova;           // Device address
+	 *  93         unsigned long           vaddr;          // Process virtual addr
+	 *  94         size_t                  size;           // Map size (bytes)
+	 *  95         int                     prot;           // IOMMU_READ/WRITE
+	 *  96         bool                    iommu_mapped;
+	 *  97         bool                    lock_cap;       // capable(CAP_IPC_LOCK)
+	 *  98         bool                    vaddr_invalid;
+	 *  99         struct task_struct      *task;
+	 * 100         struct rb_root          pfn_list;       // Ex-user pinned pfn list
+	 * 101 };
+	 *
+	 * 是根据VM的iova来寻找的
+	 */
 	dma = vfio_find_dma(iommu, iova, size);
 	if (set_vaddr) {
 		if (!dma) {
@@ -1394,6 +2462,15 @@ static int vfio_dma_do_map(struct vfio_iommu *iommu,
 			dma->vaddr = vaddr;
 			dma->vaddr_invalid = false;
 			iommu->vaddr_invalid_count--;
+			/*
+			 * 在以下使用vfio_iommu->vaddr_wait:
+			 *   - drivers/vfio/vfio_iommu_type1.c|634| <<vfio_wait>> prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
+			 *   - drivers/vfio/vfio_iommu_type1.c|638| <<vfio_wait>> finish_wait(&iommu->vaddr_wait, &wait);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1296| <<vfio_remove_dma>> wake_up_all(&iommu->vaddr_wait);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1853| <<vfio_dma_do_map>> wake_up_all(&iommu->vaddr_wait);
+			 *   - drivers/vfio/vfio_iommu_type1.c|2982| <<vfio_iommu_type1_open>> init_waitqueue_head(&iommu->vaddr_wait);
+			 *   - drivers/vfio/vfio_iommu_type1.c|3353| <<vfio_iommu_type1_notify>> wake_up_all(&iommu->vaddr_wait);
+			 */
 			wake_up_all(&iommu->vaddr_wait);
 		}
 		goto out_unlock;
@@ -1402,16 +2479,42 @@ static int vfio_dma_do_map(struct vfio_iommu *iommu,
 		goto out_unlock;
 	}
 
+	/*
+	 * 在以下使用vfio_iommu->dma_avail:
+	 *   - drivers/vfio/vfio_iommu_type1.c|1636| <<vfio_remove_dma>> iommu->dma_avail++;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2255| <<vfio_dma_do_map>> if (!iommu->dma_avail) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2274| <<vfio_dma_do_map>> iommu->dma_avail--;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3448| <<vfio_iommu_type1_open>> iommu->dma_avail = dma_entry_limit;
+	 *   - drivers/vfio/vfio_iommu_type1.c|3618| <<vfio_iommu_dma_avail_build_caps>> cap_dma_avail.avail = iommu->dma_avail;
+	 */
 	if (!iommu->dma_avail) {
 		ret = -ENOSPC;
 		goto out_unlock;
 	}
 
+	/*
+	 * 只在此处调用
+	 * 似乎是判断这个iova的range是否被iommu支持
+	 */
 	if (!vfio_iommu_iova_dma_valid(iommu, iova, iova + size - 1)) {
 		ret = -EINVAL;
 		goto out_unlock;
 	}
 
+	/*
+	 *  90 struct vfio_dma {
+	 *  91         struct rb_node          node;
+	 *  92         dma_addr_t              iova;           // Device address
+	 *  93         unsigned long           vaddr;          // Process virtual addr
+	 *  94         size_t                  size;           // Map size (bytes)
+	 *  95         int                     prot;           // IOMMU_READ/WRITE
+	 *  96         bool                    iommu_mapped;
+	 *  97         bool                    lock_cap;       // capable(CAP_IPC_LOCK)
+	 *  98         bool                    vaddr_invalid;
+	 *  99         struct task_struct      *task;
+	 * 100         struct rb_root          pfn_list;       // Ex-user pinned pfn list
+	 * 101 };
+	 */
 	dma = kzalloc(sizeof(*dma), GFP_KERNEL);
 	if (!dma) {
 		ret = -ENOMEM;
@@ -1452,17 +2555,48 @@ static int vfio_dma_do_map(struct vfio_iommu *iommu,
 	dma->task = current->group_leader;
 	dma->lock_cap = capable(CAP_IPC_LOCK);
 
+	/*
+	 * 在以下使用vfio_dma->pfn_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|266| <<vfio_find_vpfn>> struct rb_node *node = dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|287| <<vfio_link_pfn>> link = &dma->pfn_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|299| <<vfio_link_pfn>> rb_insert_color(&new->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|304| <<vfio_unlink_pfn>> rb_erase(&old->node, &dma->pfn_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1178| <<vfio_remove_dma>> WARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));
+	 *   - drivers/vfio/vfio_iommu_type1.c|1325| <<vfio_dma_do_unmap>> if (!RB_EMPTY_ROOT(&dma->pfn_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1789| <<vfio_dma_do_map>> dma->pfn_list = RB_ROOT;
+	 *   - drivers/vfio/vfio_iommu_type1.c|2633| <<vfio_iommu_unmap_unpin_reaccount>> p = rb_first(&dma->pfn_list);
+	 *
+	 * 一般没有调用
+	 * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.pin_pages = vfio_iommu_type1_pin_pages()
+	 * -> vfio_add_to_pfn_list()
+	 *    -> vfio_link_pfn()
+	 *       -> rb_insert_color(&new->node, &dma->pfn_list);
+	 */
 	dma->pfn_list = RB_ROOT;
 
+	/*
+	 * 只在此处调用
+	 *
+	 * 插入vfio_iommu的rb tree!
+	 */
 	/* Insert zero-sized and grow as we map chunks of it */
 	vfio_link_dma(iommu, dma);
 
+	/*
+	 * vfio_pin_map_dma()只在此处调用
+	 * 失败的话会调用vfio_remove_dma()从rb tree移除
+	 */
 	/* Don't pin and map if container doesn't contain IOMMU capable domain*/
 	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))
 		dma->size = size;
 	else
 		ret = vfio_pin_map_dma(iommu, dma, size);
 
+	/*
+	 * 所以如果ioctl完成的时候, 这段内存要么全被pin, 要么全部unpin.
+	 * 并且, 要么vfio_dma在vfio_iommu->dma_list, 要么被移除被free.
+	 */
+
 out_unlock:
 	mutex_unlock(&iommu->lock);
 	return ret;
@@ -1480,6 +2614,10 @@ static int vfio_bus_type(struct device *dev, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|2507| <<vfio_iommu_type1_attach_group>> ret = vfio_iommu_replay(iommu, domain);
+ */
 static int vfio_iommu_replay(struct vfio_iommu *iommu,
 			     struct vfio_domain *domain)
 {
@@ -1493,6 +2631,27 @@ static int vfio_iommu_replay(struct vfio_iommu *iommu,
 	if (ret < 0)
 		return ret;
 
+	/*
+	 * 在以下使用vfio_iommu->domain_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|182| <<IS_IOMMU_CAP_DOMAIN_IN_CONTAINER>> (!list_empty(&iommu->domain_list))
+	 *   - drivers/vfio/vfio_iommu_type1.c|1454| <<vfio_unmap_unpin>> domain = d = list_first_entry(&iommu->domain_list,
+	 *   - drivers/vfio/vfio_iommu_type1.c|1457| <<vfio_unmap_unpin>> list_for_each_entry_continue(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1575| <<vfio_pgsize_bitmap>> list_for_each_entry(domain, &iommu->domain_list, next)
+	 *   - drivers/vfio/vfio_iommu_type1.c|1754| <<vfio_iommu_map>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|1766| <<vfio_iommu_map>> list_for_each_entry_continue_reverse(d, &iommu->domain_list, next)
+	 *   - drivers/vfio/vfio_iommu_type1.c|2249| <<vfio_iommu_replay>> if (!list_empty(&iommu->domain_list))
+	 *   - drivers/vfio/vfio_iommu_type1.c|2250| <<vfio_iommu_replay>> d = list_first_entry(&iommu->domain_list,
+	 *   - drivers/vfio/vfio_iommu_type1.c|2845| <<vfio_iommu_type1_attach_group>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|2979| <<vfio_iommu_type1_attach_group>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3009| <<vfio_iommu_type1_attach_group>> list_add(&domain->next, &iommu->domain_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3113| <<vfio_iommu_aper_expand>> list_for_each_entry(domain, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3148| <<vfio_iommu_resv_refresh>> list_for_each_entry(d, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3230| <<vfio_iommu_type1_detach_group>> list_for_each_entry(domain, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3246| <<vfio_iommu_type1_detach_group>> if (list_is_singular(&iommu->domain_list)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3311| <<vfio_iommu_type1_open>> INIT_LIST_HEAD(&iommu->domain_list);
+	 *   -  drivers/vfio/vfio_iommu_type1.c|3379| <<vfio_iommu_type1_release>> list_for_each_entry_safe(domain, domain_tmp, &iommu->domain_list, next) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3396| <<vfio_domains_have_iommu_cache>> list_for_each_entry(domain, &iommu->domain_list, next) {
+	 */
 	/* Arbitrarily pick the first domain in the list for lookups */
 	if (!list_empty(&iommu->domain_list))
 		d = list_first_entry(&iommu->domain_list,
@@ -1511,6 +2670,16 @@ static int vfio_iommu_replay(struct vfio_iommu *iommu,
 			phys_addr_t phys;
 			size_t size;
 
+			/*
+			 * 在以下使用vfio_dma->iommu_mapped:
+			 *   - drivers/vfio/vfio_iommu_type1.c|1125| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+			 *   - drivers/vfio/vfio_iommu_type1.c|1556| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+			 *   - drivers/vfio/vfio_iommu_type1.c|1809| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+			 *   - drivers/vfio/vfio_iommu_type1.c|1859| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+			 *   - drivers/vfio/vfio_iommu_type1.c|1875| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+			 *   - drivers/vfio/vfio_iommu_type1.c|1885| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+			 *   - drivers/vfio/vfio_iommu_type1.c|2532| <<vfio_iommu_unmap_unpin_reaccount>> dma->iommu_mapped = false;
+			 */
 			if (dma->iommu_mapped) {
 				phys_addr_t p;
 				dma_addr_t i;
@@ -1561,6 +2730,19 @@ static int vfio_iommu_replay(struct vfio_iommu *iommu,
 			ret = iommu_map(domain->domain, iova, phys,
 					size, dma->prot | domain->prot);
 			if (ret) {
+				/*
+				 * called by:
+				 *   - drivers/vfio/vfio_iommu_type1.c|989| <<vfio_sync_unpin>> unlocked += vfio_unpin_pages_remote(dma, entry->iova,
+				 *                                                entry->phys >> PAGE_SHIFT, entry->len >> PAGE_SHIFT, false);
+				 *   - drivers/vfio/vfio_iommu_type1.c|1060| <<unmap_unpin_slow>> *unlocked += vfio_unpin_pages_remote(dma, *iova,
+				 *                                                phys >> PAGE_SHIFT, unmapped >> PAGE_SHIFT, false);
+				 *   - drivers/vfio/vfio_iommu_type1.c|1533| <<vfio_pin_map_dma_chunk>> vfio_unpin_pages_remote(dma, iova + mapped_size,
+				 *                                                pfn, npage, true);
+				 *   - drivers/vfio/vfio_iommu_type1.c|1921| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+				 *                                                size >> PAGE_SHIFT, true);
+				 *   - drivers/vfio/vfio_iommu_type1.c|1974| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+				 *                                                size >> PAGE_SHIFT, true);
+				 */
 				if (!dma->iommu_mapped)
 					vfio_unpin_pages_remote(dma, iova,
 							phys >> PAGE_SHIFT,
@@ -1615,6 +2797,19 @@ static int vfio_iommu_replay(struct vfio_iommu *iommu,
 			}
 
 			iommu_unmap(domain->domain, iova, size);
+			/*
+			 * called by:
+			 *   - drivers/vfio/vfio_iommu_type1.c|989| <<vfio_sync_unpin>> unlocked += vfio_unpin_pages_remote(dma, entry->iova,
+			 *                                                entry->phys >> PAGE_SHIFT, entry->len >> PAGE_SHIFT, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1060| <<unmap_unpin_slow>> *unlocked += vfio_unpin_pages_remote(dma, *iova,
+			 *                                                phys >> PAGE_SHIFT, unmapped >> PAGE_SHIFT, false);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1533| <<vfio_pin_map_dma_chunk>> vfio_unpin_pages_remote(dma, iova + mapped_size,
+			 *                                                pfn, npage, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1921| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+			 *                                                size >> PAGE_SHIFT, true);
+			 *   - drivers/vfio/vfio_iommu_type1.c|1974| <<vfio_iommu_replay>> vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
+			 *                                                size >> PAGE_SHIFT, true);
+			 */
 			vfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,
 						size >> PAGE_SHIFT, true);
 		}
@@ -1656,6 +2851,15 @@ static void vfio_test_domain_fgsp(struct vfio_domain *domain)
 	__free_pages(pages, order);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|3177| <<vfio_iommu_type1_attach_group>> if (find_iommu_group(d, iommu_group)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3184| <<vfio_iommu_type1_attach_group>> if (find_iommu_group(iommu->external_domain, iommu_group)) {
+ *   - drivers/vfio/vfio_iommu_type1.c|3575| <<vfio_iommu_type1_detach_group>> group = find_iommu_group(iommu->external_domain, iommu_group);
+ *   - drivers/vfio/vfio_iommu_type1.c|3601| <<vfio_iommu_type1_detach_group>> group = find_iommu_group(domain, iommu_group);
+ *
+ * 查找在vfio_domain->group_list是否有参数的iommu_group
+ */
 static struct vfio_group *find_iommu_group(struct vfio_domain *domain,
 					   struct iommu_group *iommu_group)
 {
@@ -1764,6 +2968,10 @@ static void vfio_iommu_detach_group(struct vfio_domain *domain,
 		iommu_detach_group(domain->domain, group->iommu_group);
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|3204| <<vfio_iommu_type1_attach_group>> if (vfio_bus_is_mdev(bus)) {
+ */
 static bool vfio_bus_is_mdev(struct bus_type *bus)
 {
 	struct bus_type *mdev_bus;
@@ -1778,6 +2986,11 @@ static bool vfio_bus_is_mdev(struct bus_type *bus)
 	return ret;
 }
 
+/*
+ * 在以下使用vfio_mdev_iommu_device():
+ *   - drivers/vfio/vfio_iommu_type1.c|3211| <<vfio_iommu_type1_attach_group>> ret = iommu_group_for_each_dev(iommu_group,
+ *                     &iommu_device, vfio_mdev_iommu_device);
+ */
 static int vfio_mdev_iommu_device(struct device *dev, void *data)
 {
 	struct device **old = data, *new;
@@ -2013,6 +3226,39 @@ static void vfio_iommu_iova_insert_copy(struct vfio_iommu *iommu,
 
 	list_splice_tail(iova_copy, iova);
 }
+/*
+ * 在以下使用vfio_iommu_driver_ops->attach_group:
+ *   - drivers/vfio/vfio.c|213| <<global>> .attach_group = vfio_noiommu_attach_group,
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|1368| <<global>> .attach_group = tce_iommu_attach_group,
+ *   - drivers/vfio/vfio_iommu_type1.c|3129| <<global>> .attach_group = vfio_iommu_type1_attach_group,
+ *   - drivers/vfio/vfio.c|1077| <<__vfio_container_attach_groups>> ret = driver->ops->attach_group(data, group->iommu_group);
+ *   - drivers/vfio/vfio.c|1402| <<vfio_group_set_container>> ret = driver->ops->attach_group(container->iommu_data,
+ *
+ * 3150 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3151         .name                   = "vfio-iommu-type1",
+ * 3152         .owner                  = THIS_MODULE,
+ * 3153         .open                   = vfio_iommu_type1_open,
+ * 3154         .release                = vfio_iommu_type1_release,
+ * 3155         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3156         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3157         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3158         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3159         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3160         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3161         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3162         .notify                 = vfio_iommu_type1_notify,
+ * 3163 };
+ *
+ * vfio_iommu_type1_attach_group
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * x64_sys_call
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.attach_group = vfio_iommu_type1_attach_group
+ */
 static int vfio_iommu_type1_attach_group(void *iommu_data,
 					 struct iommu_group *iommu_group)
 {
@@ -2030,6 +3276,9 @@ static int vfio_iommu_type1_attach_group(void *iommu_data,
 	mutex_lock(&iommu->lock);
 
 	list_for_each_entry(d, &iommu->domain_list, next) {
+		/*
+		 * 查找在vfio_domain->group_list是否有参数的iommu_group
+		 */
 		if (find_iommu_group(d, iommu_group)) {
 			mutex_unlock(&iommu->lock);
 			return -EINVAL;
@@ -2037,12 +3286,19 @@ static int vfio_iommu_type1_attach_group(void *iommu_data,
 	}
 
 	if (iommu->external_domain) {
+		/*
+		 * 查找在vfio_domain->group_list是否有参数的iommu_group
+		 */
 		if (find_iommu_group(iommu->external_domain, iommu_group)) {
 			mutex_unlock(&iommu->lock);
 			return -EINVAL;
 		}
 	}
 
+	/*
+	 * struct vfio_group *group;
+	 * struct vfio_domain *domain,
+	 */
 	group = kzalloc(sizeof(*group), GFP_KERNEL);
 	domain = kzalloc(sizeof(*domain), GFP_KERNEL);
 	if (!group || !domain) {
@@ -2052,6 +3308,9 @@ static int vfio_iommu_type1_attach_group(void *iommu_data,
 
 	group->iommu_group = iommu_group;
 
+	/*
+	 * struct bus_type *bus = NULL;
+	 */
 	/* Determine bus_type in order to allocate a domain */
 	ret = iommu_group_for_each_dev(iommu_group, &bus, vfio_bus_type);
 	if (ret)
@@ -2215,25 +3474,79 @@ static int vfio_iommu_type1_attach_group(void *iommu_data,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|3046| <<vfio_iommu_type1_detach_group>> vfio_iommu_unmap_unpin_all(iommu);
+ *   - drivers/vfio/vfio_iommu_type1.c|3082| <<vfio_iommu_type1_detach_group>> vfio_iommu_unmap_unpin_all(iommu);
+ *   - drivers/vfio/vfio_iommu_type1.c|3207| <<vfio_iommu_type1_release>> vfio_iommu_unmap_unpin_all(iommu);
+ */
 static void vfio_iommu_unmap_unpin_all(struct vfio_iommu *iommu)
 {
 	struct rb_node *node;
 
+	/*
+	 * 在以下使用vfio_iommu->dma_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|214| <<vfio_find_dma>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|238| <<vfio_find_dma_first_node>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|270| <<vfio_link_dma>> struct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;
+	 *   - drivers/vfio/vfio_iommu_type1.c|284| <<vfio_link_dma>> rb_insert_color(&new->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|293| <<vfio_unlink_dma>> rb_erase(&old->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2227| <<vfio_iommu_replay>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2326| <<vfio_iommu_replay>> for (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3015| <<vfio_iommu_unmap_unpin_all>> while ((node = rb_first(&iommu->dma_list)))
+	 *   - drivers/vfio/vfio_iommu_type1.c|3027| <<vfio_iommu_unmap_unpin_reaccount>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3274| <<vfio_iommu_type1_open>> iommu->dma_list = RB_ROOT;
+	 */
 	while ((node = rb_first(&iommu->dma_list)))
 		vfio_remove_dma(iommu, rb_entry(node, struct vfio_dma, node));
 }
 
+/*
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|2773| <<vfio_iommu_type1_detach_group>> vfio_iommu_unmap_unpin_reaccount(iommu);
+ */
 static void vfio_iommu_unmap_unpin_reaccount(struct vfio_iommu *iommu)
 {
 	struct rb_node *n, *p;
 
+	/*
+	 * 在以下使用vfio_iommu->dma_list:
+	 *   - drivers/vfio/vfio_iommu_type1.c|214| <<vfio_find_dma>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|238| <<vfio_find_dma_first_node>> struct rb_node *node = iommu->dma_list.rb_node;
+	 *   - drivers/vfio/vfio_iommu_type1.c|270| <<vfio_link_dma>> struct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;
+	 *   - drivers/vfio/vfio_iommu_type1.c|284| <<vfio_link_dma>> rb_insert_color(&new->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|293| <<vfio_unlink_dma>> rb_erase(&old->node, &iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2227| <<vfio_iommu_replay>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2326| <<vfio_iommu_replay>> for (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {
+	 *   - drivers/vfio/vfio_iommu_type1.c|3015| <<vfio_iommu_unmap_unpin_all>> while ((node = rb_first(&iommu->dma_list)))
+	 *   - drivers/vfio/vfio_iommu_type1.c|3027| <<vfio_iommu_unmap_unpin_reaccount>> n = rb_first(&iommu->dma_list);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3274| <<vfio_iommu_type1_open>> iommu->dma_list = RB_ROOT;
+	 */
 	n = rb_first(&iommu->dma_list);
 	for (; n; n = rb_next(n)) {
 		struct vfio_dma *dma;
 		long locked = 0, unlocked = 0;
 
 		dma = rb_entry(n, struct vfio_dma, node);
+		/*
+		 * called by:
+		 *   - drivers/vfio/vfio_iommu_type1.c|1142| <<vfio_remove_dma>> vfio_unmap_unpin(iommu, dma, true);
+		 *   - drivers/vfio/vfio_iommu_type1.c|1368| <<vfio_pin_map_dma_undo>> vfio_unmap_unpin(args->iommu, args->dma, true);
+		 *   - drivers/vfio/vfio_iommu_type1.c|2531| <<vfio_iommu_unmap_unpin_reaccount>> unlocked += vfio_unmap_unpin(iommu, dma, false);
+		 *
+		 * 只有dma->size不是0的时候才会真正生效.
+		 */
 		unlocked += vfio_unmap_unpin(iommu, dma, false);
+		/*
+		 * 在以下使用vfio_dma->iommu_mapped:
+		 *   - drivers/vfio/vfio_iommu_type1.c|1125| <<vfio_unmap_unpin>> dma->iommu_mapped = false;
+		 *   - drivers/vfio/vfio_iommu_type1.c|1556| <<vfio_pin_map_dma>> dma->iommu_mapped = true;
+		 *   - drivers/vfio/vfio_iommu_type1.c|1809| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|1859| <<vfio_iommu_replay>> if (!dma->iommu_mapped)
+		 *   - drivers/vfio/vfio_iommu_type1.c|1875| <<vfio_iommu_replay>> dma->iommu_mapped = true;
+		 *   - drivers/vfio/vfio_iommu_type1.c|1885| <<vfio_iommu_replay>> if (dma->iommu_mapped) {
+		 *   - drivers/vfio/vfio_iommu_type1.c|2532| <<vfio_iommu_unmap_unpin_reaccount>> dma->iommu_mapped = false;
+		 */
 		dma->iommu_mapped = false;
 		p = rb_first(&dma->pfn_list);
 		for (; p; p = rb_next(p)) {
@@ -2327,6 +3640,42 @@ static int vfio_iommu_resv_refresh(struct vfio_iommu *iommu,
 	return ret;
 }
 
+/*
+ * vfio_iommu_type1_detach_group
+ * vfio_group_try_dissolve_container
+ * vfio_group_put_external_user
+ * kvm_vfio_group_put_external_user
+ * kvm_vfio_destroy
+ * kvm_put_kvm
+ * kvm_vm_release
+ * __fput
+ * ____fput
+ * task_work_run
+ * do_exit
+ * do_group_exit
+ * get_signal
+ * do_signal
+ * exit_to_usermode_loop
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.detach_group = vfio_iommu_type1_detach_group()
+ */
 static void vfio_iommu_type1_detach_group(void *iommu_data,
 					  struct iommu_group *iommu_group)
 {
@@ -2338,6 +3687,9 @@ static void vfio_iommu_type1_detach_group(void *iommu_data,
 	mutex_lock(&iommu->lock);
 
 	if (iommu->external_domain) {
+		/*
+		 * 查找在vfio_domain->group_list是否有参数的iommu_group
+		 */
 		group = find_iommu_group(iommu->external_domain, iommu_group);
 		if (group) {
 			list_del(&group->next);
@@ -2364,6 +3716,9 @@ static void vfio_iommu_type1_detach_group(void *iommu_data,
 	vfio_iommu_iova_get_copy(iommu, &iova_copy);
 
 	list_for_each_entry(domain, &iommu->domain_list, next) {
+		/*
+		 * 查找在vfio_domain->group_list是否有参数的iommu_group
+		 */
 		group = find_iommu_group(domain, iommu_group);
 		if (!group)
 			continue;
@@ -2404,6 +3759,32 @@ static void vfio_iommu_type1_detach_group(void *iommu_data,
 	mutex_unlock(&iommu->lock);
 }
 
+/*
+ * vfio_iommu_type1_open
+ * do_vfs_ioctl
+ * ksys_ioctl
+ * __x64_sys_ioctl
+ * x64_sys_call
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.open = vfio_iommu_type1_open()
+ */
 static void *vfio_iommu_type1_open(unsigned long arg)
 {
 	struct vfio_iommu *iommu;
@@ -2433,6 +3814,15 @@ static void *vfio_iommu_type1_open(unsigned long arg)
 	iommu->container_open = true;
 	mutex_init(&iommu->lock);
 	BLOCKING_INIT_NOTIFIER_HEAD(&iommu->notifier);
+	/*
+	 * 在以下使用vfio_iommu->vaddr_wait:
+	 *   - drivers/vfio/vfio_iommu_type1.c|634| <<vfio_wait>> prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|638| <<vfio_wait>> finish_wait(&iommu->vaddr_wait, &wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1296| <<vfio_remove_dma>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1853| <<vfio_dma_do_map>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2982| <<vfio_iommu_type1_open>> init_waitqueue_head(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3353| <<vfio_iommu_type1_notify>> wake_up_all(&iommu->vaddr_wait);
+	 */
 	init_waitqueue_head(&iommu->vaddr_wait);
 
 	return iommu;
@@ -2454,6 +3844,24 @@ static void vfio_release_domain(struct vfio_domain *domain, bool external)
 		iommu_domain_free(domain->domain);
 }
 
+/*
+ * 3178 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3179         .name                   = "vfio-iommu-type1",
+ * 3180         .owner                  = THIS_MODULE,
+ * 3181         .open                   = vfio_iommu_type1_open,
+ * 3182         .release                = vfio_iommu_type1_release,
+ * 3183         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3184         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3185         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3186         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3187         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3188         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3189         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3190         .notify                 = vfio_iommu_type1_notify,
+ * 3191 };
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.release = vfio_iommu_type1_release()
+ */
 static void vfio_iommu_type1_release(void *iommu_data)
 {
 	struct vfio_iommu *iommu = iommu_data;
@@ -2580,6 +3988,43 @@ static int vfio_iommu_dma_avail_build_caps(struct vfio_iommu *iommu,
 	return ret;
 }
 
+/*
+ * 1167 static long vfio_fops_unl_ioctl(struct file *filep,
+ * 1168                                 unsigned int cmd, unsigned long arg)
+ * 1169 {
+ * 1170         struct vfio_container *container = filep->private_data;
+ * 1171         struct vfio_iommu_driver *driver;
+ * 1172         void *data;
+ * 1173         long ret = -EINVAL;
+ * 1174 
+ * 1175         if (!container)
+ * 1176                 return ret;
+ * 1177        
+ * 1178         switch (cmd) {
+ * 1179         case VFIO_GET_API_VERSION:
+ * 1180                 ret = VFIO_API_VERSION;
+ * 1181                 break; 
+ * 1182         case VFIO_CHECK_EXTENSION:
+ * 1183                 ret = vfio_ioctl_check_extension(container, arg);
+ * 1184                 break;
+ * 1185         case VFIO_SET_IOMMU:
+ * 1186                 ret = vfio_ioctl_set_iommu(container, arg);
+ * 1187                 break;
+ * 1188         default:       
+ * 1189                 driver = container->iommu_driver;
+ * 1190                 data = container->iommu_data;
+ * 1191        
+ * 1192                 if (driver) // passthrough all unrecognized ioctls
+ * 1193                         ret = driver->ops->ioctl(data, cmd, arg);
+ * 1194         }
+ * 1195 
+ * 1196         return ret;
+ * 1197 }
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.ioctl = vfio_iommu_type1_ioctl()
+ *
+ * 可以参考open
+ */
 static long vfio_iommu_type1_ioctl(void *iommu_data,
 				   unsigned int cmd, unsigned long arg)
 {
@@ -2659,6 +4104,18 @@ static long vfio_iommu_type1_ioctl(void *iommu_data,
 			-EFAULT : 0;
 
 	} else if (cmd == VFIO_IOMMU_MAP_DMA) {
+		/*
+		 * 829 struct vfio_iommu_type1_dma_map {
+		 * 830         __u32   argsz;
+		 * 831         __u32   flags;
+		 * 832 #define VFIO_DMA_MAP_FLAG_READ (1 << 0)         // readable from device
+		 * 833 #define VFIO_DMA_MAP_FLAG_WRITE (1 << 1)        // writable from device
+		 * 834 #define VFIO_DMA_MAP_FLAG_VADDR (1 << 2)
+		 * 835         __u64   vaddr;                          // Process virtual address
+		 * 836         __u64   iova;                           // IO virtual address
+		 * 837         __u64   size;                           // Size of mapping (bytes)
+		 * 838 };
+		 */
 		struct vfio_iommu_type1_dma_map map;
 		uint32_t mask = VFIO_DMA_MAP_FLAG_READ |
 				VFIO_DMA_MAP_FLAG_WRITE |
@@ -2672,6 +4129,9 @@ static long vfio_iommu_type1_ioctl(void *iommu_data,
 		if (map.argsz < minsz || map.flags & ~mask)
 			return -EINVAL;
 
+		/*
+		 * 只在此处调用
+		 */
 		return vfio_dma_do_map(iommu, &map);
 
 	} else if (cmd == VFIO_IOMMU_UNMAP_DMA) {
@@ -2699,6 +4159,26 @@ static long vfio_iommu_type1_ioctl(void *iommu_data,
 	return -ENOTTY;
 }
 
+/*
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * 简单的测试没见到调用
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.register_notifier = vfio_iommu_type1_register_notifier()
+ */
 static int vfio_iommu_type1_register_notifier(void *iommu_data,
 					      unsigned long *events,
 					      struct notifier_block *nb)
@@ -2715,6 +4195,26 @@ static int vfio_iommu_type1_register_notifier(void *iommu_data,
 	return blocking_notifier_chain_register(&iommu->notifier, nb);
 }
 
+/*
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * 简单的测试没看见调用
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.unregister_notifier = vfio_iommu_type1_unregister_notifier()
+ */
 static int vfio_iommu_type1_unregister_notifier(void *iommu_data,
 						struct notifier_block *nb)
 {
@@ -2723,6 +4223,37 @@ static int vfio_iommu_type1_unregister_notifier(void *iommu_data,
 	return blocking_notifier_chain_unregister(&iommu->notifier, nb);
 }
 
+/*
+ * 不管是在QEMU kill还是在VM内部shutdown:
+ * vfio_iommu_type1_notify
+ * __fput
+ * ____fput
+ * task_work_run
+ * do_exit
+ * do_group_exit
+ * get_signal
+ * do_signal
+ * exit_to_usermode_loop
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 3478 static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
+ * 3479         .name                   = "vfio-iommu-type1",
+ * 3480         .owner                  = THIS_MODULE,
+ * 3481         .open                   = vfio_iommu_type1_open,
+ * 3482         .release                = vfio_iommu_type1_release,
+ * 3483         .ioctl                  = vfio_iommu_type1_ioctl,
+ * 3484         .attach_group           = vfio_iommu_type1_attach_group,
+ * 3485         .detach_group           = vfio_iommu_type1_detach_group,
+ * 3486         .pin_pages              = vfio_iommu_type1_pin_pages,
+ * 3487         .unpin_pages            = vfio_iommu_type1_unpin_pages,
+ * 3488         .register_notifier      = vfio_iommu_type1_register_notifier,
+ * 3489         .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
+ * 3490         .notify                 = vfio_iommu_type1_notify,
+ * 3491 };
+ *
+ * struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1.notify = = vfio_iommu_type1_notify()
+ */
 static void vfio_iommu_type1_notify(void *iommu_data,
 				    enum vfio_iommu_notify_type event)
 {
@@ -2733,6 +4264,15 @@ static void vfio_iommu_type1_notify(void *iommu_data,
 	mutex_lock(&iommu->lock);
 	iommu->container_open = false;
 	mutex_unlock(&iommu->lock);
+	/*
+	 * 在以下使用vfio_iommu->vaddr_wait:
+	 *   - drivers/vfio/vfio_iommu_type1.c|634| <<vfio_wait>> prepare_to_wait(&iommu->vaddr_wait, &wait, TASK_KILLABLE);
+	 *   - drivers/vfio/vfio_iommu_type1.c|638| <<vfio_wait>> finish_wait(&iommu->vaddr_wait, &wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1296| <<vfio_remove_dma>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|1853| <<vfio_dma_do_map>> wake_up_all(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|2982| <<vfio_iommu_type1_open>> init_waitqueue_head(&iommu->vaddr_wait);
+	 *   - drivers/vfio/vfio_iommu_type1.c|3353| <<vfio_iommu_type1_notify>> wake_up_all(&iommu->vaddr_wait);
+	 */
 	wake_up_all(&iommu->vaddr_wait);
 }
 
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 5083ccf04d9f..c82885a15278 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -453,6 +453,24 @@ struct mm_struct {
 		unsigned long hiwater_vm;  /* High-water virtual memory usage */
 
 		unsigned long total_vm;	   /* Total pages mapped */
+		/*
+		 * 在以下修改mm_struct->locked_vm:
+		 *   - fs/exec.c|1087| <<exec_mmap>> mm->locked_vm = old_mm->locked_vm;
+		 *   - fs/exec.c|1371| <<flush_old_exec>> current->mm->locked_vm = current->mm->driver_pinned_vm;
+		 *   - kernel/fork.c|1029| <<mm_init>> mm->locked_vm = 0;
+		 *   - mm/debug.c|175| <<dump_mm>> pr_emerg(" ... mm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,
+		 *   - mm/mlock.c|565| <<mlock_fixup>> mm->locked_vm += nr_pages;
+		 *   - mm/mlock.c|692| <<do_mlock>> locked += current->mm->locked_vm;
+		 *   - mm/mmap.c|2002| <<mmap_region>> mm->locked_vm += (len >> PAGE_SHIFT);
+		 *   - mm/mmap.c|2567| <<expand_upwards>> mm->locked_vm += grow;
+		 *   - mm/mmap.c|2647| <<expand_downwards>> mm->locked_vm += grow;
+		 *   - mm/mmap.c|3031| <<__do_munmap>> mm->locked_vm -= vma_pages(tmp);
+		 *   - mm/mmap.c|3273| <<do_brk_flags>> mm->locked_vm += (len >> PAGE_SHIFT);
+		 *   - mm/mremap.c|435| <<move_vma>> mm->locked_vm += new_len >> PAGE_SHIFT;
+		 *   - mm/mremap.c|726| <<SYSCALL_DEFINE5(mremap)>> mm->locked_vm += pages;
+		 *   - mm/util.c|492| <<__account_locked_vm>> mm->locked_vm = locked_vm + pages;
+		 *   - mm/util.c|495| <<__account_locked_vm>> mm->locked_vm = locked_vm - pages;
+		 */
 		unsigned long locked_vm;   /* Pages that have PG_mlocked set */
 		atomic64_t    pinned_vm;   /* Refcount permanently increased */
 		unsigned long data_vm;	   /* VM_WRITE & ~VM_SHARED & ~VM_STACK */
diff --git a/include/linux/padata.h b/include/linux/padata.h
index dc3b46b1dbbf..42a1226e2486 100644
--- a/include/linux/padata.h
+++ b/include/linux/padata.h
@@ -206,6 +206,27 @@ static inline void __init padata_init(void) {}
 
 #ifdef CONFIG_LOCKDEP
 
+/*
+ * 如果是vfio_pin_map_dma()来的:
+ * 2130         struct padata_mt_job job = {
+ * 2131                 .thread_fn   = vfio_pin_map_dma_chunk,
+ * 2132                 .fn_arg      = &args,
+ * 2133                 .start       = dma->vaddr,
+ * 2134                 .size        = map_size,
+ * 2135                 .align       = PMD_SIZE,
+ * 2136                 .min_chunk   = (1ul << 27),
+ * 2137                 .undo_fn     = vfio_pin_map_dma_undo,
+ * 2138                 .max_threads = 16,
+ * 2139         };
+ *
+ * 1 << 27 是 134217728, 也就是128MB
+ *
+ * 在以下使用padata_do_multithreaded():
+ *   - drivers/vfio/vfio_iommu_type1.c|2094| <<vfio_pin_map_dma>> ret = padata_do_multithreaded(&job);
+ *   - mm/memory.c|1070| <<copy_page_range_mt>> return padata_do_multithreaded(&job);
+ *   - mm/memory.c|1425| <<unmap_page_range_mt>> padata_do_multithreaded(&job);
+ *   - mm/page_alloc.c|1931| <<deferred_init_memmap>> padata_do_multithreaded(&job);
+ */
 #define padata_do_multithreaded(job)					      \
 ({									      \
 	static struct lock_class_key __key;				      \
diff --git a/include/linux/vfio.h b/include/linux/vfio.h
index 950299a293a0..fec1f7ed88ba 100644
--- a/include/linux/vfio.h
+++ b/include/linux/vfio.h
@@ -77,6 +77,14 @@ struct vfio_iommu_driver_ops {
 	long		(*ioctl)(void *iommu_data, unsigned int cmd,
 				 unsigned long arg);
 	int		(*mmap)(void *iommu_data, struct vm_area_struct *vma);
+	/*
+	 * 在以下使用vfio_iommu_driver_ops->attach_group:
+	 *   - drivers/vfio/vfio.c|213| <<global>> .attach_group = vfio_noiommu_attach_group,
+	 *   - drivers/vfio/vfio_iommu_spapr_tce.c|1368| <<global>> .attach_group = tce_iommu_attach_group,
+	 *   - drivers/vfio/vfio_iommu_type1.c|3129| <<global>> .attach_group = vfio_iommu_type1_attach_group,
+	 *   - drivers/vfio/vfio.c|1077| <<__vfio_container_attach_groups>> ret = driver->ops->attach_group(data, group->iommu_group);
+	 *   - drivers/vfio/vfio.c|1402| <<vfio_group_set_container>> ret = driver->ops->attach_group(container->iommu_data,
+	 */
 	int		(*attach_group)(void *iommu_data,
 					struct iommu_group *group);
 	void		(*detach_group)(void *iommu_data,
diff --git a/kernel/events/core.c b/kernel/events/core.c
index c820b89db5cc..d5c30f5ad4e5 100644
--- a/kernel/events/core.c
+++ b/kernel/events/core.c
@@ -4037,6 +4037,18 @@ static void __perf_event_read(void *info)
 	raw_spin_unlock(&ctx->lock);
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4836| <<__perf_event_read_value>> total += perf_event_count(event);
+ *   - kernel/events/core.c|4845| <<__perf_event_read_value>> total += perf_event_count(child);
+ *   - kernel/events/core.c|4927| <<__perf_read_group_add>> values[n++] += perf_event_count(leader);
+ *   - kernel/events/core.c|4932| <<__perf_read_group_add>> values[n++] += perf_event_count(sub);
+ *   - kernel/events/core.c|5497| <<perf_event_update_userpage>> userpg->offset = perf_event_count(event);
+ *   - kernel/events/core.c|6363| <<perf_output_read_one>> values[n++] = perf_event_count(event);
+ *   - kernel/events/core.c|6407| <<perf_output_read_group>> values[n++] = perf_event_count(leader);
+ *   - kernel/events/core.c|6420| <<perf_output_read_group>> values[n++] = perf_event_count(sub);
+ *   - kernel/events/core.c|11667| <<sync_child_event>> child_val = perf_event_count(child_event);
+ */
 static inline u64 perf_event_count(struct perf_event *event)
 {
 	return local64_read(&event->count) + atomic64_read(&event->child_count);
@@ -4116,6 +4128,13 @@ int perf_event_read_local(struct perf_event *event, u64 *value,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4852| <<__perf_event_read_value>> (void )perf_event_read(event, false);
+ *   - kernel/events/core.c|4861| <<__perf_event_read_value>> (void )perf_event_read(child, false);
+ *   - kernel/events/core.c|4903| <<__perf_read_group_add>> ret = perf_event_read(leader, true);
+ *   - kernel/events/core.c|5118| <<_perf_event_reset>> (void )perf_event_read(event, false);
+ */
 static int perf_event_read(struct perf_event *event, bool group)
 {
 	enum perf_event_state state = READ_ONCE(event->state);
@@ -4822,6 +4841,11 @@ static int perf_release(struct inode *inode, struct file *file)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/events/core.c|4860| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+ *   - kernel/events/core.c|4991| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+ */
 static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event *child;
@@ -4832,7 +4856,26 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 
 	mutex_lock(&event->child_mutex);
 
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|4852| <<__perf_event_read_value>> (void )perf_event_read(event, false);
+	 *   - kernel/events/core.c|4861| <<__perf_event_read_value>> (void )perf_event_read(child, false);
+	 *   - kernel/events/core.c|4903| <<__perf_read_group_add>> ret = perf_event_read(leader, true);
+	 *   - kernel/events/core.c|5118| <<_perf_event_reset>> (void )perf_event_read(event, false);
+	 */
 	(void)perf_event_read(event, false);
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|4836| <<__perf_event_read_value>> total += perf_event_count(event);
+	 *   - kernel/events/core.c|4845| <<__perf_event_read_value>> total += perf_event_count(child);
+	 *   - kernel/events/core.c|4927| <<__perf_read_group_add>> values[n++] += perf_event_count(leader);
+	 *   - kernel/events/core.c|4932| <<__perf_read_group_add>> values[n++] += perf_event_count(sub);
+	 *   - kernel/events/core.c|5497| <<perf_event_update_userpage>> userpg->offset = perf_event_count(event);
+	 *   - kernel/events/core.c|6363| <<perf_output_read_one>> values[n++] = perf_event_count(event);
+	 *   - kernel/events/core.c|6407| <<perf_output_read_group>> values[n++] = perf_event_count(leader);
+	 *   - kernel/events/core.c|6420| <<perf_output_read_group>> values[n++] = perf_event_count(sub);
+	 *   - kernel/events/core.c|11667| <<sync_child_event>> child_val = perf_event_count(child_event);
+	 */
 	total += perf_event_count(event);
 
 	*enabled += event->total_time_enabled +
@@ -4851,12 +4894,22 @@ static u64 __perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *
 	return total;
 }
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/pmu-emul.c|86| <<kvm_pmu_get_counter_value>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ *   - arch/x86/kvm/pmu.h|56| <<pmc_read_counter>> counter += perf_event_read_value(pmc->perf_event, &enabled, &running);
+ */
 u64 perf_event_read_value(struct perf_event *event, u64 *enabled, u64 *running)
 {
 	struct perf_event_context *ctx;
 	u64 count;
 
 	ctx = perf_event_ctx_lock(event);
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|4860| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+	 *   - kernel/events/core.c|4991| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+	 */
 	count = __perf_event_read_value(event, enabled, running);
 	perf_event_ctx_unlock(event, ctx);
 
@@ -4988,6 +5041,11 @@ static int perf_read_one(struct perf_event *event,
 	u64 values[4];
 	int n = 0;
 
+	/*
+	 * called by:
+	 *   - kernel/events/core.c|4860| <<perf_event_read_value>> count = __perf_event_read_value(event, enabled, running);
+	 *   - kernel/events/core.c|4991| <<perf_read_one>> values[n++] = __perf_event_read_value(event, &enabled, &running);
+	 */
 	values[n++] = __perf_event_read_value(event, &enabled, &running);
 	if (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)
 		values[n++] = enabled;
@@ -5141,6 +5199,72 @@ static void perf_event_for_each(struct perf_event *event,
 		perf_event_for_each_child(sibling, func);
 }
 
+/*
+ * 908 static __initconst const struct x86_pmu amd_pmu = {
+ * 909         .name                   = "AMD",
+ * 910         .handle_irq             = amd_pmu_handle_irq,
+ * 911         .disable_all            = amd_pmu_disable_all,
+ * 912         .enable_all             = x86_pmu_enable_all,
+ * 913         .enable                 = x86_pmu_enable_event,
+ * 914         .disable                = amd_pmu_disable_event,
+ * 915         .hw_config              = amd_pmu_hw_config,
+ * 916         .schedule_events        = x86_schedule_events,
+ * 917         .eventsel               = MSR_K7_EVNTSEL0,
+ * 918         .perfctr                = MSR_K7_PERFCTR0,
+ * 919         .addr_offset            = amd_pmu_addr_offset,
+ * 920         .event_map              = amd_pmu_event_map,
+ * 921         .max_events             = ARRAY_SIZE(amd_perfmon_event_map),
+ * 922         .num_counters           = AMD64_NUM_COUNTERS,
+ * 923         .cntval_bits            = 48,
+ * 924         .cntval_mask            = (1ULL << 48) - 1,
+ * 925         .apic                   = 1,
+ * 926         // use highest bit to detect overflow
+ * 927         .max_period             = (1ULL << 47) - 1,
+ * 928         .get_event_constraints  = amd_get_event_constraints,
+ * 929         .put_event_constraints  = amd_put_event_constraints,
+ * 930
+ * 931         .format_attrs           = amd_format_attr,
+ * 932         .events_sysfs_show      = amd_event_sysfs_show,
+ * 933
+ * 934         .cpu_prepare            = amd_pmu_cpu_prepare,
+ * 935         .cpu_starting           = amd_pmu_cpu_starting,
+ * 936         .cpu_dead               = amd_pmu_cpu_dead,
+ * 937
+ * 938         .amd_nb_constraints     = 1,
+ * 939 };
+ *
+ *
+ * arch/x86/events/core.c
+ * 2395 static struct pmu pmu = {
+ * 2396         .pmu_enable             = x86_pmu_enable,
+ * 2397         .pmu_disable            = x86_pmu_disable,
+ * 2398
+ * 2399         .attr_groups            = x86_pmu_attr_groups,
+ * 2400
+ * 2401         .event_init             = x86_pmu_event_init,
+ * 2402
+ * 2403         .event_mapped           = x86_pmu_event_mapped,
+ * 2404         .event_unmapped         = x86_pmu_event_unmapped,
+ * 2405
+ * 2406         .add                    = x86_pmu_add,
+ * 2407         .del                    = x86_pmu_del,
+ * 2408         .start                  = x86_pmu_start,
+ * 2409         .stop                   = x86_pmu_stop,
+ * 2410         .read                   = x86_pmu_read,
+ * 2411
+ * 2412         .start_txn              = x86_pmu_start_txn,
+ * 2413         .cancel_txn             = x86_pmu_cancel_txn,
+ * 2414         .commit_txn             = x86_pmu_commit_txn,
+ * 2415
+ * 2416         .event_idx              = x86_pmu_event_idx,
+ * 2417         .sched_task             = x86_pmu_sched_task,
+ * 2418         .task_ctx_size          = sizeof(struct x86_perf_task_context),
+ * 2419         .check_period           = x86_pmu_check_period,
+ * 2420
+ * 2421         .aux_output_match       = x86_pmu_aux_output_match,
+ * 2422 };
+ */
+
 static void __perf_event_period(struct perf_event *event,
 				struct perf_cpu_context *cpuctx,
 				struct perf_event_context *ctx,
@@ -5173,6 +5297,11 @@ static void __perf_event_period(struct perf_event *event,
 	local64_set(&event->hw.period_left, 0);
 
 	if (active) {
+		/*
+		 * struct perf_event *event:
+		 * -> struct pmu *pmu;
+		 *    -> void (*start) (struct perf_event *event, int flags);
+		 */
 		event->pmu->start(event, PERF_EF_RELOAD);
 		perf_pmu_enable(ctx->pmu);
 	}
diff --git a/kernel/padata.c b/kernel/padata.c
index fab5338163ae..0f7525c8647d 100644
--- a/kernel/padata.c
+++ b/kernel/padata.c
@@ -37,15 +37,50 @@
 #include <linux/sysfs.h>
 #include <linux/rcupdate.h>
 
+/*
+ * 在以下使用PADATA_WORK_ONSTACK:
+ *   - kernel/padata.c|141| <<padata_work_init>> if (flags & PADATA_WORK_ONSTACK)
+ *   - kernel/padata.c|875| <<__padata_do_multithreaded>> padata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);
+ */
 #define	PADATA_WORK_ONSTACK	1	/* Work's memory is on stack */
 
 struct padata_work {
+	/*
+	 * 在以下使用padata_work->pw_work:
+	 *   - kernel/padata.c|142| <<padata_work_init>> INIT_WORK_ONSTACK(&pw->pw_work, work_fn);
+	 *   - kernel/padata.c|144| <<padata_work_init>> INIT_WORK(&pw->pw_work, work_fn);
+	 *   - kernel/padata.c|191| <<padata_parallel_worker>> struct padata_work *pw = container_of(parallel_work,
+	 *                               struct padata_work, pw_work);
+	 *   - kernel/padata.c|265| <<padata_do_parallel>> queue_work(pinst->parallel_wq, &pw->pw_work);
+	 *   - kernel/padata.c|549| <<padata_mt_helper>> struct padata_work *pw = container_of(w, struct padata_work, pw_work);
+	 *   - kernel/padata.c|872| <<__padata_do_multithreaded>> queue_work(system_unbound_wq, &pw->pw_work);
+	 *   - kernel/padata.c|883| <<__padata_do_multithreaded>> padata_mt_helper(&my_work.pw_work);
+	 *   kernel/padata.c|927| <<__padata_do_multithreaded>> destroy_work_on_stack(&my_work.pw_work);
+	 */
 	struct work_struct	pw_work;
 	struct list_head	pw_list;  /* padata_free_works linkage */
 	void			*pw_data;
 	/* holds job units from padata_mt_job::start to pw_error_start */
+	/*
+	 * 在以下使用padata_work->pw_error_offset:
+	 *   - kernel/padata.c|607| <<padata_mt_helper>> pw->pw_error_offset = position_offset;
+	 *   - kernel/padata.c|630| <<padata_error_cmp>> if (work_a->pw_error_offset < work_b->pw_error_offset)
+	 *   - kernel/padata.c|632| <<padata_error_cmp>> else if (work_a->pw_error_offset > work_b->pw_error_offset)
+	 *
+	 * 主要的应用是padata_error_cmp()
+	 */
 	unsigned long		pw_error_offset;
+	/*
+	 * 在以下使用padata_work->pw_error_start:
+	 *   - kernel/padata.c|606| <<padata_mt_helper>> pw->pw_error_start = position;
+	 *   - kernel/padata.c|690| <<padata_undo>> undo_end = failed_work->pw_error_start;
+	 */
 	unsigned long		pw_error_start;
+	/*
+	 * 在以下使用padata_work->pw_error_end:
+	 *   - kernel/padata.c|608| <<padata_mt_helper>> pw->pw_error_end = end;
+	 *   - kernel/padata.c|712| <<padata_undo>> undo_pos = failed_work->pw_error_end;
+	 */
 	unsigned long		pw_error_end;
 };
 
@@ -63,6 +98,15 @@ struct padata_mt_job_state {
 	unsigned long		chunk_size;
 	unsigned long		position;
 	unsigned long		remaining_size;
+	/*
+	 * 在以下使用padata_mt_job_state->failed_works:
+	 *   - kernel/padata.c|587| <<padata_mt_helper>> list_move(&pw->pw_list, &ps->failed_works);
+	 *   - kernel/padata.c|627| <<padata_undo>> struct list_head *failed_works = &ps->failed_works;
+	 *   - kernel/padata.c|632| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+	 *   - kernel/padata.c|639| <<padata_undo>> failed_work = list_first_entry_or_null(failed_works,
+	 *   - kernel/padata.c|661| <<padata_undo>> WARN_ON(!list_empty(failed_works));
+	 *   - kernel/padata.c|735| <<__padata_do_multithreaded>> INIT_LIST_HEAD(&ps.failed_works);
+	 */
 	struct list_head	failed_works;
 #ifdef CONFIG_LOCKDEP
 	struct lockdep_map	lockdep_map;
@@ -118,6 +162,10 @@ static void padata_work_init(struct padata_work *pw, work_func_t work_fn,
 	pw->pw_data = data;
 }
 
+/*
+ * 在以下使用padata_work_alloc_mt():
+ *   - kernel/padata.c|863| <<__padata_do_multithreaded>> ps.nworks = padata_work_alloc_mt(nworks, &ps, &works);
+ */
 static int padata_work_alloc_mt(int nworks, void *data, struct list_head *head)
 {
 	int i;
@@ -477,13 +525,56 @@ static int pd_setup_cpumasks(struct parallel_data *pd,
 	return err;
 }
 
+/*
+ * spin_lock_init(&ps.lock);
+ * init_completion(&ps.completion);
+ * INIT_LIST_HEAD(&ps.failed_works);
+ * lockdep_init_map(&ps.lockdep_map, map_name, key, 0);
+ * ps.job            = job;
+ * ps.nworks         = padata_work_alloc_mt(nworks, &ps, &works);
+ * ps.nworks_fini    = 0;
+ * ps.error          = 0;
+ * ps.position       = job->start;
+ * ps.remaining_size = job->size;
+ *
+ * ps.chunk_size = job->size / (ps.nworks * load_balance_factor);
+ * ps.chunk_size = max(ps.chunk_size, job->min_chunk);
+ * ps.chunk_size = roundup(ps.chunk_size, job->align);
+ *
+ *
+ * 1977         struct padata_mt_job job = {
+ * 1978                 .thread_fn   = vfio_pin_map_dma_chunk,
+ * 1979                 .fn_arg      = &args,
+ * 1980                 .start       = dma->vaddr,
+ * 1981                 .size        = map_size,
+ * 1982                 .align       = PMD_SIZE,
+ * 1983                 .min_chunk   = (1ul << 27),
+ * 1984                 .undo_fn     = vfio_pin_map_dma_undo,
+ * 1985                 .max_threads = 16,
+ * 1986         };
+ *
+ *
+ * 在以下使用padata_mt_helper():
+ *   - kernel/padata.c|132| <<padata_work_alloc_mt>> padata_work_init(pw, padata_mt_helper, data, 0);
+ *   - kernel/padata.c|648| <<__padata_do_multithreaded>> padata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);
+ *   - kernel/padata.c|650| <<__padata_do_multithreaded>> padata_mt_helper(&my_work.pw_work);
+ */
 static void padata_mt_helper(struct work_struct *w)
 {
+	/*
+	 * struct padata_work:
+	 * -> unsigned long pw_error_offset;
+	 * -> unsigned long pw_error_start;
+	 * -> unsigned long pw_error_end;
+	 */
 	struct padata_work *pw = container_of(w, struct padata_work, pw_work);
 	struct padata_mt_job_state *ps = pw->pw_data;
 	struct padata_mt_job *job = ps->job;
 	bool done;
 
+	/*
+	 * 这里锁上ps->lock!
+	 */
 	spin_lock(&ps->lock);
 
 	while (ps->remaining_size > 0 && ps->error == 0) {
@@ -500,12 +591,47 @@ static void padata_mt_helper(struct work_struct *w)
 		ps->position = end;
 		ps->remaining_size -= size;
 
+		/*
+		 * 这里解锁ps->lock!
+		 */
 		spin_unlock(&ps->lock);
 		lock_map_acquire(&ps->lockdep_map);
 
+		/*
+		 * spin_lock_init(&ps.lock);
+		 * init_completion(&ps.completion);
+		 * INIT_LIST_HEAD(&ps.failed_works);
+		 * lockdep_init_map(&ps.lockdep_map, map_name, key, 0);
+		 * ps.job            = job;
+		 * ps.nworks         = padata_work_alloc_mt(nworks, &ps, &works);
+		 * ps.nworks_fini    = 0;
+		 * ps.error          = 0;
+		 * ps.position       = job->start;
+		 * ps.remaining_size = job->size;
+		 *
+		 * ps.chunk_size = job->size / (ps.nworks * load_balance_factor);
+		 * ps.chunk_size = max(ps.chunk_size, job->min_chunk);
+		 * ps.chunk_size = roundup(ps.chunk_size, job->align);
+		 *
+		 * 1977         struct padata_mt_job job = {
+		 * 1978                 .thread_fn   = vfio_pin_map_dma_chunk,
+		 * 1979                 .fn_arg      = &args,
+		 * 1980                 .start       = dma->vaddr,
+		 * 1981                 .size        = map_size,
+		 * 1982                 .align       = PMD_SIZE,
+		 * 1983                 .min_chunk   = (1ul << 27),
+		 * 1984                 .undo_fn     = vfio_pin_map_dma_undo,
+		 * 1985                 .max_threads = 16,
+		 * 1986         };
+		 *
+		 * vfio_pin_map_dma_chunk()
+		 */
 		ret = job->thread_fn(position, end, job->fn_arg);
 
 		lock_map_release(&ps->lockdep_map);
+		/*
+		 * 这里锁上ps->lock!
+		 */
 		spin_lock(&ps->lock);
 
 		if (ret) {
@@ -514,7 +640,21 @@ static void padata_mt_helper(struct work_struct *w)
 				ps->error = ret;
 			/* Save information about where the job failed. */
 			if (job->undo_fn) {
+				/*
+				 * 在以下使用padata_mt_job_state->failed_works:
+				 *   - kernel/padata.c|587| <<padata_mt_helper>> list_move(&pw->pw_list, &ps->failed_works);
+				 *   - kernel/padata.c|627| <<padata_undo>> struct list_head *failed_works = &ps->failed_works;
+				 *   - kernel/padata.c|632| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+				 *   - kernel/padata.c|639| <<padata_undo>> failed_work = list_first_entry_or_null(failed_works,
+				 *   - kernel/padata.c|661| <<padata_undo>> WARN_ON(!list_empty(failed_works));
+				 *   - kernel/padata.c|735| <<__padata_do_multithreaded>> INIT_LIST_HEAD(&ps.failed_works);
+				 */
 				list_move(&pw->pw_list, &ps->failed_works);
+				/*
+				 * 在以下使用padata_work->pw_error_start:
+				 *   - kernel/padata.c|606| <<padata_mt_helper>> pw->pw_error_start = position;
+				 *   - kernel/padata.c|690| <<padata_undo>> undo_end = failed_work->pw_error_start;
+				 */
 				pw->pw_error_start = position;
 				pw->pw_error_offset = position_offset;
 				pw->pw_error_end = end;
@@ -524,18 +664,33 @@ static void padata_mt_helper(struct work_struct *w)
 
 	++ps->nworks_fini;
 	done = (ps->nworks_fini == ps->nworks);
+	/*
+	 * 这里解锁ps->lock!
+	 */
 	spin_unlock(&ps->lock);
 
 	if (done)
 		complete(&ps->completion);
 }
 
+/*
+ * 在以下使用padata_error_cmp():
+ *   - kernel/padata.c|702| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+ */
 static int padata_error_cmp(void *unused, struct list_head *a,
 			    struct list_head *b)
 {
 	struct padata_work *work_a = list_entry(a, struct padata_work, pw_list);
 	struct padata_work *work_b = list_entry(b, struct padata_work, pw_list);
 
+	/*
+	 * 在以下使用padata_work->pw_error_offset:
+	 *   - kernel/padata.c|607| <<padata_mt_helper>> pw->pw_error_offset = position_offset;
+	 *   - kernel/padata.c|630| <<padata_error_cmp>> if (work_a->pw_error_offset < work_b->pw_error_offset)
+	 *   - kernel/padata.c|632| <<padata_error_cmp>> else if (work_a->pw_error_offset > work_b->pw_error_offset)
+	 *
+	 * 主要的应用是padata_error_cmp()
+	 */
 	if (work_a->pw_error_offset < work_b->pw_error_offset)
 		return -1;
 	else if (work_a->pw_error_offset > work_b->pw_error_offset)
@@ -543,10 +698,54 @@ static int padata_error_cmp(void *unused, struct list_head *a,
 	return 0;
 }
 
+/*
+ * spin_lock_init(&ps.lock);
+ * init_completion(&ps.completion);
+ * INIT_LIST_HEAD(&ps.failed_works);
+ * lockdep_init_map(&ps.lockdep_map, map_name, key, 0);
+ * ps.job            = job;
+ * ps.nworks         = padata_work_alloc_mt(nworks, &ps, &works);
+ * ps.nworks_fini    = 0;
+ * ps.error          = 0;
+ * ps.position       = job->start;
+ * ps.remaining_size = job->size;
+ *
+ * ps.chunk_size = job->size / (ps.nworks * load_balance_factor);
+ * ps.chunk_size = max(ps.chunk_size, job->min_chunk);
+ * ps.chunk_size = roundup(ps.chunk_size, job->align);
+ *
+ *
+ * 1977         struct padata_mt_job job = {
+ * 1978                 .thread_fn   = vfio_pin_map_dma_chunk,
+ * 1979                 .fn_arg      = &args,
+ * 1980                 .start       = dma->vaddr,
+ * 1981                 .size        = map_size,
+ * 1982                 .align       = PMD_SIZE,
+ * 1983                 .min_chunk   = (1ul << 27),
+ * 1984                 .undo_fn     = vfio_pin_map_dma_undo,
+ * 1985                 .max_threads = 16,
+ * 1986         };
+ *
+ *
+ * called by:
+ *   - kernel/padata.c|680| <<__padata_do_multithreaded>> padata_undo(&ps, &works, &my_work);
+ *
+ * padata_undo()就是要把成功的unpin了
+ * 不是把失败的unpin了
+ */
 static void padata_undo(struct padata_mt_job_state *ps,
 			struct list_head *works_list,
 			struct padata_work *stack_work)
 {
+	/*
+	 * 在以下使用padata_mt_job_state->failed_works:
+	 *   - kernel/padata.c|587| <<padata_mt_helper>> list_move(&pw->pw_list, &ps->failed_works);
+	 *   - kernel/padata.c|627| <<padata_undo>> struct list_head *failed_works = &ps->failed_works;
+	 *   - kernel/padata.c|632| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+	 *   - kernel/padata.c|639| <<padata_undo>> failed_work = list_first_entry_or_null(failed_works,
+	 *   - kernel/padata.c|661| <<padata_undo>> WARN_ON(!list_empty(failed_works));
+	 *   - kernel/padata.c|735| <<__padata_do_multithreaded>> INIT_LIST_HEAD(&ps.failed_works);
+	 */
 	struct list_head *failed_works = &ps->failed_works;
 	struct padata_mt_job *job = ps->job;
 	unsigned long undo_pos = job->start;
@@ -554,23 +753,69 @@ static void padata_undo(struct padata_mt_job_state *ps,
 	/* Sort so the failed ranges can be checked as we go. */
 	list_sort(NULL, failed_works, padata_error_cmp);
 
+	/*
+	 * 42 struct padata_work {
+	 * 43         struct work_struct      pw_work;
+	 * 44         struct list_head        pw_list;  // padata_free_works linkage
+	 * 45         void                    *pw_data;
+	 * 46         // holds job units from padata_mt_job::start to pw_error_start
+	 * 47         unsigned long           pw_error_offset;
+	 * 48         unsigned long           pw_error_start;
+	 * 49         unsigned long           pw_error_end;
+	 * 50 };
+	 */
 	/* Undo completed work on this node, skipping failed ranges. */
 	while (undo_pos != ps->position) {
 		struct padata_work *failed_work;
 		unsigned long undo_end;
 
+		/*
+		 * 在以下使用padata_mt_job_state->failed_works:
+		 *   - kernel/padata.c|587| <<padata_mt_helper>> list_move(&pw->pw_list, &ps->failed_works);
+		 *   - kernel/padata.c|627| <<padata_undo>> struct list_head *failed_works = &ps->failed_works;
+		 *   - kernel/padata.c|632| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+		 *   - kernel/padata.c|639| <<padata_undo>> failed_work = list_first_entry_or_null(failed_works,
+		 *   - kernel/padata.c|661| <<padata_undo>> WARN_ON(!list_empty(failed_works));
+		 *   - kernel/padata.c|735| <<__padata_do_multithreaded>> INIT_LIST_HEAD(&ps.failed_works);
+		 */
 		failed_work = list_first_entry_or_null(failed_works,
 						       struct padata_work,
 						       pw_list);
+		/*
+		 * 在以下使用padata_work->pw_error_start:
+		 *   - kernel/padata.c|606| <<padata_mt_helper>> pw->pw_error_start = position;
+		 *   - kernel/padata.c|690| <<padata_undo>> undo_end = failed_work->pw_error_start;
+		 */
 		if (failed_work)
 			undo_end = failed_work->pw_error_start;
 		else
 			undo_end = ps->position;
 
+		/*
+		 * 1977         struct padata_mt_job job = {
+		 * 1978                 .thread_fn   = vfio_pin_map_dma_chunk,
+		 * 1979                 .fn_arg      = &args,
+		 * 1980                 .start       = dma->vaddr,
+		 * 1981                 .size        = map_size,
+		 * 1982                 .align       = PMD_SIZE,
+		 * 1983                 .min_chunk   = (1ul << 27),
+		 * 1984                 .undo_fn     = vfio_pin_map_dma_undo,
+		 * 1985                 .max_threads = 16,
+		 * 1986         };
+		 *
+		 * 在以下使用vfio_pin_map_dma_undo():
+		 *   - drivers/vfio/vfio_iommu_type1.c|1891| <<vfio_pin_map_dma_chunk>> vfio_pin_map_dma_undo(start_vaddr, start_vaddr + mapped_size,
+		 *   - drivers/vfio/vfio_iommu_type1.c|1915| <<vfio_pin_map_dma>> .undo_fn = vfio_pin_map_dma_undo,
+		 *
+		 * 1 << 27 是 134217728, 也就是128MB
+		 */
 		if (undo_pos != undo_end)
 			job->undo_fn(undo_pos, undo_end, job->fn_arg);
 
 		if (failed_work) {
+			/*
+			 * 两处修改undo_pos的地方之一
+			 */
 			undo_pos = failed_work->pw_error_end;
 			/* main thread's stack_work stays off works_list */
 			if (failed_work == stack_work)
@@ -578,9 +823,21 @@ static void padata_undo(struct padata_mt_job_state *ps,
 			else
 				list_move(&failed_work->pw_list, works_list);
 		} else {
+			/*
+			 * 两处修改undo_pos的地方之一
+			 */
 			undo_pos = undo_end;
 		}
 	}
+	/*
+	 * 在以下使用padata_mt_job_state->failed_works:
+	 *   - kernel/padata.c|587| <<padata_mt_helper>> list_move(&pw->pw_list, &ps->failed_works);
+	 *   - kernel/padata.c|627| <<padata_undo>> struct list_head *failed_works = &ps->failed_works;
+	 *   - kernel/padata.c|632| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+	 *   - kernel/padata.c|639| <<padata_undo>> failed_work = list_first_entry_or_null(failed_works,
+	 *   - kernel/padata.c|661| <<padata_undo>> WARN_ON(!list_empty(failed_works));
+	 *   - kernel/padata.c|735| <<__padata_do_multithreaded>> INIT_LIST_HEAD(&ps.failed_works);
+	 */
 	WARN_ON(!list_empty(failed_works));
 }
 
@@ -592,6 +849,25 @@ static void padata_undo(struct padata_mt_job_state *ps,
  *
  * Return: 0 for success or a client-specific nonzero error code.
  */
+/*
+ * 如果是vfio_pin_map_dma()来的:
+ * 2130         struct padata_mt_job job = {
+ * 2131                 .thread_fn   = vfio_pin_map_dma_chunk,
+ * 2132                 .fn_arg      = &args,
+ * 2133                 .start       = dma->vaddr,
+ * 2134                 .size        = map_size,
+ * 2135                 .align       = PMD_SIZE,
+ * 2136                 .min_chunk   = (1ul << 27),
+ * 2137                 .undo_fn     = vfio_pin_map_dma_undo,
+ * 2138                 .max_threads = 16,
+ * 2139         };
+ *
+ * 1 << 27 是 134217728, 也就是128MB
+ *
+ * called by:
+ *   - nclude/linux/padata.h|214| <<padata_do_multithreaded>> __padata_do_multithreaded((job), &__key, __map_name); \
+ *   - include/linux/padata.h|220| <<padata_do_multithreaded>> __padata_do_multithreaded((job), NULL, NULL)
+ */
 int __padata_do_multithreaded(struct padata_mt_job *job,
 			      struct lock_class_key *key,
 			      const char *map_name)
@@ -617,15 +893,47 @@ int __padata_do_multithreaded(struct padata_mt_job *job,
 		return ret;
 	}
 
+	/*
+	 * 56 struct padata_mt_job_state {
+	 * 57         spinlock_t              lock;
+	 * 58         struct completion       completion;
+	 * 59         struct padata_mt_job    *job;
+	 * 60         int                     nworks;
+	 * 61         int                     nworks_fini;
+	 * 62         int                     error; // first error from thread_fn
+	 * 63         unsigned long           chunk_size;
+	 * 64         unsigned long           position;
+	 * 65         unsigned long           remaining_size;
+	 * 66         struct list_head        failed_works;
+	 * 67 #ifdef CONFIG_LOCKDEP
+	 * 68         struct lockdep_map      lockdep_map;
+	 * 69 #endif
+	 * 70 };
+	 */
 	spin_lock_init(&ps.lock);
 	init_completion(&ps.completion);
+	/*
+	 * 在以下使用padata_mt_job_state->failed_works:
+	 *   - kernel/padata.c|587| <<padata_mt_helper>> list_move(&pw->pw_list, &ps->failed_works);
+	 *   - kernel/padata.c|627| <<padata_undo>> struct list_head *failed_works = &ps->failed_works;
+	 *   - kernel/padata.c|632| <<padata_undo>> list_sort(NULL, failed_works, padata_error_cmp);
+	 *   - kernel/padata.c|639| <<padata_undo>> failed_work = list_first_entry_or_null(failed_works,
+	 *   - kernel/padata.c|661| <<padata_undo>> WARN_ON(!list_empty(failed_works));
+	 *   - kernel/padata.c|735| <<__padata_do_multithreaded>> INIT_LIST_HEAD(&ps.failed_works);
+	 */
 	INIT_LIST_HEAD(&ps.failed_works);
 	lockdep_init_map(&ps.lockdep_map, map_name, key, 0);
 	ps.job		  = job;
+	/*
+	 * 会把分配的work插入works链表
+	 */
 	ps.nworks	  = padata_work_alloc_mt(nworks, &ps, &works);
 	ps.nworks_fini	  = 0;
 	ps.error	  = 0;
 	ps.position	  = job->start;
+	/*
+	 * 一开始remaining_size就是size!!!
+	 */
 	ps.remaining_size = job->size;
 
 	/*
@@ -641,17 +949,77 @@ int __padata_do_multithreaded(struct padata_mt_job *job,
 	lock_map_acquire(&ps.lockdep_map);
 	lock_map_release(&ps.lockdep_map);
 
+	/*
+	 * 在以下使用padata_work->pw_work:
+	 *   - kernel/padata.c|142| <<padata_work_init>> INIT_WORK_ONSTACK(&pw->pw_work, work_fn);
+	 *   - kernel/padata.c|144| <<padata_work_init>> INIT_WORK(&pw->pw_work, work_fn);
+	 *   - kernel/padata.c|191| <<padata_parallel_worker>> struct padata_work *pw = container_of(parallel_work,
+	 *                               struct padata_work, pw_work);
+	 *   - kernel/padata.c|265| <<padata_do_parallel>> queue_work(pinst->parallel_wq, &pw->pw_work);
+	 *   - kernel/padata.c|549| <<padata_mt_helper>> struct padata_work *pw = container_of(w, struct padata_work, pw_work);
+	 *   - kernel/padata.c|872| <<__padata_do_multithreaded>> queue_work(system_unbound_wq, &pw->pw_work);
+	 *   - kernel/padata.c|883| <<__padata_do_multithreaded>> padata_mt_helper(&my_work.pw_work);
+	 *   kernel/padata.c|927| <<__padata_do_multithreaded>> destroy_work_on_stack(&my_work.pw_work);
+	 *
+	 * pw->pw_work的fn是padata_mt_helper()
+	 */
 	list_for_each_entry(pw, &works, pw_list)
 		queue_work(system_unbound_wq, &pw->pw_work);
 
 	/* Use the current thread, which saves starting a workqueue worker. */
 	padata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);
 	INIT_LIST_HEAD(&my_work.pw_list);
+	/*
+	 * 在以下使用padata_mt_helper():
+	 *   - kernel/padata.c|132| <<padata_work_alloc_mt>> padata_work_init(pw, padata_mt_helper, data, 0);
+	 *   - kernel/padata.c|648| <<__padata_do_multithreaded>> padata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);
+	 *   - kernel/padata.c|650| <<__padata_do_multithreaded>> padata_mt_helper(&my_work.pw_work);
+	 */
 	padata_mt_helper(&my_work.pw_work);
 
 	/* Wait for all the helpers to finish. */
 	wait_for_completion(&ps.completion);
 
+	/*
+	 * 56 struct padata_mt_job_state {
+	 * 57         spinlock_t              lock;
+	 * 58         struct completion       completion;
+	 * 59         struct padata_mt_job    *job;
+	 * 60         int                     nworks;
+	 * 61         int                     nworks_fini;
+	 * 62         int                     error; // first error from thread_fn
+	 * 63         unsigned long           chunk_size;
+	 * 64         unsigned long           position;
+	 * 65         unsigned long           remaining_size;
+	 * 66         struct list_head        failed_works;
+	 * 67 #ifdef CONFIG_LOCKDEP     
+	 * 68         struct lockdep_map      lockdep_map;
+	 * 69 #endif          
+	 * 70 };
+	 *
+	 * 1977         struct padata_mt_job job = {
+	 * 1978                 .thread_fn   = vfio_pin_map_dma_chunk,
+	 * 1979                 .fn_arg      = &args,
+	 * 1980                 .start       = dma->vaddr,
+	 * 1981                 .size        = map_size,
+	 * 1982                 .align       = PMD_SIZE,
+	 * 1983                 .min_chunk   = (1ul << 27),
+	 * 1984                 .undo_fn     = vfio_pin_map_dma_undo,
+	 * 1985                 .max_threads = 16,
+	 * 1986         };
+	 *
+	 * 1 << 27 是 134217728, 也就是128MB
+	 *
+	 * struct padata_work my_work, *pw;
+	 * struct padata_mt_job_state ps;
+	 * LIST_HEAD(works);
+	 *
+	 * 只在此处调用padata_undo()
+	 *
+	 * 上面那些代码有失败的直接在vfio_pin_map_dma_chunk()内部unpin了
+	 * 可是还有很多成功的
+	 * padata_undo()就是要把成功的unpin了
+	 */
 	if (ps.error && job->undo_fn)
 		padata_undo(&ps, &works, &my_work);
 
diff --git a/mm/gup.c b/mm/gup.c
index 12ac0980ce7c..a61702f3f7ee 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -870,6 +870,23 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
  * instead of __get_user_pages. __get_user_pages should be used only if
  * you need some special @gup_flags.
  */
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ */
 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
@@ -1095,6 +1112,31 @@ int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 }
 EXPORT_SYMBOL_GPL(fixup_user_fault);
 
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ *
+ * called by:
+ *   - mm/gup.c|1348| <<get_user_pages_remote>> return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
+ *   - mm/gup.c|1692| <<check_and_migrate_cma_pages>> nr_pages = __get_user_pages_locked(tsk, mm, start, nr_pages,
+ *   - mm/gup.c|1747| <<__gup_longterm_locked>> rc = __get_user_pages_locked(tsk, mm, start, nr_pages, pages,
+ *   - mm/gup.c|1780| <<__gup_longterm_locked>> return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
+ *   - mm/gup.c|1835| <<get_user_pages_locked>> return __get_user_pages_locked(current, current->mm, start, nr_pages,
+ *   - mm/gup.c|1873| <<get_user_pages_unlocked>> ret = __get_user_pages_locked(current, mm, start, nr_pages, pages, NULL,
+ */
 static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
 						struct mm_struct *mm,
 						unsigned long start,
@@ -1120,6 +1162,23 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
 	pages_done = 0;
 	lock_dropped = false;
 	for (;;) {
+		/*
+		 * 可能返回-512 (-ERESTARTSYS).
+		 *
+		 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+		 * 886                 unsigned long start, unsigned long nr_pages,
+		 * 887                 unsigned int gup_flags, struct page **pages,
+		 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+		 * ... ...
+		 * 933                 //
+		 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+		 * 935                 // potentially allocating memory.
+		 * 936                  //
+		 * 937                 if (fatal_signal_pending(current)) {
+		 * 938                         ret = -ERESTARTSYS;
+		 * 939                         goto out;
+		 * 940                 }
+		 */
 		ret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,
 				       vmas, locked);
 		if (!locked)
@@ -1246,6 +1305,23 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
  * should use get_user_pages because it cannot pass
  * FAULT_FLAG_ALLOW_RETRY to handle_mm_fault.
  */
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ */
 long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 		unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
@@ -1260,6 +1336,23 @@ long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 	if (WARN_ON_ONCE(gup_flags & FOLL_LONGTERM))
 		return -EINVAL;
 
+	/*
+	 * 可能返回-512 (-ERESTARTSYS).
+	 *
+	 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+	 * 886                 unsigned long start, unsigned long nr_pages,
+	 * 887                 unsigned int gup_flags, struct page **pages,
+	 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+	 * ... ...
+	 * 933                 //
+	 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+	 * 935                 // potentially allocating memory.
+	 * 936                  //
+	 * 937                 if (fatal_signal_pending(current)) {
+	 * 938                         ret = -ERESTARTSYS;
+	 * 939                         goto out;
+	 * 940                 }
+	 */
 	return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
 				       locked,
 				       gup_flags | FOLL_TOUCH | FOLL_REMOTE);
@@ -1530,6 +1623,23 @@ static struct page *new_non_cma_page(struct page *page, unsigned long private)
 	return __alloc_pages_node(nid, gfp_mask, 0);
 }
 
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+1117  *
+1118  * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+1119  * 886                 unsigned long start, unsigned long nr_pages,
+1120  * 887                 unsigned int gup_flags, struct page **pages,
+1121  * 888                 struct vm_area_struct **vmas, int *nonblocking)
+1122  * ... ...
+1123  * 933                 //
+1124  * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+1125  * 935                 // potentially allocating memory.
+1126  * 936                  //
+1127  * 937                 if (fatal_signal_pending(current)) {
+1128  * 938                         ret = -ERESTARTSYS;
+1129  * 939                         goto out;
+1130  * 940                 }
+ */
 static long check_and_migrate_cma_pages(struct task_struct *tsk,
 					struct mm_struct *mm,
 					unsigned long start,
@@ -1604,6 +1714,23 @@ static long check_and_migrate_cma_pages(struct task_struct *tsk,
 		 * again migrating any new CMA pages which we failed to isolate
 		 * earlier.
 		 */
+		/*
+		 * 可能返回-512 (-ERESTARTSYS).
+		 *
+		 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+		 * 886                 unsigned long start, unsigned long nr_pages,
+		 * 887                 unsigned int gup_flags, struct page **pages,
+		 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+		 * ... ...
+		 * 933                 //
+		 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+		 * 935                 // potentially allocating memory.
+		 * 936                  //
+		 * 937                 if (fatal_signal_pending(current)) {
+		 * 938                         ret = -ERESTARTSYS;
+		 * 939                         goto out;
+		 * 940                 }
+		 */
 		nr_pages = __get_user_pages_locked(tsk, mm, start, nr_pages,
 						   pages, vmas, NULL,
 						   gup_flags);
@@ -1633,6 +1760,23 @@ static long check_and_migrate_cma_pages(struct task_struct *tsk,
  * __gup_longterm_locked() is a wrapper for __get_user_pages_locked which
  * allows us to process the FOLL_LONGTERM flag.
  */
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ */
 static long __gup_longterm_locked(struct task_struct *tsk,
 				  struct mm_struct *mm,
 				  unsigned long start,
@@ -1659,6 +1803,23 @@ static long __gup_longterm_locked(struct task_struct *tsk,
 		flags = memalloc_nocma_save();
 	}
 
+	/*
+	 * 可能返回-512 (-ERESTARTSYS).
+	 *
+	 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+	 * 886                 unsigned long start, unsigned long nr_pages,
+	 * 887                 unsigned int gup_flags, struct page **pages,
+	 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+	 * ... ...
+	 * 933                 //
+	 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+	 * 935                 // potentially allocating memory.
+	 * 936                  //
+	 * 937                 if (fatal_signal_pending(current)) {
+	 * 938                         ret = -ERESTARTSYS;
+	 * 939                         goto out;
+	 * 940                 }
+	 */
 	rc = __get_user_pages_locked(tsk, mm, start, nr_pages, pages,
 				     vmas_tmp, NULL, gup_flags);
 
@@ -1684,6 +1845,23 @@ static long __gup_longterm_locked(struct task_struct *tsk,
 	return rc;
 }
 #else /* !CONFIG_FS_DAX && !CONFIG_CMA */
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ */
 static __always_inline long __gup_longterm_locked(struct task_struct *tsk,
 						  struct mm_struct *mm,
 						  unsigned long start,
@@ -1692,6 +1870,23 @@ static __always_inline long __gup_longterm_locked(struct task_struct *tsk,
 						  struct vm_area_struct **vmas,
 						  unsigned int flags)
 {
+	/*
+	 * 可能返回-512 (-ERESTARTSYS).
+	 *
+	 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+	 * 886                 unsigned long start, unsigned long nr_pages,
+	 * 887                 unsigned int gup_flags, struct page **pages,
+	 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+	 * ... ...
+	 * 933                 //
+	 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+	 * 935                 // potentially allocating memory.
+	 * 936                  //
+	 * 937                 if (fatal_signal_pending(current)) {
+	 * 938                         ret = -ERESTARTSYS;
+	 * 939                         goto out;
+	 * 940                 }
+	 */
 	return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
 				       NULL, flags);
 }
@@ -1704,6 +1899,28 @@ static __always_inline long __gup_longterm_locked(struct task_struct *tsk,
  * passing of a locked parameter.  We also obviously don't pass
  * FOLL_REMOTE in here.
  */
+/*
+ * called by:
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|106| <<mm_iommu_do_alloc>> ret = get_user_pages(ua + (entry << PAGE_SHIFT), n,
+ *   - arch/x86/kernel/cpu/sgx/ioctl.c|247| <<__sgx_encl_add_page>> ret = get_user_pages(src, 1, 0, &src_page, NULL);
+ *   - arch/x86/mm/mpx.c|423| <<mpx_resolve_fault>> gup_ret = get_user_pages((unsigned long )addr, nr_pages,
+ *   - drivers/gpu/drm/radeon/radeon_ttm.c|514| <<radeon_ttm_tt_pin_userptr>> r = get_user_pages(userptr, num_pages, write ? FOLL_WRITE : 0,
+ *   - drivers/infiniband/core/umem.c|216| <<ib_umem_get>> ret = get_user_pages(cur_base,
+ *   - drivers/infiniband/hw/qib/qib_user_pages.c|111| <<qib_get_user_pages>> ret = get_user_pages(start_page + got * PAGE_SIZE,
+ *   - drivers/infiniband/hw/usnic/usnic_uiom.c|144| <<usnic_uiom_get_pages>> ret = get_user_pages(cur_base,
+ *   - drivers/infiniband/sw/siw/siw_mem.c|429| <<siw_umem_get>> rv = get_user_pages(first_page_va, nents,
+ *   - drivers/media/v4l2-core/videobuf-dma-sg.c|186| <<videobuf_dma_init_user_locked>> err = get_user_pages(data & PAGE_MASK, dma->nr_pages,
+ *   - drivers/misc/sgi-gru/grufault.c|188| <<non_atomic_pte_lookup>> if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &page, NULL) <= 0)
+ *   - drivers/staging/kpc2000/kpc_dma/fileops.c|80| <<kpc_dma_transfer>> rv = get_user_pages(iov_base, acd->page_count, FOLL_TOUCH | FOLL_WRITE | FOLL_GET, acd->user_pages, NULL);
+ *   - drivers/vfio/vfio_iommu_type1.c|751| <<vaddr_get_pfn>> ret = get_user_pages(vaddr, 1, flags | FOLL_LONGTERM, page,
+ *   - drivers/vhost/vdpa.c|953| <<vhost_vdpa_pa_map>> pinned = get_user_pages(cur_base, sz2pin,
+ *   - io_uring/io_uring.c|8942| <<io_sqe_buffer_register>> pret = get_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,
+ *   - mm/gup_benchmark.c|58| <<__gup_benchmark_ioctl>> nr = get_user_pages(addr, nr,
+ *   - mm/gup_benchmark.c|63| <<__gup_benchmark_ioctl>> nr = get_user_pages(addr, nr, gup->flags & 1, pages + i,
+ *   - net/rds/rdma.c|173| <<rds_pin_pages>> ret = get_user_pages(user_addr, nr_pages, gup_flags, pages, NULL);
+ *   - net/xdp/xdp_umem.c|294| <<xdp_umem_pin_pages>> npgs = get_user_pages(umem->address, umem->npgs,
+ *   - virt/kvm/kvm_main.c|1606| <<check_user_page_hwpoison>> rc = get_user_pages(addr, 1, flags, NULL, NULL);
+ */
 long get_user_pages(unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
 		struct vm_area_struct **vmas)
@@ -1734,6 +1951,23 @@ EXPORT_SYMBOL(get_user_pages);
  *      if (locked)
  *          up_read(&mm->mmap_sem);
  */
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ */
 long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
 			   unsigned int gup_flags, struct page **pages,
 			   int *locked)
@@ -1747,6 +1981,23 @@ long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
 	if (WARN_ON_ONCE(gup_flags & FOLL_LONGTERM))
 		return -EINVAL;
 
+	/*
+	 * 可能返回-512 (-ERESTARTSYS).
+	 *
+	 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+	 * 886                 unsigned long start, unsigned long nr_pages,
+	 * 887                 unsigned int gup_flags, struct page **pages,
+	 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+	 * ... ...
+	 * 933                 //
+	 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+	 * 935                 // potentially allocating memory.
+	 * 936                  //
+	 * 937                 if (fatal_signal_pending(current)) {
+	 * 938                         ret = -ERESTARTSYS;
+	 * 939                         goto out;
+	 * 940                 }
+	 */
 	return __get_user_pages_locked(current, current->mm, start, nr_pages,
 				       pages, NULL, locked,
 				       gup_flags | FOLL_TOUCH);
@@ -1768,6 +2019,23 @@ EXPORT_SYMBOL(get_user_pages_locked);
  * get_user_pages_fast should be used instead if specific gup_flags
  * (e.g. FOLL_FORCE) are not required.
  */
+/*
+ * 可能返回-512 (-ERESTARTSYS).
+ *
+ * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+ * 886                 unsigned long start, unsigned long nr_pages,
+ * 887                 unsigned int gup_flags, struct page **pages,
+ * 888                 struct vm_area_struct **vmas, int *nonblocking)
+ * ... ...
+ * 933                 //
+ * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+ * 935                 // potentially allocating memory.
+ * 936                  //
+ * 937                 if (fatal_signal_pending(current)) {
+ * 938                         ret = -ERESTARTSYS;
+ * 939                         goto out;
+ * 940                 }
+ */
 long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
 			     struct page **pages, unsigned int gup_flags)
 {
@@ -1785,6 +2053,23 @@ long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
 		return -EINVAL;
 
 	down_read(&mm->mmap_sem);
+	/*
+	 * 可能返回-512 (-ERESTARTSYS).
+	 *
+	 * 885 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
+	 * 886                 unsigned long start, unsigned long nr_pages,
+	 * 887                 unsigned int gup_flags, struct page **pages,
+	 * 888                 struct vm_area_struct **vmas, int *nonblocking)
+	 * ... ...
+	 * 933                 //
+	 * 934                 // If we have a pending SIGKILL, don't keep faulting pages and
+	 * 935                 // potentially allocating memory.
+	 * 936                  //
+	 * 937                 if (fatal_signal_pending(current)) {
+	 * 938                         ret = -ERESTARTSYS;
+	 * 939                         goto out;
+	 * 940                 }
+	 */
 	ret = __get_user_pages_locked(current, mm, start, nr_pages, pages, NULL,
 				      &locked, gup_flags | FOLL_TOUCH);
 	if (locked)
diff --git a/mm/util.c b/mm/util.c
index 04ebc76588aa..9e39e24f0e92 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -453,6 +453,26 @@ void arch_pick_mmap_layout(struct mm_struct *mm, struct rlimit *rlim_stack)
  * * 0       on success
  * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.
  */
+/*
+ * 在以下调用account_locked_vm():
+ *   - arch/powerpc/kvm/book3s_64_vio.c|266| <<kvm_spapr_tce_release>> account_locked_vm(current->mm, kvmppc_stt_pages(kvmppc_tce_pages(stt->size)), false);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|291| <<kvm_vm_ioctl_create_spapr_tce>> ret = account_locked_vm(current->mm, kvmppc_stt_pages(npages), true);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|337| <<kvm_vm_ioctl_create_spapr_tce>> account_locked_vm(current->mm, kvmppc_stt_pages(npages), false);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|66| <<mm_iommu_do_alloc>> ret = account_locked_vm(mm, entries, true);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|181| <<mm_iommu_do_alloc>> account_locked_vm(mm, locked_entries, false);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|281| <<mm_iommu_put>> account_locked_vm(mm, unlock_entries, false);
+ *   - drivers/fpga/dfl-afu-dma-region.c|50| <<afu_dma_pin_pages>> ret = account_locked_vm(current->mm, npages, true);
+ *   - drivers/fpga/dfl-afu-dma-region.c|79| <<afu_dma_pin_pages>> account_locked_vm(current->mm, npages, false);
+ *   - drivers/fpga/dfl-afu-dma-region.c|99| <<afu_dma_unpin_pages>> account_locked_vm(current->mm, npages, false);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|292| <<tce_iommu_enable>> ret = account_locked_vm(container->mm, locked, true);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|311| <<tce_iommu_disable>> account_locked_vm(container->mm, container->locked_pages, false);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|621| <<tce_iommu_create_table>> ret = account_locked_vm(container->mm, table_size >> PAGE_SHIFT, true);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|640| <<tce_iommu_free_table>> account_locked_vm(container->mm, pages, false);
+ *
+ * called by:
+ *   - drivers/vfio/vfio_iommu_type1.c|359| <<vfio_lock_acct>> ret = __account_locked_vm(mm, abs(npage), npage > 0, dma->task, dma->lock_cap);
+ *   - mm/util.c|507| <<account_locked_vm>> ret = __account_locked_vm(mm, pages, inc, current, capable(CAP_IPC_LOCK));
+ */
 int __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,
 			struct task_struct *task, bool bypass_rlim)
 {
@@ -461,6 +481,24 @@ int __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,
 
 	lockdep_assert_held_write(&mm->mmap_sem);
 
+	/*
+	 * 在以下修改mm_struct->locked_vm:
+	 *   - fs/exec.c|1087| <<exec_mmap>> mm->locked_vm = old_mm->locked_vm;
+	 *   - fs/exec.c|1371| <<flush_old_exec>> current->mm->locked_vm = current->mm->driver_pinned_vm;
+	 *   - kernel/fork.c|1029| <<mm_init>> mm->locked_vm = 0;
+	 *   - mm/debug.c|175| <<dump_mm>> pr_emerg(" ... mm->hiwater_rss, mm->hiwater_vm, mm->total_vm, mm->locked_vm,
+	 *   - mm/mlock.c|565| <<mlock_fixup>> mm->locked_vm += nr_pages;
+	 *   - mm/mlock.c|692| <<do_mlock>> locked += current->mm->locked_vm;
+	 *   - mm/mmap.c|2002| <<mmap_region>> mm->locked_vm += (len >> PAGE_SHIFT);
+	 *   - mm/mmap.c|2567| <<expand_upwards>> mm->locked_vm += grow;
+	 *   - mm/mmap.c|2647| <<expand_downwards>> mm->locked_vm += grow;
+	 *   - mm/mmap.c|3031| <<__do_munmap>> mm->locked_vm -= vma_pages(tmp);
+	 *   - mm/mmap.c|3273| <<do_brk_flags>> mm->locked_vm += (len >> PAGE_SHIFT);
+	 *   - mm/mremap.c|435| <<move_vma>> mm->locked_vm += new_len >> PAGE_SHIFT;
+	 *   - mm/mremap.c|726| <<SYSCALL_DEFINE5(mremap)>> mm->locked_vm += pages;
+	 *   - mm/util.c|492| <<__account_locked_vm>> mm->locked_vm = locked_vm + pages;
+	 *   - mm/util.c|495| <<__account_locked_vm>> mm->locked_vm = locked_vm - pages;
+	 */
 	locked_vm = mm->locked_vm;
 	if (inc) {
 		if (!bypass_rlim) {
@@ -496,6 +534,22 @@ EXPORT_SYMBOL_GPL(__account_locked_vm);
  * * 0       on success, or if mm is NULL
  * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.
  */
+/*
+ * called by:
+ *   - arch/powerpc/kvm/book3s_64_vio.c|266| <<kvm_spapr_tce_release>> account_locked_vm(current->mm, kvmppc_stt_pages(kvmppc_tce_pages(stt->size)), false);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|291| <<kvm_vm_ioctl_create_spapr_tce>> ret = account_locked_vm(current->mm, kvmppc_stt_pages(npages), true);
+ *   - arch/powerpc/kvm/book3s_64_vio.c|337| <<kvm_vm_ioctl_create_spapr_tce>> account_locked_vm(current->mm, kvmppc_stt_pages(npages), false);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|66| <<mm_iommu_do_alloc>> ret = account_locked_vm(mm, entries, true);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|181| <<mm_iommu_do_alloc>> account_locked_vm(mm, locked_entries, false);
+ *   - arch/powerpc/mm/book3s64/iommu_api.c|281| <<mm_iommu_put>> account_locked_vm(mm, unlock_entries, false);
+ *   - drivers/fpga/dfl-afu-dma-region.c|50| <<afu_dma_pin_pages>> ret = account_locked_vm(current->mm, npages, true);
+ *   - drivers/fpga/dfl-afu-dma-region.c|79| <<afu_dma_pin_pages>> account_locked_vm(current->mm, npages, false);
+ *   - drivers/fpga/dfl-afu-dma-region.c|99| <<afu_dma_unpin_pages>> account_locked_vm(current->mm, npages, false);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|292| <<tce_iommu_enable>> ret = account_locked_vm(container->mm, locked, true);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|311| <<tce_iommu_disable>> account_locked_vm(container->mm, container->locked_pages, false);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|621| <<tce_iommu_create_table>> ret = account_locked_vm(container->mm, table_size >> PAGE_SHIFT, true);
+ *   - drivers/vfio/vfio_iommu_spapr_tce.c|640| <<tce_iommu_free_table>> account_locked_vm(container->mm, pages, false);
+ */
 int account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc)
 {
 	int ret;
-- 
2.39.5 (Apple Git-154)

