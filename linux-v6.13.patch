From 9a6a181294184a5359851a04443665f5b9fa0958 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Mon, 24 Mar 2025 08:09:08 -0700
Subject: [PATCH 1/1] linux-v6.13

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/entry/calling.h                      |   40 +
 arch/x86/events/core.c                        |    9 +
 arch/x86/events/intel/core.c                  |    6 +
 arch/x86/include/asm/debugreg.h               |   16 +
 arch/x86/include/asm/disabled-features.h      |   13 +
 arch/x86/include/asm/kvm_host.h               |  235 +++
 arch/x86/include/asm/msr-index.h              |   48 +
 arch/x86/include/asm/nospec-branch.h          |   73 +
 arch/x86/include/asm/spec-ctrl.h              |   56 +
 arch/x86/include/asm/svm.h                    |    8 +
 arch/x86/kernel/alternative.c                 |   13 +
 arch/x86/kernel/cpu/bugs.c                    |  767 +++++++
 arch/x86/kernel/cpu/common.c                  |   28 +
 arch/x86/kernel/cpu/cpu.h                     |    7 +
 arch/x86/kernel/irq.c                         |    5 +
 arch/x86/kernel/kvmclock.c                    |   32 +
 arch/x86/kernel/process.c                     |   10 +
 arch/x86/kernel/process.h                     |   10 +
 arch/x86/kernel/smpboot.c                     |   11 +
 arch/x86/kvm/cpuid.c                          |   21 +
 arch/x86/kvm/cpuid.h                          |   10 +
 arch/x86/kvm/hyperv.c                         |   13 +
 arch/x86/kvm/i8254.c                          |    7 +
 arch/x86/kvm/ioapic.c                         |   21 +
 arch/x86/kvm/irq_comm.c                       |   71 +
 arch/x86/kvm/lapic.c                          |  303 +++
 arch/x86/kvm/lapic.h                          |   34 +
 arch/x86/kvm/mmu.h                            |   45 +
 arch/x86/kvm/mmu/mmu.c                        | 1255 ++++++++++++
 arch/x86/kvm/mmu/mmu_internal.h               |   74 +
 arch/x86/kvm/mmu/paging_tmpl.h                |   86 +
 arch/x86/kvm/mmu/spte.c                       |   32 +
 arch/x86/kvm/mmu/spte.h                       |   30 +
 arch/x86/kvm/mmu/tdp_mmu.c                    |  228 +++
 arch/x86/kvm/pmu.c                            |  245 +++
 arch/x86/kvm/pmu.h                            |   43 +
 arch/x86/kvm/svm/avic.c                       |  442 ++++
 arch/x86/kvm/svm/pmu.c                        |   15 +
 arch/x86/kvm/svm/svm.c                        |  316 +++
 arch/x86/kvm/svm/svm.h                        |   20 +
 arch/x86/kvm/vmx/nested.c                     |   13 +
 arch/x86/kvm/vmx/pmu_intel.c                  |   51 +
 arch/x86/kvm/vmx/vmx.c                        |  211 ++
 arch/x86/kvm/vmx/vmx.h                        |   18 +
 arch/x86/kvm/x86.c                            |  544 +++++
 arch/x86/kvm/x86.h                            |    9 +
 arch/x86/mm/tlb.c                             |   78 +
 drivers/block/virtio_blk.c                    |   19 +
 drivers/idle/intel_idle.c                     |   11 +
 drivers/iommu/amd/init.c                      |    4 +
 drivers/iommu/amd/iommu.c                     |   41 +
 .../net/ethernet/mellanox/mlx5/core/en/txrx.h |   14 +
 .../net/ethernet/mellanox/mlx5/core/en_tx.c   |   13 +
 .../net/ethernet/mellanox/mlx5/core/lib/eq.h  |    7 +
 drivers/net/virtio_net.c                      |   72 +
 drivers/net/xen-netfront.c                    |    4 +
 drivers/scsi/virtio_scsi.c                    |  228 +++
 drivers/target/loopback/tcm_loop.c            |   17 +
 drivers/target/target_core_configfs.c         |   32 +
 drivers/target/target_core_file.c             |   34 +
 drivers/target/target_core_hba.c              |    8 +
 drivers/target/target_core_iblock.c           |   14 +
 drivers/target/target_core_pr.c               |    8 +
 drivers/target/target_core_sbc.c              |   39 +
 drivers/target/target_core_spc.c              |    5 +
 drivers/target/target_core_transport.c        |   61 +
 drivers/vdpa/ifcvf/ifcvf_main.c               |   16 +
 drivers/vdpa/mlx5/net/mlx5_vnet.c             |   27 +
 drivers/vdpa/vdpa.c                           |   60 +
 drivers/vdpa/vdpa_sim/vdpa_sim.c              |   26 +
 drivers/vdpa/vdpa_sim/vdpa_sim_blk.c          |   11 +
 drivers/vdpa/vdpa_sim/vdpa_sim_net.c          |   20 +
 drivers/vdpa/vdpa_user/vduse_dev.c            |   16 +
 drivers/vdpa/virtio_pci/vp_vdpa.c             |   16 +
 drivers/vfio/pci/vfio_pci_intrs.c             |   23 +
 drivers/vhost/net.c                           |  254 +++
 drivers/vhost/scsi.c                          | 1815 +++++++++++++++++
 drivers/vhost/test.c                          |   55 +
 drivers/vhost/vdpa.c                          |   42 +
 drivers/vhost/vhost.c                         | 1260 ++++++++++++
 drivers/vhost/vhost.h                         |  177 ++
 drivers/vhost/vringh.c                        |    8 +
 drivers/vhost/vsock.c                         |   96 +
 drivers/virtio/virtio.c                       |   47 +
 drivers/virtio/virtio_ring.c                  |   31 +
 drivers/virtio/virtio_vdpa.c                  |   20 +
 include/linux/kvm_host.h                      |   32 +
 include/linux/mempool.h                       |   83 +
 include/linux/mmu_notifier.h                  |   37 +
 include/linux/scatterlist.h                   |   58 +
 include/linux/sched.h                         |   72 +
 include/linux/sched/smt.h                     |   31 +
 include/linux/sched/sysctl.h                  |   21 +
 include/linux/skbuff.h                        |   10 +
 include/linux/vdpa.h                          |   27 +
 include/linux/virtio.h                        |    6 +
 include/target/target_core_base.h             |   15 +
 include/uapi/linux/vhost_types.h              |   11 +
 include/uapi/linux/virtio_ring.h              |   17 +
 kernel/sched/core.c                           |   64 +
 kernel/sched/fair.c                           |  253 +++
 kernel/vhost_task.c                           |    8 +
 lib/iov_iter.c                                |   57 +
 lib/scatterlist.c                             |   20 +
 lib/sg_pool.c                                 |  126 ++
 mm/huge_memory.c                              |    4 +
 mm/mempolicy.c                                |   27 +
 mm/mmu_notifier.c                             |   38 +
 mm/mprotect.c                                 |   40 +
 net/core/skbuff.c                             |   10 +
 net/netfilter/nf_tables_core.c                |   13 +
 virt/kvm/dirty_ring.c                         |   12 +
 virt/kvm/eventfd.c                            |  101 +
 virt/kvm/guest_memfd.c                        |   28 +
 virt/kvm/kvm_main.c                           |  117 ++
 virt/kvm/kvm_mm.h                             |   12 +
 virt/kvm/pfncache.c                           |    5 +
 virt/lib/irqbypass.c                          |   23 +
 118 files changed, 11651 insertions(+)

diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h
index ea8177062..a5b3b399f 100644
--- a/arch/x86/entry/calling.h
+++ b/arch/x86/entry/calling.h
@@ -289,6 +289,17 @@ For 32-bit we have the following conventions - kernel is built with
 
 #endif
 
+/*
+ * 在以下使用X86_FEATURE_KERNEL_IBRS:
+ *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+ *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+ *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+ *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+ *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+ *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+ */
 /*
  * IBRS kernel mitigation for Spectre_v2.
  *
@@ -326,6 +337,35 @@ For 32-bit we have the following conventions - kernel is built with
 #endif
 .endm
 
+/*
+ * 在以下使用X86_FEATURE_KERNEL_IBRS:
+ *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+ *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+ *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+ *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+ *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+ *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+ *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+ *
+ * 103 #define MSR_IA32_SPEC_CTRL              0x00000048 // Speculation Control
+ * 104 #define SPEC_CTRL_IBRS                  BIT(0)     // Indirect Branch Restricted Speculation
+ * 105 #define SPEC_CTRL_STIBP_SHIFT           1          // Single Thread Indirect Branch Predictor (STIBP) bit
+ * 106 #define SPEC_CTRL_STIBP                 BIT(SPEC_CTRL_STIBP_SHIFT)      // STIBP mask
+ * 107 #define SPEC_CTRL_SSBD_SHIFT            2          // Speculative Store Bypass Disable bit
+ * 108 #define SPEC_CTRL_SSBD                  BIT(SPEC_CTRL_SSBD_SHIFT)       // Speculative Store Bypass Disable
+ * 109 #define SPEC_CTRL_RRSBA_DIS_S_SHIFT     6          // Disable RRSBA behavior
+ * 110 #define SPEC_CTRL_RRSBA_DIS_S           BIT(SPEC_CTRL_RRSBA_DIS_S_SHIFT)
+ * 111 #define SPEC_CTRL_BHI_DIS_S_SHIFT       10         // Disable Branch History Injection behavior
+ * 112 #define SPEC_CTRL_BHI_DIS_S             BIT(SPEC_CTRL_BHI_DIS_S_SHIFT)
+ * 113        
+ * 114 // A mask for bits which the kernel toggles when controlling mitigations
+ * 115 #define SPEC_CTRL_MITIGATIONS_MASK      (SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD \
+ * 116                                                         | SPEC_CTRL_RRSBA_DIS_S \
+ * 117                                                         | SPEC_CTRL_BHI_DIS_S)
+ *
+ * 这里exit的时候只clear SPEC_CTRL_IBRS.
+ */
 /*
  * Similar to IBRS_ENTER, requires KERNEL GS,CR3 and clobbers (AX, CX, DX)
  * regs. Must be called after the last RET.
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index c75c482d4..5854a4e58 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -3056,6 +3056,15 @@ unsigned long perf_arch_misc_flags(struct pt_regs *regs)
 	return flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.h|198| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_init_pmu_capability()
+ *       -> perf_get_x86_pmu_capability()
+ */
 void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
 {
 	/* This API doesn't currently support enumerating hybrid PMUs. */
diff --git a/arch/x86/events/intel/core.c b/arch/x86/events/intel/core.c
index 99c590da0..68ba2d046 100644
--- a/arch/x86/events/intel/core.c
+++ b/arch/x86/events/intel/core.c
@@ -4238,6 +4238,12 @@ static struct perf_guest_switch_msr *intel_guest_get_msrs(int *nr, void *data)
 	};
 
 	if (x86_pmu.intel_cap.pebs_baseline) {
+		/*
+		 * 在以下使用kvm_pmu->pebs_data_cfg:
+		 *   - arch/x86/events/intel/core.c|4244| <<intel_guest_get_msrs>> .guest = kvm_pmu->pebs_data_cfg,
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|315| <<intel_pmu_get_msr(MSR_PEBS_DATA_CFG)>> msr_info->data = pmu->pebs_data_cfg;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|377| <<intel_pmu_set_msr(MSR_PEBS_DATA_CFG)>> pmu->pebs_data_cfg = data;
+		 */
 		arr[(*nr)++] = (struct perf_guest_switch_msr){
 			.msr = MSR_PEBS_DATA_CFG,
 			.host = cpuc->active_pebs_data_cfg,
diff --git a/arch/x86/include/asm/debugreg.h b/arch/x86/include/asm/debugreg.h
index fdbbbfec7..e9f4e0d72 100644
--- a/arch/x86/include/asm/debugreg.h
+++ b/arch/x86/include/asm/debugreg.h
@@ -161,6 +161,14 @@ static inline unsigned long amd_get_dr_addr_mask(unsigned int dr)
 }
 #endif
 
+/*
+ * 在以下调用get_debugctlmsr():
+ *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+ *   - arch/x86/events/intel/ds.c|832| <<intel_pmu_enable_bts>> debugctlmsr = get_debugctlmsr();
+ *   - arch/x86/events/intel/ds.c|856| <<intel_pmu_disable_bts>> debugctlmsr = get_debugctlmsr();
+ *   - arch/x86/kernel/step.c|188| <<set_task_blockstep>> debugctl = get_debugctlmsr();
+ *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+ */
 static inline unsigned long get_debugctlmsr(void)
 {
 	unsigned long debugctlmsr = 0;
@@ -174,6 +182,14 @@ static inline unsigned long get_debugctlmsr(void)
 	return debugctlmsr;
 }
 
+/*
+ * 在以下调用update_debugctlmsr():
+ *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+ *   - arch/x86/events/intel/ds.c|845| <<intel_pmu_enable_bts>> update_debugctlmsr(debugctlmsr);
+ *   - arch/x86/events/intel/ds.c|862| <<intel_pmu_disable_bts>> update_debugctlmsr(debugctlmsr);
+ *   - arch/x86/kernel/step.c|197| <<set_task_blockstep>> update_debugctlmsr(debugctl);
+ *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+ */
 static inline void update_debugctlmsr(unsigned long debugctlmsr)
 {
 #ifndef CONFIG_X86_DEBUGCTLMSR
diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h
index c492bdc97..b280367d2 100644
--- a/arch/x86/include/asm/disabled-features.h
+++ b/arch/x86/include/asm/disabled-features.h
@@ -50,6 +50,19 @@
 # define DISABLE_PTI		(1 << (X86_FEATURE_PTI & 31))
 #endif
 
+/*
+ * 在以下使用X86_FEATURE_RETPOLINE:
+ *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+ *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ *   - arch/x86/include/asm/nospec-branch.h|450| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+ *   - arch/x86/include/asm/nospec-branch.h|480| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+ *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1993| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|3327| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+ *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ *   - net/netfilter/nf_tables_core.c|35| <<nf_skip_indirect_calls_enable>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ */
 #ifdef CONFIG_MITIGATION_RETPOLINE
 # define DISABLE_RETPOLINE	0
 #else
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e159e44a6..2a23197e1 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -89,8 +89,38 @@
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *
+ * 处理的函数: process_nmi()
+ */
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *
+ * 处理的函数: kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理的函数: kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
@@ -280,6 +310,19 @@ enum x86_intercept_stage;
  * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
  * when the guest was accessing private memory.
  */
+/*
+ * 在以下使用PFERR_PRIVATE_ACCESS:
+ *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+ *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+ *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+ *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+ *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+ *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+ *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+ *
+ * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+ * when the guest was accessing private memory.
+ */
 #define PFERR_PRIVATE_ACCESS   BIT_ULL(49)
 #define PFERR_SYNTHETIC_MASK   (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
 
@@ -519,6 +562,18 @@ struct kvm_pmc {
 	 * PMC events triggered by KVM emulation that haven't been fully
 	 * processed, i.e. haven't undergone overflow detection.
 	 */
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	u64 emulated_counter;
 	u64 eventsel;
 	struct perf_event *perf_event;
@@ -573,8 +628,25 @@ struct kvm_pmu {
 	DECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);
 
 	u64 ds_area;
+	/*
+	 * 在以下使用kvm_pmu->pebs_enable:
+	 *   - arch/x86/kvm/pmu.c|289| <<pmc_reprogram_counter>> bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
+	 *   - arch/x86/kvm/pmu.c|376| <<pmc_resume_counter>> if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
+	 *          (!!pmc->perf_event->attr.precise_ip))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|309| <<intel_pmu_get_msr(MSR_IA32_PEBS_ENABLE)>> msr_info->data = pmu->pebs_enable;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|361| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> if (pmu->pebs_enable != data) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|362| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> diff = pmu->pebs_enable ^ data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> pmu->pebs_enable = data;
+	 *   - arch/x86/kvm/vmx/vmx.c|7240| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+	 */
 	u64 pebs_enable;
 	u64 pebs_enable_rsvd;
+	/*
+	 * 在以下使用kvm_pmu->pebs_data_cfg:
+	 *   - arch/x86/events/intel/core.c|4244| <<intel_guest_get_msrs>> .guest = kvm_pmu->pebs_data_cfg,
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|315| <<intel_pmu_get_msr(MSR_PEBS_DATA_CFG)>> msr_info->data = pmu->pebs_data_cfg;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|377| <<intel_pmu_set_msr(MSR_PEBS_DATA_CFG)>> pmu->pebs_data_cfg = data;
+	 */
 	u64 pebs_data_cfg;
 	u64 pebs_data_cfg_rsvd;
 
@@ -996,6 +1068,17 @@ struct kvm_vcpu_arch {
 
 	/* pv related host specific info */
 	struct {
+		/*
+		 * 在以下使用pv_unhalted:
+		 *   - arch/x86/kvm/lapic.c|1356| <<__apic_accept_irq>> vcpu->arch.pv.pv_unhalted = 1;
+		 *   - arch/x86/kvm/svm/sev.c|3833| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/svm/sev.c|3872| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11352| <<kvm_vcpu_has_events>> if (vcpu->arch.pv.pv_unhalted)
+		 *   - arch/x86/kvm/x86.c|11453| <<vcpu_block>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11532| <<__kvm_emulate_halt>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11599| <<kvm_arch_dy_runnable>> if (READ_ONCE(vcpu->arch.pv.pv_unhalted))
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu->arch.pv.pv_unhalted)
+		 */
 		bool pv_unhalted;
 	} pv;
 
@@ -1194,6 +1277,70 @@ struct kvm_x86_pmu_event_filter {
 	__u64 events[];
 };
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * 在以下设置APICV_INHIBIT_REASON_DISABLED:
+ *   - arch/x86/kvm/x86.c|10028| <<kvm_apicv_init>> enum kvm_apicv_inhibit reason = enable_apicv ?
+ *              APICV_INHIBIT_REASON_ABSENT : APICV_INHIBIT_REASON_DISABLED;
+ *
+ * 在以下设置APICV_INHIBIT_REASON_HYPERV:
+ *   - arch/x86/kvm/hyperv.c|162| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+ *              APICV_INHIBIT_REASON_HYPERV, !!hv->synic_auto_eoi_used); 
+ *
+ * 在以下设置APICV_INHIBIT_REASON_ABSENT:
+ *   - arch/x86/kvm/x86.c|6546| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|7094| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|10027| <<kvm_apicv_init>> enum kvm_apicv_inhibit reason = enable_apicv ?
+ *              APICV_INHIBIT_REASON_ABSENT : APICV_INHIBIT_REASON_DISABLED;
+ *
+ * 在以下设置APICV_INHIBIT_REASON_BLOCKIRQ:
+ *   - arch/x86/kvm/x86.c|12232| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+ *              APICV_INHIBIT_REASON_BLOCKIRQ, set);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED:
+ *   - arch/x86/kvm/lapic.c|466| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_APIC_ID_MODIFIED:
+ *   - arch/x86/kvm/lapic.c|476| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/lapic.c|478| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_APIC_BASE_MODIFIED:
+ *   - arch/x86/kvm/lapic.c|2628| <<__kvm_apic_set_base>> APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_NESTED:
+ *   - arch/x86/kvm/svm/avic.c|545| <<avic_vcpu_get_apicv_inhibit_reasons>> return APICV_INHIBIT_REASON_NESTED;
+ *
+ * 在以下设置APICV_INHIBIT_REASON_IRQWIN:
+ *   - arch/x86/kvm/svm/svm.c|3262| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3907| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_PIT_REINJ:
+ *   - arch/x86/kvm/i8254.c|315| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/i8254.c|321| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_SEV:
+ *   - arch/x86/kvm/svm/sev.c|463| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED:
+ *   - arch/x86/kvm/lapic.c|471| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|473| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ */
+
 enum kvm_apicv_inhibit {
 
 	/********************************************************************/
@@ -1318,6 +1465,15 @@ struct kvm_arch {
 	 * guest attempts to execute from the region then KVM obviously can't
 	 * create an NX huge page (without hanging the guest).
 	 */
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+	 *              &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+	 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	struct list_head possible_nx_huge_pages;
 #ifdef CONFIG_KVM_EXTERNAL_WRITE_TRACKING
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1349,8 +1505,32 @@ struct kvm_arch {
 	bool apic_access_memslot_enabled;
 	bool apic_access_memslot_inhibited;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	/* Protects apicv_inhibit_reasons */
 	struct rw_semaphore apicv_update_lock;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
@@ -1422,6 +1602,31 @@ struct kvm_arch {
 	bool triple_fault_event;
 
 	bool bus_lock_detection_enabled;
+	/*
+	 * 在以下使用global的enable_pmu:
+	 *   - arch/x86/kvm/cpuid.c|1063| <<__do_cpuid_func>> if (!enable_pmu || !static_cpu_has(X86_FEATURE_ARCH_PERFMON)) {
+	 *   - arch/x86/kvm/cpuid.c|1399| <<__do_cpuid_func>> if (!enable_pmu || !kvm_cpu_cap_has(X86_FEATURE_PERFMON_V2)) {
+	 *   - arch/x86/kvm/pmu.h|216| <<kvm_init_pmu_capability>> enable_pmu = false;
+	 *   - arch/x86/kvm/pmu.h|218| <<kvm_init_pmu_capability>> if (enable_pmu) {
+	 *   - arch/x86/kvm/pmu.h|229| <<kvm_init_pmu_capability>> enable_pmu = false;
+	 *   - arch/x86/kvm/pmu.h|231| <<kvm_init_pmu_capability>> enable_pmu = false;
+	 *   - arch/x86/kvm/pmu.h|234| <<kvm_init_pmu_capability>> if (!enable_pmu) {
+	 *   - arch/x86/kvm/svm/svm.c|5272| <<svm_set_cpu_caps>> if (enable_pmu) {
+	 *   - arch/x86/kvm/svm/svm.c|5450| <<svm_hardware_setup>> if (!enable_pmu)
+	 *   - arch/x86/kvm/vmx/vmx.c|7968| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+	 *   - arch/x86/kvm/vmx/vmx.c|8037| <<vmx_set_cpu_caps>> if (!enable_pmu)
+	 *   - arch/x86/kvm/vmx/vmx.c|8627| <<vmx_hardware_setup>> if (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())
+	 *   - arch/x86/kvm/x86.c|4803| <<kvm_vm_ioctl_check_extension>> r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
+	 *   - arch/x86/kvm/x86.c|6726| <<kvm_vm_ioctl_enable_cap>> if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
+	 *   - arch/x86/kvm/x86.c|7542| <<kvm_init_msr_lists>> if (enable_pmu) {
+	 *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+	 *
+	 * 在以下使用kvm_arch->enable_pmu:
+	 *   - arch/x86/kvm/pmu.c|938| <<kvm_pmu_refresh>> if (!vcpu->kvm->arch.enable_pmu)
+	 *   - arch/x86/kvm/svm/pmu.c|44| <<get_gp_pmc_amd>> if (!vcpu->kvm->arch.enable_pmu)
+	 *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+	 *   - arch/x86/kvm/x86.c|6731| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+	 */
 	bool enable_pmu;
 
 	u32 notify_window;
@@ -1443,6 +1648,16 @@ struct kvm_arch {
 	bool sgx_provisioning_allowed;
 
 	struct kvm_x86_pmu_event_filter __rcu *pmu_event_filter;
+	/*
+	 * 在以下使用kvm_arch->nx_huge_page_recovery_thread:
+	 *   - arch/x86/kvm/mmu/mmu.c|7778| <<set_nx_huge_pages>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7907| <<set_nx_huge_pages_recovery_param>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8071| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread = vhost_task_create(
+	 *   - arch/x86/kvm/mmu/mmu.c|8075| <<kvm_mmu_post_init_vm>> if (!kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8078| <<kvm_mmu_post_init_vm>> vhost_task_start(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8084| <<kvm_mmu_pre_destroy_vm>> if (kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8085| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 */
 	struct vhost_task *nx_huge_page_recovery_thread;
 	u64 nx_huge_page_last;
 
@@ -2169,15 +2384,35 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/lapic.c|466| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|471| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|476| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/lapic.c|2623| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+ *   - arch/x86/kvm/svm/sev.c|463| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+ *   - arch/x86/kvm/svm/svm.c|3883| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ */
 static inline void kvm_set_apicv_inhibit(struct kvm *kvm,
 					 enum kvm_apicv_inhibit reason)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/include/asm/kvm_host.h|2175| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+	 *   - arch/x86/include/asm/kvm_host.h|2181| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+	 */
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
 }
 
 static inline void kvm_clear_apicv_inhibit(struct kvm *kvm,
 					   enum kvm_apicv_inhibit reason)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/include/asm/kvm_host.h|2175| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+	 *   - arch/x86/include/asm/kvm_host.h|2181| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+	 */
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
 }
 
diff --git a/arch/x86/include/asm/msr-index.h b/arch/x86/include/asm/msr-index.h
index 3ae84c3b8..cb959e5e5 100644
--- a/arch/x86/include/asm/msr-index.h
+++ b/arch/x86/include/asm/msr-index.h
@@ -67,6 +67,39 @@
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT	29
 #define MSR_TEST_CTRL_SPLIT_LOCK_DETECT		BIT(MSR_TEST_CTRL_SPLIT_LOCK_DETECT_BIT)
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|123| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|381| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|563| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|74| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|93| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|139| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1407| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|3021| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3167| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3193| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4444| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2059| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2321| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2346| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7319| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7330| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|14128| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|14130| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|14133| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ */
 #define MSR_IA32_SPEC_CTRL		0x00000048 /* Speculation Control */
 #define SPEC_CTRL_IBRS			BIT(0)	   /* Indirect Branch Restricted Speculation */
 #define SPEC_CTRL_STIBP_SHIFT		1	   /* Single Thread Indirect Branch Predictor (STIBP) bit */
@@ -83,6 +116,21 @@
 							| SPEC_CTRL_RRSBA_DIS_S \
 							| SPEC_CTRL_BHI_DIS_S)
 
+/*
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|119| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|124| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - tools/arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/include/asm/nospec-branch.h|536| <<indirect_branch_prediction_barrier>> alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|557| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|608| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4668| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7957| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3914| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3941| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
 #define MSR_IA32_PRED_CMD		0x00000049 /* Prediction Command */
 #define PRED_CMD_IBPB			BIT(0)	   /* Indirect Branch Prediction Barrier */
 #define PRED_CMD_SBPB			BIT(7)	   /* Selective Branch Prediction Barrier */
diff --git a/arch/x86/include/asm/nospec-branch.h b/arch/x86/include/asm/nospec-branch.h
index 96b410b1d..1a282e4e7 100644
--- a/arch/x86/include/asm/nospec-branch.h
+++ b/arch/x86/include/asm/nospec-branch.h
@@ -438,6 +438,19 @@ static inline void call_depth_return_thunk(void) {}
 
 #ifdef CONFIG_X86_64
 
+/*
+ * 在以下使用X86_FEATURE_RETPOLINE:
+ *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+ *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+ *   - arch/x86/include/asm/nospec-branch.h|450| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+ *   - arch/x86/include/asm/nospec-branch.h|480| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+ *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1993| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/kernel/cpu/bugs.c|3327| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+ *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+ *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ *   - net/netfilter/nf_tables_core.c|35| <<nf_skip_indirect_calls_enable>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+ */
 /*
  * Inline asm uses the %V modifier which is only in newer GCC
  * which is ensured when CONFIG_MITIGATION_RETPOLINE is defined.
@@ -531,8 +544,61 @@ void alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)
 
 extern u64 x86_pred_cmd;
 
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ */
 static inline void indirect_branch_prediction_barrier(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_USE_IBPB:
+	 *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+	 *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+	 */
 	alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
 }
 
@@ -548,6 +614,13 @@ extern u64 spec_ctrl_current(void);
  *
  * (Implemented as CPP macros due to header hell.)
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/apm_32.c|605| <<__apm_bios_call>> firmware_restrict_branch_speculation_start();
+ *   - arch/x86/kernel/apm_32.c|686| <<__apm_bios_call_simple>> firmware_restrict_branch_speculation_start();
+ *   - arch/x86/platform/efi/efi_32.c|147| <<arch_efi_call_virt_setup>> firmware_restrict_branch_speculation_start();
+ *   - arch/x86/platform/efi/efi_64.c|452| <<arch_efi_call_virt_setup>> firmware_restrict_branch_speculation_start();
+ */
 #define firmware_restrict_branch_speculation_start()			\
 do {									\
 	preempt_disable();						\
diff --git a/arch/x86/include/asm/spec-ctrl.h b/arch/x86/include/asm/spec-ctrl.h
index 658b690b2..8fbd88c4e 100644
--- a/arch/x86/include/asm/spec-ctrl.h
+++ b/arch/x86/include/asm/spec-ctrl.h
@@ -27,6 +27,21 @@ extern void x86_virt_spec_ctrl(u64 guest_virt_spec_ctrl, bool guest);
 static inline
 void x86_spec_ctrl_set_guest(u64 guest_virt_spec_ctrl)
 {
+	/*
+	 * 在以下使用x86_virt_spec_ctrl(): -->似乎只SVM使用:
+	 *   - arch/x86/include/asm/spec-ctrl.h|30| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, true);
+	 *   - arch/x86/include/asm/spec-ctrl.h|44| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, false);
+	 *
+	 * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+	 * the guest has, while on VMEXIT we restore the host view. This
+	 * would be easier if SPEC_CTRL were architecturally maskable or
+	 * shadowable for guests but this is not (currently) the case.
+	 * Takes the guest view of SPEC_CTRL MSR as a parameter and also
+	 * the guest's version of VIRT_SPEC_CTRL, if emulated.
+	 *
+	 * NOTE: This function is *only* called for SVM, since Intel uses
+	 * MSR_IA32_SPEC_CTRL for SSBD.
+	 */
 	x86_virt_spec_ctrl(guest_virt_spec_ctrl, true);
 }
 
@@ -38,9 +53,28 @@ void x86_spec_ctrl_set_guest(u64 guest_virt_spec_ctrl)
  *
  * Avoids writing to the MSR if the content/bits are the same
  */
+/*
+ * 在以下调用x86_spec_ctrl_restore_host():
+ *   - arch/x86/kvm/svm/svm.c|4548| <<svm_vcpu_run>> x86_spec_ctrl_restore_host(svm->virt_spec_ctrl);
+ */
 static inline
 void x86_spec_ctrl_restore_host(u64 guest_virt_spec_ctrl)
 {
+	/*
+	 * 在以下使用x86_virt_spec_ctrl(): -->似乎只SVM使用:
+	 *   - arch/x86/include/asm/spec-ctrl.h|30| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, true);
+	 *   - arch/x86/include/asm/spec-ctrl.h|44| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, false);
+	 *
+	 * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+	 * the guest has, while on VMEXIT we restore the host view. This
+	 * would be easier if SPEC_CTRL were architecturally maskable or
+	 * shadowable for guests but this is not (currently) the case.
+	 * Takes the guest view of SPEC_CTRL MSR as a parameter and also
+	 * the guest's version of VIRT_SPEC_CTRL, if emulated.
+	 *
+	 * NOTE: This function is *only* called for SVM, since Intel uses
+	 * MSR_IA32_SPEC_CTRL for SSBD.
+	 */
 	x86_virt_spec_ctrl(guest_virt_spec_ctrl, false);
 }
 
@@ -66,6 +100,9 @@ static inline unsigned long ssbd_spec_ctrl_to_tif(u64 spec_ctrl)
 	return (spec_ctrl & SPEC_CTRL_SSBD) << (TIF_SSBD - SPEC_CTRL_SSBD_SHIFT);
 }
 
+/*
+ * 没人调用
+ */
 static inline unsigned long stibp_spec_ctrl_to_tif(u64 spec_ctrl)
 {
 	BUILD_BUG_ON(TIF_SPEC_IB < SPEC_CTRL_STIBP_SHIFT);
@@ -81,8 +118,27 @@ static inline u64 ssbd_tif_to_amd_ls_cfg(u64 tifn)
  * This can be used in noinstr functions & should only be called in bare
  * metal context.
  */
+/*
+ * 在以下使用__update_spec_ctrl():
+ *   - arch/x86/kernel/smpboot.c|1405| <<native_play_dead>> __update_spec_ctrl(0);
+ *   - drivers/idle/intel_idle.c|186| <<intel_idle_ibrs>> __update_spec_ctrl(0);
+ *   - drivers/idle/intel_idle.c|191| <<intel_idle_ibrs>> __update_spec_ctrl(spec_ctrl);
+ */
 static __always_inline void __update_spec_ctrl(u64 val)
 {
+	/*
+	 * 在以下使用percpu的x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|341| <<global>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|550| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|125| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|126| <<global>> EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|92| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|150| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|153| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|165| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	__this_cpu_write(x86_spec_ctrl_current, val);
 	native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
 }
diff --git a/arch/x86/include/asm/svm.h b/arch/x86/include/asm/svm.h
index 2b59b9951..7f120192c 100644
--- a/arch/x86/include/asm/svm.h
+++ b/arch/x86/include/asm/svm.h
@@ -247,6 +247,14 @@ struct __attribute__ ((__packed__)) vmcb_control_area {
 
 #define AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK	GENMASK_ULL(11, 0)
 #define AVIC_PHYSICAL_ID_ENTRY_BACKING_PAGE_MASK	(0xFFFFFFFFFFULL << 12)
+/*
+ * 在以下使用AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK:
+ *   - arch/x86/kvm/svm/avic.c|839| <<svm_ir_list_add>> if (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)
+ *   - arch/x86/kvm/svm/avic.c|1060| <<avic_vcpu_load>> WARN_ON_ONCE(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
+ *   - arch/x86/kvm/svm/avic.c|1064| <<avic_vcpu_load>> entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ *   - arch/x86/kvm/svm/avic.c|1090| <<avic_vcpu_put>> if (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))
+ *   - arch/x86/kvm/svm/avic.c|1105| <<avic_vcpu_put>> entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+ */
 #define AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK		(1ULL << 62)
 #define AVIC_PHYSICAL_ID_ENTRY_VALID_MASK		(1ULL << 63)
 #define AVIC_PHYSICAL_ID_TABLE_SIZE_MASK		(0xFFULL)
diff --git a/arch/x86/kernel/alternative.c b/arch/x86/kernel/alternative.c
index 243843e44..bdefdd227 100644
--- a/arch/x86/kernel/alternative.c
+++ b/arch/x86/kernel/alternative.c
@@ -665,6 +665,19 @@ static int patch_retpoline(void *addr, struct insn *insn, u8 *bytes)
 	/* If anyone ever does: CALL/JMP *%rsp, we're in deep trouble. */
 	BUG_ON(reg == 4);
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|450| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+	 *   - arch/x86/include/asm/nospec-branch.h|480| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1993| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3327| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *   - net/netfilter/nf_tables_core.c|35| <<nf_skip_indirect_calls_enable>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
 	    !cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {
 		if (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))
diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index 47a01d402..694cb3270 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -34,6 +34,177 @@
 
 #include "cpu.h"
 
+/*
+ * IBRS:
+ * commit 2dbb887e875b1de3ca8f40ddf26bcfe55798c609
+ * Author: Peter Zijlstra <peterz@infradead.org>
+ * Date:   Tue Jun 14 23:15:53 2022 +0200
+ *
+ * x86/entry: Add kernel IBRS implementation
+ *
+ *
+ * STIBP:
+ * commit 53c613fe6349994f023245519265999eed75957f
+ * Author: Jiri Kosina <jikos@kernel.org>
+ * Date:   Tue Sep 25 14:38:55 2018 +0200
+ *
+ * x86/speculation: Enable cross-hyperthread spectre v2 STIBP mitigation
+ * 好像一直开着, 不是好像, 应该是就是 :)
+ *
+ *
+ * IBPB:
+ * commit 20ffa1caecca4db8f79fe665acdeaa5af815a24d
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 25 16:14:15 2018 +0000
+ *
+ * x86/speculation: Add basic IBPB (Indirect Branch Prediction Barrier) support
+ * 请看下面关于IBPB
+ *
+ *
+ * RETPOLINE.
+ * commit 76b043848fd22dbf7f8bf3a1452f8c70d557b860
+ * Author: David Woodhouse <dwmw@amazon.co.uk>
+ * Date:   Thu Jan 11 21:46:25 2018 +0000
+ *
+ * x86/retpoline: Add initial retpoline support
+ * 主要依靠编译器的选项.
+ * 还有依靠汇编的CALL_NOSPEC, JMP_NOSPEC, RETPOLINE_CALL, RETPOLINE_JMP.
+ */
+
+/*
+ * hv# cpuid -l 0x7 -1 | grep IBP
+ *       IBRS/IBPB: indirect branch restrictions  = true
+ *       STIBP: 1 thr indirect branch predictor   = true
+ *
+ * hv# cpuid -l 0x80000008 -1
+ * CPU:
+ *    Physical Address and Linear Address Size (0x80000008/eax):
+ *       maximum physical address bits         = 0x2e (46)
+ *       maximum linear (virtual) address bits = 0x30 (48)
+ *       maximum guest physical address bits   = 0x0 (0)
+ *    Extended Feature Extensions ID (0x80000008/ebx):
+ *       CLZERO instruction                       = false
+ *       instructions retired count support       = false
+ *       always save/restore error pointers       = false
+ *       INVLPGB instruction                      = false
+ *       RDPRU instruction                        = false
+ *       memory bandwidth enforcement             = false
+ *       MCOMMIT instruction                      = false
+ *       WBNOINVD instruction                     = false
+ *       IBPB: indirect branch prediction barrier = false
+ *       interruptible WBINVD, WBNOINVD           = false
+ *       IBRS: indirect branch restr speculation  = false
+ *       STIBP: 1 thr indirect branch predictor   = false
+ *       CPU prefers: IBRS always on              = false
+ *       CPU prefers: STIBP always on             = false
+ *       IBRS preferred over software solution    = false
+ *       IBRS provides same mode protection       = false
+ *       EFER[LMSLE] not supported                = false
+ *       INVLPGB supports TLB flush guest nested  = false
+ *       ppin processor id number supported       = false
+ *       SSBD: speculative store bypass disable   = false
+ *       virtualized SSBD                         = false
+ *       SSBD fixed in hardware                   = false
+ *       CPPC: collaborative processor perf ctrl  = false
+ *       PSFD: predictive store forward disable   = false
+ *       not vulnerable to branch type confusion  = false
+ *       branch sampling feature support          = false
+ *    Size Identifiers (0x80000008/ecx):
+ *       ApicIdCoreIdSize                    = 0x0 (0)
+ *       performance time-stamp counter size = 40 bits (0)
+ *    Feature Extended Size (0x80000008/edx):
+ *       max page count for INVLPGB instruction = 0x0 (0)
+ *       RDPRU instruction max input support    = 0x0 (0)
+ */
+
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
+
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|123| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|381| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|563| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|74| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|93| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|139| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1407| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|3021| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3167| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3193| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4444| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2059| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2321| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2346| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7319| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7330| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|14128| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|14130| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|14133| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|119| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|124| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - tools/arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/include/asm/nospec-branch.h|536| <<indirect_branch_prediction_barrier>> alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|557| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|608| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4668| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7957| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3914| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3941| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 static void __init spectre_v1_select_mitigation(void);
 static void __init spectre_v2_select_mitigation(void);
 static void __init retbleed_select_mitigation(void);
@@ -50,26 +221,115 @@ static void __init l1d_flush_select_mitigation(void);
 static void __init srso_select_mitigation(void);
 static void __init gds_select_mitigation(void);
 
+/*
+ * 在以下使用x86_spec_ctrl_base:
+ *   - arch/x86/kernel/cpu/bugs.c|173| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|180| <<cpu_select_mitigations>> x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
+ *   - arch/x86/kernel/cpu/bugs.c|1611| <<spec_ctrl_disable_kernel_rrsba>> x86_spec_ctrl_base |= SPEC_CTRL_RRSBA_DIS_S;
+ *   - arch/x86/kernel/cpu/bugs.c|1612| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|1672| <<spec_ctrl_bhi_dis>> x86_spec_ctrl_base |= SPEC_CTRL_BHI_DIS_S;
+ *   - arch/x86/kernel/cpu/bugs.c|1673| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|1811| <<spectre_v2_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+ *   - arch/x86/kernel/cpu/bugs.c|1812| <<spectre_v2_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|1929| <<update_stibp_msr>> u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+ *   - arch/x86/kernel/cpu/bugs.c|1936| <<update_stibp_strict>> u64 mask = x86_spec_ctrl_base & ~SPEC_CTRL_STIBP;
+ *   - arch/x86/kernel/cpu/bugs.c|1941| <<update_stibp_strict>> if (mask == x86_spec_ctrl_base)
+ *   - arch/x86/kernel/cpu/bugs.c|1946| <<update_stibp_strict>> x86_spec_ctrl_base = mask;
+ *   - arch/x86/kernel/cpu/bugs.c|2162| <<__ssb_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_SSBD;
+ *   - arch/x86/kernel/cpu/bugs.c|2163| <<__ssb_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2417| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/process.c|617| <<__speculation_ctrl_update>> u64 msr = x86_spec_ctrl_base;
+ */
 /* The base value of the SPEC_CTRL MSR without task-specific bits set */
 u64 x86_spec_ctrl_base;
 EXPORT_SYMBOL_GPL(x86_spec_ctrl_base);
 
+/*
+ * 在以下使用percpu的x86_spec_ctrl_current:
+ *   - arch/x86/entry/calling.h|341| <<global>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+ *   - arch/x86/include/asm/nospec-branch.h|550| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/kernel/cpu/bugs.c|125| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+ *   - arch/x86/kernel/cpu/bugs.c|126| <<global>> EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
+ *   - arch/x86/include/asm/spec-ctrl.h|92| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|150| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+ *   - arch/x86/kernel/cpu/bugs.c|153| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+ *   - arch/x86/kernel/cpu/bugs.c|165| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+ *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+ */
 /* The current value of the SPEC_CTRL MSR with task-specific bits set */
 DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
 EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
 
+/*
+ * 在以下使用x86_pred_cmd:
+ *   - arch/x86/include/asm/nospec-branch.h|545| <<indirect_branch_prediction_barrier>> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *              x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/kernel/cpu/bugs.c|2666| <<srso_select_mitigation>> x86_pred_cmd = PRED_CMD_SBPB;
+ */
 u64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;
 EXPORT_SYMBOL_GPL(x86_pred_cmd);
 
+/*
+ * 在以下使用x86_arch_cap_msr:
+ *   - arch/x86/kernel/cpu/bugs.c|216| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr(); 
+ *   - arch/x86/kernel/cpu/bugs.c|415| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+ *   - arch/x86/kernel/cpu/bugs.c|416| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+ *   - arch/x86/kernel/cpu/bugs.c|507| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+ *   - arch/x86/kernel/cpu/bugs.c|517| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+ *   - arch/x86/kernel/cpu/bugs.c|520| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+ *   - arch/x86/kernel/cpu/bugs.c|578| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+ *   - arch/x86/kernel/cpu/bugs.c|738| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) && !boot_cpu_has(X86_FEATURE_RTM) &&
+ *   - arch/x86/kernel/cpu/bugs.c|878| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+ *   - arch/x86/kernel/cpu/bugs.c|1636| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+ *   - arch/x86/kernel/cpu/bugs.c|2016| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+ *
+ * 很多cap, 有security的, 也有其他的.
+ */
 static u64 __ro_after_init x86_arch_cap_msr;
 
 static DEFINE_MUTEX(spec_ctrl_mutex);
 
+/*
+ * 在以下使用x86_return_thunk:
+ *   - arch/x86/kernel/cpu/bugs.c|135| <<global>> void (*x86_return_thunk)(void ) __ro_after_init = __x86_return_thunk;
+ *   - arch/x86/kernel/alternative.c|802| <<patch_return>> __text_gen_insn(bytes, JMP32_INSN_OPCODE, addr, x86_return_thunk, i);
+ *   - arch/x86/kernel/cpu/bugs.c|1174| <<retbleed_select_mitigation>> x86_return_thunk = retbleed_return_thunk;
+ *   - arch/x86/kernel/cpu/bugs.c|1210| <<retbleed_select_mitigation>> x86_return_thunk = call_depth_return_thunk;
+ *   - arch/x86/kernel/cpu/bugs.c|2713| <<srso_select_mitigation>> x86_return_thunk = srso_alias_return_thunk;
+ *   - arch/x86/kernel/cpu/bugs.c|2716| <<srso_select_mitigation>> x86_return_thunk = srso_return_thunk;
+ *   - arch/x86/kernel/ftrace.c|361| <<create_trampoline>> __text_gen_insn(ip, JMP32_INSN_OPCODE, ip, x86_return_thunk, JMP32_INSN_SIZE);
+ *   - arch/x86/kernel/static_call.c|85| <<__static_call_transform>> code = text_gen_insn(JMP32_INSN_OPCODE, insn, x86_return_thunk);
+ *   - arch/x86/kernel/static_call.c|94| <<__static_call_transform>> func = x86_return_thunk;
+ *   - arch/x86/net/bpf_jit_comp.c|679| <<emit_return>> emit_jump(&prog, x86_return_thunk, ip);
+ */
 void (*x86_return_thunk)(void) __ro_after_init = __x86_return_thunk;
 
+/*
+ * 在以下调用update_spec_ctrl():
+ *   - arch/x86/kernel/cpu/bugs.c|1645| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|1706| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|1849| <<spectre_v2_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|1967| <<update_stibp_msr>> update_spec_ctrl(val);
+ *   - arch/x86/kernel/cpu/bugs.c|2200| <<__ssb_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+ *   - arch/x86/kernel/cpu/bugs.c|2468| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+ */
 /* Update SPEC_CTRL MSR and its cached copy unconditionally */
 static void update_spec_ctrl(u64 val)
 {
+	/*
+	 * 在以下使用percpu的x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|341| <<global>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|550| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|125| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|126| <<global>> EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|92| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|150| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|153| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|165| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	this_cpu_write(x86_spec_ctrl_current, val);
 	wrmsrl(MSR_IA32_SPEC_CTRL, val);
 }
@@ -78,13 +338,41 @@ static void update_spec_ctrl(u64 val)
  * Keep track of the SPEC_CTRL MSR value for the current task, which may differ
  * from x86_spec_ctrl_base due to STIBP/SSB in __speculation_ctrl_update().
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/process.c|643| <<__speculation_ctrl_update>> update_spec_ctrl_cond(msr);
+ */
 void update_spec_ctrl_cond(u64 val)
 {
+	/*
+	 * 在以下使用percpu的x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|341| <<global>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|550| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|125| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|126| <<global>> EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|92| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|150| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|153| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|165| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	if (this_cpu_read(x86_spec_ctrl_current) == val)
 		return;
 
 	this_cpu_write(x86_spec_ctrl_current, val);
 
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+	 *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	/*
 	 * When KERNEL_IBRS this MSR is written on return-to-user, unless
 	 * forced the update can be delayed until that time.
@@ -93,8 +381,28 @@ void update_spec_ctrl_cond(u64 val)
 		wrmsrl(MSR_IA32_SPEC_CTRL, val);
 }
 
+/*
+ * called by:
+ *   - arch/x86/include/asm/nospec-branch.h|571| <<firmware_restrict_branch_speculation_start>> spec_ctrl_current() | SPEC_CTRL_IBRS, \
+ *   - arch/x86/include/asm/nospec-branch.h|580| <<firmware_restrict_branch_speculation_end>> spec_ctrl_current(), \
+ *   - arch/x86/kernel/cpu/bugs.c|1966| <<update_stibp_msr>> u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
+ *   - drivers/idle/intel_idle.c|182| <<intel_idle_ibrs>> u64 spec_ctrl = spec_ctrl_current();
+ */
 noinstr u64 spec_ctrl_current(void)
 {
+	/*
+	 * 在以下使用percpu的x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|341| <<global>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|550| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|125| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|126| <<global>> EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|92| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|150| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|153| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|165| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	return this_cpu_read(x86_spec_ctrl_current);
 }
 EXPORT_SYMBOL_GPL(spec_ctrl_current);
@@ -106,10 +414,37 @@ EXPORT_SYMBOL_GPL(spec_ctrl_current);
 u64 __ro_after_init x86_amd_ls_cfg_base;
 u64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;
 
+/*
+ * 在以下使用switch_to_cond_stibp:
+ *   - arch/x86/include/asm/nospec-branch.h|585| <<global>> DECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+ *   - arch/x86/kernel/cpu/bugs.c|177| <<global>> DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+ *   - arch/x86/kernel/cpu/bugs.c|1991| <<update_indir_branch_cond>> static_branch_enable(&switch_to_cond_stibp);
+ *   - arch/x86/kernel/cpu/bugs.c|1993| <<update_indir_branch_cond>> static_branch_disable(&switch_to_cond_stibp);
+ *   - arch/x86/kernel/cpu/bugs.c|2898| <<stibp_state>> if (static_key_enabled(&switch_to_cond_stibp))
+ *   - arch/x86/kernel/process.c|637| <<__speculation_ctrl_update>> static_branch_unlikely(&switch_to_cond_stibp)) {
+ *   - arch/x86/kernel/process.h|26| <<switch_to_extra>> if (!static_branch_likely(&switch_to_cond_stibp)) {
+ */
 /* Control conditional STIBP in switch_to() */
 DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+/*
+ * 在以下使用switch_mm_cond_ibpb:
+ *   - arch/x86/include/asm/nospec-branch.h|586| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+ *   - arch/x86/kernel/cpu/bugs.c|179| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+ *   - arch/x86/kernel/cpu/bugs.c|1450| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_cond_ibpb);
+ *   - arch/x86/kernel/cpu/bugs.c|2909| <<ibpb_state>> if (static_key_enabled(&switch_mm_cond_ibpb))
+ *   - arch/x86/mm/tlb.c|408| <<cond_mitigation>> if (static_branch_likely(&switch_mm_cond_ibpb)) {
+ */
 /* Control conditional IBPB in switch_mm() */
 DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+/*
+ * 在以下使用switch_mm_always_ibpb:
+ *   - arch/x86/include/asm/nospec-branch.h|587| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+ *   - arch/x86/kernel/cpu/bugs.c|181| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+ *   - arch/x86/kernel/cpu/bugs.c|1444| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_always_ibpb);
+ *   - arch/x86/kernel/cpu/bugs.c|1455| <<spectre_v2_user_select_mitigation>> static_key_enabled(&switch_mm_always_ibpb) ?
+ *   - arch/x86/kernel/cpu/bugs.c|2907| <<ibpb_state>> if (static_key_enabled(&switch_mm_always_ibpb))
+ *   - arch/x86/mm/tlb.c|457| <<cond_mitigation>> if (static_branch_unlikely(&switch_mm_always_ibpb)) {
+ */
 /* Control unconditional IBPB in switch_mm() */
 DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
 
@@ -128,6 +463,10 @@ DEFINE_STATIC_KEY_FALSE(switch_mm_cond_l1d_flush);
 DEFINE_STATIC_KEY_FALSE(mmio_stale_data_clear);
 EXPORT_SYMBOL_GPL(mmio_stale_data_clear);
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|2365| <<arch_cpu_finalize_init>> cpu_select_mitigations();
+ */
 void __init cpu_select_mitigations(void)
 {
 	/*
@@ -138,6 +477,12 @@ void __init cpu_select_mitigations(void)
 	if (cpu_feature_enabled(X86_FEATURE_MSR_SPEC_CTRL)) {
 		rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
 
+		/*
+		 * 114 // A mask for bits which the kernel toggles when controlling mitigations
+		 * 115 #define SPEC_CTRL_MITIGATIONS_MASK      (SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD \
+		 * 116                                                         | SPEC_CTRL_RRSBA_DIS_S \
+		 * 117                                                         | SPEC_CTRL_BHI_DIS_S
+		 */
 		/*
 		 * Previously running kernel (kexec), may have some controls
 		 * turned ON. Clear them and let the mitigations setup below
@@ -146,6 +491,22 @@ void __init cpu_select_mitigations(void)
 		x86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;
 	}
 
+	/*
+	 * 在以下使用x86_arch_cap_msr:
+	 *   - arch/x86/kernel/cpu/bugs.c|216| <<cpu_select_mitigations>> x86_arch_cap_msr = x86_read_arch_cap_msr();
+	 *   - arch/x86/kernel/cpu/bugs.c|415| <<taa_select_mitigation>> if ( (x86_arch_cap_msr & ARCH_CAP_MDS_NO) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|416| <<taa_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_TSX_CTRL_MSR))
+	 *   - arch/x86/kernel/cpu/bugs.c|507| <<mmio_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_FBSDP_NO))
+	 *   - arch/x86/kernel/cpu/bugs.c|517| <<mmio_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_FB_CLEAR) ||
+	 *   - arch/x86/kernel/cpu/bugs.c|520| <<mmio_select_mitigation>> !(x86_arch_cap_msr & ARCH_CAP_MDS_NO)))
+	 *   - arch/x86/kernel/cpu/bugs.c|578| <<rfds_select_mitigation>> if (x86_arch_cap_msr & ARCH_CAP_RFDS_CLEAR)
+	 *   - arch/x86/kernel/cpu/bugs.c|738| <<srbds_select_mitigation>> if ((x86_arch_cap_msr & ARCH_CAP_MDS_NO) && !boot_cpu_has(X86_FEATURE_RTM) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|878| <<gds_select_mitigation>> if (!(x86_arch_cap_msr & ARCH_CAP_GDS_CTRL)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1636| <<spec_ctrl_disable_kernel_rrsba>> if (!(x86_arch_cap_msr & ARCH_CAP_RRSBA)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2016| <<update_mds_branch_idle>> (x86_arch_cap_msr & ARCH_CAP_FBSDP_NO)) {
+	 *
+	 * 很多cap, 有security的, 也有其他的.
+	 */
 	x86_arch_cap_msr = x86_read_arch_cap_msr();
 
 	/* Select the proper CPU mitigations before patching alternatives: */
@@ -177,10 +538,30 @@ void __init cpu_select_mitigations(void)
 	gds_select_mitigation();
 }
 
+/*
+ * 在以下使用x86_virt_spec_ctrl(): -->似乎只SVM使用:
+ *   - arch/x86/include/asm/spec-ctrl.h|30| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, true);
+ *   - arch/x86/include/asm/spec-ctrl.h|44| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, false);
+ *
+ * On VMENTER we must preserve whatever view of the SPEC_CTRL MSR
+ * the guest has, while on VMEXIT we restore the host view. This
+ * would be easier if SPEC_CTRL were architecturally maskable or
+ * shadowable for guests but this is not (currently) the case.
+ * Takes the guest view of SPEC_CTRL MSR as a parameter and also
+ * the guest's version of VIRT_SPEC_CTRL, if emulated.
+ *
+ * NOTE: This function is *only* called for SVM, since Intel uses
+ * MSR_IA32_SPEC_CTRL for SSBD.
+ */
 /*
  * NOTE: This function is *only* called for SVM, since Intel uses
  * MSR_IA32_SPEC_CTRL for SSBD.
  */
+/*
+ * 在以下使用x86_virt_spec_ctrl():
+ *   - arch/x86/include/asm/spec-ctrl.h|45| <<x86_spec_ctrl_set_guest>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, true);
+ *   - arch/x86/include/asm/spec-ctrl.h|74| <<x86_spec_ctrl_restore_host>> x86_virt_spec_ctrl(guest_virt_spec_ctrl, false);
+ */
 void
 x86_virt_spec_ctrl(u64 guest_virt_spec_ctrl, bool setguest)
 {
@@ -871,6 +1252,15 @@ enum spectre_v1_mitigation {
 	SPECTRE_V1_MITIGATION_AUTO,
 };
 
+/*
+ * 在以下使用spectre_v1_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|1077| <<global>> static enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =
+ *   - arch/x86/kernel/cpu/bugs.c|1110| <<spectre_v1_select_mitigation>> spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ *   - arch/x86/kernel/cpu/bugs.c|1114| <<spectre_v1_select_mitigation>> if (spectre_v1_mitigation == SPECTRE_V1_MITIGATION_AUTO) {
+ *   - arch/x86/kernel/cpu/bugs.c|1150| <<spectre_v1_select_mitigation>> pr_info("%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ *   - arch/x86/kernel/cpu/bugs.c|1155| <<nospectre_v1_cmdline>> spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
+ *   - arch/x86/kernel/cpu/bugs.c|3219| <<cpu_show_common>> return sysfs_emit(buf, "%s\n", spectre_v1_strings[spectre_v1_mitigation]);
+ */
 static enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =
 	IS_ENABLED(CONFIG_MITIGATION_SPECTRE_V1) ?
 		SPECTRE_V1_MITIGATION_AUTO : SPECTRE_V1_MITIGATION_NONE;
@@ -884,6 +1274,10 @@ static const char * const spectre_v1_strings[] = {
  * Does SMAP provide full mitigation against speculative kernel access to
  * userspace?
  */
+/*
+ * 在以下调用smap_works_speculatively():
+ *   - arch/x86/kernel/cpu/bugs.c|1128| <<spectre_v1_select_mitigation>> !smap_works_speculatively()) {
+ */
 static bool smap_works_speculatively(void)
 {
 	if (!boot_cpu_has(X86_FEATURE_SMAP))
@@ -903,6 +1297,10 @@ static bool smap_works_speculatively(void)
 
 static void __init spectre_v1_select_mitigation(void)
 {
+	/*
+	 * 关于cpu_mitigations_off():
+	 *   如果cpu_mitigations == CPU_MITIGATIONS_OFF
+	 */
 	if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {
 		spectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;
 		return;
@@ -940,6 +1338,12 @@ static void __init spectre_v1_select_mitigation(void)
 			 * paths, to prevent user entry from speculatively
 			 * skipping swapgs.
 			 */
+			/*
+			 * 在以下使用X86_FEATURE_FENCE_SWAPGS_KERNEL:
+			 *   - arch/x86/entry/calling.h|366| <<global>> ALTERNATIVE "", "lfence", X86_FEATURE_FENCE_SWAPGS_KERNEL
+			 *   - arch/x86/include/asm/cpufeatures.h|291| <<global>> #define X86_FEATURE_FENCE_SWAPGS_KERNEL (11*32+ 5) 
+			 *   - arch/x86/kernel/cpu/bugs.c|1146| <<spectre_v1_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_KERNEL);
+			 */
 			setup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_KERNEL);
 		}
 	}
@@ -954,6 +1358,23 @@ static int __init nospectre_v1_cmdline(char *str)
 }
 early_param("nospectre_v1", nospectre_v1_cmdline);
 
+/*
+ * 在以下使用spectre_v2_enabled:
+ *   - arch/x86/kernel/cpu/bugs.c|1160| <<global>> enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init = SPECTRE_V2_NONE;
+ *   - arch/x86/kernel/cpu/amd.c|1066| <<init_amd>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1272| <<retbleed_select_mitigation>> spectre_v2_enabled == SPECTRE_V2_RETPOLINE) {
+ *   - arch/x86/kernel/cpu/bugs.c|1362| <<retbleed_select_mitigation>> switch (spectre_v2_enabled) {
+ *   - arch/x86/kernel/cpu/bugs.c|1393| <<retpoline_module_ok>> if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+ *   - arch/x86/kernel/cpu/bugs.c|1422| <<unpriv_ebpf_notify>> switch (spectre_v2_enabled) {
+ *   - arch/x86/kernel/cpu/bugs.c|1631| <<spectre_v2_user_select_mitigation>> (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|2046| <<spectre_v2_select_mitigation>> spectre_v2_enabled = mode;
+ *   - arch/x86/kernel/cpu/bugs.c|2198| <<cpu_bugs_smt_update>> spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)
+ *   - arch/x86/kernel/cpu/bugs.c|3053| <<stibp_state>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|3144| <<spectre_v2_show_state>> if (spectre_v2_enabled == SPECTRE_V2_LFENCE)
+ *   - arch/x86/kernel/cpu/bugs.c|3147| <<spectre_v2_show_state>> if (spectre_v2_enabled == SPECTRE_V2_EIBRS && unprivileged_ebpf_enabled())
+ *   - arch/x86/kernel/cpu/bugs.c|3151| <<spectre_v2_show_state>> spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)
+ *   - arch/x86/kernel/cpu/bugs.c|3155| <<spectre_v2_show_state>> spectre_v2_strings[spectre_v2_enabled],
+ */
 enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init = SPECTRE_V2_NONE;
 
 #undef pr_fmt
@@ -1177,14 +1598,45 @@ static void __init retbleed_select_mitigation(void)
 #undef pr_fmt
 #define pr_fmt(fmt)     "Spectre V2 : " fmt
 
+/*
+ * 493 // The Spectre V2 mitigation variants
+ * 494 enum spectre_v2_mitigation {
+ * 495         SPECTRE_V2_NONE,
+ * 496         SPECTRE_V2_RETPOLINE,
+ * 497         SPECTRE_V2_LFENCE,
+ * 498         SPECTRE_V2_EIBRS,
+ * 499         SPECTRE_V2_EIBRS_RETPOLINE,
+ * 500         SPECTRE_V2_EIBRS_LFENCE,
+ * 501         SPECTRE_V2_IBRS,
+ * 502 };
+ * 503
+ * 504 // The indirect branch speculation control variants
+ * 505 enum spectre_v2_user_mitigation {
+ * 506         SPECTRE_V2_USER_NONE,
+ * 507         SPECTRE_V2_USER_STRICT,
+ * 508         SPECTRE_V2_USER_STRICT_PREFERRED,
+ * 509         SPECTRE_V2_USER_PRCTL,
+ * 510         SPECTRE_V2_USER_SECCOMP,
+ * 511 };
+ */
+
 static enum spectre_v2_user_mitigation spectre_v2_user_stibp __ro_after_init =
 	SPECTRE_V2_USER_NONE;
 static enum spectre_v2_user_mitigation spectre_v2_user_ibpb __ro_after_init =
 	SPECTRE_V2_USER_NONE;
 
 #ifdef CONFIG_MITIGATION_RETPOLINE
+/*
+ * 在以下使用spectre_v2_bad_module:
+ *   - arch/x86/kernel/cpu/bugs.c|1437| <<retpoline_module_ok>> spectre_v2_bad_module = true;
+ *   - arch/x86/kernel/cpu/bugs.c|1443| <<spectre_v2_module_string>> return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
+ */
 static bool spectre_v2_bad_module;
 
+/*
+ * called by:
+ *   - kernel/module/main.c|2262| <<check_modinfo_retpoline>> if (retpoline_module_ok(get_modinfo(info, "retpoline")))
+ */
 bool retpoline_module_ok(bool has_retpoline)
 {
 	if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
@@ -1195,6 +1647,10 @@ bool retpoline_module_ok(bool has_retpoline)
 	return false;
 }
 
+/*
+ * 在以下使用spectre_v2_module_string():
+ *   - arch/x86/kernel/cpu/bugs.c|3203| <<spectre_v2_show_state>> spectre_v2_module_string());
+ */
 static inline const char *spectre_v2_module_string(void)
 {
 	return spectre_v2_bad_module ? " - vulnerable module loaded" : "";
@@ -1283,12 +1739,21 @@ static const struct {
 	{ "seccomp,ibpb",	SPECTRE_V2_USER_CMD_SECCOMP_IBPB,	false },
 };
 
+/*
+ * 在以下使用spec_v2_user_print_cond():
+ *   - arch/x86/kernel/cpu/bugs.c|1564| <<spectre_v2_parse_user_cmdline>> spec_v2_user_print_cond(v2_user_options[i].option,
+ */
 static void __init spec_v2_user_print_cond(const char *reason, bool secure)
 {
 	if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2) != secure)
 		pr_info("spectre_v2_user=%s forced on command line.\n", reason);
 }
 
+/*
+ * 在以下使用spectre_v2_cmd:
+ *   - arch/x86/kernel/cpu/bugs.c|1367| <<spectre_v2_parse_user_cmdline>> switch (spectre_v2_cmd) {
+ *   - arch/x86/kernel/cpu/bugs.c|1961| <<spectre_v2_select_mitigation>> spectre_v2_cmd = cmd;
+ */
 static __ro_after_init enum spectre_v2_mitigation_cmd spectre_v2_cmd;
 
 static enum spectre_v2_user_cmd __init
@@ -1365,6 +1830,45 @@ spectre_v2_user_select_mitigation(void)
 
 	/* Initialize Indirect Branch Prediction Barrier */
 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *
+		 * 在以下使用X86_FEATURE_USE_IBPB:
+		 *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+		 *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+		 *
+		 * 部分例子.
+		 *
+		 * context_switch()
+		 * -> switch_mm_irqs_off()
+		 *    -> cond_mitigation()
+		 *       -> indirect_branch_prediction_barrier()
+		 *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+		 *                                   x86_pred_cmd,
+		 *                                   X86_FEATURE_USE_IBPB);
+		 *
+		 * switch_mm()
+		 * -> switch_mm_irqs_off()
+		 *    -> cond_mitigation()
+		 *       -> indirect_branch_prediction_barrier()
+		 *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+		 *                                   x86_pred_cmd,
+		 *                                   X86_FEATURE_USE_IBPB);
+		 */
 		setup_force_cpu_cap(X86_FEATURE_USE_IBPB);
 
 		spectre_v2_user_ibpb = mode;
@@ -1380,10 +1884,27 @@ spectre_v2_user_select_mitigation(void)
 		case SPECTRE_V2_USER_CMD_PRCTL:
 		case SPECTRE_V2_USER_CMD_AUTO:
 		case SPECTRE_V2_USER_CMD_SECCOMP:
+			/*
+			 * 在以下使用switch_mm_cond_ibpb:
+			 *   - arch/x86/include/asm/nospec-branch.h|586| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+			 *   - arch/x86/kernel/cpu/bugs.c|179| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+			 *   - arch/x86/kernel/cpu/bugs.c|1450| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_cond_ibpb);
+			 *   - arch/x86/kernel/cpu/bugs.c|2909| <<ibpb_state>> if (static_key_enabled(&switch_mm_cond_ibpb))
+			 *   - arch/x86/mm/tlb.c|408| <<cond_mitigation>> if (static_branch_likely(&switch_mm_cond_ibpb)) {
+			 */
 			static_branch_enable(&switch_mm_cond_ibpb);
 			break;
 		}
 
+		/*
+		 * 在以下使用switch_mm_always_ibpb:
+		 *   - arch/x86/include/asm/nospec-branch.h|587| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|181| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|1444| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_always_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|1455| <<spectre_v2_user_select_mitigation>> static_key_enabled(&switch_mm_always_ibpb) ?
+		 *   - arch/x86/kernel/cpu/bugs.c|2907| <<ibpb_state>> if (static_key_enabled(&switch_mm_always_ibpb))
+		 *   - arch/x86/mm/tlb.c|457| <<cond_mitigation>> if (static_branch_unlikely(&switch_mm_always_ibpb)) {
+		 */
 		pr_info("mitigation: Enabling %s Indirect Branch Prediction Barrier\n",
 			static_key_enabled(&switch_mm_always_ibpb) ?
 			"always-on" : "conditional");
@@ -1548,6 +2069,11 @@ static enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)
 	return cmd;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2118| <<spectre_v2_select_mitigation>> mode = spectre_v2_select_retpoline();
+ *   - arch/x86/kernel/cpu/bugs.c|2131| <<spectre_v2_select_mitigation>> mode = spectre_v2_select_retpoline();
+ */
 static enum spectre_v2_mitigation __init spectre_v2_select_retpoline(void)
 {
 	if (!IS_ENABLED(CONFIG_MITIGATION_RETPOLINE)) {
@@ -1674,6 +2200,19 @@ static void __init bhi_select_mitigation(void)
 	if (bhi_mitigation == BHI_MITIGATION_OFF)
 		return;
 
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|450| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+	 *   - arch/x86/include/asm/nospec-branch.h|480| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1993| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3327| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *   - net/netfilter/nf_tables_core.c|35| <<nf_skip_indirect_calls_enable>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 */
 	/* Retpoline mitigates against BHI unless the CPU has RRSBA behavior */
 	if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
 	    !boot_cpu_has(X86_FEATURE_RETPOLINE_LFENCE)) {
@@ -1700,11 +2239,52 @@ static void __init bhi_select_mitigation(void)
 	setup_force_cpu_cap(X86_FEATURE_CLEAR_BHB_LOOP_ON_VMEXIT);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|187| <<cpu_select_mitigations>> spectre_v2_select_mitigation();
+ */
 static void __init spectre_v2_select_mitigation(void)
 {
+	/*
+	 * 1518 // The kernel command line selection for spectre v2
+	 * 1519 enum spectre_v2_mitigation_cmd {
+	 * 1520         SPECTRE_V2_CMD_NONE,
+	 * 1521         SPECTRE_V2_CMD_AUTO,
+	 * 1522         SPECTRE_V2_CMD_FORCE,
+	 * 1523         SPECTRE_V2_CMD_RETPOLINE,
+	 * 1524         SPECTRE_V2_CMD_RETPOLINE_GENERIC,
+	 * 1525         SPECTRE_V2_CMD_RETPOLINE_LFENCE,
+	 * 1526         SPECTRE_V2_CMD_EIBRS,
+	 * 1527         SPECTRE_V2_CMD_EIBRS_RETPOLINE,
+	 * 1528         SPECTRE_V2_CMD_EIBRS_LFENCE,
+	 * 1529         SPECTRE_V2_CMD_IBRS,
+	 * 1530 };
+	 *
+	 * 493 // The Spectre V2 mitigation variants
+	 * 494 enum spectre_v2_mitigation {
+	 * 495         SPECTRE_V2_NONE,
+	 * 496         SPECTRE_V2_RETPOLINE,
+	 * 497         SPECTRE_V2_LFENCE,
+	 * 498         SPECTRE_V2_EIBRS,
+	 * 499         SPECTRE_V2_EIBRS_RETPOLINE,
+	 * 500         SPECTRE_V2_EIBRS_LFENCE,
+	 * 501         SPECTRE_V2_IBRS,
+	 * 502 };
+	 */
 	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
 	enum spectre_v2_mitigation mode = SPECTRE_V2_NONE;
 
+	/*
+	 * 在以下使用X86_BUG_SPECTRE_V2:
+	 *   - arch/x86/include/asm/cpufeatures.h|511| <<global>> #define X86_BUG_SPECTRE_V2 X86_BUG(16) 
+	 *   - arch/x86/kernel/cpu/bugs.c|1570| <<spec_v2_user_print_cond>> if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2) != secure)
+	 *   - arch/x86/kernel/cpu/bugs.c|1767| <<spec_v2_print_cond>> if (boot_cpu_has_bug(X86_BUG_SPECTRE_V2) != secure)
+	 *   - arch/x86/kernel/cpu/bugs.c|2020| <<spectre_v2_select_mitigation>> if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|2678| <<ib_prctl_get>> if (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))
+	 *   - arch/x86/kernel/cpu/bugs.c|3300| <<cpu_show_common>> case X86_BUG_SPECTRE_V2:
+	 *   - arch/x86/kernel/cpu/bugs.c|3358| <<cpu_show_spectre_v2>> return cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V2);
+	 *   - arch/x86/kernel/cpu/common.c|1333| <<cpu_set_bug_bits>> setup_force_cpu_bug(X86_BUG_SPECTRE_V2);
+	 */
 	/*
 	 * If the CPU is not affected and the command line mode is NONE or AUTO
 	 * then nothing to do.
@@ -1719,6 +2299,21 @@ static void __init spectre_v2_select_mitigation(void)
 
 	case SPECTRE_V2_CMD_FORCE:
 	case SPECTRE_V2_CMD_AUTO:
+		/*
+		 * # cat /sys/devices/system/cpu/vulnerabilities/spectre_v2
+		 * Mitigation: Retpolines; IBPB: conditional; IBRS_FW; STIBP: conditional; RSB filling; PBRSB-eIBRS: Not affected; BHI: Not affected
+		 * 
+		 * # cat /sys/devices/system/cpu/vulnerabilities/spectre_v2
+		 * Mitigation: Enhanced / Automatic IBRS; IBPB: conditional; STIBP: always-on; RSB filling; PBRSB-eIBRS: Not affected; BHI: Not affected
+		 *
+		 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+		 *   - arch/x86/include/asm/cpufeatures.h|222| <<global>> #define X86_FEATURE_IBRS_ENHANCED ( 7*32+30) 
+		 *   - arch/x86/kernel/cpu/bugs.c|1812| <<spectre_v2_parse_cmdline>> !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2030| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2097| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+		 *   - arch/x86/kernel/cpu/common.c|1350| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+		 *   - arch/x86/kernel/cpu/common.c|1440| <<cpu_set_bug_bits>> (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+		 */
 		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
 			mode = SPECTRE_V2_EIBRS;
 			break;
@@ -1767,6 +2362,11 @@ static void __init spectre_v2_select_mitigation(void)
 		break;
 	}
 
+	/*
+	 * 注释:
+	 * WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks
+	 * possible via Spectre v2 BHB attacks!
+	 */
 	if (mode == SPECTRE_V2_EIBRS && unprivileged_ebpf_enabled())
 		pr_err(SPECTRE_V2_EIBRS_EBPF_MSG);
 
@@ -1774,7 +2374,26 @@ static void __init spectre_v2_select_mitigation(void)
 		if (boot_cpu_has(X86_FEATURE_AUTOIBRS)) {
 			msr_set_bit(MSR_EFER, _EFER_AUTOIBRS);
 		} else {
+			/*
+			 * 在以下使用SPEC_CTRL_IBRS:
+			 *   - arch/x86/entry/calling.h|342| <<global>> andl $(~SPEC_CTRL_IBRS), %edx
+			 *   - arch/x86/include/asm/msr-index.h|104| <<global>> #define SPEC_CTRL_IBRS BIT(0)
+			 *   - arch/x86/include/asm/msr-index.h|115| <<SPEC_CTRL_MITIGATIONS_MASK>> #define
+			 *      SPEC_CTRL_MITIGATIONS_MASK (SPEC_CTRL_IBRS | SPEC_CTRL_STIBP | SPEC_CTRL_SSBD \
+			 *   - arch/x86/include/asm/nospec-branch.h|571| <<firmware_restrict_branch_speculation_start>>
+			 *      spec_ctrl_current() | SPEC_CTRL_IBRS, \
+			 *   - arch/x86/kernel/cpu/bugs.c|2134| <<spectre_v2_select_mitigation>> x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+			 */
 			x86_spec_ctrl_base |= SPEC_CTRL_IBRS;
+			/*
+			 * 在以下调用update_spec_ctrl():
+			 *   - arch/x86/kernel/cpu/bugs.c|1645| <<spec_ctrl_disable_kernel_rrsba>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|1706| <<spec_ctrl_bhi_dis>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|1849| <<spectre_v2_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|1967| <<update_stibp_msr>> update_spec_ctrl(val);
+			 *   - arch/x86/kernel/cpu/bugs.c|2200| <<__ssb_select_mitigation>> update_spec_ctrl(x86_spec_ctrl_base);
+			 *   - arch/x86/kernel/cpu/bugs.c|2468| <<x86_spec_ctrl_setup_ap>> update_spec_ctrl(x86_spec_ctrl_base);
+			 */
 			update_spec_ctrl(x86_spec_ctrl_base);
 		}
 	}
@@ -1785,7 +2404,27 @@ static void __init spectre_v2_select_mitigation(void)
 		break;
 
 	case SPECTRE_V2_IBRS:
+		/*
+		 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+		 *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+		 *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+		 *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+		 *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+		 *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+		 *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+		 *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+		 *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+		 */
 		setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+		/*
+		 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+		 *   - arch/x86/include/asm/cpufeatures.h|222| <<global>> #define X86_FEATURE_IBRS_ENHANCED ( 7*32+30)
+		 *   - arch/x86/kernel/cpu/bugs.c|1812| <<spectre_v2_parse_cmdline>> !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2030| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+		 *   - arch/x86/kernel/cpu/bugs.c|2097| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+		 *   - arch/x86/kernel/cpu/common.c|1350| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+		 *   - arch/x86/kernel/cpu/common.c|1440| <<cpu_set_bug_bits>> (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+		 */
 		if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
 			pr_warn(SPECTRE_V2_IBRS_PERF_MSG);
 		break;
@@ -1797,6 +2436,19 @@ static void __init spectre_v2_select_mitigation(void)
 
 	case SPECTRE_V2_RETPOLINE:
 	case SPECTRE_V2_EIBRS_RETPOLINE:
+		/*
+		 * 在以下使用X86_FEATURE_RETPOLINE:
+		 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+		 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+		 *   - arch/x86/include/asm/nospec-branch.h|450| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+		 *   - arch/x86/include/asm/nospec-branch.h|480| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+		 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|1993| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/kernel/cpu/bugs.c|3327| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+		 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+		 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 *   - net/netfilter/nf_tables_core.c|35| <<nf_skip_indirect_calls_enable>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+		 */
 		setup_force_cpu_cap(X86_FEATURE_RETPOLINE);
 		break;
 	}
@@ -1815,6 +2467,23 @@ static void __init spectre_v2_select_mitigation(void)
 		bhi_select_mitigation();
 
 	spectre_v2_enabled = mode;
+	/*
+	 * 在以下使用spectre_v2_enabled:
+	 *   - arch/x86/kernel/cpu/bugs.c|1160| <<global>> enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init = SPECTRE_V2_NONE;
+	 *   - arch/x86/kernel/cpu/amd.c|1066| <<init_amd>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1272| <<retbleed_select_mitigation>> spectre_v2_enabled == SPECTRE_V2_RETPOLINE) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1362| <<retbleed_select_mitigation>> switch (spectre_v2_enabled) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1393| <<retpoline_module_ok>> if (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)
+	 *   - arch/x86/kernel/cpu/bugs.c|1422| <<unpriv_ebpf_notify>> switch (spectre_v2_enabled) {
+	 *   - arch/x86/kernel/cpu/bugs.c|1631| <<spectre_v2_user_select_mitigation>> (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|2046| <<spectre_v2_select_mitigation>> spectre_v2_enabled = mode;
+	 *   - arch/x86/kernel/cpu/bugs.c|2198| <<cpu_bugs_smt_update>> spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)
+	 *   - arch/x86/kernel/cpu/bugs.c|3053| <<stibp_state>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3144| <<spectre_v2_show_state>> if (spectre_v2_enabled == SPECTRE_V2_LFENCE)
+	 *   - arch/x86/kernel/cpu/bugs.c|3147| <<spectre_v2_show_state>> if (spectre_v2_enabled == SPECTRE_V2_EIBRS && unprivileged_ebpf_enabled())
+	 *   - arch/x86/kernel/cpu/bugs.c|3151| <<spectre_v2_show_state>> spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)
+	 *   - arch/x86/kernel/cpu/bugs.c|3155| <<spectre_v2_show_state>> spectre_v2_strings[spectre_v2_enabled],
+	 */
 	pr_info("%s\n", spectre_v2_strings[mode]);
 
 	/*
@@ -1855,6 +2524,13 @@ static void __init spectre_v2_select_mitigation(void)
 	 *
 	 * FIXME: Is this pointless for retbleed-affected AMD?
 	 */
+	/*
+	 * 在以下使用X86_FEATURE_RSB_CTXSW:
+	 *   - arch/x86/include/asm/cpufeatures.h|211| <<global>> #define X86_FEATURE_RSB_CTXSW ( 7*32+19)
+	 *   - arch/x86/kernel/cpu/bugs.c|2481| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
+	 *   - arch/x86/kernel/cpu/bugs.c|3593| <<spectre_v2_show_state>> return sysfs_emit(...
+	 *               boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? "; RSB filling" : "",
+	 */
 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
 
@@ -1890,12 +2566,20 @@ static void __init spectre_v2_select_mitigation(void)
 	spectre_v2_cmd = cmd;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2415| <<update_stibp_strict>> on_each_cpu(update_stibp_msr, NULL, 1);
+ */
 static void update_stibp_msr(void * __unused)
 {
 	u64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);
 	update_spec_ctrl(val);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2479| <<cpu_bugs_smt_update>> update_stibp_strict();
+ */
 /* Update x86_spec_ctrl_base in case SMT state changed. */
 static void update_stibp_strict(void)
 {
@@ -1916,6 +2600,16 @@ static void update_stibp_strict(void)
 /* Update the static key controlling the evaluation of TIF_SPEC_IB */
 static void update_indir_branch_cond(void)
 {
+	/*
+	 * 在以下使用switch_to_cond_stibp:
+	 *   - arch/x86/include/asm/nospec-branch.h|585| <<global>> DECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|177| <<global>> DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|1991| <<update_indir_branch_cond>> static_branch_enable(&switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|1993| <<update_indir_branch_cond>> static_branch_disable(&switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|2898| <<stibp_state>> if (static_key_enabled(&switch_to_cond_stibp))
+	 *   - arch/x86/kernel/process.c|637| <<__speculation_ctrl_update>> static_branch_unlikely(&switch_to_cond_stibp)) {
+	 *   - arch/x86/kernel/process.h|26| <<switch_to_extra>> if (!static_branch_likely(&switch_to_cond_stibp)) {
+	 */
 	if (sched_smt_active())
 		static_branch_enable(&switch_to_cond_stibp);
 	else
@@ -1951,6 +2645,10 @@ static void update_mds_branch_idle(void)
 #define TAA_MSG_SMT "TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.\n"
 #define MMIO_MSG_SMT "MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.\n"
 
+/*
+ * 在以下调用cpu_bugs_smt_update():
+ *   - arch/x86/kernel/cpu/common.c|2359| <<arch_smt_update>> cpu_bugs_smt_update();
+ */
 void cpu_bugs_smt_update(void)
 {
 	mutex_lock(&spec_ctrl_mutex);
@@ -2226,6 +2924,11 @@ static bool is_spec_ib_user_controlled(void)
 		spectre_v2_user_stibp == SPECTRE_V2_USER_SECCOMP;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|2326| <<arch_prctl_spec_ctrl_set>> return ib_prctl_set(task, ctrl);
+ *   - arch/x86/kernel/cpu/bugs.c|2341| <<arch_seccomp_spec_mitigate>> ib_prctl_set(task, PR_SPEC_FORCE_DISABLE);
+ */
 static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 {
 	switch (ctrl) {
@@ -2273,6 +2976,15 @@ static int ib_prctl_set(struct task_struct *task, unsigned long ctrl)
 		if (ctrl == PR_SPEC_FORCE_DISABLE)
 			task_set_spec_ib_force_disable(task);
 		task_update_spec_tif(task);
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (task == current)
 			indirect_branch_prediction_barrier();
 		break;
@@ -2392,6 +3104,34 @@ EXPORT_SYMBOL_GPL(itlb_multihit_kvm_mitigation);
 #undef pr_fmt
 #define pr_fmt(fmt)	"L1TF: " fmt
 
+/*
+ * 在以下使用l1tf_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|3055| <<global>> enum l1tf_mitigations l1tf_mitigation __ro_after_init =
+ *   - arch/x86/kernel/cpu/bugs.c|3058| <<global>> EXPORT_SYMBOL_GPL(l1tf_mitigation);
+ *   - arch/x86/kernel/cpu/bugs.c|3110| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3112| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3116| <<l1tf_select_mitigation>> switch (l1tf_mitigation) {
+ *   - arch/x86/kernel/cpu/bugs.c|3136| <<l1tf_select_mitigation>> if (l1tf_mitigation != L1TF_MITIGATION_OFF &&
+ *   - arch/x86/kernel/cpu/bugs.c|3158| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3160| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOWARN;
+ *   - arch/x86/kernel/cpu/bugs.c|3162| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH;
+ *   - arch/x86/kernel/cpu/bugs.c|3164| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3166| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL;
+ *   - arch/x86/kernel/cpu/bugs.c|3168| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL_FORCE;
+ *   - arch/x86/kvm/vmx/vmx.c|272| <<vmx_setup_l1d_flush>> switch (l1tf_mitigation) {
+ *   - arch/x86/kvm/vmx/vmx.c|286| <<vmx_setup_l1d_flush>> } else if (l1tf_mitigation == L1TF_MITIGATION_FULL_FORCE) {
+ *   - arch/x86/kvm/vmx/vmx.c|7787| <<vmx_vm_init>> switch (l1tf_mitigation) {
+ *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+ *
+ * enum l1tf_mitigations {
+ *     L1TF_MITIGATION_OFF,
+ *     L1TF_MITIGATION_FLUSH_NOWARN,
+ *     L1TF_MITIGATION_FLUSH,
+ *     L1TF_MITIGATION_FLUSH_NOSMT,
+ *     L1TF_MITIGATION_FULL,
+ *     L1TF_MITIGATION_FULL_FORCE
+ * };
+ */
 /* Default mitigation for L1TF-affected CPUs */
 enum l1tf_mitigations l1tf_mitigation __ro_after_init =
 	IS_ENABLED(CONFIG_MITIGATION_L1TF) ? L1TF_MITIGATION_FLUSH : L1TF_MITIGATION_OFF;
@@ -2810,6 +3550,16 @@ static char *stibp_state(void)
 		return "; STIBP: always-on";
 	case SPECTRE_V2_USER_PRCTL:
 	case SPECTRE_V2_USER_SECCOMP:
+		/*
+		 * 在以下使用switch_to_cond_stibp:
+		 *   - arch/x86/include/asm/nospec-branch.h|585| <<global>> DECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|177| <<global>> DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|1991| <<update_indir_branch_cond>> static_branch_enable(&switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|1993| <<update_indir_branch_cond>> static_branch_disable(&switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|2898| <<stibp_state>> if (static_key_enabled(&switch_to_cond_stibp))
+		 *   - arch/x86/kernel/process.c|637| <<__speculation_ctrl_update>> static_branch_unlikely(&switch_to_cond_stibp)) {
+		 *   - arch/x86/kernel/process.h|26| <<switch_to_extra>> if (!static_branch_likely(&switch_to_cond_stibp)) {
+		 */
 		if (static_key_enabled(&switch_to_cond_stibp))
 			return "; STIBP: conditional";
 	}
@@ -2819,8 +3569,25 @@ static char *stibp_state(void)
 static char *ibpb_state(void)
 {
 	if (boot_cpu_has(X86_FEATURE_IBPB)) {
+		/*
+		 * 在以下使用switch_mm_always_ibpb:
+		 *   - arch/x86/include/asm/nospec-branch.h|587| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|181| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|1444| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_always_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|1455| <<spectre_v2_user_select_mitigation>> static_key_enabled(&switch_mm_always_ibpb) ?
+		 *   - arch/x86/kernel/cpu/bugs.c|2907| <<ibpb_state>> if (static_key_enabled(&switch_mm_always_ibpb))
+		 *   - arch/x86/mm/tlb.c|457| <<cond_mitigation>> if (static_branch_unlikely(&switch_mm_always_ibpb)) {
+		 */
 		if (static_key_enabled(&switch_mm_always_ibpb))
 			return "; IBPB: always-on";
+		/*
+		 * 在以下使用switch_mm_cond_ibpb:
+		 *   - arch/x86/include/asm/nospec-branch.h|586| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|179| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|1450| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_cond_ibpb);
+		 *   - arch/x86/kernel/cpu/bugs.c|2909| <<ibpb_state>> if (static_key_enabled(&switch_mm_cond_ibpb))
+		 *   - arch/x86/mm/tlb.c|408| <<cond_mitigation>> if (static_branch_likely(&switch_mm_cond_ibpb)) {
+		 */
 		if (static_key_enabled(&switch_mm_cond_ibpb))
 			return "; IBPB: conditional";
 		return "; IBPB: disabled";
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 3e9037690..c5c7e60b1 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1337,6 +1337,15 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	   !cpu_has(c, X86_FEATURE_AMD_SSB_NO))
 		setup_force_cpu_bug(X86_BUG_SPEC_STORE_BYPASS);
 
+	/*
+	 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+	 *   - arch/x86/include/asm/cpufeatures.h|222| <<global>> #define X86_FEATURE_IBRS_ENHANCED ( 7*32+30)
+	 *   - arch/x86/kernel/cpu/bugs.c|1812| <<spectre_v2_parse_cmdline>> !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2030| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2097| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+	 *   - arch/x86/kernel/cpu/common.c|1350| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+	 *   - arch/x86/kernel/cpu/common.c|1440| <<cpu_set_bug_bits>> (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+	 */
 	/*
 	 * AMD's AutoIBRS is equivalent to Intel's eIBRS - use the Intel feature
 	 * flag and protect from vendor-specific bugs via the whitelist.
@@ -1434,6 +1443,15 @@ static void __init cpu_set_bug_bits(struct cpuinfo_x86 *c)
 	if (vulnerable_to_rfds(x86_arch_cap_msr))
 		setup_force_cpu_bug(X86_BUG_RFDS);
 
+	/*
+	 * 在以下使用X86_FEATURE_IBRS_ENHANCED:
+	 *   - arch/x86/include/asm/cpufeatures.h|222| <<global>> #define X86_FEATURE_IBRS_ENHANCED ( 7*32+30)
+	 *   - arch/x86/kernel/cpu/bugs.c|1812| <<spectre_v2_parse_cmdline>> !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2030| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {
+	 *   - arch/x86/kernel/cpu/bugs.c|2097| <<spectre_v2_select_mitigation>> if (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))
+	 *   - arch/x86/kernel/cpu/common.c|1350| <<cpu_set_bug_bits>> setup_force_cpu_cap(X86_FEATURE_IBRS_ENHANCED);
+	 *   - arch/x86/kernel/cpu/common.c|1440| <<cpu_set_bug_bits>> (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED) ||
+	 */
 	/* When virtualized, eIBRS could be hidden, assume vulnerable */
 	if (!(x86_arch_cap_msr & ARCH_CAP_BHI_NO) &&
 	    !cpu_matches(cpu_vuln_whitelist, NO_BHI) &&
@@ -2335,6 +2353,12 @@ void microcode_check(struct cpuinfo_x86 *prev_info)
 /*
  * Invoked from core CPU hotplug code after hotplug operations
  */
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/common.c|2389| <<arch_cpu_finalize_init>> arch_smt_update();
+ *   - kernel/cpu.c|1460| <<_cpu_down>> arch_smt_update();
+ *   - kernel/cpu.c|1693| <<_cpu_up>> arch_smt_update();
+ */
 void arch_smt_update(void)
 {
 	/* Handle the speculative execution misfeatures */
@@ -2343,6 +2367,10 @@ void arch_smt_update(void)
 	apic_smt_update();
 }
 
+/*
+ * called by:
+ *   - init/main.c|1068| <<start_kernel>> arch_cpu_finalize_init();
+ */
 void __init arch_cpu_finalize_init(void)
 {
 	struct cpuinfo_x86 *c = this_cpu_ptr(&cpu_info);
diff --git a/arch/x86/kernel/cpu/cpu.h b/arch/x86/kernel/cpu/cpu.h
index 1beccefba..30001c724 100644
--- a/arch/x86/kernel/cpu/cpu.h
+++ b/arch/x86/kernel/cpu/cpu.h
@@ -92,6 +92,13 @@ extern void update_gds_msr(void);
 
 extern enum spectre_v2_mitigation spectre_v2_enabled;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/amd.c|1066| <<init_amd>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|1615| <<spectre_v2_in_ibrs_mode>> return spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;
+ *   - arch/x86/kernel/cpu/bugs.c|1710| <<spectre_v2_user_select_mitigation>> (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ *   - arch/x86/kernel/cpu/bugs.c|3132| <<stibp_state>> if (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&
+ */
 static inline bool spectre_v2_in_eibrs_mode(enum spectre_v2_mitigation mode)
 {
 	return mode == SPECTRE_V2_EIBRS ||
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 385e3a5fc..135157497 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -405,6 +405,11 @@ void intel_posted_msi_init(void)
  * instead of:
  *		read, xchg, read, xchg, read, xchg, read, xchg
  */
+/*
+ * 在以下调用handle_pending_pir():
+ *   - arch/x86/kernel/irq.c|460| <<DEFINE_IDTENTRY_SYSVEC(sysvec_posted_msi_notification)>> if (!handle_pending_pir(pid->pir64, regs))
+ *   - arch/x86/kernel/irq.c|475| <<DEFINE_IDTENTRY_SYSVEC(sysvec_posted_msi_notification)>> handle_pending_pir(pid->pir64, regs);
+ */
 static __always_inline bool handle_pending_pir(u64 *pir, struct pt_regs *regs)
 {
 	int i, vec = FIRST_EXTERNAL_VECTOR;
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 5b2c15214..2786cef9d 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -25,6 +25,14 @@
 static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init;
+/*
+ * 在以下使用msr_kvm_wall_clock:
+ *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+ *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+ *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+ *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+ *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+ */
 static int msr_kvm_wall_clock __ro_after_init;
 static u64 kvm_sched_clock_offset __ro_after_init;
 
@@ -60,6 +68,14 @@ EXPORT_PER_CPU_SYMBOL_GPL(hv_clock_per_cpu);
  */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
+	/*
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+	 *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
 	preempt_disable();
 	pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
@@ -292,6 +308,14 @@ void __init kvmclock_init(void)
 	if (!kvm_para_available() || !kvmclock)
 		return;
 
+	/*
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+	 *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE2)) {
 		msr_kvm_system_time = MSR_KVM_SYSTEM_TIME_NEW;
 		msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
@@ -307,6 +331,14 @@ void __init kvmclock_init(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+	 *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	pr_info("kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index f63f8fd00..e475de8e2 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -632,6 +632,16 @@ static __always_inline void __speculation_ctrl_update(unsigned long tifp,
 		msr |= ssbd_tif_to_spec_ctrl(tifn);
 	}
 
+	/*
+	 * 在以下使用switch_to_cond_stibp:
+	 *   - arch/x86/include/asm/nospec-branch.h|585| <<global>> DECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|177| <<global>> DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|1991| <<update_indir_branch_cond>> static_branch_enable(&switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|1993| <<update_indir_branch_cond>> static_branch_disable(&switch_to_cond_stibp);
+	 *   - arch/x86/kernel/cpu/bugs.c|2898| <<stibp_state>> if (static_key_enabled(&switch_to_cond_stibp))
+	 *   - arch/x86/kernel/process.c|637| <<__speculation_ctrl_update>> static_branch_unlikely(&switch_to_cond_stibp)) {
+	 *   - arch/x86/kernel/process.h|26| <<switch_to_extra>> if (!static_branch_likely(&switch_to_cond_stibp)) {
+	 */
 	/* Only evaluate TIF_SPEC_IB if conditional STIBP is enabled. */
 	if (IS_ENABLED(CONFIG_SMP) &&
 	    static_branch_unlikely(&switch_to_cond_stibp)) {
diff --git a/arch/x86/kernel/process.h b/arch/x86/kernel/process.h
index 76b547b83..1c4a16e94 100644
--- a/arch/x86/kernel/process.h
+++ b/arch/x86/kernel/process.h
@@ -23,6 +23,16 @@ static inline void switch_to_extra(struct task_struct *prev,
 		 * TIF_SPEC_IB. For CONFIG_SMP=n TIF_SPEC_IB is not
 		 * in the TIF_WORK_CTXSW masks.
 		 */
+		/*
+		 * 在以下使用switch_to_cond_stibp:
+		 *   - arch/x86/include/asm/nospec-branch.h|585| <<global>> DECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|177| <<global>> DEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|1991| <<update_indir_branch_cond>> static_branch_enable(&switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|1993| <<update_indir_branch_cond>> static_branch_disable(&switch_to_cond_stibp);
+		 *   - arch/x86/kernel/cpu/bugs.c|2898| <<stibp_state>> if (static_key_enabled(&switch_to_cond_stibp))
+		 *   - arch/x86/kernel/process.c|637| <<__speculation_ctrl_update>> static_branch_unlikely(&switch_to_cond_stibp)) {
+		 *   - arch/x86/kernel/process.h|26| <<switch_to_extra>> if (!static_branch_likely(&switch_to_cond_stibp)) {
+		 */
 		if (!static_branch_likely(&switch_to_cond_stibp)) {
 			prev_tif &= ~_TIF_SPEC_IB;
 			next_tif &= ~_TIF_SPEC_IB;
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index b5a8f0891..d98ca063b 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -1401,6 +1401,17 @@ void __noreturn hlt_play_dead(void)
  */
 void native_play_dead(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+	 *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
 		__update_spec_ctrl(0);
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index ae0b438a2..a97cdd175 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -368,6 +368,10 @@ static bool guest_cpuid_is_amd_or_hygon(struct kvm_vcpu *vcpu)
 	       is_guest_vendor_hygon(entry->ebx, entry->ecx, entry->edx);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|504| <<kvm_set_cpuid>> kvm_vcpu_after_set_cpuid(vcpu);
+ */
 static void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -454,6 +458,11 @@ u64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)
 	return rsvd_bits(cpuid_maxphyaddr(vcpu), 63);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|545| <<kvm_vcpu_ioctl_set_cpuid>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ *   - arch/x86/kvm/cpuid.c|571| <<kvm_vcpu_ioctl_set_cpuid2>> r = kvm_set_cpuid(vcpu, e2, cpuid->nent);
+ */
 static int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,
                         int nent)
 {
@@ -552,6 +561,10 @@ int kvm_vcpu_ioctl_set_cpuid(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+/*
+ * 处理KVM_SET_CPUID2:
+ *   - arch/x86/kvm/x86.c|6028| <<kvm_arch_vcpu_ioctl>> r = kvm_vcpu_ioctl_set_cpuid2(vcpu, &cpuid, cpuid_arg->entries);
+ */
 int kvm_vcpu_ioctl_set_cpuid2(struct kvm_vcpu *vcpu,
 			      struct kvm_cpuid2 *cpuid,
 			      struct kvm_cpuid_entry2 __user *entries)
@@ -963,6 +976,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1444| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -1497,6 +1514,10 @@ static bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,
 	return false;
 }
 
+/*
+ * 处理KVM_GET_SUPPORTED_CPUID和KVM_GET_EMULATED_CPUID:
+ *   - arch/x86/kvm/x86.c|4905| <<kvm_arch_dev_ioctl>> r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries, ioctl);
+ */
 int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 			    struct kvm_cpuid_entry2 __user *entries,
 			    unsigned int type)
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index f16a7b2c2..47b7ed554 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,6 +42,16 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|53| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|274| <<kvm_vcpu_is_legal_cr3>> return kvm_vcpu_is_legal_gpa(vcpu, cr3);
+ *   - arch/x86/kvm/svm/nested.c|256| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|257| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|834| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ *   - arch/x86/kvm/vmx/nested.c|2823| <<nested_vmx_check_eptp>> if (CC(!kvm_vcpu_is_legal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/vmx.c|5829| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && !kvm_vcpu_is_legal_gpa(vcpu, gpa)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 4f0a94346..5bbb81eab 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -134,6 +134,19 @@ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&vcpu->kvm->arch.apicv_update_lock);
 
 	if (auto_eoi_new)
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index cd57a517d..62f4c2730 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -288,6 +288,13 @@ static inline void kvm_pit_reset_reinject(struct kvm_pit *pit)
 	atomic_set(&pit->pit_state.irq_ack, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|702| <<kvm_create_pit>> kvm_pit_set_reinject(pit, true);
+ *   - arch/x86/kvm/i8254.c|727| <<kvm_create_pit>> kvm_pit_set_reinject(pit, false);
+ *   - arch/x86/kvm/i8254.c|745| <<kvm_free_pit>> kvm_pit_set_reinject(pit, false);
+ *   - arch/x86/kvm/x86.c|6473| <<kvm_vm_ioctl_reinject>> kvm_pit_set_reinject(pit, control->pit_reinject);
+ */
 void kvm_pit_set_reinject(struct kvm_pit *pit, bool reinject)
 {
 	struct kvm_kpit_state *ps = &pit->pit_state;
diff --git a/arch/x86/kvm/ioapic.c b/arch/x86/kvm/ioapic.c
index 995eb5054..9f528d1e2 100644
--- a/arch/x86/kvm/ioapic.c
+++ b/arch/x86/kvm/ioapic.c
@@ -119,6 +119,13 @@ static void __rtc_irq_eoi_tracking_restore_one(struct kvm_vcpu *vcpu)
 				 kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+	 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+	 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+	 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+	 */
 	new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
 	old_val = test_bit(vcpu->vcpu_id, dest_map->map);
 
@@ -188,6 +195,13 @@ static void ioapic_lazy_update_eoi(struct kvm_ioapic *ioapic, int irq)
 	union kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];
 
 	kvm_for_each_vcpu(i, vcpu, ioapic->kvm) {
+		/*
+		 * 在以下使用kvm_apic_pending_eoi():
+		 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+		 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+		 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+		 */
 		if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 					 entry->fields.dest_id,
 					 entry->fields.dest_mode) ||
@@ -296,6 +310,13 @@ void kvm_ioapic_scan_entry(struct kvm_vcpu *vcpu, ulong *ioapic_handled_vectors)
 		    index == RTC_GSI) {
 			u16 dm = kvm_lapic_irq_dest_mode(!!e->fields.dest_mode);
 
+			/*
+			 * 在以下使用kvm_apic_pending_eoi():
+			 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+			 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+			 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+			 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+			 */
 			if (kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 						e->fields.dest_id, dm) ||
 			    kvm_apic_pending_eoi(vcpu, e->fields.vector))
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 8136695f7..48773b8f4 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -44,6 +44,16 @@ static int kvm_set_ioapic_irq(struct kvm_kernel_irq_routing_entry *e,
 				line_status);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|508| <<synic_set_irq>> ret = kvm_irq_delivery_to_apic(vcpu->kvm, vcpu->arch.apic, &irq, NULL);
+ *   - arch/x86/kvm/ioapic.c|494| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe,
+ *   - arch/x86/kvm/ioapic.c|498| <<ioapic_service>> ret = kvm_irq_delivery_to_apic(ioapic->kvm, NULL, &irqe, NULL);
+ *   - arch/x86/kvm/irq_comm.c|173| <<kvm_set_msi>> return kvm_irq_delivery_to_apic(kvm, NULL, &irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1740| <<kvm_apic_send_ipi>> kvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);
+ *   - arch/x86/kvm/x86.c|10111| <<kvm_pv_kick_cpu_op>> kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
+ *   - arch/x86/kvm/xen.c|574| <<kvm_xen_inject_vcpu_vector>> kvm_irq_delivery_to_apic(v->kvm, NULL, &irq, NULL);
+ */
 int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, struct dest_map *dest_map)
 {
@@ -101,6 +111,35 @@ int kvm_irq_delivery_to_apic(struct kvm *kvm, struct kvm_lapic *src,
 	return r;
 }
 
+/*
+ * kvm_set_msi_irq
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * kvm_set_msi_irq
+ * kvm_set_msi
+ * kvm_send_userspace_msi
+ * kvm_vm_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 void kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 		     struct kvm_lapic_irq *irq)
 {
@@ -156,6 +195,31 @@ static int kvm_hv_set_sint(struct kvm_kernel_irq_routing_entry *e,
 }
 #endif
 
+/*
+ * kvm_set_msi_irq
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * called by:
+ *   - virt/kvm/eventfd.c|273| <<irqfd_wakeup>> if (kvm_arch_set_irq_inatomic(&irq,
+ *              kvm, KVM_USERSPACE_IRQ_SOURCE_ID, 1, false) == -EWOULDBLOCK)
+ */
 int kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,
 			      struct kvm *kvm, int irq_source_id, int level,
 			      bool line_status)
@@ -424,6 +488,13 @@ void kvm_scan_ioapic_routes(struct kvm_vcpu *vcpu,
 
 			kvm_set_msi_irq(vcpu->kvm, entry, &irq);
 
+			/*
+			 * 在以下调用kvm_apic_pending_eoi():
+			 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+			 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+			 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+			 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+			 */
 			if (irq.trig_mode &&
 			    (kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 						 irq.dest_id, irq.dest_mode) ||
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 3c83951c6..4d1765c49 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -117,6 +117,13 @@ static inline int apic_test_vector(int vec, void *bitmap)
 	return test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+ *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+ *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+ */
 bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -698,6 +705,10 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6930| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -714,6 +725,36 @@ static inline int apic_search_irr(struct kvm_lapic *apic)
 	return find_highest_vector(apic->regs + APIC_IRR);
 }
 
+/*
+ * kvm_write_guest_cached
+ * apic_sync_pv_eoi_from_guest
+ * kvm_lapic_sync_from_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_write_guest_cached
+ * kvm_lapic_sync_to_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|751| <<apic_clear_irr>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|837| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+ *   - arch/x86/kvm/lapic.c|960| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+ *   - arch/x86/kvm/lapic.c|3239| <<kvm_apic_set_state>> apic_find_highest_irr(apic));
+ *   - arch/x86/kvm/lapic.c|3349| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+ */
 static inline int apic_find_highest_irr(struct kvm_lapic *apic)
 {
 	int result;
@@ -776,6 +817,28 @@ static inline void apic_set_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * kvm_write_guest_cached
+ * apic_sync_pv_eoi_from_guest
+ * kvm_lapic_sync_from_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_write_guest_cached
+ * kvm_lapic_sync_to_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static inline int apic_find_highest_isr(struct kvm_lapic *apic)
 {
 	int result;
@@ -816,6 +879,11 @@ static inline void apic_clear_isr(int vec, struct kvm_lapic *apic)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6987| <<vmx_sync_pir_to_irr>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ *   - arch/x86/kvm/x86.c|10432| <<update_cr8_intercept>> max_irr = kvm_lapic_find_highest_irr(vcpu);
+ */
 int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 {
 	/* This may race with setting of irr in __apic_accept_irq() and
@@ -823,6 +891,14 @@ int kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)
 	 * will cause vmexit immediately and the value will be recalculated
 	 * on the next vmentry.
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|751| <<apic_clear_irr>> apic_find_highest_irr(apic));
+	 *   - arch/x86/kvm/lapic.c|837| <<kvm_lapic_find_highest_irr>> return apic_find_highest_irr(vcpu->arch.apic);
+	 *   - arch/x86/kvm/lapic.c|960| <<apic_has_interrupt_for_ppr>> highest_irr = apic_find_highest_irr(apic);
+	 *   - arch/x86/kvm/lapic.c|3239| <<kvm_apic_set_state>> apic_find_highest_irr(apic));
+	 *   - arch/x86/kvm/lapic.c|3349| <<kvm_lapic_sync_to_vapic>> max_irr = apic_find_highest_irr(apic);
+	 */
 	return apic_find_highest_irr(vcpu->arch.apic);
 }
 EXPORT_SYMBOL_GPL(kvm_lapic_find_highest_irr);
@@ -831,11 +907,29 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 			     int vector, int level, int trig_mode,
 			     struct dest_map *dest_map);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|854| <<stimer_notify_direct>> return !kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/hyperv.c|2227| <<kvm_hv_send_ipi_to_many>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ *   - arch/x86/kvm/irq_comm.c|77| <<kvm_irq_delivery_to_apic>> r += kvm_apic_set_irq(vcpu, irq, dest_map);
+ *   - arch/x86/kvm/irq_comm.c|99| <<kvm_irq_delivery_to_apic>> r = kvm_apic_set_irq(lowest, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|932| <<__pv_send_ipi>> count += kvm_apic_set_irq(vcpu, irq, NULL);
+ *   - arch/x86/kvm/lapic.c|1379| <<kvm_irq_delivery_to_apic_fast>> *r = kvm_apic_set_irq(src->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/lapic.c|1392| <<kvm_irq_delivery_to_apic_fast>> *r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);
+ *   - arch/x86/kvm/x86.c|14002| <<kvm_arch_async_page_present>> kvm_apic_set_irq(vcpu, &irq, NULL);
+ */
 int kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,
 		     struct dest_map *dest_map)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|915| <<kvm_apic_set_irq>> return __apic_accept_irq(apic,
+	 *          irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+	 *   - arch/x86/kvm/lapic.c|3054| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic,
+	 *          mode, vector, 1, trig_mode, NULL);
+	 */
 	return __apic_accept_irq(apic, irq->delivery_mode, irq->vector,
 			irq->level, irq->trig_mode, dest_map);
 }
@@ -940,6 +1034,31 @@ static bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)
 	return val;
 }
 
+/*
+ * apic_has_interrupt_for_ppr
+ * kvm_cpu_has_injectable_intr
+ * kvm_check_and_inject_events
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * apic_has_interrupt_for_ppr
+ * kvm_cpu_has_interrupt
+ * kvm_vcpu_has_events
+ * kvm_arch_vcpu_runnable
+ * kvm_vcpu_check_block
+ * kvm_vcpu_halt
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)
 {
 	int highest_irr;
@@ -1216,6 +1335,53 @@ static inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,
 	return true;
 }
 
+/*
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_irq_delivery_to_apic
+ * kvm_set_msi
+ * kvm_send_userspace_msi
+ * kvm_vm_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_irq_delivery_to_apic
+ * kvm_apic_send_ipi
+ * kvm_x2apic_icr_write
+ * handle_fastpath_set_msr_irqoff
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/irq_comm.c|55| <<kvm_irq_delivery_to_apic>> if (kvm_irq_delivery_to_apic_fast(kvm, src, irq, &r, dest_map))
+ *   - arch/x86/kvm/irq_comm.c|229| <<kvm_arch_set_irq_inatomic>> if (kvm_irq_delivery_to_apic_fast(kvm, NULL, &irq, &r, NULL))
+ */
 bool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,
 		struct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)
 {
@@ -1295,6 +1461,69 @@ bool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,
 	return ret;
 }
 
+/*
+ * __apic_accept_irq
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_irq_delivery_to_apic
+ * kvm_set_msi
+ * kvm_send_userspace_msi
+ * kvm_vm_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * __apic_accept_irq
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * __apic_accept_irq
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_irq_delivery_to_apic
+ * kvm_apic_send_ipi
+ * kvm_x2apic_icr_write
+ * handle_fastpath_set_msr_irqoff
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * __apic_accept_irq
+ * kvm_apic_local_deliver
+ * kvm_inject_apic_timer_irqs
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|915| <<kvm_apic_set_irq>> return __apic_accept_irq(apic,
+ *          irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+ *   - arch/x86/kvm/lapic.c|3054| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic,
+ *          mode, vector, 1, trig_mode, NULL);
+ */
 /*
  * Add a pending IRQ into lapic.
  * Return 1 if successfully added and 0 if discarded.
@@ -1336,6 +1565,10 @@ static int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,
 						       apic->regs + APIC_TMR);
 		}
 
+		/*
+		 * vmx_deliver_interrupt()
+		 * svm_deliver_interrupt()
+		 */
 		kvm_x86_call(deliver_interrupt)(apic, delivery_mode,
 						trig_mode, vector);
 		break;
@@ -2836,6 +3069,13 @@ int apic_has_pending_timer(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|2097| <<kvm_apic_inject_pending_timer_irqs>> kvm_apic_local_deliver(apic, APIC_LVTT);
+ *   - rch/x86/kvm/lapic.c|3068| <<kvm_apic_nmi_wd_deliver>> kvm_apic_local_deliver(apic, APIC_LVT0);
+ *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_deliver_pmi>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTPC);
+ *   - arch/x86/kvm/x86.c|5426| <<kvm_vcpu_x86_set_ucna>> kvm_apic_local_deliver(vcpu->arch.apic, APIC_LVTCMCI);
+ */
 int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 {
 	u32 reg = kvm_lapic_get_reg(apic, lvt_type);
@@ -2847,6 +3087,13 @@ int kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)
 		mode = reg & APIC_MODE_MASK;
 		trig_mode = reg & APIC_LVT_LEVEL_TRIGGER;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/lapic.c|915| <<kvm_apic_set_irq>> return __apic_accept_irq(apic,
+		 *          irq->delivery_mode, irq->vector, irq->level, irq->trig_mode, dest_map);
+		 *   - arch/x86/kvm/lapic.c|3054| <<kvm_apic_local_deliver>> r = __apic_accept_irq(apic,
+		 *          mode, vector, 1, trig_mode, NULL);
+		 */
 		r = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);
 		if (r && lvt_type == APIC_LVTPC &&
 		    guest_cpuid_is_intel_compatible(apic->vcpu))
@@ -3187,6 +3434,15 @@ void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
 	if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 		return;
 
+	/*
+	 * 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3369| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3447| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+	 *   - arch/x86/kvm/lapic.c|3455| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+	 */
 	if (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				  sizeof(u32)))
 		return;
@@ -3220,6 +3476,31 @@ static void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,
 	pv_eoi_set_pending(apic->vcpu);
 }
 
+/*
+ * kvm_write_guest_cached
+ * apic_sync_pv_eoi_from_guest
+ * kvm_lapic_sync_from_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_write_guest_cached
+ * kvm_lapic_sync_to_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|11276| <<vcpu_enter_guest>> kvm_lapic_sync_to_vapic(vcpu);
+ */
 void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 {
 	u32 data, tpr;
@@ -3240,13 +3521,35 @@ void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)
 		max_isr = 0;
 	data = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);
 
+	/*
+	 * 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3369| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3447| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+	 *   - arch/x86/kvm/lapic.c|3455| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+	 */
 	kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 				sizeof(u32));
 }
 
+/*
+ * 处理KVM_SET_VAPIC_ADDR:
+ *   - arch/x86/kvm/x86.c|6115| <<kvm_arch_vcpu_ioctl(KVM_SET_VAPIC_ADDR)>> r = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);
+ */
 int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
 {
 	if (vapic_addr) {
+		/*
+		 * 在以下使用kvm_lapic->vapic_cache:
+		 *   - arch/x86/kvm/lapic.c|3369| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm,
+		 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32)))
+		 *   - arch/x86/kvm/lapic.c|3447| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+		 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+		 *   - arch/x86/kvm/lapic.c|3455| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+		 *              &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+		 */
 		if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
 					&vcpu->arch.apic->vapic_cache,
 					vapic_addr, sizeof(u32)))
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 24add38be..630fb6fe2 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -61,6 +61,26 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下使用kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|706| <<kvm_apic_update_irr>> if (unlikely(!apic->apicv_active && irr_updated))
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> if (unlikely(apic->apicv_active)) {
+	 *   - arch/x86/kvm/lapic.c|765| <<apic_set_isr>> if (unlikely(apic->apicv_active))
+	 *   - arch/x86/kvm/lapic.c|810| <<apic_clear_isr>> if (unlikely(apic->apicv_active))
+	 *   - arch/x86/kvm/lapic.c|1798| <<lapic_timer_int_injected>> if (apic->apicv_active)
+	 *   - arch/x86/kvm/lapic.c|1913| <<apic_timer_expired>> if (!from_timer_fn && apic->apicv_active) {
+	 *   - arch/x86/kvm/lapic.c|2672| <<kvm_apic_update_apicv>> if (apic->apicv_active)
+	 *   - arch/x86/kvm/lapic.c|2806| <<kvm_lapic_reset>> if (apic->apicv_active) {
+	 *   - arch/x86/kvm/lapic.c|2942| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/lapic.c|3122| <<kvm_apic_set_state>> if (apic->apicv_active) {
+	 *   - arch/x86/kvm/lapic.h|222| <<kvm_vcpu_apicv_active>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->apicv_active;
+	 *   - arch/x86/kvm/svm/svm.c|3689| <<svm_complete_interrupt_delivery>> if (!READ_ONCE(vcpu->arch.apic->apicv_active)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4280| <<vmx_deliver_posted_interrupt>> if (!vcpu->arch.apic->apicv_active)
+	 *   - arch/x86/kvm/x86.c|10265| <<update_cr8_intercept>> if (vcpu->arch.apic->apicv_active)
+	 *   - arch/x86/kvm/x86.c|10641| <<__kvm_vcpu_update_apicv>> if (apic->apicv_active == activate)
+	 *   - arch/x86/kvm/x86.c|10644| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *   - arch/x86/kvm/x86.c|10654| <<__kvm_vcpu_update_apicv>> if (!apic->apicv_active)
+	 */
 	bool apicv_active;
 	bool sw_enabled;
 	bool irr_pending;
@@ -76,6 +96,15 @@ struct kvm_lapic {
 	 */
 	void *regs;
 	gpa_t vapic_addr;
+	/*
+	 * 在以下使用kvm_lapic->vapic_cache:
+	 *   - arch/x86/kvm/lapic.c|3369| <<kvm_lapic_sync_from_vapic>> if (kvm_read_guest_cached(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32)))
+	 *   - arch/x86/kvm/lapic.c|3447| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, &data, sizeof(u32));
+	 *   - arch/x86/kvm/lapic.c|3455| <<kvm_lapic_set_vapic_addr>> if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
+	 *              &vcpu->arch.apic->vapic_cache, vapic_addr, sizeof(u32)))
+	 */
 	struct gfn_to_hva_cache vapic_cache;
 	unsigned long pending_events;
 	unsigned int sipi_vector;
@@ -155,6 +184,11 @@ static inline void kvm_lapic_set_vector(int vec, void *bitmap)
 	set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * 在以下调用kvm_lapic_set_irr():
+ *   - arch/x86/kvm/svm/svm.c|3767| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ *   - arch/x86/kvm/vmx/vmx.c|4319| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+ */
 static inline void kvm_lapic_set_irr(int vec, struct kvm_lapic *apic)
 {
 	kvm_lapic_set_vector(vec, apic->regs + APIC_IRR);
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index e93223586..f2e3443f2 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -244,6 +244,24 @@ extern bool tdp_mmu_enabled;
 #define tdp_mmu_enabled false
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|100| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1467| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1490| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1577| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1776| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1894| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1907| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4387| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|7475| <<kvm_rmap_zap_gfn_range>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|7537| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7810| <<kvm_mmu_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|7835| <<kvm_mmu_slot_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7908| <<kvm_mmu_recover_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7924| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|13204| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !tdp_mmu_enabled || kvm_shadow_root_allocated(kvm);
@@ -270,6 +288,33 @@ kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|535| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|1526| <<__rmap_add>> kvm_update_page_stats(kvm, sp->role.level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|512| <<handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1341| <<tdp_mmu_split_huge_page>> kvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 2401606db..d28ed13d7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -59,10 +59,65 @@
 
 #include "trace.h"
 
+/*
+ * https://docs.kernel.org/virt/kvm/locking.html
+ *
+ * 在NUMA系统上,不同CPU和内存之间的距离不同,为了提高系统性能,
+ * Linux内核里会进行所谓的NUMA balance,它的目的是动态的调整程序
+ * 运行的CPU或者程序使用的内存,使得CPU和内存尽量在一个NUMA域里.
+ *
+ * 对于内存的调整,内核会周期性的断开VA到PA的页表映射,这样当访问
+ * 内存时就会触发缺页异常,内核在处理缺页异常时进行必要的内存迁移.
+ * 
+ * 需要注意的是,打开NUMA balance后,内核一定会周期性的断开VA到PA的映射,
+ * 对于不需要做内存迁移的情况,内核把页表重新配置好,这个开销相对来
+ * 说是比较小的.
+ */
+
+/*
+ * 在以下使用nx_hugepage_mitigation_hard_disabled:
+ *   - arch/x86/kvm/mmu/mmu.c|62| <<global>> static bool nx_hugepage_mitigation_hard_disabled;
+ *   - arch/x86/kvm/mmu/mmu.c|7720| <<get_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7742| <<set_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7760| <<set_nx_huge_pages>> nx_hugepage_mitigation_hard_disabled = true;
+ *   - arch/x86/kvm/mmu/mmu.c|7889| <<set_nx_huge_pages_recovery_param>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|8059| <<kvm_mmu_post_init_vm>> if (nx_hugepage_mitigation_hard_disabled)
+ *
+ * 默认是0
+ */
 static bool nx_hugepage_mitigation_hard_disabled;
 
+/*
+ * 在以下使用nx_huge_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|64| <<global>> int __read_mostly nx_huge_pages = -1;
+ *   - arch/x86/kvm/mmu/mmu.c|87| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|88| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ *   - arch/x86/kvm/mmu/mmu.c|7235| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ *   - arch/x86/kvm/mmu/mmu.c|7240| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+ *   - arch/x86/kvm/mmu/mmu.c|7295| <<kvm_mmu_x86_module_init>> if (nx_huge_pages == -1)
+ *   - arch/x86/kvm/mmu/mmu.c|7369| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|187| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
+ */
 int __read_mostly nx_huge_pages = -1;
+/*
+ * 在以下使用nx_huge_pages_recovery_period_ms:
+ *   - arch/x86/kvm/mmu/mmu.c|76| <<global>> static uint __read_mostly nx_huge_pages_recovery_period_ms;
+ *   - arch/x86/kvm/mmu/mmu.c|103| <<global>> module_param_cb(nx_huge_pages_recovery_period_ms,
+ *              &nx_huge_pages_recovery_param_ops, &nx_huge_pages_recovery_period_ms, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|105| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
+ *   - arch/x86/kvm/mmu/mmu.c|7874| <<calc_nx_huge_pages_recovery_period>> *period = READ_ONCE(nx_huge_pages_recovery_period_ms);
+ */
 static uint __read_mostly nx_huge_pages_recovery_period_ms;
+/*
+ * 在以下使用nx_huge_pages_recovery_ratio:
+ *   - arch/x86/kvm/mmu/mmu.c|79| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|81| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 60;
+ *   - arch/x86/kvm/mmu/mmu.c|100| <<global>> module_param_cb(nx_huge_pages_recovery_ratio, &nx_huge_pages_recovery_param_ops,
+ *              &nx_huge_pages_recovery_ratio, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|102| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, "uint");
+ *   - arch/x86/kvm/mmu/mmu.c|7869| <<calc_nx_huge_pages_recovery_period>> uint ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+ *   - arch/x86/kvm/mmu/mmu.c|7936| <<kvm_recover_nx_huge_pages>> ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+ */
 #ifdef CONFIG_PREEMPT_RT
 /* Recovery can cause latency spikes, disable it for PREEMPT_RT.  */
 static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
@@ -105,6 +160,12 @@ module_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);
  */
 bool tdp_enabled = false;
 
+/*
+ * 在以下使用tdp_mmu_allowed:
+ *   - arch/x86/kvm/mmu/mmu.c|148| <<global>> static bool __ro_after_init tdp_mmu_allowed;
+ *   - arch/x86/kvm/mmu/mmu.c|6954| <<kvm_configure_mmu>> tdp_mmu_enabled = tdp_mmu_allowed && tdp_enabled;
+ *   - arch/x86/kvm/mmu/mmu.c|7948| <<kvm_mmu_x86_module_init>> tdp_mmu_allowed = tdp_mmu_enabled;
+ */
 static bool __ro_after_init tdp_mmu_allowed;
 
 #ifdef CONFIG_X86_64
@@ -113,7 +174,23 @@ module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0444);
 #endif
 
 static int max_huge_page_level __read_mostly;
+/*
+ * 在以下使用tdp_root_level:
+ *   - arch/x86/kvm/mmu/mmu.c|156| <<global>> static int tdp_root_level __read_mostly;
+ *   - arch/x86/kvm/mmu/mmu.c|6067| <<kvm_mmu_get_tdp_level>> if (tdp_root_level)
+ *   - arch/x86/kvm/mmu/mmu.c|6068| <<kvm_mmu_get_tdp_level>> return tdp_root_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6079| <<kvm_mmu_get_max_tdp_level>> return tdp_root_level ? tdp_root_level : max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6950| <<kvm_configure_mmu>> tdp_root_level = tdp_forced_root_level;
+ */
 static int tdp_root_level __read_mostly;
+/*
+ * 在以下使用max_tdp_level:
+ *   - arch/x86/kvm/mmu/mmu.c|157| <<global>> static int max_tdp_level __read_mostly;
+ *   - arch/x86/kvm/mmu/mmu.c|6071| <<kvm_mmu_get_tdp_level>> if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)
+ *   - arch/x86/kvm/mmu/mmu.c|6074| <<kvm_mmu_get_tdp_level>> return max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6079| <<kvm_mmu_get_max_tdp_level>> return tdp_root_level ? tdp_root_level : max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6951| <<kvm_configure_mmu>> max_tdp_level = tdp_max_root_level;
+ */
 static int max_tdp_level __read_mostly;
 
 #define PTE_PREFETCH_NUM		8
@@ -171,6 +248,11 @@ struct kvm_shadow_walk_iterator {
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3857| <<fast_pf_get_last_sptep>> for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {
+ *   - arch/x86/kvm/mmu/mmu.c|4694| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -288,6 +370,11 @@ static void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)
 	kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3133| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|5573| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -512,6 +599,33 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 	return leaf_spte_change_needs_tlb_flush(old_spte, new_spte);
 }
 
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+ *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+ *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+ */
 /*
  * Rules for using mmu_spte_clear_track_bits:
  * It sets the sptep from present to nonpresent, and track the
@@ -590,6 +704,13 @@ static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4632| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|4758| <<kvm_tdp_mmu_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|5887| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|800| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+ */
 static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu, bool maybe_indirect)
 {
 	int r;
@@ -621,6 +742,11 @@ static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 	kvm_mmu_free_memory_cache(&vcpu->arch.mmu_page_header_cache);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1162| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(head_desc);
+ *   - arch/x86/kvm/mmu/mmu.c|1224| <<kvm_zap_all_rmap_sptes>> mmu_free_pte_list_desc(desc);
+ */
 static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
 {
 	kmem_cache_free(pte_list_desc_cache, pte_list_desc);
@@ -684,6 +810,11 @@ static void kvm_mmu_page_set_translation(struct kvm_mmu_page *sp, int index,
 	          sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2926| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|965| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+ */
 static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
 				    unsigned int access)
 {
@@ -696,6 +827,14 @@ static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
  * Return the pointer to the large page information for a given gfn,
  * handling slots that are not large page aligned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|746| <<update_gfn_disallow_lpage_count>> linfo = lpage_info_slot(gfn, slot, i);
+ *   - arch/x86/kvm/mmu/mmu.c|3191| <<__kvm_mmu_max_mapping_level>> linfo = lpage_info_slot(gfn, slot, max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|7971| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7977| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7983| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+ */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
 		const struct kvm_memory_slot *slot, int level)
 {
@@ -713,6 +852,11 @@ static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
  */
 #define KVM_LPAGE_MIXED_FLAG	BIT(31)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|756| <<kvm_mmu_gfn_disallow_lpage>> update_gfn_disallow_lpage_count(slot, gfn, 1);
+ *   - arch/x86/kvm/mmu/mmu.c|761| <<kvm_mmu_gfn_allow_lpage>> update_gfn_disallow_lpage_count(slot, gfn, -1);
+ */
 static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 					    gfn_t gfn, int count)
 {
@@ -728,16 +872,30 @@ static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|788| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|107| <<__kvm_write_track_add_gfn>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|834| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|130| <<__kvm_write_track_remove_gfn>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2209| <<kvm_mmu_alloc_shadow_page>> account_shadowed(kvm, sp);
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -768,6 +926,11 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 		kvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|818| <<account_nx_huge_page>> track_possible_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1195| <<kvm_tdp_mmu_map>> track_possible_nx_huge_page(kvm, sp);
+ */
 void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	/*
@@ -782,19 +945,65 @@ void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 		return;
 
 	++kvm->stat.nx_lpage_splits;
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+	 *              &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+	 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+	 *
+	 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+	 * replaced by an NX huge page.  A shadow page is on this list if its
+	 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+	 * and there are no other conditions that prevent a huge page, e.g.
+	 * the backing host page is huge, dirtly logging is not enabled for its
+	 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+	 * guarantee an NX huge page will be created in its stead, e.g. if the
+	 * guest attempts to execute from the region then KVM obviously can't
+	 * create an NX huge page (without hanging the guest).
+	 */
 	list_add_tail(&sp->possible_nx_huge_page_link,
 		      &kvm->arch.possible_nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3599| <<direct_map>> account_nx_huge_page(vcpu->kvm, sp,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|792| <<FNAME(fetch)>> account_nx_huge_page(vcpu->kvm, sp,
+ */
 static void account_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				 bool nx_huge_page_possible)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->nx_huge_page_disallowed = true;
 
 	if (nx_huge_page_possible)
 		track_possible_nx_huge_page(kvm, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2684| <<__kvm_mmu_prepare_zap_page>> unaccount_shadowed(kvm, sp);
+ */
 static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -811,6 +1020,11 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|850| <<unaccount_nx_huge_page>> untrack_possible_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|321| <<tdp_mmu_unlink_sp>> untrack_possible_nx_huge_page(kvm, sp);
+ */
 void untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	if (list_empty(&sp->possible_nx_huge_page_link))
@@ -820,13 +1034,40 @@ void untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 	list_del_init(&sp->possible_nx_huge_page_link);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2576| <<__kvm_mmu_prepare_zap_page>> unaccount_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/mmu.c|7862| <<kvm_recover_nx_huge_pages>> unaccount_nx_huge_page(kvm, sp);
+ */
 static void unaccount_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->nx_huge_page_disallowed = false;
 
 	untrack_possible_nx_huge_page(kvm, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2985| <<kvm_mmu_prefetch_sptes>> slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK);
+ */
 static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu,
 							   gfn_t gfn,
 							   bool no_dirty_log)
@@ -851,6 +1092,11 @@ static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu
  */
 #define KVM_RMAP_MANY	BIT(0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1564| <<__rmap_add>> rmap_count = pte_list_add(cache, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1702| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+ */
 /*
  * Returns the number of pointers in the rmap chain, not counting the new one.
  */
@@ -924,9 +1170,20 @@ static void pte_list_desc_remove_entry(struct kvm *kvm,
 		rmap_head->val = 0;
 	else
 		rmap_head->val = (unsigned long)head_desc->more | KVM_RMAP_MANY;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1162| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(head_desc);
+	 *   - arch/x86/kvm/mmu/mmu.c|1224| <<kvm_zap_all_rmap_sptes>> mmu_free_pte_list_desc(desc);
+	 */
 	mmu_free_pte_list_desc(head_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+ */
 static void pte_list_remove(struct kvm *kvm, u64 *spte,
 			    struct kvm_rmap_head *rmap_head)
 {
@@ -958,13 +1215,39 @@ static void pte_list_remove(struct kvm *kvm, u64 *spte,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7907| <<kvm_mmu_zap_collapsible_spte>> kvm_zap_one_rmap_spte(kvm, rmap_head, sptep);
+ */
 static void kvm_zap_one_rmap_spte(struct kvm *kvm,
 				  struct kvm_rmap_head *rmap_head, u64 *sptep)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+	 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+	 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+	 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+	 */
 	mmu_spte_clear_track_bits(kvm, sptep);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(kvm, sptep, rmap_head);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1853| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+ *
+ * struct kvm_rmap_head {
+ *     unsigned long val;
+ * };
+ */
 /* Return true if at least one SPTE was zapped, false otherwise */
 static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 				   struct kvm_rmap_head *rmap_head)
@@ -976,6 +1259,13 @@ static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 		return false;
 
 	if (!(rmap_head->val & KVM_RMAP_MANY)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+		 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+		 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+		 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+		 */
 		mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
 		goto out;
 	}
@@ -983,9 +1273,21 @@ static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 	desc = (struct pte_list_desc *)(rmap_head->val & ~KVM_RMAP_MANY);
 
 	for (; desc; desc = next) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+		 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+		 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+		 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+		 */
 		for (i = 0; i < desc->spte_count; i++)
 			mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
 		next = desc->more;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1162| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(head_desc);
+		 *   - arch/x86/kvm/mmu/mmu.c|1224| <<kvm_zap_all_rmap_sptes>> mmu_free_pte_list_desc(desc);
+		 */
 		mmu_free_pte_list_desc(desc);
 	}
 out:
@@ -1037,6 +1339,12 @@ static void rmap_remove(struct kvm *kvm, u64 *spte)
 	slot = __gfn_to_memslot(slots, gfn);
 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(kvm, spte, rmap_head);
 }
 
@@ -1112,12 +1420,28 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 	return sptep;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1399| <<rmap_write_protect>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1427| <<__rmap_clear_dirty>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1822| <<kvm_rmap_age_gfn_range>> for_each_rmap_spte(iterator.rmap, &iter, sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|1953| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|7691| <<shadow_mmu_try_split_huge_pages>> for_each_rmap_spte(rmap_head, &iter, huge_sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|7815| <<kvm_mmu_zap_collapsible_spte>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ */
 #define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
 	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
 	     _spte_; _spte_ = rmap_get_next(_iter_))
 
 static void drop_spte(struct kvm *kvm, u64 *sptep)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+	 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+	 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+	 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+	 */
 	u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
 
 	if (is_shadow_present_pte(old_spte))
@@ -1256,6 +1580,12 @@ static void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/dirty_ring.c|73| <<kvm_reset_dirty_gfn>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2215| <<kvm_get_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2332| <<kvm_clear_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ */
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
 				gfn_t gfn_offset, unsigned long mask)
@@ -1308,6 +1638,14 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|899| <<account_shadowed>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ *   - arch/x86/kvm/mmu/mmu.c|1507| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1512| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, end, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1564| <<kvm_vcpu_write_protect_gfn>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|109| <<__kvm_write_track_add_gfn>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level)
@@ -1338,9 +1676,23 @@ static bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
 	return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
 }
 
+/*
+ * 在以下使用kvm_zap_rmap():
+ *   - arch/x86/kvm/mmu/mmu.c|1761| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+ *          kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1, can_yield, true, flush);
+ */
 static bool kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			 const struct kvm_memory_slot *slot)
 {
+	/*
+	 * struct kvm_rmap_head {
+	 *     unsigned long val;
+	 * };
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1853| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+	 */
 	return kvm_zap_all_rmap_sptes(kvm, rmap_head);
 }
 
@@ -1406,6 +1758,13 @@ static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
 	rmap_walk_init_level(iterator, iterator->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1671| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level,
+ *                      start_gfn, end_gfn, &iterator) {
+ *   - arch/x86/kvm/mmu/mmu.c|1820| <<kvm_rmap_age_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K,
+ *                      KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator) {
+ */
 #define for_each_slot_rmap_range(_slot_, _start_level_, _end_level_,	\
 	   _start_gfn, _end_gfn, _iter_)				\
 	for (slot_rmap_walk_init(_iter_, _slot_, _start_level_,		\
@@ -1418,6 +1777,17 @@ typedef bool (*slot_rmaps_handler) (struct kvm *kvm,
 				    struct kvm_rmap_head *rmap_head,
 				    const struct kvm_memory_slot *slot);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+ *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+ *              true, flush_on_yield, false); 
+ *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+ *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+ *              can_yield, true, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+ *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+ */
 static __always_inline bool __walk_slot_rmaps(struct kvm *kvm,
 					      const struct kvm_memory_slot *slot,
 					      slot_rmaps_handler fn,
@@ -1430,6 +1800,13 @@ static __always_inline bool __walk_slot_rmaps(struct kvm *kvm,
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1671| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level,
+	 *                      start_gfn, end_gfn, &iterator) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1820| <<kvm_rmap_age_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K,
+	 *                      KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator) {
+	 */
 	for_each_slot_rmap_range(slot, start_level, end_level, start_gfn,
 			end_gfn, &iterator) {
 		if (iterator.rmap)
@@ -1457,6 +1834,17 @@ static __always_inline bool walk_slot_rmaps(struct kvm *kvm,
 					    int start_level, int end_level,
 					    bool flush_on_yield)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+	 *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+	 *              true, flush_on_yield, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+	 *              can_yield, true, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+	 *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+	 */
 	return __walk_slot_rmaps(kvm, slot, fn, start_level, end_level,
 				 slot->base_gfn, slot->base_gfn + slot->npages - 1,
 				 true, flush_on_yield, false);
@@ -1470,16 +1858,43 @@ static __always_inline bool walk_slot_rmaps_4k(struct kvm *kvm,
 	return walk_slot_rmaps(kvm, slot, fn, PG_LEVEL_4K, PG_LEVEL_4K, flush_on_yield);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1777| <<kvm_unmap_gfn_range>> flush = __kvm_rmap_zap_gfn_range(kvm, range->slot,
+ *   - arch/x86/kvm/mmu/mmu.c|7488| <<kvm_rmap_zap_gfn_range>> flush = __kvm_rmap_zap_gfn_range(kvm, memslot, start,
+ */
 static bool __kvm_rmap_zap_gfn_range(struct kvm *kvm,
 				     const struct kvm_memory_slot *slot,
 				     gfn_t start, gfn_t end, bool can_yield,
 				     bool flush)
 {
+	/*
+	 * 在以下使用kvm_zap_rmap():
+	 *   - arch/x86/kvm/mmu/mmu.c|1761| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *          kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1, can_yield, true, flush);
+	 *
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+	 *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+	 *              true, flush_on_yield, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+	 *              can_yield, true, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+	 *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+	 */
 	return __walk_slot_rmaps(kvm, slot, kvm_zap_rmap,
 				 PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
 				 start, end - 1, can_yield, true, flush);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7773| <<kvm_mmu_zap_memslot>> flush = kvm_unmap_gfn_range(kvm, &range);
+ *   - arch/x86/kvm/mmu/mmu.c|8276| <<kvm_arch_pre_set_memory_attributes>> return kvm_unmap_gfn_range(kvm, range);
+ *   - virt/kvm/kvm_main.c|703| <<kvm_mmu_unmap_gfn_range>> return kvm_unmap_gfn_range(kvm, range);
+ */
 bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool flush = false;
@@ -1495,6 +1910,24 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	lockdep_assert_once(kvm->mmu_invalidate_in_progress ||
 			    lockdep_is_held(&kvm->slots_lock));
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/debugfs.c|100| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1467| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1490| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1577| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1776| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1894| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1907| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|4387| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+	 *   - arch/x86/kvm/mmu/mmu.c|7475| <<kvm_rmap_zap_gfn_range>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|7537| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7810| <<kvm_mmu_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|7835| <<kvm_mmu_slot_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7908| <<kvm_mmu_recover_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7924| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/x86.c|13204| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+	 */
 	if (kvm_memslots_have_rmaps(kvm))
 		flush = __kvm_rmap_zap_gfn_range(kvm, range->slot,
 						 range->start, range->end,
@@ -1512,6 +1945,11 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 
 #define RMAP_RECYCLE_THRESHOLD 1000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1544| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|6730| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+ */
 static void __rmap_add(struct kvm *kvm,
 		       struct kvm_mmu_memory_cache *cache,
 		       const struct kvm_memory_slot *slot,
@@ -1523,6 +1961,13 @@ static void __rmap_add(struct kvm *kvm,
 
 	sp = sptep_to_sp(spte);
 	kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|535| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+	 *   - arch/x86/kvm/mmu/mmu.c|1526| <<__rmap_add>> kvm_update_page_stats(kvm, sp->role.level, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|512| <<handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1341| <<tdp_mmu_split_huge_page>> kvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);
+	 */
 	kvm_update_page_stats(kvm, sp->role.level, 1);
 
 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
@@ -1531,16 +1976,30 @@ static void __rmap_add(struct kvm *kvm,
 	if (rmap_count > kvm->stat.max_mmu_rmap_size)
 		kvm->stat.max_mmu_rmap_size = rmap_count;
 	if (rmap_count > RMAP_RECYCLE_THRESHOLD) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+		 *   - arch/x86/kvm/mmu/mmu.c|1853| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+		 */
 		kvm_zap_all_rmap_sptes(kvm, rmap_head);
 		kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2923| <<mmu_set_spte>> rmap_add(vcpu, slot, sptep, gfn, pte_access);
+ */
 static void rmap_add(struct kvm_vcpu *vcpu, const struct kvm_memory_slot *slot,
 		     u64 *spte, gfn_t gfn, unsigned int access)
 {
 	struct kvm_mmu_memory_cache *cache = &vcpu->arch.mmu_pte_list_desc_cache;
 
+	/*
+	 * called by: 
+	 *   - arch/x86/kvm/mmu/mmu.c|1544| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+	 *   - arch/x86/kvm/mmu/mmu.c|6730| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+	 */
 	__rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
 }
 
@@ -1552,6 +2011,13 @@ static bool kvm_rmap_age_gfn_range(struct kvm *kvm,
 	bool young = false;
 	u64 *sptep;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1671| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level,
+	 *                      start_gfn, end_gfn, &iterator) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1820| <<kvm_rmap_age_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K,
+	 *                      KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator) {
+	 */
 	for_each_slot_rmap_range(range->slot, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
 				 range->start, range->end - 1, &iterator) {
 		for_each_rmap_spte(iterator.rmap, &iter, sptep) {
@@ -1621,12 +2087,20 @@ static void kvm_mmu_check_sptes_at_free(struct kvm_mmu_page *sp)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2416| <<kvm_mmu_alloc_shadow_page>> kvm_account_mmu_page(kvm, sp);
+ */
 static void kvm_account_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	kvm->arch.n_used_mmu_pages++;
 	kvm_account_pgtable_pages((void *)sp->spt, +1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2784| <<__kvm_mmu_prepare_zap_page>> kvm_unaccount_mmu_page(kvm, sp);
+ */
 static void kvm_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	kvm->arch.n_used_mmu_pages--;
@@ -1661,6 +2135,12 @@ static void mmu_page_add_parent_pte(struct kvm_mmu_memory_cache *cache,
 static void mmu_page_remove_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 				       u64 *parent_pte)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
 }
 
@@ -2047,6 +2527,10 @@ static void clear_sp_write_flooding_count(u64 *spte)
  * order to read guest page tables.  Direct shadow pages are never
  * unsync, thus @vcpu can be NULL if @role.direct is true.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2371| <<__kvm_mmu_get_shadow_page>> sp = kvm_mmu_find_shadow_page(kvm, vcpu, gfn, sp_list, role);
+ */
 static struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,
 						     struct kvm_vcpu *vcpu,
 						     gfn_t gfn,
@@ -2167,6 +2651,11 @@ static struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2391| <<kvm_mmu_get_shadow_page>> return __kvm_mmu_get_shadow_page(vcpu->kvm, vcpu, &caches, gfn, role);
+ *   - arch/x86/kvm/mmu/mmu.c|7379| <<shadow_mmu_get_sp_for_split>> return __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);
+ */
 /* Note, @vcpu may be NULL if @role.direct is true; see kvm_mmu_find_shadow_page. */
 static struct kvm_mmu_page *__kvm_mmu_get_shadow_page(struct kvm *kvm,
 						      struct kvm_vcpu *vcpu,
@@ -2249,6 +2738,12 @@ static union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+ */
 static struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,
 						 u64 *sptep, gfn_t gfn,
 						 bool direct, unsigned int access)
@@ -2360,6 +2855,12 @@ static void __link_shadow_page(struct kvm *kvm,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2465,6 +2966,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2699| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2749| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|6997| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|7602| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2516,6 +3024,24 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 		zapped_root = !is_obsolete_sp(kvm, sp);
 	}
 
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (sp->nx_huge_page_disallowed)
 		unaccount_nx_huge_page(kvm, sp);
 
@@ -2564,6 +3090,13 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2852| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm,
+ *              KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2877| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm,
+ *              kvm->arch.n_used_mmu_pages - goal_nr_mmu_pages);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2610,6 +3143,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3703| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3835| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4646| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|833| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2637,11 +3177,23 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
  * Changing the number of mmu pages allocated to the vm
  * Note: if goal_nr_mmu_pages is too small, you will get dead lock
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6343| <<kvm_vm_ioctl_set_nr_mmu_pages>> kvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);
+ *   - arch/x86/kvm/x86.c|13398| <<kvm_arch_commit_memory_region>> kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
+ */
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
 {
 	write_lock(&kvm->mmu_lock);
 
 	if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|2852| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm,
+		 *              KVM_REFILL_PAGES - avail);
+		 *   - arch/x86/kvm/mmu/mmu.c|2877| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm,
+		 *              kvm->arch.n_used_mmu_pages - goal_nr_mmu_pages);
+		 */
 		kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
 						  goal_nr_mmu_pages);
 
@@ -2815,6 +3367,15 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+ *              gfn, page_to_pfn(pages[i]), NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *              base_gfn, fault->pfn, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+ *              gw->pte_access, base_gfn, fault->pfn, fault);
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2860,6 +3421,16 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			was_rmapped = 1;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+	 *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+	 *                  fault->prefetch, false, fault->map_writable, &new_spte);
+	 */
 	wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,
 			   false, host_writable, &spte);
 
@@ -2878,8 +3449,16 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 
 	if (!was_rmapped) {
 		WARN_ON_ONCE(ret == RET_PF_SPURIOUS);
+		/*
+		 * 只在此处调用
+		 */
 		rmap_add(vcpu, slot, sptep, gfn, pte_access);
 	} else {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|2926| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|965| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+		 */
 		/* Already rmapped but the pte_access bits may have changed. */
 		kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
 	}
@@ -2887,6 +3466,11 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2976| <<direct_pte_prefetch_many>> return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
+ */
 static bool kvm_mmu_prefetch_sptes(struct kvm_vcpu *vcpu, gfn_t gfn, u64 *sptep,
 				   int nr_pages, unsigned int access)
 {
@@ -2906,6 +3490,15 @@ static bool kvm_mmu_prefetch_sptes(struct kvm_vcpu *vcpu, gfn_t gfn, u64 *sptep,
 		return false;
 
 	for (i = 0; i < nr_pages; i++, gfn++, sptep++) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+		 *              gfn, page_to_pfn(pages[i]), NULL);
+		 *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+		 *              base_gfn, fault->pfn, fault);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+		 *              gw->pte_access, base_gfn, fault->pfn, fault);
+		 */
 		mmu_set_spte(vcpu, slot, sptep, access, gfn,
 			     page_to_pfn(pages[i]), NULL);
 
@@ -2931,6 +3524,11 @@ static bool direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 	gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
 	unsigned int access = sp->role.access;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<direct_pte_prefetch_many>> return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
+	 */
 	return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
 }
 
@@ -3012,6 +3610,10 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
  * the above "rules" ensure KVM will not _consume_ the result of the walk if a
  * race with the primary MMU occurs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3115| <<__kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, slot);
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -3109,11 +3711,37 @@ int kvm_mmu_max_mapping_level(struct kvm *kvm,
 	return __kvm_mmu_max_mapping_level(kvm, slot, gfn, PG_LEVEL_NUM, is_private);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ */
 void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下设置kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 *
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 *
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -3132,9 +3760,27 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->req_level = __kvm_mmu_max_mapping_level(vcpu->kvm, slot,
 						       fault->gfn, fault->max_level,
 						       fault->is_private);
+	/*
+	 * 在以下设置kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 *
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 */
 	if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
 		return;
 
+	/*
+	 * 注释:
+	 * Page size that will be created based on the req_level and
+	 * huge_page_disallowed.
+	 */
 	/*
 	 * mmu_invalidate_retry() was successful and mmu_lock is held, so
 	 * the pmd can't be split from under us.
@@ -3145,8 +3791,32 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->pfn &= ~mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3225| <<direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|721| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1111| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+ */
 void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (cur_level > PG_LEVEL_4K &&
 	    cur_level == fault->goal_level &&
 	    is_shadow_present_pte(spte) &&
@@ -3166,6 +3836,33 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4650| <<direct_page_fault>> r = direct_map(vcpu, fault);
+ */
 static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_shadow_walk_iterator it;
@@ -3173,14 +3870,39 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	int ret;
 	gfn_t base_gfn = fault->gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在3个地方trace这个trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3664| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|736| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1127| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
 		 */
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3225| <<direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|721| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1111| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
 
@@ -3188,11 +3910,36 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		if (it.level == fault->goal_level)
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
 		if (sp == ERR_PTR(-EEXIST))
 			continue;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 */
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		if (fault->huge_page_disallowed)
 			account_nx_huge_page(vcpu->kvm, sp,
 					     fault->req_level >= it.level);
@@ -3201,6 +3948,15 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+	 *              gfn, page_to_pfn(pages[i]), NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+	 *              base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+	 *              gw->pte_access, base_gfn, fault->pfn, fault);
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -3390,6 +4146,39 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * 注释:
+ * Fast page fault:
+ *
+ * Fast page fault is the fast path which fixes the guest page fault out of the
+ * mmu-lock on x86. Currently, the page fault can be fast in one of the
+ * following two cases:
+ *
+ * Access Tracking: The SPTE is not present, but it is marked for access
+ * tracking. That means we need to restore the saved R/X bits. This is
+ * described in more detail later below.
+ *
+ * Write-Protection: The SPTE is present and the fault is caused by
+ * write-protect. That means we just need to change the W bit of the spte.
+ *
+ * What we use to avoid all the races is the Host-writable bit and MMU-writable
+ * bit on the spte:
+ *
+ * Host-writable means the gfn is writable in the host kernel page tables and
+ * in its KVM memslot.
+ *
+ * MMU-writable means the gfn is writable in the guest’s mmu and it is not
+ * write-protected by shadow page write-protection.
+ *
+ * On fast page fault path, we will use cmpxchg to atomically set the spte W
+ * bit if spte.HOST_WRITEABLE = 1 and spte.WRITE_PROTECT = 1, to restore the
+ * saved R/X bits if for an access-traced spte, or both. This is safe because
+ * whenever changing these bits can be detected by cmpxchg.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4628| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4754| <<kvm_tdp_mmu_page_fault>> r = fast_page_fault(vcpu, fault);
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3411,6 +4200,28 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		else
 			sptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
 
+		/*
+		 * 在以下使用FROZEN_SPTE:
+		 *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+		 *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+		 *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+		 *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+		 *                                                    level, shared);
+		 *
+		 * If a thread running without exclusive control of the MMU lock must perform a
+		 * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+		 * non-present intermediate value. Other threads which encounter this value
+		 * should not modify the SPTE.
+		 *
+		 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+		 * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+		 * vulnerability.
+		 *
+		 * Only used by the TDP MMU.
+		 */
 		/*
 		 * It's entirely possible for the mapping to have been zapped
 		 * by a different task, but the root page should always be
@@ -3630,6 +4441,13 @@ void kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_free_guest_mode_roots);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+ *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+ *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+ */
 static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 			    u8 level)
 {
@@ -3665,6 +4483,13 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 		goto out_unlock;
 
 	if (shadow_root_level >= PT64_ROOT_4LEVEL) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
 		mmu->root.hpa = root;
 	} else if (shadow_root_level == PT32E_ROOT_LEVEL) {
@@ -3676,6 +4501,13 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 		for (i = 0; i < 4; ++i) {
 			WARN_ON_ONCE(IS_VALID_PAE_ROOT(mmu->pae_root[i]));
 
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+			 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+			 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+			 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+			 */
 			root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0,
 					      PT32_ROOT_LEVEL);
 			mmu->pae_root[i] = root | PT_PRESENT_MASK |
@@ -3801,6 +4633,13 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	 * write-protect the guests page table root.
 	 */
 	if (mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, root_gfn, 0,
 				      mmu->root_role.level);
 		mmu->root.hpa = root;
@@ -3855,6 +4694,13 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 		 */
 		quadrant = (mmu->cpu_role.base.level == PT32_ROOT_LEVEL) ? i : 0;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
 		mmu->pae_root[i] = root | pm_mask;
 	}
@@ -3981,6 +4827,12 @@ static bool is_unsync_root(hpa_t root)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6464| <<kvm_mmu_load>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|3628| <<kvm_vcpu_flush_tlb_guest>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|10911| <<vcpu_enter_guest>> kvm_mmu_sync_roots(vcpu);
+ */
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -4142,6 +4994,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6158| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4176,6 +5032,12 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/mmu.c|4751| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|795| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 struct kvm_page_fault *fault)
 {
@@ -4236,6 +5098,19 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
 		return;
 
@@ -4251,6 +5126,12 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 	      work->arch.cr3 != kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+	 *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+	 *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+	 */
 	r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
 				  true, NULL, NULL);
 
@@ -4375,6 +5256,12 @@ static int __kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4539| <<direct_page_fault>> r = kvm_mmu_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4630| <<kvm_tdp_mmu_page_fault>> r = kvm_mmu_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|804| <<FNAME(page_fault)>> r = kvm_mmu_faultin_pfn(vcpu, fault, walker.pte_access);
+ */
 static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 			       struct kvm_page_fault *fault, unsigned int access)
 {
@@ -4488,6 +5375,12 @@ static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
  * Returns true if the page fault is stale and needs to be retried, i.e. if the
  * root was invalidated by a memslot update or a relevant mmu_notifier fired.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4643| <<direct_page_fault>> if (is_page_fault_stale(vcpu, fault))
+ *   - arch/x86/kvm/mmu/mmu.c|4769| <<kvm_tdp_mmu_page_fault>> if (is_page_fault_stale(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|830| <<FNAME(page_fault)>> if (is_page_fault_stale(vcpu, fault))
+ */
 static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4517,6 +5410,34 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_invalidate_retry_gfn(vcpu->kvm, fault->mmu_seq, fault->gfn);
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4566| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4669| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	int r;
@@ -4525,6 +5446,12 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (WARN_ON_ONCE(kvm_mmu_is_dummy_root(vcpu->arch.mmu->root.hpa)))
 		return RET_PF_RETRY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/mmu.c|4751| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|795| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+	 */
 	if (page_fault_handle_page_track(vcpu, fault))
 		return RET_PF_WRITE_PROTECTED;
 
@@ -4532,6 +5459,13 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r != RET_PF_INVALID)
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4632| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|4758| <<kvm_tdp_mmu_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|5887| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|800| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+	 */
 	r = mmu_topup_memory_caches(vcpu, false);
 	if (r)
 		return r;
@@ -4546,10 +5480,20 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (is_page_fault_stale(vcpu, fault))
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3703| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|3835| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|4646| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|833| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+	 */
 	r = make_mmu_pages_available(vcpu);
 	if (r)
 		goto out_unlock;
 
+	/*
+	 * 只在此处调用
+	 */
 	r = direct_map(vcpu, fault);
 
 out_unlock:
@@ -4595,6 +5539,17 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 	if (!flags) {
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+		 *                      insn_len);
+		 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+		 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+		 *                      svm->vmcb->control.insn_bytes : NULL,
+		 *                      svm->vmcb->control.insn_len);
+		 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+		 */
 		r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
 				insn_len);
 	} else if (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {
@@ -4611,6 +5566,30 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
 #ifdef CONFIG_X86_64
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ */
 static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 				  struct kvm_page_fault *fault)
 {
@@ -4659,6 +5638,36 @@ bool kvm_mmu_may_ignore_guest_pat(void)
 	return shadow_memtype_mask;
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ * 在以下使用kvm_tdp_page_fault():
+ *  - arch/x86/kvm/mmu/mmu.c|4681| <<kvm_tdp_map_page>> if (vcpu->arch.mmu->page_fault != kvm_tdp_page_fault)
+ *  - arch/x86/kvm/mmu/mmu.c|5450| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *  - arch/x86/kvm/mmu/mmu_internal.h|306| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *  - arch/x86/kvm/mmu/mmu_internal.h|325| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 #ifdef CONFIG_X86_64
@@ -4669,6 +5678,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4799| <<kvm_arch_vcpu_pre_fault_memory>> r = kvm_tdp_map_page(vcpu, range->gpa, error_code, &level);
+ */
 static int kvm_tdp_map_page(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code,
 			    u8 *level)
 {
@@ -4685,6 +5698,12 @@ static int kvm_tdp_map_page(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code,
 		if (signal_pending(current))
 			return -EINTR;
 		cond_resched();
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+		 *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+		 *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
 	} while (r == RET_PF_RETRY);
 
@@ -4728,6 +5747,19 @@ long kvm_arch_vcpu_pre_fault_memory(struct kvm_vcpu *vcpu,
 	if (r)
 		return r;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	if (kvm_arch_has_private_mem(vcpu->kvm) &&
 	    kvm_mem_is_private(vcpu->kvm, gpa_to_gfn(range->gpa)))
 		error_code |= PFERR_PRIVATE_ACCESS;
@@ -4736,6 +5768,9 @@ long kvm_arch_vcpu_pre_fault_memory(struct kvm_vcpu *vcpu,
 	 * Shadow paging uses GVA for kvm page fault, so restrict to
 	 * two-dimensional paging.
 	 */
+	/*
+	 * 只在此处调用
+	 */
 	r = kvm_tdp_map_page(vcpu, range->gpa, error_code, &level);
 	if (r < 0)
 		return r;
@@ -5885,6 +6920,10 @@ static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/page_track.h|55| <<kvm_page_track_write>> kvm_mmu_track_write(vcpu, gpa, new, bytes);
+ */
 void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			 int bytes)
 {
@@ -5946,6 +6985,10 @@ static bool is_write_to_guest_page_table(u64 error_code)
 	return (error_code & mask) == mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6111| <<kvm_mmu_page_fault>> r = kvm_mmu_write_protect_fault(vcpu, cr2_or_gpa, error_code,
+ */
 static int kvm_mmu_write_protect_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				       u64 error_code, int *emulation_type)
 {
@@ -6039,6 +7082,17 @@ static int kvm_mmu_write_protect_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return RET_PF_EMULATE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *                      insn_len);
+ *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                      svm->vmcb->control.insn_bytes : NULL, 
+ *                      svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -6048,6 +7102,19 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (WARN_ON_ONCE(!VALID_PAGE(vcpu->arch.mmu->root.hpa)))
 		return RET_PF_RETRY;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	/*
 	 * Except for reserved faults (emulated MMIO is shared-only), set the
 	 * PFERR_PRIVATE_ACCESS flag for software-protected VMs based on the gfn's
@@ -6067,6 +7134,9 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
 			return -EFAULT;
 
+		/*
+		 * 只在此处调用
+		 */
 		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
 		if (r == RET_PF_EMULATE)
 			goto emulate;
@@ -6075,6 +7145,12 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (r == RET_PF_INVALID) {
 		vcpu->stat.pf_taken++;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+		 *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+		 *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
 					  &emulation_type, NULL);
 		if (KVM_BUG_ON(r == RET_PF_INVALID, vcpu->kvm))
@@ -6084,6 +7160,9 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (r < 0)
 		return r;
 
+	/*
+	 * 只在此处调用kvm_mmu_write_protect_fault()
+	 */
 	if (r == RET_PF_WRITE_PROTECTED)
 		r = kvm_mmu_write_protect_fault(vcpu, cr2_or_gpa, error_code,
 						&emulation_type);
@@ -6099,6 +7178,12 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		return 1;
 
 emulate:
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
 				       insn_len);
 }
@@ -6230,6 +7315,13 @@ void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5353| <<svm_hardware_setup>> kvm_configure_mmu(npt_enabled, get_npt_level(),
+ *              get_npt_level(), PG_LEVEL_1G);
+ *   - arch/x86/kvm/vmx/vmx.c|8542| <<vmx_hardware_setup>> kvm_configure_mmu(enable_ept, 0, vmx_get_max_ept_level(),
+ *              ept_caps_to_lpage_level(vmx_capability.ept));
+ */
 void kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,
 		       int tdp_max_root_level, int tdp_huge_page_level)
 {
@@ -6265,6 +7357,11 @@ static void free_mmu_pages(struct kvm_mmu *mmu)
 	free_page((unsigned long)mmu->pml5_root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6362| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+ */
 static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 {
 	struct page *page;
@@ -6318,6 +7415,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12424| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -6336,6 +7437,11 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6362| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+	 */
 	ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
 	if (ret)
 		return ret;
@@ -6474,6 +7580,25 @@ void kvm_mmu_init_vm(struct kvm *kvm)
 {
 	kvm->arch.shadow_mmio_value = shadow_mmio_value;
 	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	/*
+	 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+	 * replaced by an NX huge page.  A shadow page is on this list if its
+	 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+	 * and there are no other conditions that prevent a huge page, e.g.
+	 * the backing host page is huge, dirtly logging is not enabled for its
+	 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+	 * guarantee an NX huge page will be created in its stead, e.g. if the
+	 * guest attempts to execute from the region then KVM obviously can't
+	 * create an NX huge page (without hanging the guest).
+	 *
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+	 *              &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+	 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
 	spin_lock_init(&kvm->arch.mmu_unsync_pages_lock);
 
@@ -6642,6 +7767,10 @@ static int topup_split_caches(struct kvm *kvm)
 	return kvm_mmu_topup_memory_cache(&kvm->arch.split_shadow_page_cache, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7399| <<shadow_mmu_split_huge_page>> sp = shadow_mmu_get_sp_for_split(kvm, huge_sptep);
+ */
 static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *huge_sptep)
 {
 	struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
@@ -6669,6 +7798,10 @@ static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *hu
 	return __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6773| <<shadow_mmu_try_split_huge_page>> shadow_mmu_split_huge_page(kvm, slot, huge_sptep);
+ */
 static void shadow_mmu_split_huge_page(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot,
 				       u64 *huge_sptep)
@@ -6715,6 +7848,10 @@ static void shadow_mmu_split_huge_page(struct kvm *kvm,
 	__link_shadow_page(kvm, cache, huge_sptep, sp, flush);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6809| <<shadow_mmu_try_split_huge_pages>> r = shadow_mmu_try_split_huge_page(kvm, slot, huge_sptep);
+ */
 static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
 					  const struct kvm_memory_slot *slot,
 					  u64 *huge_sptep)
@@ -6754,6 +7891,10 @@ static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6840| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot, shadow_mmu_try_split_huge_pages,
+ */
 static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 					    struct kvm_rmap_head *rmap_head,
 					    const struct kvm_memory_slot *slot)
@@ -6800,6 +7941,11 @@ static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6854| <<kvm_mmu_try_split_huge_pages>> kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
+ *   - arch/x86/kvm/mmu/mmu.c|6876| <<kvm_mmu_slot_try_split_huge_pages>> kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
+ */
 static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 						const struct kvm_memory_slot *slot,
 						gfn_t start, gfn_t end,
@@ -6807,6 +7953,17 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 {
 	int level;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+	 *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+	 *              true, flush_on_yield, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+	 *              can_yield, true, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+	 *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+	 */
 	/*
 	 * Split huge pages starting with KVM_MAX_HUGEPAGE_LEVEL and working
 	 * down to the target level. This ensures pages are recursively split
@@ -6838,6 +7995,10 @@ void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13296| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_try_split_huge_pages(kvm, new, PG_LEVEL_4K);
+ */
 void kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,
 					const struct kvm_memory_slot *memslot,
 					int target_level)
@@ -6963,6 +8124,10 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7860| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ */
 static void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -7234,6 +8399,12 @@ void kvm_mmu_vendor_module_exit(void)
  * Calculate the effective recovery period, accounting for '0' meaning "let KVM
  * select a halving time of 1 hour".  Returns true if recovery is enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7892| <<set_nx_huge_pages_recovery_param>> was_recovery_enabled = calc_nx_huge_pages_recovery_period(&old_period);
+ *   - arch/x86/kvm/mmu/mmu.c|7898| <<set_nx_huge_pages_recovery_param>> is_recovery_enabled = calc_nx_huge_pages_recovery_period(&new_period);
+ *   - arch/x86/kvm/mmu/mmu.c|8039| <<kvm_nx_huge_page_recovery_worker>> enabled = calc_nx_huge_pages_recovery_period(&period);
+ */
 static bool calc_nx_huge_pages_recovery_period(uint *period)
 {
 	/*
@@ -7246,6 +8417,14 @@ static bool calc_nx_huge_pages_recovery_period(uint *period)
 	if (!enabled || !ratio)
 		return false;
 
+	/*
+	 * 在以下使用nx_huge_pages_recovery_period_ms:
+	 *   - arch/x86/kvm/mmu/mmu.c|76| <<global>> static uint __read_mostly nx_huge_pages_recovery_period_ms;
+	 *   - arch/x86/kvm/mmu/mmu.c|103| <<global>> module_param_cb(nx_huge_pages_recovery_period_ms,
+	 *              &nx_huge_pages_recovery_param_ops, &nx_huge_pages_recovery_period_ms, 0644);
+	 *   - arch/x86/kvm/mmu/mmu.c|105| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
+	 *   - arch/x86/kvm/mmu/mmu.c|7874| <<calc_nx_huge_pages_recovery_period>> *period = READ_ONCE(nx_huge_pages_recovery_period_ms);
+	 */
 	*period = READ_ONCE(nx_huge_pages_recovery_period_ms);
 	if (!*period) {
 		/* Make sure the period is not less than one second.  */
@@ -7287,8 +8466,20 @@ static int set_nx_huge_pages_recovery_param(const char *val, const struct kernel
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8052| <<kvm_nx_huge_page_recovery_worker>> kvm_recover_nx_huge_pages(kvm);
+ */
 static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_vm_stat->nx_lpage_splits:
+	 *   - arch/x86/kvm/x86.c|244| <<global>> STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+	 *   - arch/x86/kvm/mmu/mmu.c|839| <<track_possible_nx_huge_page>> ++kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|897| <<untrack_possible_nx_huge_page>> --kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7917| <<kvm_recover_nx_huge_pages>> unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7937| <<kvm_recover_nx_huge_pages>> to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
+	 */
 	unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
 	struct kvm_memory_slot *slot;
 	int rcu_idx;
@@ -7308,9 +8499,38 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 	 */
 	rcu_read_lock();
 
+	/*
+	 * 在以下使用nx_huge_pages_recovery_ratio:
+	 *   - arch/x86/kvm/mmu/mmu.c|79| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|81| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 60;
+	 *   - arch/x86/kvm/mmu/mmu.c|100| <<global>> module_param_cb(nx_huge_pages_recovery_ratio, &nx_huge_pages_recovery_param_ops,
+	 *              &nx_huge_pages_recovery_ratio, 0644);
+	 *   - arch/x86/kvm/mmu/mmu.c|102| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, "uint");
+	 *   - arch/x86/kvm/mmu/mmu.c|7869| <<calc_nx_huge_pages_recovery_period>> uint ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+	 *   - arch/x86/kvm/mmu/mmu.c|7936| <<kvm_recover_nx_huge_pages>> ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+	 */
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+		 * replaced by an NX huge page.  A shadow page is on this list if its
+		 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+		 * and there are no other conditions that prevent a huge page, e.g.
+		 * the backing host page is huge, dirtly logging is not enabled for its
+		 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+		 * guarantee an NX huge page will be created in its stead, e.g. if the
+		 * guest attempts to execute from the region then KVM obviously can't
+		 * create an NX huge page (without hanging the guest).
+		 *
+		 * 在以下使用kvm_arch->possible_nx_huge_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+		 *              &kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+		 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+		 */
 		if (list_empty(&kvm->arch.possible_nx_huge_pages))
 			break;
 
@@ -7324,6 +8544,24 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 		sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
 				      struct kvm_mmu_page,
 				      possible_nx_huge_page_link);
+		/*
+		 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+		 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+		 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+		 *
+		 * The shadow page can't be replaced by an equivalent huge page
+		 * because it is being used to map an executable page in the guest
+		 * and the NX huge page mitigation is enabled.
+		 */
 		WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
 		WARN_ON_ONCE(!sp->role.direct);
 
@@ -7417,6 +8655,23 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 		return 0;
 
 	kvm->arch.nx_huge_page_last = get_jiffies_64();
+	/*
+	 * 在以下使用kvm_arch->nx_huge_page_recovery_thread:
+	 *   - arch/x86/kvm/mmu/mmu.c|7778| <<set_nx_huge_pages>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7907| <<set_nx_huge_pages_recovery_param>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8071| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread = vhost_task_create(
+	 *   - arch/x86/kvm/mmu/mmu.c|8075| <<kvm_mmu_post_init_vm>> if (!kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8078| <<kvm_mmu_post_init_vm>> vhost_task_start(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8084| <<kvm_mmu_pre_destroy_vm>> if (kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8085| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7420| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread =
+	 *            vhost_task_create(kvm_nx_huge_page_recovery_worker,
+	 *                              kvm_nx_huge_page_recovery_worker_kill, kvm, "kvm-nx-lpage-recovery");
+	 * drivers/vhost/vhost.c|701| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+	 *            vhost_worker_killed, worker, name);
+	 */
 	kvm->arch.nx_huge_page_recovery_thread = vhost_task_create(
 		kvm_nx_huge_page_recovery_worker, kvm_nx_huge_page_recovery_worker_kill,
 		kvm, "kvm-nx-lpage-recovery");
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index b00abbe3f..83a5e0e59 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -71,6 +71,24 @@ struct kvm_mmu_page {
 	  * because it is being used to map an executable page in the guest
 	  * and the NX huge page mitigation is enabled.
 	  */
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	bool nx_huge_page_disallowed;
 
 	/*
@@ -78,6 +96,11 @@ struct kvm_mmu_page {
 	 * hash table.
 	 */
 	union kvm_mmu_page_role role;
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 * 这个值就是当前shadow页表对应的虚机页表在guest os中的gfn
+	 */
 	gfn_t gfn;
 
 	u64 *spt;
@@ -184,6 +207,17 @@ unsigned int pte_list_count(struct kvm_rmap_head *rmap_head);
 extern int nx_huge_pages;
 static inline bool is_nx_huge_page_enabled(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|64| <<global>> int __read_mostly nx_huge_pages = -1;
+	 *   - arch/x86/kvm/mmu/mmu.c|87| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+	 *   - arch/x86/kvm/mmu/mmu.c|88| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+	 *   - arch/x86/kvm/mmu/mmu.c|7235| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+	 *   - arch/x86/kvm/mmu/mmu.c|7240| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|7295| <<kvm_mmu_x86_module_init>> if (nx_huge_pages == -1)
+	 *   - arch/x86/kvm/mmu/mmu.c|7369| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|187| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
+	 */
 	return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
 }
 
@@ -203,12 +237,33 @@ struct kvm_page_fault {
 	/* Derived from mmu and global state.  */
 	const bool is_tdp;
 	const bool is_private;
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	const bool nx_huge_page_workaround_enabled;
 
 	/*
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
 	 */
+	/*
+	 * 在以下设置kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 *
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 */
 	bool huge_page_disallowed;
 
 	/*
@@ -290,10 +345,29 @@ static inline void kvm_mmu_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 				      fault->is_private);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+ *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+ *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u64 err, bool prefetch,
 					int *emulation_type, u8 *level)
 {
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	struct kvm_page_fault fault = {
 		.addr = cr2_or_gpa,
 		.error_code = err,
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index f4711674c..125b5e0bd 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -543,6 +543,11 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	pte_access = sp->role.access & FNAME(gpte_access)(gpte);
 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<direct_pte_prefetch_many>> return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
+	 */
 	return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
 }
 
@@ -659,6 +664,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 		table_gfn = gw->table_gfn[it.level - 2];
 		access = gw->pt_access[it.level - 2];
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn,
 					  false, access);
 
@@ -695,6 +706,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 		if (FNAME(gpte_changed)(vcpu, gw, it.level - 1))
 			return RET_PF_RETRY;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 */
 		if (sp != ERR_PTR(-EEXIST))
 			link_shadow_page(vcpu, it.sptep, sp);
 
@@ -708,11 +725,31 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	 * are being shadowed by KVM, i.e. allocating a new shadow page may
 	 * affect the allowed hugepage size.
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在3个地方trace这个trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3664| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|736| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1127| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 */
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
@@ -726,12 +763,37 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 		validate_direct_spte(vcpu, it.sptep, direct_access);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn,
 					  true, direct_access);
 		if (sp == ERR_PTR(-EEXIST))
 			continue;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 */
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		if (fault->huge_page_disallowed)
 			account_nx_huge_page(vcpu->kvm, sp,
 					     fault->req_level >= it.level);
@@ -740,6 +802,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+	 *              gfn, page_to_pfn(pages[i]), NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+	 *              base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+	 *              gw->pte_access, base_gfn, fault->pfn, fault);
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -943,6 +1014,11 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	if (kvm_mmu_page_get_access(sp, i) == pte_access)
 		return 0;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2926| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|965| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+	 */
 	/* Update the shadowed access bits in case they changed. */
 	kvm_mmu_page_set_access(sp, i, pte_access);
 
@@ -950,6 +1026,16 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	spte = *sptep;
 	host_writable = spte & shadow_host_writable_mask;
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+	 *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+	 *                  fault->prefetch, false, fault->map_writable, &new_spte);
+	 */
 	make_spte(vcpu, sp, slot, pte_access, gfn,
 		  spte_to_pfn(spte), spte, true, true,
 		  host_writable, &spte);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 22551e2f1..e7c13f311 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -150,6 +150,16 @@ bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+ *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+ *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+ *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+ *                  fault->prefetch, false, fault->map_writable, &new_spte);
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
@@ -414,6 +424,28 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 				  SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
 		mmio_value = 0;
 
+	/*
+	 * 在以下使用FROZEN_SPTE:
+	 *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+	 *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+	 *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+	 *                                                    level, shared);
+	 *
+	 * If a thread running without exclusive control of the MMU lock must perform a
+	 * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+	 * non-present intermediate value. Other threads which encounter this value
+	 * should not modify the SPTE.
+	 *
+	 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+	 * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+	 * vulnerability.
+	 *
+	 * Only used by the TDP MMU.
+	 */
 	/*
 	 * The masked MMIO value must obviously match itself and a frozen SPTE
 	 * must not get a false positive.  Frozen SPTEs and MMIO SPTEs should
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index af10bc038..27d02909f 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -221,11 +221,41 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用FROZEN_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+ *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+ *                                                    level, shared);
+ *
+ * If a thread running without exclusive control of the MMU lock must perform a
+ * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+ * non-present intermediate value. Other threads which encounter this value
+ * should not modify the SPTE.
+ *
+ * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+ * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+ * vulnerability.
+ *
+ * Only used by the TDP MMU.
+ */
 #define FROZEN_SPTE	(SHADOW_NONPRESENT_VALUE | 0x5a0ULL)
 
 /* Frozen SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|387| <<handle_removed_pt>> if (!is_frozen_spte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|518| <<handle_changed_spte>> !is_frozen_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|554| <<__tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_frozen_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|629| <<tdp_mmu_set_spte>> WARN_ON_ONCE(is_frozen_spte(old_spte) || is_frozen_spte(new_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> if (is_frozen_spte(iter.old_spte))
+ */
 static inline bool is_frozen_spte(u64 spte)
 {
 	return spte == FROZEN_SPTE;
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 2f15e0e33..6f33b6739 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -313,6 +313,24 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	tdp_unaccount_mmu_page(kvm, sp);
 
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (!sp->nx_huge_page_disallowed)
 		return;
 
@@ -339,6 +357,10 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|540| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ */
 static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 {
 	struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
@@ -365,6 +387,28 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			 * value to the frozen SPTE value.
 			 */
 			for (;;) {
+				/*
+				 * 在以下使用FROZEN_SPTE:
+				 *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+				 *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+				 *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+				 *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+				 *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+				 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+				 *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+				 *                                                    level, shared);
+				 *
+				 * If a thread running without exclusive control of the MMU lock must perform a
+				 * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+				 * non-present intermediate value. Other threads which encounter this value
+				 * should not modify the SPTE.
+				 *
+				 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+				 * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+				 * vulnerability.
+				 *
+				 * Only used by the TDP MMU.
+				 */
 				old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
 				if (!is_frozen_spte(old_spte))
 					break;
@@ -415,6 +459,16 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte,
 							  FROZEN_SPTE, level);
 		}
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+		 *
+		 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+		 * dirty logging updates are handled in common code, not here (see make_spte()
+		 * and fast_pf_fix_direct_spte()).
+		 */
 		handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
 				    old_spte, FROZEN_SPTE, level, shared);
 	}
@@ -438,6 +492,16 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
  * dirty logging updates are handled in common code, not here (see make_spte()
  * and fast_pf_fix_direct_spte()).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+ *
+ * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+ * dirty logging updates are handled in common code, not here (see make_spte()
+ * and fast_pf_fix_direct_spte()).
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -522,6 +586,10 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 		handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|649| <<tdp_mmu_set_spte_atomic>> ret = __tdp_mmu_set_spte_atomic(iter, new_spte);
+ */
 static inline int __must_check __tdp_mmu_set_spte_atomic(struct tdp_iter *iter,
 							 u64 new_spte)
 {
@@ -565,6 +633,15 @@ static inline int __must_check __tdp_mmu_set_spte_atomic(struct tdp_iter *iter,
  *            no side-effects other than setting iter->old_spte to the last
  *            known value of the spte.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|822| <<__tdp_mmu_zap_root>> else if (tdp_mmu_set_spte_atomic(kvm, &iter, SHADOW_NONPRESENT_VALUE))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1100| <<tdp_mmu_map_handle_target_level>> else if (tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1148| <<tdp_mmu_link_sp>> ret = tdp_mmu_set_spte_atomic(kvm, iter, spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1451| <<wrprot_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1671| <<clear_dirty_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, iter.old_spte & ~dbit))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1831| <<recover_huge_pages_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, huge_spte))
+ */
 static inline int __must_check tdp_mmu_set_spte_atomic(struct kvm *kvm,
 						       struct tdp_iter *iter,
 						       u64 new_spte)
@@ -577,6 +654,16 @@ static inline int __must_check tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 *
+	 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+	 * dirty logging updates are handled in common code, not here (see make_spte()
+	 * and fast_pf_fix_direct_spte()).
+	 */
 	handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			    new_spte, iter->level, true);
 
@@ -596,6 +683,16 @@ static inline int __must_check tdp_mmu_set_spte_atomic(struct kvm *kvm,
  * Returns the old SPTE value, which _may_ be different than @old_spte if the
  * SPTE had voldatile bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|641| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id,
+ *              iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|809| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp),
+ *              sp->ptep, old_spte, SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
+ *
+ * Returns the old SPTE value, which _may_ be different than @old_spte if the
+ * SPTE had voldatile bits.
+ */
 static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 			    u64 old_spte, u64 new_spte, gfn_t gfn, int level)
 {
@@ -612,6 +709,16 @@ static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 
 	old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 *
+	 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+	 * dirty logging updates are handled in common code, not here (see make_spte()
+	 * and fast_pf_fix_direct_spte()).
+	 */
 	handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
 	return old_spte;
 }
@@ -620,6 +727,16 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					 u64 new_spte)
 {
 	WARN_ON_ONCE(iter->yielded);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|641| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id,
+	 *              iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|809| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp),
+	 *              sp->ptep, old_spte, SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
+	 *
+	 * Returns the old SPTE value, which _may_ be different than @old_spte if the
+	 * SPTE had voldatile bits.
+	 */
 	iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep,
 					  iter->old_spte, new_spte,
 					  iter->gfn, iter->level);
@@ -788,6 +905,16 @@ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	if (WARN_ON_ONCE(!is_shadow_present_pte(old_spte)))
 		return false;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|641| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id,
+	 *              iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|809| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp),
+	 *              sp->ptep, old_spte, SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
+	 *
+	 * Returns the old SPTE value, which _may_ be different than @old_spte if the
+	 * SPTE had voldatile bits.
+	 */
 	tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte,
 			 SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
 
@@ -990,6 +1117,16 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 	    is_last_spte(iter->old_spte, iter->level))
 		return RET_PF_SPURIOUS;
 
+	/*
+	 * 在以下调用make_spte():
+	 *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+	 *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+	 *                  fault->prefetch, false, fault->map_writable, &new_spte);
+	 */
 	if (unlikely(!fault->slot))
 		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
 	else
@@ -1062,6 +1199,34 @@ static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 				   struct kvm_mmu_page *sp, bool shared);
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5374| <<kvm_tdp_mmu_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 /*
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
@@ -1074,8 +1239,20 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	struct kvm_mmu_page *sp;
 	int ret = RET_PF_RETRY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在3个地方trace这个trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3664| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|736| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1127| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	rcu_read_lock();
@@ -1083,6 +1260,14 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
 		int r;
 
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
 
@@ -1108,6 +1293,36 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		sp = tdp_mmu_alloc_sp(vcpu);
 		tdp_mmu_init_child_sp(sp, &iter);
 
+		/*
+		 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+		 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+		 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+		 *
+		 * The shadow page can't be replaced by an equivalent huge page
+		 * because it is being used to map an executable page in the guest
+		 * and the NX huge page mitigation is enabled.
+		 *
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
 
 		if (is_shadow_present_pte(iter.old_spte))
@@ -1124,6 +1339,19 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 			goto retry;
 		}
 
+		/*
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		if (fault->huge_page_disallowed &&
 		    fault->req_level >= iter.level) {
 			spin_lock(&kvm->arch.tdp_mmu_pages_lock);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 47a46283c..7204dd98a 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,15 +23,80 @@
 #include "lapic.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - tools/testing/selftests/kvm/include/x86_64/pmu.h|10| <<global>> #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|30| <<global>> __u64 events[KVM_PMU_EVENT_FILTER_MAX_EVENTS];
+ *   - arch/x86/kvm/pmu.c|1004| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|22| <<PMU_EVENT_FILTER_INVALID_NEVENTS>> #define PMU_EVENT_FILTER_INVALID_NEVENTS
+ *               (KVM_PMU_EVENT_FILTER_MAX_EVENTS + 1)
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|649| <<test_masked_events>> int nevents = KVM_PMU_EVENT_FILTER_MAX_EVENTS - MAX_TEST_EVENTS;
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|650| <<test_masked_events>> uint64_t events[KVM_PMU_EVENT_FILTER_MAX_EVENTS];
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|656| <<test_masked_events>> add_dummy_events(events, KVM_PMU_EVENT_FILTER_MAX_EVENTS);
+ */
 /* This is enough to filter the vast majority of currently defined events. */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/cpuid.c|1064| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|1065| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|1066| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|1067| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len;
+ *   - arch/x86/kvm/cpuid.c|1068| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|1069| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|1071| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|1077| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/cpuid.c|1403| <<__do_cpuid_func>> ebx.split.num_core_pmc = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/pmu.h|198| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|206| <<kvm_init_pmu_capability>> if (!kvm_pmu_cap.num_counters_gp || WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|207| <<kvm_init_pmu_capability>> if (!kvm_pmu_cap.num_counters_gp || WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|209| <<kvm_init_pmu_capability>> else if (is_intel && !kvm_pmu_cap.version)
+ *   - arch/x86/kvm/pmu.h|214| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|218| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|219| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_gp = min(kvm_pmu_cap.num_counters_gp, pmu_ops->MAX_NR_GP_COUNTERS);
+ *   - arch/x86/kvm/pmu.h|221| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed, KVM_MAX_NR_FIXED_COUNTERS);
+ *   - arch/x86/kvm/svm/pmu.c|199| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int, pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5274| <<svm_set_cpu_caps>> if (kvm_pmu_cap.num_counters_gp < AMD64_NUM_COUNTERS_CORE)
+ *   - arch/x86/kvm/svm/svm.c|5275| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5276| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5280| <<svm_set_cpu_caps>> if (kvm_pmu_cap.version != 2 || !kvm_cpu_cap_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/vmx/capabilities.h|393| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|447| <<intel_get_fixed_pmc_eventsel>> WARN_ON_ONCE(!eventsel && index < kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|491| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|493| <<intel_pmu_refresh>> eax.split.bit_width = min_t(int, eax.split.bit_width, kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> eax.split.mask_length = min_t(int, eax.split.mask_length, kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min_t(int, edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<intel_pmu_refresh>> edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed, kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|7437| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_PERFCTR0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7443| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_EVENTSEL0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7449| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_FIXED_CTR0 >= kvm_pmu_cap.num_counters_fixed)
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
+/*
+ * 在以下使用kvm_pmu_eventsel:
+ *   - arch/x86/kvm/pmu.c|78| <<global>> struct kvm_pmu_emulated_event_selectors __read_mostly kvm_pmu_eventsel;
+ *   - arch/x86/kvm/pmu.h|233| <<kvm_init_pmu_capability>> kvm_pmu_eventsel.INSTRUCTIONS_RETIRED = perf_get_hw_event_config(PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/pmu.h|235| <<kvm_init_pmu_capability>> kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED = perf_get_hw_event_config(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/vmx/nested.c|3687| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|8945| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9276| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9278| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *
+ * struct kvm_pmu_emulated_event_selectors {
+ *     u64 INSTRUCTIONS_RETIRED;
+ *     u64 BRANCH_INSTRUCTIONS_RETIRED;
+ * };
+ */
 struct kvm_pmu_emulated_event_selectors __read_mostly kvm_pmu_eventsel;
 EXPORT_SYMBOL_GPL(kvm_pmu_eventsel);
 
+/*
+ * 在以下使用vmx_pebs_pdir_cpu[]:
+ *   - arch/x86/kvm/pmu.c|216| <<pmc_get_pebs_precise_level>> (pmc->idx == 32 && x86_match_cpu(vmx_pebs_pdir_cpu)))
+ */
 /* Precise Distribution of Instructions Retired (PDIR) */
 static const struct x86_cpu_id vmx_pebs_pdir_cpu[] = {
 	X86_MATCH_VFM(INTEL_ICELAKE_D, NULL),
@@ -77,6 +142,20 @@ static const struct x86_cpu_id vmx_pebs_pdist_cpu[] = {
 
 static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 
+/*
+ * 在以下调用KVM_X86_PMU_OP():
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|2| <<global>> #if !defined(KVM_X86_PMU_OP) || !defined(KVM_X86_PMU_OP_OPTIONAL)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|14| <<BUILD_BUG_ON>> KVM_X86_PMU_OP(rdpmc_ecx_to_pmc)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|15| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(msr_idx_to_pmc)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|17| <<KVM_X86_PMU_OP_OPTIONAL>> KVM_X86_PMU_OP(is_valid_msr)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|18| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(get_msr)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|19| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(set_msr)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|20| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(refresh)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|21| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(init)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|26| <<KVM_X86_PMU_OP_OPTIONAL>> #undef KVM_X86_PMU_OP
+ *   - arch/x86/kvm/pmu.c|144| <<KVM_X86_PMU_OP_OPTIONAL>> #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
+ *   - arch/x86/kvm/pmu.c|153| <<kvm_pmu_ops_update>> #define KVM_X86_PMU_OP(func) \
+ */
 #define KVM_X86_PMU_OP(func)					     \
 	DEFINE_STATIC_CALL_NULL(kvm_x86_pmu_##func,			     \
 				*(((struct kvm_pmu_ops *)0)->func));
@@ -96,6 +175,11 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|201| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|549| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -119,10 +203,23 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	if (pmc->intr && !skip_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   -  arch/x86/kvm/pmu.c|277| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -189,6 +286,17 @@ static int pmc_reprogram_counter(struct kvm_pmc *pmc, u32 type, u64 config,
 		.exclude_kernel = exclude_kernel,
 		.config = config,
 	};
+	/*
+	 * 在以下使用kvm_pmu->pebs_enable:
+	 *   - arch/x86/kvm/pmu.c|289| <<pmc_reprogram_counter>> bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
+	 *   - arch/x86/kvm/pmu.c|376| <<pmc_resume_counter>> if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
+	 *          (!!pmc->perf_event->attr.precise_ip))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|309| <<intel_pmu_get_msr(MSR_IA32_PEBS_ENABLE)>> msr_info->data = pmu->pebs_enable;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|361| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> if (pmu->pebs_enable != data) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|362| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> diff = pmu->pebs_enable ^ data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> pmu->pebs_enable = data;
+	 *   - arch/x86/kvm/vmx/vmx.c|7240| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+	 */
 	bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
 
 	attr.sample_period = get_sample_period(pmc, pmc->counter);
@@ -244,6 +352,18 @@ static bool pmc_pause_counter(struct kvm_pmc *pmc)
 	 */
 	prev_counter = counter & pmc_bitmask(pmc);
 
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	counter += pmc->emulated_counter;
 	pmc->counter = counter & pmc_bitmask(pmc);
 
@@ -264,6 +384,17 @@ static bool pmc_resume_counter(struct kvm_pmc *pmc)
 			      get_sample_period(pmc, pmc->counter)))
 		return false;
 
+	/*
+	 * 在以下使用kvm_pmu->pebs_enable:
+	 *   - arch/x86/kvm/pmu.c|289| <<pmc_reprogram_counter>> bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
+	 *   - arch/x86/kvm/pmu.c|376| <<pmc_resume_counter>> if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
+	 *          (!!pmc->perf_event->attr.precise_ip))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|309| <<intel_pmu_get_msr(MSR_IA32_PEBS_ENABLE)>> msr_info->data = pmu->pebs_enable;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|361| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> if (pmu->pebs_enable != data) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|362| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> diff = pmu->pebs_enable ^ data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> pmu->pebs_enable = data;
+	 *   - arch/x86/kvm/vmx/vmx.c|7240| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+	 */
 	if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
 	    (!!pmc->perf_event->attr.precise_ip))
 		return false;
@@ -313,6 +444,18 @@ void pmc_write_counter(struct kvm_pmc *pmc, u64 val)
 	 * reset it to '0'.  Note, this very sneakily offsets the accumulated
 	 * emulated count too, by using pmc_read_counter()!
 	 */
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	pmc->emulated_counter = 0;
 	pmc->counter += val - pmc_read_counter(pmc);
 	pmc->counter &= pmc_bitmask(pmc);
@@ -492,6 +635,21 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 				     eventsel & ARCH_PERFMON_EVENTSEL_INT);
 }
 
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *
+ * 处理的函数: kvm_pmu_handle_event()
+ *
+ *
+ * 在以下调用kvm_pmu_handle_event():
+ *   - arch/x86/kvm/x86.c|11024| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
@@ -604,6 +762,18 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理的函数: kvm_pmu_deliver_pmi()
+ *
+ * 在以下调用kvm_pmu_deliver_pmi():
+ *   - arch/x86/kvm/x86.c|11026| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
@@ -732,6 +902,18 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 	kvm_for_each_pmc(pmu, pmc, i, pmu->all_valid_pmc_idx) {
 		pmc_stop_counter(pmc);
 		pmc->counter = 0;
+		/*
+		 * 在以下使用kvm_pmc->emulated_counter:
+		 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+		 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+		 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+		 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+		 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+		 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+		 *
+		 * PMC events triggered by KVM emulation that haven't been fully
+		 * processed, i.e. haven't undergone overflow detection.
+		 */
 		pmc->emulated_counter = 0;
 
 		if (pmc_is_gp(pmc))
@@ -740,6 +922,22 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 
 	pmu->fixed_ctr_ctrl = pmu->global_ctrl = pmu->global_status = 0;
 
+	/*
+	 * 在以下使用kvm_pmu_call():
+	 *   - arch/x86/include/asm/kvm_host.h|2081| <<global>> #define kvm_pmu_call(func) static_call(kvm_x86_pmu_##func)
+	 *   - arch/x86/kvm/pmu.c|681| <<kvm_pmu_check_rdpmc_early>> return kvm_pmu_call(check_rdpmc_early)(vcpu, idx);
+	 *   - arch/x86/kvm/pmu.c|730| <<kvm_pmu_rdpmc>> pmc = kvm_pmu_call(rdpmc_ecx_to_pmc)(vcpu, idx, &mask);
+	 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_deliver_pmi>> kvm_pmu_call(deliver_pmi)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|773| <<kvm_pmu_is_valid_msr>> return kvm_pmu_call(msr_idx_to_pmc)(vcpu, msr) ||
+	 *   - arch/x86/kvm/pmu.c|774| <<kvm_pmu_is_valid_msr>> kvm_pmu_call(is_valid_msr)(vcpu, msr);
+	 *   - arch/x86/kvm/pmu.c|780| <<kvm_pmu_mark_pmc_in_use>> struct kvm_pmc *pmc = kvm_pmu_call(msr_idx_to_pmc)(vcpu, msr);
+	 *   - arch/x86/kvm/pmu.c|805| <<kvm_pmu_get_msr>> return kvm_pmu_call(get_msr)(vcpu, msr_info);
+	 *   - arch/x86/kvm/pmu.c|864| <<kvm_pmu_set_msr>> return kvm_pmu_call(set_msr)(vcpu, msr_info);
+	 *   - arch/x86/kvm/pmu.c|903| <<kvm_pmu_reset>> kvm_pmu_call(reset)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|941| <<kvm_pmu_refresh>> kvm_pmu_call(refresh)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|959| <<kvm_pmu_init>> kvm_pmu_call(init)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|981| <<kvm_pmu_cleanup>> kvm_pmu_call(cleanup)(vcpu);
+	 */
 	kvm_pmu_call(reset)(vcpu);
 }
 
@@ -748,6 +946,14 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
  * Refresh the PMU configuration for the vCPU, e.g. if userspace changes CPUID
  * and/or PERF_CAPABILITIES.
  */
+/*
+ * 没见到guest wrmsr MSR_IA32_PERF_CAPABILITIES. 应该是QEMU的行为.
+ *
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|416| <<kvm_vcpu_after_set_cpuid>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/pmu.c|960| <<kvm_pmu_init>> kvm_pmu_refresh(vcpu);
+ *   - arch/x86/kvm/x86.c|3850| <<kvm_set_msr_common(MSR_IA32_PERF_CAPABILITIES)>> kvm_pmu_refresh(vcpu);
+ */
 void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -778,6 +984,22 @@ void kvm_pmu_refresh(struct kvm_vcpu *vcpu)
 	if (!vcpu->kvm->arch.enable_pmu)
 		return;
 
+	/*
+	 * 在以下使用kvm_pmu_call():
+	 *   - arch/x86/include/asm/kvm_host.h|2081| <<global>> #define kvm_pmu_call(func) static_call(kvm_x86_pmu_##func)
+	 *   - arch/x86/kvm/pmu.c|681| <<kvm_pmu_check_rdpmc_early>> return kvm_pmu_call(check_rdpmc_early)(vcpu, idx);
+	 *   - arch/x86/kvm/pmu.c|730| <<kvm_pmu_rdpmc>> pmc = kvm_pmu_call(rdpmc_ecx_to_pmc)(vcpu, idx, &mask);
+	 *   - arch/x86/kvm/pmu.c|758| <<kvm_pmu_deliver_pmi>> kvm_pmu_call(deliver_pmi)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|773| <<kvm_pmu_is_valid_msr>> return kvm_pmu_call(msr_idx_to_pmc)(vcpu, msr) ||
+	 *   - arch/x86/kvm/pmu.c|774| <<kvm_pmu_is_valid_msr>> kvm_pmu_call(is_valid_msr)(vcpu, msr);
+	 *   - arch/x86/kvm/pmu.c|780| <<kvm_pmu_mark_pmc_in_use>> struct kvm_pmc *pmc = kvm_pmu_call(msr_idx_to_pmc)(vcpu, msr);
+	 *   - arch/x86/kvm/pmu.c|805| <<kvm_pmu_get_msr>> return kvm_pmu_call(get_msr)(vcpu, msr_info);
+	 *   - arch/x86/kvm/pmu.c|864| <<kvm_pmu_set_msr>> return kvm_pmu_call(set_msr)(vcpu, msr_info);
+	 *   - arch/x86/kvm/pmu.c|903| <<kvm_pmu_reset>> kvm_pmu_call(reset)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|941| <<kvm_pmu_refresh>> kvm_pmu_call(refresh)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|959| <<kvm_pmu_init>> kvm_pmu_call(init)(vcpu);
+	 *   - arch/x86/kvm/pmu.c|981| <<kvm_pmu_cleanup>> kvm_pmu_call(cleanup)(vcpu);
+	 */
 	kvm_pmu_call(refresh)(vcpu);
 
 	/*
@@ -828,8 +1050,24 @@ void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|944| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	pmc->emulated_counter++;
 	kvm_pmu_request_counter_reprogram(pmc);
 }
@@ -861,6 +1099,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 							 select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3687| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|8945| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9276| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9278| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 eventsel)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index ad89d0bd6..6cd907ef3 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -106,6 +106,18 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
 
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	counter = pmc->counter + pmc->emulated_counter;
 
 	if (pmc->perf_event && !pmc->is_paused)
@@ -180,6 +192,15 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 extern struct x86_pmu_capability kvm_pmu_cap;
 extern struct kvm_pmu_emulated_event_selectors kvm_pmu_eventsel;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9774| <<kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_init_pmu_capability()
+ *       -> perf_get_x86_pmu_capability()
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -230,6 +251,17 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -242,6 +274,17 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 
 	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 		set_bit(bit, pmu->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 }
 
diff --git a/arch/x86/kvm/svm/avic.c b/arch/x86/kvm/svm/avic.c
index 65fd245a9..60aae32ce 100644
--- a/arch/x86/kvm/svm/avic.c
+++ b/arch/x86/kvm/svm/avic.c
@@ -59,6 +59,12 @@
 
 static_assert(__AVIC_GATAG(AVIC_VM_ID_MASK, AVIC_VCPU_ID_MASK) == -1u);
 
+/*
+ * 在以下使用force_avic:
+ *   - arch/x86/kvm/svm/avic.c|63| <<global>> module_param_unsafe(force_avic, bool, 0444);
+ *   - arch/x86/kvm/svm/avic.c|1194| <<avic_hardware_setup>> if (!boot_cpu_has(X86_FEATURE_AVIC) && !force_avic) {
+ *   - arch/x86/kvm/svm/avic.c|1210| <<avic_hardware_setup>> } else if (force_avic) {
+ */
 static bool force_avic;
 module_param_unsafe(force_avic, bool, 0444);
 
@@ -68,10 +74,28 @@ module_param_unsafe(force_avic, bool, 0444);
  * a particular vCPU.
  */
 #define SVM_VM_DATA_HASH_BITS	8
+/*
+ * 在以下使用svm_vm_data_hash:
+ *   - arch/x86/kvm/svm/avic.c|77| <<global>> static DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);
+ *   - arch/x86/kvm/svm/avic.c|183| <<avic_ga_log_notifier>> hash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {
+ *   - arch/x86/kvm/svm/avic.c|256| <<avic_vm_init>> hash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {
+ *   - arch/x86/kvm/svm/avic.c|262| <<avic_vm_init>> hash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);
+ */
 static DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);
 static u32 next_vm_id = 0;
 static bool next_vm_id_wrapped = 0;
 static DEFINE_SPINLOCK(svm_vm_data_hash_lock);
+/*
+ * 在以下设置x2avic_enabled:
+ *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+ * 在以下使用x2avic_enabled:
+ *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+ *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+ *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+ *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+ *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+ *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+ */
 bool x2avic_enabled;
 
 /*
@@ -82,8 +106,41 @@ struct amd_svm_iommu_ir {
 	void *data;		/* Storing pointer to struct amd_ir_data */
 };
 
+/*
+ * 在以下调用avic_activate_vmcb():
+ *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm)) avic_activate_vmcb(svm);
+ *   - arch/x86/kvm/svm/avic.c|1207| <<avic_refresh_virtual_apic_mode>> if (kvm_vcpu_apicv_active(vcpu)) { avic_activate_vmcb(svm);
+ */
 static void avic_activate_vmcb(struct vcpu_svm *svm)
 {
+	/*
+	 * 527 struct vmcb {
+	 * 528         struct vmcb_control_area control;
+	 * 529         union {
+	 * 530                 struct vmcb_save_area save;
+	 * 531         
+	 * 532                 //
+	 * 533                  * For SEV-ES VMs, the save area in the VMCB is used only to
+	 * 534                  * save/load host state.  Guest state resides in a separate
+	 * 535                  * page, the aptly named VM Save Area (VMSA), that is encrypted
+	 * 536                  * with the guest's private key.
+	 * 537                  //
+	 * 538                 struct sev_es_save_area host_sev_es_save;
+	 * 539         };
+	 * 540 } __packed;
+	 *
+	 * 131 struct kvm_vmcb_info {
+	 * 132         struct vmcb *ptr;
+	 * 133         unsigned long pa;
+	 * 134         int cpu;
+	 * 135         uint64_t asid_generation;
+	 * 136 };
+	 *
+	 * struct vcpu_svm *svm:
+	 * -> struct vmcb *vmcb;
+	 * -> struct kvm_vmcb_info vmcb01;
+	 * -> struct kvm_vmcb_info *current_vmcb;
+	 */
 	struct vmcb *vmcb = svm->vmcb01.ptr;
 
 	vmcb->control.int_ctl &= ~(AVIC_ENABLE_MASK | X2APIC_MODE_MASK);
@@ -98,9 +155,26 @@ static void avic_activate_vmcb(struct vcpu_svm *svm)
 	 * (deletes the memslot) if any vCPU has x2APIC enabled, thus enabling
 	 * AVIC in hybrid mode activates only the doorbell mechanism.
 	 */
+	/*
+	 * 在以下设置x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+	 * 在以下使用x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+	 */
 	if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
 		vmcb->control.int_ctl |= X2APIC_MODE_MASK;
 		vmcb->control.avic_physical_id |= X2AVIC_MAX_PHYSICAL_ID;
+		/*
+		 * 在以下调用svm_set_x2apic_msr_interception():
+		 *   - arch/x86/kvm/svm/avic.c|133| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, false);
+		 *   - arch/x86/kvm/svm/avic.c|144| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+		 *   - arch/x86/kvm/svm/avic.c|164| <<avic_deactivate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+		 */
 		/* Disabling MSR intercept for x2APIC registers */
 		svm_set_x2apic_msr_interception(svm, false);
 	} else {
@@ -112,6 +186,12 @@ static void avic_activate_vmcb(struct vcpu_svm *svm)
 
 		/* For xAVIC and hybrid-xAVIC modes */
 		vmcb->control.avic_physical_id |= AVIC_MAX_PHYSICAL_ID;
+		/*
+		 * 在以下调用svm_set_x2apic_msr_interception():
+		 *   - arch/x86/kvm/svm/avic.c|133| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, false);
+		 *   - arch/x86/kvm/svm/avic.c|144| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+		 *   - arch/x86/kvm/svm/avic.c|164| <<avic_deactivate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+		 */
 		/* Enabling MSR intercept for x2APIC registers */
 		svm_set_x2apic_msr_interception(svm, true);
 	}
@@ -132,6 +212,12 @@ static void avic_deactivate_vmcb(struct vcpu_svm *svm)
 	    vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_MSR_PROT))
 		return;
 
+	/*
+	 * 在以下调用svm_set_x2apic_msr_interception():
+	 *   - arch/x86/kvm/svm/avic.c|133| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, false);
+	 *   - arch/x86/kvm/svm/avic.c|144| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+	 *   - arch/x86/kvm/svm/avic.c|164| <<avic_deactivate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+	 */
 	/* Enabling MSR intercept for x2APIC registers */
 	svm_set_x2apic_msr_interception(svm, true);
 }
@@ -140,6 +226,23 @@ static void avic_deactivate_vmcb(struct vcpu_svm *svm)
  * This function is called from IOMMU driver to notify
  * SVM to schedule in a particular vCPU of a particular VM.
  */
+/*
+ * avic_ga_log_notifier
+ * iommu_poll_ga_log
+ * amd_iommu_int_thread_galog
+ * irq_thread_fn
+ * irq_thread
+ * kthread
+ * ret_from_fork
+ * ret_from_fork_asm
+ *
+ * 在以下使用avic_ga_log_notifier():
+ *   - arch/x86/kvm/svm/avic.c|1313| <<avic_hardware_setup>> amd_iommu_register_ga_log_notifier(&avic_ga_log_notifier);
+ *
+ * avic_ga_log_notifier()的注释:
+ * This function is called from IOMMU driver to notify
+ * SVM to schedule in a particular vCPU of a particular VM.
+ */
 int avic_ga_log_notifier(u32 ga_tag)
 {
 	unsigned long flags;
@@ -152,6 +255,13 @@ int avic_ga_log_notifier(u32 ga_tag)
 	trace_kvm_avic_ga_log(vm_id, vcpu_id);
 
 	spin_lock_irqsave(&svm_vm_data_hash_lock, flags);
+	/*
+	 * 在以下使用svm_vm_data_hash:
+	 *   - arch/x86/kvm/svm/avic.c|77| <<global>> static DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);
+	 *   - arch/x86/kvm/svm/avic.c|183| <<avic_ga_log_notifier>> hash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {
+	 *   - arch/x86/kvm/svm/avic.c|256| <<avic_vm_init>> hash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {
+	 *   - arch/x86/kvm/svm/avic.c|262| <<avic_vm_init>> hash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);
+	 */
 	hash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {
 		if (kvm_svm->avic_vm_id != vm_id)
 			continue;
@@ -171,6 +281,11 @@ int avic_ga_log_notifier(u32 ga_tag)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|268| <<avic_vm_init>> avic_vm_destroy(kvm);
+ *   - arch/x86/kvm/svm/svm.c|5030| <<svm_vm_destroy>> avic_vm_destroy(kvm);
+ */
 void avic_vm_destroy(struct kvm *kvm)
 {
 	unsigned long flags;
@@ -179,6 +294,15 @@ void avic_vm_destroy(struct kvm *kvm)
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_svm->avic_logical_id_table_page:
+	 *   - arch/x86/kvm/svm/avic.c|210| <<avic_vm_destroy>> if (kvm_svm->avic_logical_id_table_page)
+	 *   - arch/x86/kvm/svm/avic.c|211| <<avic_vm_destroy>> __free_page(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|245| <<avic_vm_init>> kvm_svm->avic_logical_id_table_page = l_page;
+	 *   - arch/x86/kvm/svm/avic.c|276| <<avic_init_vmcb>> phys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));
+	 *   - arch/x86/kvm/svm/avic.c|504| <<avic_kick_target_vcpus_fast>> avic_logical_id_table = page_address(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|627| <<avic_get_logical_id_entry>> logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
+	 */
 	if (kvm_svm->avic_logical_id_table_page)
 		__free_page(kvm_svm->avic_logical_id_table_page);
 	if (kvm_svm->avic_physical_id_table_page)
@@ -189,6 +313,10 @@ void avic_vm_destroy(struct kvm *kvm)
 	spin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5052| <<svm_vm_init>> int ret = avic_vm_init(kvm);
+ */
 int avic_vm_init(struct kvm *kvm)
 {
 	unsigned long flags;
@@ -214,6 +342,15 @@ int avic_vm_init(struct kvm *kvm)
 	if (!l_page)
 		goto free_avic;
 
+	/*
+	 * 在以下使用kvm_svm->avic_logical_id_table_page:
+	 *   - arch/x86/kvm/svm/avic.c|210| <<avic_vm_destroy>> if (kvm_svm->avic_logical_id_table_page)
+	 *   - arch/x86/kvm/svm/avic.c|211| <<avic_vm_destroy>> __free_page(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|245| <<avic_vm_init>> kvm_svm->avic_logical_id_table_page = l_page;
+	 *   - arch/x86/kvm/svm/avic.c|276| <<avic_init_vmcb>> phys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));
+	 *   - arch/x86/kvm/svm/avic.c|504| <<avic_kick_target_vcpus_fast>> avic_logical_id_table = page_address(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|627| <<avic_get_logical_id_entry>> logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
+	 */
 	kvm_svm->avic_logical_id_table_page = l_page;
 
 	spin_lock_irqsave(&svm_vm_data_hash_lock, flags);
@@ -223,6 +360,13 @@ int avic_vm_init(struct kvm *kvm)
 		next_vm_id_wrapped = 1;
 		goto again;
 	}
+	/*
+	 * 在以下使用svm_vm_data_hash:
+	 *   - arch/x86/kvm/svm/avic.c|77| <<global>> static DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);
+	 *   - arch/x86/kvm/svm/avic.c|183| <<avic_ga_log_notifier>> hash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {
+	 *   - arch/x86/kvm/svm/avic.c|256| <<avic_vm_init>> hash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {
+	 *   - arch/x86/kvm/svm/avic.c|262| <<avic_vm_init>> hash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);
+	 */
 	/* Is it still in use? Only possible if wrapped at least once */
 	if (next_vm_id_wrapped) {
 		hash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {
@@ -231,6 +375,13 @@ int avic_vm_init(struct kvm *kvm)
 		}
 	}
 	kvm_svm->avic_vm_id = vm_id;
+	/*
+	 * 在以下使用svm_vm_data_hash:
+	 *   - arch/x86/kvm/svm/avic.c|77| <<global>> static DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);
+	 *   - arch/x86/kvm/svm/avic.c|183| <<avic_ga_log_notifier>> hash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {
+	 *   - arch/x86/kvm/svm/avic.c|256| <<avic_vm_init>> hash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {
+	 *   - arch/x86/kvm/svm/avic.c|262| <<avic_vm_init>> hash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);
+	 */
 	hash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);
 	spin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);
 
@@ -241,18 +392,49 @@ int avic_vm_init(struct kvm *kvm)
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1375| <<init_vmcb>> avic_init_vmcb(svm, vmcb);
+ */
 void avic_init_vmcb(struct vcpu_svm *svm, struct vmcb *vmcb)
 {
 	struct kvm_svm *kvm_svm = to_kvm_svm(svm->vcpu.kvm);
+	/*
+	 * 在以下使用vmcb_control_area->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+	 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+	 * 在以下使用vcpu_svm->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+	 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+	 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+	 */
 	phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
 	phys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));
 	phys_addr_t ppa = __sme_set(page_to_phys(kvm_svm->avic_physical_id_table_page));
 
+	/*
+	 * 在以下使用vmcb_control_area->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+	 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+	 * 在以下使用vcpu_svm->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+	 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+	 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+	 */
 	vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
 	vmcb->control.avic_logical_id = lpa & AVIC_HPA_MASK;
 	vmcb->control.avic_physical_id = ppa & AVIC_HPA_MASK;
 	vmcb->control.avic_vapic_bar = APIC_DEFAULT_PHYS_BASE & VMCB_AVIC_APIC_BAR_MASK;
 
+	/*
+	 * 在以下调用avic_activate_vmcb():
+	 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm)) avic_activate_vmcb(svm);
+	 *   - arch/x86/kvm/svm/avic.c|1207| <<avic_refresh_virtual_apic_mode>> if (kvm_vcpu_apicv_active(vcpu)) { avic_activate_vmcb(svm);
+	 */
 	if (kvm_apicv_activated(svm->vcpu.kvm))
 		avic_activate_vmcb(svm);
 	else
@@ -265,6 +447,17 @@ static u64 *avic_get_physical_id_entry(struct kvm_vcpu *vcpu,
 	u64 *avic_physical_id_table;
 	struct kvm_svm *kvm_svm = to_kvm_svm(vcpu->kvm);
 
+	/*
+	 * 在以下设置x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+	 * 在以下使用x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+	 */
 	if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
 	    (index > X2AVIC_MAX_PHYSICAL_ID))
 		return NULL;
@@ -280,6 +473,17 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	int id = vcpu->vcpu_id;
 	struct vcpu_svm *svm = to_svm(vcpu);
 
+	/*
+	 * 在以下设置x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+	 * 在以下使用x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+	 */
 	if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
 	    (id > X2AVIC_MAX_PHYSICAL_ID))
 		return -EINVAL;
@@ -301,6 +505,17 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 			return ret;
 	}
 
+	/*
+	 * 在以下使用vmcb_control_area->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+	 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+	 * 在以下使用vcpu_svm->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+	 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+	 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+	 */
 	svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
 
 	/* Setting AVIC backing page address in the phy APIC ID table */
@@ -308,6 +523,17 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	if (!entry)
 		return -EINVAL;
 
+	/*
+	 * 在以下使用vmcb_control_area->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+	 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+	 * 在以下使用vcpu_svm->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+	 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+	 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+	 */
 	new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
 			      AVIC_PHYSICAL_ID_ENTRY_BACKING_PAGE_MASK) |
 			      AVIC_PHYSICAL_ID_ENTRY_VALID_MASK);
@@ -318,6 +544,10 @@ static int avic_init_backing_page(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|3727| <<svm_complete_interrupt_delivery>> avic_ring_doorbell(vcpu);
+ */
 void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 {
 	/*
@@ -337,9 +567,21 @@ void avic_ring_doorbell(struct kvm_vcpu *vcpu)
 }
 
 
+/*
+ * 在以下调用avic_kick_vcpu():
+ *   - arch/x86/kvm/svm/avic.c|416| <<avic_kick_vcpu_by_physical_id>> avic_kick_vcpu(target_vcpu, icrl);
+ *   - arch/x86/kvm/svm/avic.c|540| <<avic_kick_target_vcpus>> avic_kick_vcpu(vcpu, icrl);
+ */
 static void avic_kick_vcpu(struct kvm_vcpu *vcpu, u32 icrl)
 {
 	vcpu->arch.apic->irr_pending = true;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/avic.c|343| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu, icrl & APIC_MODE_MASK,
+	 *                                      icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|3750| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu, delivery_mode,
+	 *                                      trig_mode, vector);
+	 */
 	svm_complete_interrupt_delivery(vcpu,
 					icrl & APIC_MODE_MASK,
 					icrl & APIC_INT_LEVELTRIG,
@@ -444,6 +686,15 @@ static int avic_kick_target_vcpus_fast(struct kvm *kvm, struct kvm_lapic *source
 		if (unlikely(!bitmap))
 			return 0;
 
+		/*
+		 * 在以下使用kvm_svm->avic_logical_id_table_page:
+		 *   - arch/x86/kvm/svm/avic.c|210| <<avic_vm_destroy>> if (kvm_svm->avic_logical_id_table_page)
+		 *   - arch/x86/kvm/svm/avic.c|211| <<avic_vm_destroy>> __free_page(kvm_svm->avic_logical_id_table_page);
+		 *   - arch/x86/kvm/svm/avic.c|245| <<avic_vm_init>> kvm_svm->avic_logical_id_table_page = l_page;
+		 *   - arch/x86/kvm/svm/avic.c|276| <<avic_init_vmcb>> phys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));
+		 *   - arch/x86/kvm/svm/avic.c|504| <<avic_kick_target_vcpus_fast>> avic_logical_id_table = page_address(kvm_svm->avic_logical_id_table_page);
+		 *   - arch/x86/kvm/svm/avic.c|627| <<avic_get_logical_id_entry>> logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
+		 */
 		if (apic_x2apic_mode(source))
 			avic_logical_id_table = NULL;
 		else
@@ -462,6 +713,10 @@ static int avic_kick_target_vcpus_fast(struct kvm *kvm, struct kvm_lapic *source
 	return 0;
 }
 
+/*
+ * 只在下面调用:
+ *   - arch/x86/kvm/svm/avic.c|581| <<avic_incomplete_ipi_interception>> avic_kick_target_vcpus(vcpu->kvm, apic, icrl, icrh, index);
+ */
 static void avic_kick_target_vcpus(struct kvm *kvm, struct kvm_lapic *source,
 				   u32 icrl, u32 icrh, u32 index)
 {
@@ -487,6 +742,10 @@ static void avic_kick_target_vcpus(struct kvm *kvm, struct kvm_lapic *source,
 	}
 }
 
+/*
+ * 在以下使用avic_incomplete_ipi_interception():
+ *   - arch/x86/kvm/svm/svm.c|3392| <<global>> [SVM_EXIT_AVIC_INCOMPLETE_IPI] = avic_incomplete_ipi_interception,
+ */
 int avic_incomplete_ipi_interception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -539,6 +798,10 @@ int avic_incomplete_ipi_interception(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * 在以下使用avic_vcpu_get_apicv_inhibit_reasons():
+ *   - arch/x86/kvm/svm/svm.c|5208| <<global>> .vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+ */
 unsigned long avic_vcpu_get_apicv_inhibit_reasons(struct kvm_vcpu *vcpu)
 {
 	if (is_guest_mode(vcpu))
@@ -570,6 +833,15 @@ static u32 *avic_get_logical_id_entry(struct kvm_vcpu *vcpu, u32 ldr, bool flat)
 		return NULL;
 	index += (cluster << 2);
 
+	/*
+	 * 在以下使用kvm_svm->avic_logical_id_table_page:
+	 *   - arch/x86/kvm/svm/avic.c|210| <<avic_vm_destroy>> if (kvm_svm->avic_logical_id_table_page)
+	 *   - arch/x86/kvm/svm/avic.c|211| <<avic_vm_destroy>> __free_page(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|245| <<avic_vm_init>> kvm_svm->avic_logical_id_table_page = l_page;
+	 *   - arch/x86/kvm/svm/avic.c|276| <<avic_init_vmcb>> phys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));
+	 *   - arch/x86/kvm/svm/avic.c|504| <<avic_kick_target_vcpus_fast>> avic_logical_id_table = page_address(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|627| <<avic_get_logical_id_entry>> logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
+	 */
 	logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
 
 	return &logical_apic_id_table[index];
@@ -735,12 +1007,21 @@ int avic_init_vcpu(struct vcpu_svm *svm)
 	return ret;
 }
 
+/*
+ * 在以下使用avic_apicv_post_state_restore():
+ *   - arch/x86/kvm/svm/svm.c|5152| <<global>> .apicv_post_state_restore = avic_apicv_post_state_restore,
+ *   - arch/x86/kvm/svm/avic.c|1206| <<avic_refresh_virtual_apic_mode>> avic_apicv_post_state_restore(vcpu);
+ */
 void avic_apicv_post_state_restore(struct kvm_vcpu *vcpu)
 {
 	avic_handle_dfr_update(vcpu);
 	avic_handle_ldr_update(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|1427| <<avic_refresh_apicv_exec_ctrl>> avic_set_pi_irte_mode(vcpu, activated);
+ */
 static int avic_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)
 {
 	int ret = 0;
@@ -789,6 +1070,19 @@ static void svm_ir_list_del(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
 }
 
+/*
+ * struct amd_iommu_pi_data {
+ *     u32 ga_tag;
+ *     u32 prev_ga_tag;
+ *     u64 base;
+ *     bool is_guest_mode;
+ *     struct vcpu_data *vcpu_data;
+ *     void *ir_data;
+ * };
+ *
+ * 只在下面调用svm_ir_list_add():
+ *   - arch/x86/kvm/svm/avic.c|1183| <<avic_pi_update_irte>> svm_ir_list_add(svm, &pi);
+ */
 static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 {
 	int ret = 0;
@@ -836,6 +1130,19 @@ static int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)
 	 * See also avic_vcpu_load().
 	 */
 	entry = READ_ONCE(*(svm->avic_physical_id_cache));
+	/*
+	 * 在以下使用AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK:
+	 *   - arch/x86/kvm/svm/avic.c|839| <<svm_ir_list_add>> if (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)
+	 *   - arch/x86/kvm/svm/avic.c|1060| <<avic_vcpu_load>> WARN_ON_ONCE(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
+	 *   - arch/x86/kvm/svm/avic.c|1064| <<avic_vcpu_load>> entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+	 *   - arch/x86/kvm/svm/avic.c|1090| <<avic_vcpu_put>> if (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))
+	 *   - arch/x86/kvm/svm/avic.c|1105| <<avic_vcpu_put>> entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/svm/avic.c|1057| <<svm_ir_list_add>> amd_iommu_update_ga(entry & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK,
+	 *                                         true, pi->ir_data);
+	 *   - arch/x86/kvm/svm/avic.c|1243| <<avic_update_iommu_vcpu_affinity>> ret = amd_iommu_update_ga(cpu, r, ir->data);
+	 */
 	if (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)
 		amd_iommu_update_ga(entry & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK,
 				    true, pi->ir_data);
@@ -876,6 +1183,17 @@ get_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
 	pr_debug("SVM: %s: use GA mode for irq %u\n", __func__,
 		 irq.vector);
 	*svm = to_svm(vcpu);
+	/*
+	 * 在以下使用vmcb_control_area->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+	 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+	 * 在以下使用vcpu_svm->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+	 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+	 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+	 */
 	vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
 	vcpu_info->vector = irq.vector;
 
@@ -891,6 +1209,36 @@ get_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,
  * @set: set or unset PI
  * returns 0 on success, < 0 on failure
  */
+/*
+ * avic_pi_update_irte
+ * kvm_arch_irq_bypass_del_producer
+ * __disconnect
+ * irq_bypass_unregister_producer
+ * vfio_msi_set_vector_signal
+ * vfio_msi_disable
+ * vfio_pci_set_msi_trigger
+ * vfio_pci_core_ioctl
+ * vfio_device_fops_unl_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * avic_pi_update_irte
+ * kvm_arch_irq_bypass_add_producer
+ * __connect
+ * irq_bypass_register_producer
+ * vfio_msi_set_vector_signal
+ * vfio_msi_set_block
+ * vfio_pci_set_msi_trigger
+ * vfio_pci_core_ioctl
+ * vfio_device_fops_unl_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用avic_pi_update_irte():
+ *   - arch/x86/kvm/svm/svm.c|5180| <<global>> .pi_update_irte = avic_pi_update_irte,
+ */
 int avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 			uint32_t guest_irq, bool set)
 {
@@ -931,8 +1279,29 @@ int avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 		 */
 		if (!get_pi_vcpu_info(kvm, e, &vcpu_info, &svm) && set &&
 		    kvm_vcpu_apicv_active(&svm->vcpu)) {
+			/*
+			 * struct amd_iommu_pi_data {
+			 *     u32 ga_tag;
+			 *     u32 prev_ga_tag;
+			 *     u64 base;
+			 *     bool is_guest_mode;
+			 *     struct vcpu_data *vcpu_data;
+			 *     void *ir_data;
+			 * };
+			 */
 			struct amd_iommu_pi_data pi;
 
+			/*
+			 * 在以下使用vmcb_control_area->avic_backing_page:
+			 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+			 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+			 * 在以下使用vcpu_svm->avic_backing_page:
+			 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+			 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+			 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+			 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+			 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+			 */
 			/* Try to enable guest_mode in IRTE */
 			pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
 					    AVIC_HPA_MASK);
@@ -998,6 +1367,11 @@ int avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|1137| <<avic_vcpu_load>> avic_update_iommu_vcpu_affinity(vcpu, h_physical_id, true);
+ *   - arch/x86/kvm/svm/avic.c|1181| <<avic_vcpu_put>> avic_update_iommu_vcpu_affinity(vcpu, -1, 0);
+ */
 static inline int
 avic_update_iommu_vcpu_affinity(struct kvm_vcpu *vcpu, int cpu, bool r)
 {
@@ -1018,6 +1392,12 @@ avic_update_iommu_vcpu_affinity(struct kvm_vcpu *vcpu, int cpu, bool r)
 		return 0;
 
 	list_for_each_entry(ir, &svm->ir_list, node) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/svm/avic.c|1057| <<svm_ir_list_add>> amd_iommu_update_ga(entry & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK,
+		 *                                         true, pi->ir_data);
+		 *   - arch/x86/kvm/svm/avic.c|1243| <<avic_update_iommu_vcpu_affinity>> ret = amd_iommu_update_ga(cpu, r, ir->data);
+		 */
 		ret = amd_iommu_update_ga(cpu, r, ir->data);
 		if (ret)
 			return ret;
@@ -1025,6 +1405,12 @@ avic_update_iommu_vcpu_affinity(struct kvm_vcpu *vcpu, int cpu, bool r)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|1413| <<avic_refresh_apicv_exec_ctrl>> avic_vcpu_load(vcpu, vcpu->cpu);
+ *   - arch/x86/kvm/svm/avic.c|1446| <<avic_vcpu_unblocking>> avic_vcpu_load(vcpu, vcpu->cpu);
+ *   - arch/x86/kvm/svm/svm.c|1584| <<svm_vcpu_load>> avic_vcpu_load(vcpu, cpu);
+ */
 void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	u64 entry;
@@ -1057,6 +1443,14 @@ void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	spin_lock_irqsave(&svm->ir_list_lock, flags);
 
 	entry = READ_ONCE(*(svm->avic_physical_id_cache));
+	/*
+	 * 在以下使用AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK:
+	 *   - arch/x86/kvm/svm/avic.c|839| <<svm_ir_list_add>> if (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)
+	 *   - arch/x86/kvm/svm/avic.c|1060| <<avic_vcpu_load>> WARN_ON_ONCE(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
+	 *   - arch/x86/kvm/svm/avic.c|1064| <<avic_vcpu_load>> entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+	 *   - arch/x86/kvm/svm/avic.c|1090| <<avic_vcpu_put>> if (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))
+	 *   - arch/x86/kvm/svm/avic.c|1105| <<avic_vcpu_put>> entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+	 */
 	WARN_ON_ONCE(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
 
 	entry &= ~AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK;
@@ -1064,6 +1458,11 @@ void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
 
 	WRITE_ONCE(*(svm->avic_physical_id_cache), entry);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/avic.c|1137| <<avic_vcpu_load>> avic_update_iommu_vcpu_affinity(vcpu, h_physical_id, true);
+	 *   - arch/x86/kvm/svm/avic.c|1181| <<avic_vcpu_put>> avic_update_iommu_vcpu_affinity(vcpu, -1, 0);
+	 */
 	avic_update_iommu_vcpu_affinity(vcpu, h_physical_id, true);
 
 	spin_unlock_irqrestore(&svm->ir_list_lock, flags);
@@ -1086,6 +1485,14 @@ void avic_vcpu_put(struct kvm_vcpu *vcpu)
 	 */
 	entry = READ_ONCE(*(svm->avic_physical_id_cache));
 
+	/*
+	 * 在以下使用AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK:
+	 *   - arch/x86/kvm/svm/avic.c|839| <<svm_ir_list_add>> if (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)
+	 *   - arch/x86/kvm/svm/avic.c|1060| <<avic_vcpu_load>> WARN_ON_ONCE(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);
+	 *   - arch/x86/kvm/svm/avic.c|1064| <<avic_vcpu_load>> entry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+	 *   - arch/x86/kvm/svm/avic.c|1090| <<avic_vcpu_put>> if (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))
+	 *   - arch/x86/kvm/svm/avic.c|1105| <<avic_vcpu_put>> entry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;
+	 */
 	/* Nothing to do if IsRunning == '0' due to vCPU blocking. */
 	if (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))
 		return;
@@ -1109,6 +1516,11 @@ void avic_vcpu_put(struct kvm_vcpu *vcpu)
 
 }
 
+/*
+ * 在以下使用avic_refresh_virtual_apic_mode():
+ *   - arch/x86/kvm/svm/svm.c|5150| <<global>> .set_virtual_apic_mode = avic_refresh_virtual_apic_mode,
+ *   - arch/x86/kvm/svm/avic.c|1221| <<avic_refresh_apicv_exec_ctrl>> avic_refresh_virtual_apic_mode(vcpu);
+ */
 void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -1126,6 +1538,11 @@ void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 		 * accordingly before re-activating.
 		 */
 		avic_apicv_post_state_restore(vcpu);
+		/*
+		 * 在以下调用avic_activate_vmcb():
+		 *   - arch/x86/kvm/svm/avic.c|285| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm)) avic_activate_vmcb(svm);
+		 *   - arch/x86/kvm/svm/avic.c|1207| <<avic_refresh_virtual_apic_mode>> if (kvm_vcpu_apicv_active(vcpu)) { avic_activate_vmcb(svm);
+		 */
 		avic_activate_vmcb(svm);
 	} else {
 		avic_deactivate_vmcb(svm);
@@ -1133,6 +1550,10 @@ void avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)
 	vmcb_mark_dirty(vmcb, VMCB_AVIC);
 }
 
+/*
+ * 在以下使用avic_refresh_apicv_exec_ctrl():
+ *   - arch/x86/kvm/svm/svm.c|5157| <<global>> .refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,
+ */
 void avic_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)
 {
 	bool activated = kvm_vcpu_apicv_active(vcpu);
@@ -1216,11 +1637,32 @@ bool avic_hardware_setup(void)
 		pr_warn("Your system might crash and burn");
 	}
 
+	/*
+	 * 在以下设置x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+	 * 在以下使用x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+	 */
 	/* AVIC is a prerequisite for x2AVIC. */
 	x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
 	if (x2avic_enabled)
 		pr_info("x2AVIC enabled\n");
 
+	/*
+	 * 在以下使用avic_ga_log_notifier():
+	 *   - arch/x86/kvm/svm/avic.c|1313| <<avic_hardware_setup>> amd_iommu_register_ga_log_notifier(&avic_ga_log_notifier);
+	 *
+	 * avic_ga_log_notifier()的注释:
+	 * This function is called from IOMMU driver to notify
+	 * SVM to schedule in a particular vCPU of a particular VM.
+	 *
+	 * 只在此处使用amd_iommu_register_ga_log_notifier()
+	 */
 	amd_iommu_register_ga_log_notifier(&avic_ga_log_notifier);
 
 	return true;
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index 22d5a65b4..dd9655a35 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -173,6 +173,21 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * 230 struct kvm_pmu_ops amd_pmu_ops __initdata = {
+ * 231         .rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc,
+ * 232         .msr_idx_to_pmc = amd_msr_idx_to_pmc,
+ * 233         .check_rdpmc_early = amd_check_rdpmc_early,
+ * 234         .is_valid_msr = amd_is_valid_msr,
+ * 235         .get_msr = amd_pmu_get_msr,
+ * 236         .set_msr = amd_pmu_set_msr,
+ * 237         .refresh = amd_pmu_refresh,
+ * 238         .init = amd_pmu_init,
+ * 239         .EVENTSEL_EVENT = AMD64_EVENTSEL_EVENT,
+ * 240         .MAX_NR_GP_COUNTERS = KVM_MAX_NR_AMD_GP_COUNTERS,
+ * 241         .MIN_NR_GP_COUNTERS = AMD64_NUM_COUNTERS,
+ * 242 };
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 21dacd312..0eadab464 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -52,6 +52,29 @@
 #include "kvm_onhyperv.h"
 #include "svm_onhyperv.h"
 
+/*
+ * 似乎是avic=0的重要函数.
+ * svm_inject_irq
+ * kvm_check_and_inject_events
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * cr_interception
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
+
 MODULE_AUTHOR("Qumranet");
 MODULE_DESCRIPTION("KVM support for SVM (AMD-V) extensions");
 MODULE_LICENSE("GPL");
@@ -227,6 +250,12 @@ module_param(tsc_scaling, int, 0444);
  * enable / disable AVIC.  Because the defaults differ for APICv
  * support between VMX and SVM we cannot use module_param_named.
  */
+/*
+ * 在以下使用avic:
+ *   - arch/x86/kvm/svm/svm.c|230| <<global>> static bool avic;
+ *   - arch/x86/kvm/svm/svm.c|231| <<global>> module_param(avic, bool, 0444);
+ *   - arch/x86/kvm/svm/svm.c|5454| <<svm_hardware_setup>> enable_apicv = avic = avic && avic_hardware_setup();
+ */
 static bool avic;
 module_param(avic, bool, 0444);
 
@@ -894,6 +923,12 @@ void svm_vcpu_init_msrpm(struct kvm_vcpu *vcpu, u32 *msrpm)
 	}
 }
 
+/*
+ * 在以下调用svm_set_x2apic_msr_interception():
+ *   - arch/x86/kvm/svm/avic.c|133| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, false);
+ *   - arch/x86/kvm/svm/avic.c|144| <<avic_activate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+ *   - arch/x86/kvm/svm/avic.c|164| <<avic_deactivate_vmcb>> svm_set_x2apic_msr_interception(svm, true);
+ */
 void svm_set_x2apic_msr_interception(struct vcpu_svm *svm, bool intercept)
 {
 	int i;
@@ -901,6 +936,17 @@ void svm_set_x2apic_msr_interception(struct vcpu_svm *svm, bool intercept)
 	if (intercept == svm->x2avic_msrs_intercepted)
 		return;
 
+	/*
+	 * 在以下设置x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+	 * 在以下使用x2avic_enabled:
+	 *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+	 *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+	 *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+	 *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+	 */
 	if (!x2avic_enabled)
 		return;
 
@@ -1560,6 +1606,45 @@ static void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	if (sd->current_vmcb != svm->vmcb) {
 		sd->current_vmcb = svm->vmcb;
 
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *
+		 * 在以下使用X86_FEATURE_USE_IBPB:
+		 *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+		 *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+		 *
+		 * 部分例子.
+		 *
+		 * context_switch()
+		 * -> switch_mm_irqs_off()
+		 *    -> cond_mitigation()
+		 *       -> indirect_branch_prediction_barrier()
+		 *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+		 *                                   x86_pred_cmd,
+		 *                                   X86_FEATURE_USE_IBPB);
+		 *
+		 * switch_mm()
+		 * -> switch_mm_irqs_off()
+		 *    -> cond_mitigation()
+		 *       -> indirect_branch_prediction_barrier()
+		 *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+		 *                                   x86_pred_cmd,
+		 *                                   X86_FEATURE_USE_IBPB);
+		 */
 		if (!cpu_feature_enabled(X86_FEATURE_IBPB_ON_VMEXIT))
 			indirect_branch_prediction_barrier();
 	}
@@ -2053,6 +2138,16 @@ static int pf_interception(struct kvm_vcpu *vcpu)
 			svm->vmcb->control.insn_len);
 }
 
+/*
+ * npf_interception
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static int npf_interception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -2070,10 +2165,34 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 	if (WARN_ON_ONCE(error_code & PFERR_SYNTHETIC_MASK))
 		error_code &= ~PFERR_SYNTHETIC_MASK;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	if (sev_snp_guest(vcpu->kvm) && (error_code & PFERR_GUEST_ENC_MASK))
 		error_code |= PFERR_PRIVATE_ACCESS;
 
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *                      insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                      svm->vmcb->control.insn_bytes : NULL,
+	 *                      svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
 				static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
 				svm->vmcb->control.insn_bytes : NULL,
@@ -2645,6 +2764,16 @@ static bool check_selective_cr0_intercepted(struct kvm_vcpu *vcpu,
 
 #define CR_VALID (1ULL << 63)
 
+/*
+ * cr_interception
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static int cr_interception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3010,6 +3139,19 @@ static int svm_set_vm_cr(struct kvm_vcpu *vcpu, u64 data)
 	return 0;
 }
 
+/*
+ * svm_set_msr
+ * __kvm_set_msr
+ * kvm_set_msr_ignored_check
+ * kvm_emulate_wrmsr
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3654,6 +3796,35 @@ static bool svm_set_vnmi_pending(struct kvm_vcpu *vcpu)
 	return true;
 }
 
+/*
+ * 5.15上关于Intel的例子.
+ * -> vcpu_enter_guest()
+ *    -> inject_pending_event()
+ *       -> kvm_cpu_has_injectable_intr()
+ *          -> kvm_apic_has_interrupt()
+ *             -> apic_has_interrupt_for_ppr()
+ *                -> kvm_x86_sync_pir_to_irr = vmx_sync_pir_to_irr()
+ *                   -> suppose another interrupt comes and pi_test_on(&vmx->pi_desc)
+ *                   -> pi_clear_on(&vmx->pi_desc);
+ *                   -> kvm_apic_update_irr() returns true
+ *                      -> kvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu), false)
+ *                         -> kvm_cpu_get_interrupt()
+ *                            -> kvm_get_apic_interrupt()
+ *                               -> kvm_apic_has_interrupt()
+ *                                  -> apic_has_interrupt_for_ppr()
+ *                                     -> kvm_x86_sync_pir_to_irr = vmx_sync_pir_to_irr()
+ *                                        -> kvm_lapic_find_highest_irr() returns -1?
+ *
+ * svm_inject_irq
+ * kvm_check_and_inject_events
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static void svm_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -3672,10 +3843,23 @@ static void svm_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)
 			   vcpu->arch.interrupt.soft, reinjected);
 	++vcpu->stat.irq_injections;
 
+	/*
+	 * struct vcpu_svm *svm:
+	 * -> struct vmcb *vmcb;
+	 *    -> struct vmcb_control_area control;
+	 *       -> u32 event_inj;
+	 */
 	svm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |
 				       SVM_EVTINJ_VALID | type;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|343| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu, icrl & APIC_MODE_MASK,
+ *                                      icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+ *   - arch/x86/kvm/svm/svm.c|3750| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, 
+ *                                      trig_mode, vector);
+ */
 void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 				     int trig_mode, int vector)
 {
@@ -3700,6 +3884,9 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 		 * the vCPU exits the guest before the doorbell chimes, hardware
 		 * will automatically process AVIC interrupts at the next VMRUN.
 		 */
+		/*
+		 * 只在此处调用
+		 */
 		avic_ring_doorbell(vcpu);
 	} else {
 		/*
@@ -3710,9 +3897,78 @@ void svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,
 	}
 }
 
+/*
+ * svm_deliver_interrupt
+ * __apic_accept_irq
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_irq_delivery_to_apic
+ * kvm_set_msi
+ * kvm_send_userspace_msi
+ * kvm_vm_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * svm_deliver_interrupt
+ * __apic_accept_irq
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_arch_set_irq_inatomic
+ * irqfd_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * svm_deliver_interrupt
+ * __apic_accept_irq
+ * kvm_irq_delivery_to_apic_fast
+ * kvm_irq_delivery_to_apic
+ * kvm_apic_send_ipi
+ * kvm_x2apic_icr_write
+ * handle_fastpath_set_msr_irqoff
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * svm_deliver_interrupt
+ * __apic_accept_irq
+ * kvm_apic_local_deliver
+ * kvm_inject_apic_timer_irqs
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * 在以下使用svm_deliver_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|5179| <<global>> .deliver_interrupt = svm_deliver_interrupt,
+ */
 static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 				  int trig_mode, int vector)
 {
+	/*
+	 * 在以下调用kvm_lapic_set_irr():
+	 *   - arch/x86/kvm/svm/svm.c|3767| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+	 *   - arch/x86/kvm/vmx/vmx.c|4319| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+	 */
 	kvm_lapic_set_irr(vector, apic);
 
 	/*
@@ -3723,6 +3979,13 @@ static void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,
 	 * will signal the doorbell if the CPU has already entered the guest.
 	 */
 	smp_mb__after_atomic();
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/svm/avic.c|343| <<avic_kick_vcpu>> svm_complete_interrupt_delivery(vcpu, icrl & APIC_MODE_MASK,
+	 *                                      icrl & APIC_INT_LEVELTRIG, icrl & APIC_VECTOR_MASK);
+	 *   - arch/x86/kvm/svm/svm.c|3750| <<svm_deliver_interrupt>> svm_complete_interrupt_delivery(apic->vcpu, delivery_mode,
+	 *                                      trig_mode, vector);
+	 */
 	svm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);
 }
 
@@ -3856,6 +4119,21 @@ static int svm_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 	return 1;
 }
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ */
 static void svm_enable_irq_window(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4016,6 +4294,10 @@ static inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4264| <<svm_vcpu_run>> sync_lapic_to_cr8(vcpu);
+ */
 static inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -4062,6 +4344,11 @@ static void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 		kvm_rip_write(vcpu, svm->soft_int_old_rip);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|4181| <<svm_cancel_injection>> svm_complete_interrupts(vcpu);
+ *   - arch/x86/kvm/svm/svm.c|4355| <<svm_vcpu_run>> svm_complete_interrupts(vcpu);
+ */
 static void svm_complete_interrupts(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -5172,6 +5459,10 @@ static __init void svm_adjust_mmio_mask(void)
 	kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
 }
 
+/*
+ * caled by:
+ *   - arch/x86/kvm/svm/svm.c|5441| <<svm_hardware_setup>> svm_set_cpu_caps();
+ */
 static __init void svm_set_cpu_caps(void)
 {
 	kvm_set_cpu_caps();
@@ -5250,6 +5541,14 @@ static __init void svm_set_cpu_caps(void)
 	kvm_cpu_cap_clear(X86_FEATURE_BUS_LOCK_DETECT);
 }
 
+/*
+ * 5470 static struct kvm_x86_init_ops svm_init_ops __initdata = {
+ * 5471         .hardware_setup = svm_hardware_setup,
+ * 5472
+ * 5473         .runtime_ops = &svm_x86_ops,
+ * 5474         .pmu_ops = &amd_pmu_ops,
+ * 5475 };
+ */
 static __init int svm_hardware_setup(void)
 {
 	int cpu;
@@ -5357,6 +5656,12 @@ static __init int svm_hardware_setup(void)
 			goto err;
 	}
 
+	/*
+	 * 在以下使用avic:
+	 *   - arch/x86/kvm/svm/svm.c|230| <<global>> static bool avic;
+	 *   - arch/x86/kvm/svm/svm.c|231| <<global>> module_param(avic, bool, 0444);
+	 *   - arch/x86/kvm/svm/svm.c|5454| <<svm_hardware_setup>> enable_apicv = avic = avic && avic_hardware_setup();
+	 */
 	enable_apicv = avic = avic && avic_hardware_setup();
 
 	if (!enable_apicv) {
@@ -5364,6 +5669,17 @@ static __init int svm_hardware_setup(void)
 		svm_x86_ops.vcpu_unblocking = NULL;
 		svm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;
 	} else if (!x2avic_enabled) {
+		/*
+		 * 在以下设置x2avic_enabled:
+		 *   - arch/x86/kvm/svm/avic.c|1220| <<avic_hardware_setup>> x2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);
+		 * 在以下使用x2avic_enabled:
+		 *   - arch/x86/kvm/svm/avic.c|101| <<avic_activate_vmcb>> if (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {
+		 *   - arch/x86/kvm/svm/avic.c|268| <<avic_get_physical_id_entry>> if ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||
+		 *   - arch/x86/kvm/svm/avic.c|283| <<avic_init_backing_page>> if ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||
+		 *   - arch/x86/kvm/svm/avic.c|1221| <<avic_hardware_setup>> if (x2avic_enabled)
+		 *   - arch/x86/kvm/svm/svm.c|904| <<svm_set_x2apic_msr_interception>> if (!x2avic_enabled)
+		 *   - arch/x86/kvm/svm/svm.c|5417| <<svm_hardware_setup>> } else if (!x2avic_enabled) {
+		 */
 		svm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization = true;
 	}
 
diff --git a/arch/x86/kvm/svm/svm.h b/arch/x86/kvm/svm/svm.h
index 43fa6a16e..c8381a793 100644
--- a/arch/x86/kvm/svm/svm.h
+++ b/arch/x86/kvm/svm/svm.h
@@ -119,6 +119,15 @@ struct kvm_svm {
 
 	/* Struct members for AVIC */
 	u32 avic_vm_id;
+	/*
+	 * 在以下使用kvm_svm->avic_logical_id_table_page:
+	 *   - arch/x86/kvm/svm/avic.c|210| <<avic_vm_destroy>> if (kvm_svm->avic_logical_id_table_page)
+	 *   - arch/x86/kvm/svm/avic.c|211| <<avic_vm_destroy>> __free_page(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|245| <<avic_vm_init>> kvm_svm->avic_logical_id_table_page = l_page;
+	 *   - arch/x86/kvm/svm/avic.c|276| <<avic_init_vmcb>> phys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));
+	 *   - arch/x86/kvm/svm/avic.c|504| <<avic_kick_target_vcpus_fast>> avic_logical_id_table = page_address(kvm_svm->avic_logical_id_table_page);
+	 *   - arch/x86/kvm/svm/avic.c|627| <<avic_get_logical_id_entry>> logical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);
+	 */
 	struct page *avic_logical_id_table_page;
 	struct page *avic_physical_id_table_page;
 	struct hlist_node hnode;
@@ -301,6 +310,17 @@ struct vcpu_svm {
 
 	u32 ldr_reg;
 	u32 dfr_reg;
+	/*
+	 * 在以下使用vmcb_control_area->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|406| <<avic_init_vmcb>> vmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;
+	 *   - arch/x86/kvm/svm/svm.c|3452| <<dump_vmcb>> pr_err("%-20s%016llx\n", "avic_backing_page:", control->avic_backing_page);
+	 * 在以下使用vcpu_svm->avic_backing_page:
+	 *   - arch/x86/kvm/svm/avic.c|402| <<avic_init_vmcb>> phys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|486| <<avic_init_backing_page>> svm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);
+	 *   - arch/x86/kvm/svm/avic.c|493| <<avic_init_backing_page>> new_entry = __sme_set((page_to_phys(svm->avic_backing_page) &
+	 *   - arch/x86/kvm/svm/avic.c|1142| <<get_pi_vcpu_info>> vcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));
+	 *   - arch/x86/kvm/svm/avic.c|1240| <<avic_pi_update_irte>> pi.base = __sme_set(page_to_phys(svm->avic_backing_page) &
+	 */
 	struct page *avic_backing_page;
 	u64 *avic_physical_id_cache;
 
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index aa78b6f38..ddd82d5a8 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -2634,6 +2634,10 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3589| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
@@ -5015,6 +5019,15 @@ void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,
 	 * doesn't isolate different VMCSs, i.e. in this case, doesn't provide
 	 * separate modes for L2 vs L1.
 	 */
+	/*
+	 * 在以下使用indirect_branch_prediction_barrier():
+	 *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+	 */
 	if (guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))
 		indirect_branch_prediction_barrier();
 
diff --git a/arch/x86/kvm/vmx/pmu_intel.c b/arch/x86/kvm/vmx/pmu_intel.c
index 9c9d4a336..618ca7616 100644
--- a/arch/x86/kvm/vmx/pmu_intel.c
+++ b/arch/x86/kvm/vmx/pmu_intel.c
@@ -306,12 +306,29 @@ static int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		msr_info->data = pmu->fixed_ctr_ctrl;
 		break;
 	case MSR_IA32_PEBS_ENABLE:
+		/*
+		 * 在以下使用kvm_pmu->pebs_enable:
+		 *   - arch/x86/kvm/pmu.c|289| <<pmc_reprogram_counter>> bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
+		 *   - arch/x86/kvm/pmu.c|376| <<pmc_resume_counter>> if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
+		 *          (!!pmc->perf_event->attr.precise_ip))
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|309| <<intel_pmu_get_msr(MSR_IA32_PEBS_ENABLE)>> msr_info->data = pmu->pebs_enable;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|361| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> if (pmu->pebs_enable != data) {
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|362| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> diff = pmu->pebs_enable ^ data;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> pmu->pebs_enable = data;
+		 *   - arch/x86/kvm/vmx/vmx.c|7240| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+		 */
 		msr_info->data = pmu->pebs_enable;
 		break;
 	case MSR_IA32_DS_AREA:
 		msr_info->data = pmu->ds_area;
 		break;
 	case MSR_PEBS_DATA_CFG:
+		/*
+		 * 在以下使用kvm_pmu->pebs_data_cfg:
+		 *   - arch/x86/events/intel/core.c|4244| <<intel_guest_get_msrs>> .guest = kvm_pmu->pebs_data_cfg,
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|315| <<intel_pmu_get_msr(MSR_PEBS_DATA_CFG)>> msr_info->data = pmu->pebs_data_cfg;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|377| <<intel_pmu_set_msr(MSR_PEBS_DATA_CFG)>> pmu->pebs_data_cfg = data;
+		 */
 		msr_info->data = pmu->pebs_data_cfg;
 		break;
 	default:
@@ -355,6 +372,17 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 			reprogram_fixed_counters(pmu, data);
 		break;
 	case MSR_IA32_PEBS_ENABLE:
+		/*
+		 * 在以下使用kvm_pmu->pebs_enable:
+		 *   - arch/x86/kvm/pmu.c|289| <<pmc_reprogram_counter>> bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
+		 *   - arch/x86/kvm/pmu.c|376| <<pmc_resume_counter>> if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
+		 *          (!!pmc->perf_event->attr.precise_ip))
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|309| <<intel_pmu_get_msr(MSR_IA32_PEBS_ENABLE)>> msr_info->data = pmu->pebs_enable;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|361| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> if (pmu->pebs_enable != data) {
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|362| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> diff = pmu->pebs_enable ^ data;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> pmu->pebs_enable = data;
+		 *   - arch/x86/kvm/vmx/vmx.c|7240| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+		 */
 		if (data & pmu->pebs_enable_rsvd)
 			return 1;
 
@@ -374,6 +402,12 @@ static int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 		if (data & pmu->pebs_data_cfg_rsvd)
 			return 1;
 
+		/*
+		 * 在以下使用kvm_pmu->pebs_data_cfg:
+		 *   - arch/x86/events/intel/core.c|4244| <<intel_guest_get_msrs>> .guest = kvm_pmu->pebs_data_cfg,
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|315| <<intel_pmu_get_msr(MSR_PEBS_DATA_CFG)>> msr_info->data = pmu->pebs_data_cfg;
+		 *   - arch/x86/kvm/vmx/pmu_intel.c|377| <<intel_pmu_set_msr(MSR_PEBS_DATA_CFG)>> pmu->pebs_data_cfg = data;
+		 */
 		pmu->pebs_data_cfg = data;
 		break;
 	default:
@@ -456,6 +490,23 @@ static void intel_pmu_enable_fixed_counter_bits(struct kvm_pmu *pmu, u64 bits)
 		pmu->fixed_ctr_ctrl_rsvd &= ~intel_fixed_bits_by_idx(i, bits);
 }
 
+/*
+ * 728 struct kvm_pmu_ops intel_pmu_ops __initdata = {
+ * 729         .rdpmc_ecx_to_pmc = intel_rdpmc_ecx_to_pmc,
+ * 730         .msr_idx_to_pmc = intel_msr_idx_to_pmc,
+ * 731         .is_valid_msr = intel_is_valid_msr,
+ * 732         .get_msr = intel_pmu_get_msr,
+ * 733         .set_msr = intel_pmu_set_msr,
+ * 734         .refresh = intel_pmu_refresh,
+ * 735         .init = intel_pmu_init,
+ * 736         .reset = intel_pmu_reset,
+ * 737         .deliver_pmi = intel_pmu_deliver_pmi,
+ * 738         .cleanup = intel_pmu_cleanup,
+ * 739         .EVENTSEL_EVENT = ARCH_PERFMON_EVENTSEL_EVENT,
+ * 740         .MAX_NR_GP_COUNTERS = KVM_MAX_NR_INTEL_GP_COUNTERS,
+ * 741         .MIN_NR_GP_COUNTERS = 1,
+ * 742 };
+ */
 static void intel_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 893366e53..fdfe7b511 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -247,6 +247,34 @@ static const struct {
 #define L1D_CACHE_ORDER 4
 static void *vmx_l1d_flush_pages;
 
+/*
+ * 在以下使用l1tf_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|3055| <<global>> enum l1tf_mitigations l1tf_mitigation __ro_after_init =
+ *   - arch/x86/kernel/cpu/bugs.c|3058| <<global>> EXPORT_SYMBOL_GPL(l1tf_mitigation);
+ *   - arch/x86/kernel/cpu/bugs.c|3110| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3112| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3116| <<l1tf_select_mitigation>> switch (l1tf_mitigation) {
+ *   - arch/x86/kernel/cpu/bugs.c|3136| <<l1tf_select_mitigation>> if (l1tf_mitigation != L1TF_MITIGATION_OFF &&
+ *   - arch/x86/kernel/cpu/bugs.c|3158| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3160| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOWARN;
+ *   - arch/x86/kernel/cpu/bugs.c|3162| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH;
+ *   - arch/x86/kernel/cpu/bugs.c|3164| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3166| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL;
+ *   - arch/x86/kernel/cpu/bugs.c|3168| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL_FORCE;
+ *   - arch/x86/kvm/vmx/vmx.c|272| <<vmx_setup_l1d_flush>> switch (l1tf_mitigation) {
+ *   - arch/x86/kvm/vmx/vmx.c|286| <<vmx_setup_l1d_flush>> } else if (l1tf_mitigation == L1TF_MITIGATION_FULL_FORCE) {
+ *   - arch/x86/kvm/vmx/vmx.c|7787| <<vmx_vm_init>> switch (l1tf_mitigation) {
+ *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+ *
+ * enum l1tf_mitigations {
+ *     L1TF_MITIGATION_OFF,
+ *     L1TF_MITIGATION_FLUSH_NOWARN,
+ *     L1TF_MITIGATION_FLUSH,
+ *     L1TF_MITIGATION_FLUSH_NOSMT,
+ *     L1TF_MITIGATION_FULL,
+ *     L1TF_MITIGATION_FULL_FORCE
+ */
+
 static int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)
 {
 	struct page *page;
@@ -1477,6 +1505,45 @@ void vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,
 		 * performs IBPB on nested VM-Exit (a single nested transition
 		 * may switch the active VMCS multiple times).
 		 */
+		/*
+		 * 在以下调用switch_mm_irqs_off():
+		 *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+		 *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+		 *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+		 *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+		 *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+		 *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+		 *
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *
+		 * 在以下使用X86_FEATURE_USE_IBPB:
+		 *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+		 *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+		 *
+		 * 部分例子.
+		 *
+		 * context_switch()
+		 * -> switch_mm_irqs_off()
+		 *    -> cond_mitigation()
+		 *       -> indirect_branch_prediction_barrier()
+		 *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+		 *                                   x86_pred_cmd,
+		 *                                   X86_FEATURE_USE_IBPB);
+		 *
+		 * switch_mm()
+		 * -> switch_mm_irqs_off()
+		 *    -> cond_mitigation()
+		 *       -> indirect_branch_prediction_barrier()
+		 *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+		 *                                   x86_pred_cmd,
+		 *                                   X86_FEATURE_USE_IBPB);
+		 */
 		if (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))
 			indirect_branch_prediction_barrier();
 	}
@@ -1523,6 +1590,19 @@ void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 
 	vmx_vcpu_pi_load(vcpu, cpu);
 
+	/*
+	 * 在以下使用vcpu_vmx->host_debugctlmsr:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_run>> if (vmx->host_debugctlmsr)
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 *
+	 * 在以下调用get_debugctlmsr():
+	 *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+	 *   - arch/x86/events/intel/ds.c|832| <<intel_pmu_enable_bts>> debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/events/intel/ds.c|856| <<intel_pmu_disable_bts>> debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kernel/step.c|188| <<set_task_blockstep>> debugctl = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 */
 	vmx->host_debugctlmsr = get_debugctlmsr();
 }
 
@@ -4303,6 +4383,11 @@ void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,
 	struct kvm_vcpu *vcpu = apic->vcpu;
 
 	if (vmx_deliver_posted_interrupt(vcpu, vector)) {
+		/*
+		 * 在以下调用kvm_lapic_set_irr():
+		 *   - arch/x86/kvm/svm/svm.c|3767| <<svm_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+		 *   - arch/x86/kvm/vmx/vmx.c|4319| <<vmx_deliver_interrupt>> kvm_lapic_set_irr(vector, apic);
+		 */
 		kvm_lapic_set_irr(vector, apic);
 		kvm_make_request(KVM_REQ_EVENT, vcpu);
 		kvm_vcpu_kick(vcpu);
@@ -5818,6 +5903,16 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 		error_code |= (exit_qualification & EPT_VIOLATION_GVA_TRANSLATED) ?
 			      PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 
+	/*
+	 * 在以下使用kvm_vcpu_is_legal_gpa():
+	 *   - arch/x86/kvm/cpuid.h|53| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+	 *   - arch/x86/kvm/cpuid.h|274| <<kvm_vcpu_is_legal_cr3>> return kvm_vcpu_is_legal_gpa(vcpu, cr3);
+	 *   - arch/x86/kvm/svm/nested.c|256| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+	 *   - arch/x86/kvm/svm/nested.c|257| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+	 *   - arch/x86/kvm/vmx/nested.c|834| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+	 *   - arch/x86/kvm/vmx/nested.c|2823| <<nested_vmx_check_eptp>> if (CC(!kvm_vcpu_is_legal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+	 *   - arch/x86/kvm/vmx/vmx.c|5829| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && !kvm_vcpu_is_legal_gpa(vcpu, gpa)))
+	 */
 	/*
 	 * Check that the GPA doesn't exceed physical memory limits, as that is
 	 * a guest page fault.  We have to emulate the instruction here, because
@@ -5829,6 +5924,17 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 	if (unlikely(allow_smaller_maxphyaddr && !kvm_vcpu_is_legal_gpa(vcpu, gpa)))
 		return kvm_emulate_instruction(vcpu, 0);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *                      insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                      svm->vmcb->control.insn_bytes : NULL,
+	 *                      svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
@@ -5850,6 +5956,17 @@ static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *                      insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                      svm->vmcb->control.insn_bytes : NULL,
+	 *                      svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 }
 
@@ -5865,6 +5982,11 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5927| <<handle_invalid_guest_state>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|5951| <<vmx_vcpu_pre_run>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ */
 static bool vmx_emulation_required_with_pending_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7180,6 +7302,10 @@ void vmx_cancel_injection(struct kvm_vcpu *vcpu)
 	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|7476| <<vmx_vcpu_run>> atomic_switch_perf_msrs(vmx);
+ */
 static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 {
 	int i, nr_msrs;
@@ -7187,6 +7313,17 @@ static void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)
 	struct kvm_pmu *pmu = vcpu_to_pmu(&vmx->vcpu);
 
 	pmu->host_cross_mapped_mask = 0;
+	/*
+	 * 在以下使用kvm_pmu->pebs_enable:
+	 *   - arch/x86/kvm/pmu.c|289| <<pmc_reprogram_counter>> bool pebs = test_bit(pmc->idx, (unsigned long *)&pmu->pebs_enable);
+	 *   - arch/x86/kvm/pmu.c|376| <<pmc_resume_counter>> if (test_bit(pmc->idx, (unsigned long *)&pmc_to_pmu(pmc)->pebs_enable) !=
+	 *          (!!pmc->perf_event->attr.precise_ip))
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|309| <<intel_pmu_get_msr(MSR_IA32_PEBS_ENABLE)>> msr_info->data = pmu->pebs_enable;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|361| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> if (pmu->pebs_enable != data) {
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|362| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> diff = pmu->pebs_enable ^ data;
+	 *   - arch/x86/kvm/vmx/pmu_intel.c|363| <<intel_pmu_set_msr(MSR_IA32_PEBS_ENABLE)>> pmu->pebs_enable = data;
+	 *   - arch/x86/kvm/vmx/vmx.c|7240| <<atomic_switch_perf_msrs>> if (pmu->pebs_enable & pmu->global_ctrl)
+	 */
 	if (pmu->pebs_enable & pmu->global_ctrl)
 		intel_pmu_cross_mapped_check(pmu);
 
@@ -7240,6 +7377,19 @@ void noinstr vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)
 void noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,
 					unsigned int flags)
 {
+	/*
+	 * 在以下使用percpu的x86_spec_ctrl_current:
+	 *   - arch/x86/entry/calling.h|341| <<global>> movq PER_CPU_VAR(x86_spec_ctrl_current), %rdx
+	 *   - arch/x86/include/asm/nospec-branch.h|550| <<global>> DECLARE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|125| <<global>> DEFINE_PER_CPU(u64, x86_spec_ctrl_current);
+	 *   - arch/x86/kernel/cpu/bugs.c|126| <<global>> EXPORT_PER_CPU_SYMBOL_GPL(x86_spec_ctrl_current);
+	 *   - arch/x86/include/asm/spec-ctrl.h|92| <<__update_spec_ctrl>> __this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|140| <<update_spec_ctrl>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|150| <<update_spec_ctrl_cond>> if (this_cpu_read(x86_spec_ctrl_current) == val)
+	 *   - arch/x86/kernel/cpu/bugs.c|153| <<update_spec_ctrl_cond>> this_cpu_write(x86_spec_ctrl_current, val);
+	 *   - arch/x86/kernel/cpu/bugs.c|165| <<spec_ctrl_current>> return this_cpu_read(x86_spec_ctrl_current);
+	 *   - arch/x86/kvm/vmx/vmx.c|7322| <<vmx_spec_ctrl_restore_host>> u64 hostval = this_cpu_read(x86_spec_ctrl_current);
+	 */
 	u64 hostval = this_cpu_read(x86_spec_ctrl_current);
 
 	if (!cpu_feature_enabled(X86_FEATURE_MSR_SPEC_CTRL))
@@ -7255,6 +7405,17 @@ void noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,
 	 * transitioning from a less privileged predictor mode, regardless of
 	 * whether the guest/host values differ.
 	 */
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+	 *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
 	    vmx->spec_ctrl != hostval)
 		native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
@@ -7445,6 +7606,19 @@ fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu, bool force_immediate_exit)
 		current_evmcs->hv_vp_id = kvm_hv_get_vpindex(vcpu);
 	}
 
+	/*
+	 * 在以下使用vcpu_vmx->host_debugctlmsr:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_run>> if (vmx->host_debugctlmsr)
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 *
+	 * 在以下调用update_debugctlmsr():
+	 *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+	 *   - arch/x86/events/intel/ds.c|845| <<intel_pmu_enable_bts>> update_debugctlmsr(debugctlmsr);
+	 *   - arch/x86/events/intel/ds.c|862| <<intel_pmu_disable_bts>> update_debugctlmsr(debugctlmsr);
+	 *   - arch/x86/kernel/step.c|197| <<set_task_blockstep>> update_debugctlmsr(debugctl);
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 */
 	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 	if (vmx->host_debugctlmsr)
 		update_debugctlmsr(vmx->host_debugctlmsr);
@@ -7629,6 +7803,34 @@ int vmx_vcpu_create(struct kvm_vcpu *vcpu)
 	return err;
 }
 
+/*
+ * 在以下使用l1tf_mitigation:
+ *   - arch/x86/kernel/cpu/bugs.c|3055| <<global>> enum l1tf_mitigations l1tf_mitigation __ro_after_init =
+ *   - arch/x86/kernel/cpu/bugs.c|3058| <<global>> EXPORT_SYMBOL_GPL(l1tf_mitigation);
+ *   - arch/x86/kernel/cpu/bugs.c|3110| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3112| <<l1tf_select_mitigation>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3116| <<l1tf_select_mitigation>> switch (l1tf_mitigation) {
+ *   - arch/x86/kernel/cpu/bugs.c|3136| <<l1tf_select_mitigation>> if (l1tf_mitigation != L1TF_MITIGATION_OFF &&
+ *   - arch/x86/kernel/cpu/bugs.c|3158| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_OFF;
+ *   - arch/x86/kernel/cpu/bugs.c|3160| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOWARN;
+ *   - arch/x86/kernel/cpu/bugs.c|3162| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH;
+ *   - arch/x86/kernel/cpu/bugs.c|3164| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;
+ *   - arch/x86/kernel/cpu/bugs.c|3166| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL;
+ *   - arch/x86/kernel/cpu/bugs.c|3168| <<l1tf_cmdline>> l1tf_mitigation = L1TF_MITIGATION_FULL_FORCE;
+ *   - arch/x86/kvm/vmx/vmx.c|272| <<vmx_setup_l1d_flush>> switch (l1tf_mitigation) {
+ *   - arch/x86/kvm/vmx/vmx.c|286| <<vmx_setup_l1d_flush>> } else if (l1tf_mitigation == L1TF_MITIGATION_FULL_FORCE) {
+ *   - arch/x86/kvm/vmx/vmx.c|7787| <<vmx_vm_init>> switch (l1tf_mitigation) {
+ *   - arch/x86/mm/init.c|1042| <<arch_max_swapfile_size>> if (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {
+ *
+ * enum l1tf_mitigations {
+ *     L1TF_MITIGATION_OFF,
+ *     L1TF_MITIGATION_FLUSH_NOWARN,
+ *     L1TF_MITIGATION_FLUSH,
+ *     L1TF_MITIGATION_FLUSH_NOSMT,
+ *     L1TF_MITIGATION_FULL,
+ *     L1TF_MITIGATION_FULL_FORCE
+ */
+
 #define L1TF_MSG_SMT "L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n"
 #define L1TF_MSG_L1D "L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\n"
 
@@ -8338,6 +8540,15 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 43f573f6c..7b892e994 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -184,6 +184,18 @@ struct nested_vmx {
 	 */
 	bool enlightened_vmcs_enabled;
 
+	/*
+	 * 在以下设置nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|3748| <<nested_vmx_run>> vmx->nested.nested_run_pending = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|3788| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|3793| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|3803| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<vmx_leave_nested>> to_vmx(vcpu)->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|6859| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = !!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);
+	 *   - arch/x86/kvm/vmx/nested.c|6911| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7510| <<vmx_vcpu_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8277| <<vmx_leave_smm>> vmx->nested.nested_run_pending = 1;
+	 */
 	/* L2 must run next, and mustn't decide to exit to L1. */
 	bool nested_run_pending;
 
@@ -336,6 +348,12 @@ struct vcpu_vmx {
 	/* apic deadline value in host tsc */
 	u64 hv_deadline_tsc;
 
+	/*
+	 * 在以下使用vcpu_vmx->host_debugctlmsr:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_run>> if (vmx->host_debugctlmsr)
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 */
 	unsigned long host_debugctlmsr;
 
 	/*
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c79a8cc57..f4833f287 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -89,6 +89,54 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+/*
+ * 在以下使用MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/kvm/svm/svm.c|123| <<global>> { .index = MSR_IA32_SPEC_CTRL, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|171| <<global>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/x86.c|381| <<global>> MSR_IA32_SPEC_CTRL, MSR_IA32_TSX_CTRL,
+ *   - tools/arch/x86/include/asm/msr-index.h|70| <<global>> #define MSR_IA32_SPEC_CTRL 0x00000048
+ *   - arch/x86/include/asm/nospec-branch.h|554| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/nospec-branch.h|563| <<firmware_restrict_branch_speculation_end>> alternative_msr_write(MSR_IA32_SPEC_CTRL, \
+ *   - arch/x86/include/asm/spec-ctrl.h|87| <<__update_spec_ctrl>> native_wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|74| <<update_spec_ctrl>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|93| <<update_spec_ctrl_cond>> wrmsrl(MSR_IA32_SPEC_CTRL, val);
+ *   - arch/x86/kernel/cpu/bugs.c|139| <<cpu_select_mitigations>> rdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);
+ *   - arch/x86/kvm/svm/svm.c|1407| <<init_vmcb>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|3021| <<svm_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3167| <<svm_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/svm/svm.c|3193| <<svm_set_msr>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);
+ *   - arch/x86/kvm/svm/svm.c|4444| <<svm_vcpu_run>> bool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/nested.c|709| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);
+ *   - arch/x86/kvm/vmx/vmx.c|955| <<__vmx_vcpu_run_flags>> if (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))
+ *   - arch/x86/kvm/vmx/vmx.c|2059| <<vmx_get_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2321| <<vmx_set_msr>> case MSR_IA32_SPEC_CTRL:
+ *   - arch/x86/kvm/vmx/vmx.c|2346| <<vmx_set_msr>> MSR_IA32_SPEC_CTRL,
+ *   - arch/x86/kvm/vmx/vmx.c|7319| <<vmx_spec_ctrl_restore_host>> vmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);
+ *   - arch/x86/kvm/vmx/vmx.c|7330| <<vmx_spec_ctrl_restore_host>> native_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);
+ *   - arch/x86/kvm/x86.c|14128| <<kvm_spec_ctrl_test_value>> if (rdmsrl_safe(MSR_IA32_SPEC_CTRL, &saved_value))
+ *   - arch/x86/kvm/x86.c|14130| <<kvm_spec_ctrl_test_value>> else if (wrmsrl_safe(MSR_IA32_SPEC_CTRL, value))
+ *   - arch/x86/kvm/x86.c|14133| <<kvm_spec_ctrl_test_value>> wrmsrl(MSR_IA32_SPEC_CTRL, saved_value);
+ *   - arch/x86/power/cpu.c|484| <<pm_save_spec_msr>> { MSR_IA32_SPEC_CTRL, X86_FEATURE_MSR_SPEC_CTRL },
+ *   - arch/x86/xen/suspend.c|42| <<xen_vcpu_notify_restore>> wrmsrl(MSR_IA32_SPEC_CTRL, this_cpu_read(spec_ctrl));
+ *   - arch/x86/xen/suspend.c|58| <<xen_vcpu_notify_suspend>> rdmsrl(MSR_IA32_SPEC_CTRL, tmp);
+ *   - arch/x86/xen/suspend.c|60| <<xen_vcpu_notify_suspend>> wrmsrl(MSR_IA32_SPEC_CTRL, 0);
+ *
+ * 在以下使用MSR_IA32_PRED_CMD:
+ *   - arch/x86/include/asm/msr-index.h|119| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/kvm/svm/svm.c|124| <<global>> { .index = MSR_IA32_PRED_CMD, .always = false },
+ *   - arch/x86/kvm/vmx/vmx.c|172| <<global>> MSR_IA32_PRED_CMD,
+ *   - tools/arch/x86/include/asm/msr-index.h|86| <<global>> #define MSR_IA32_PRED_CMD 0x00000049
+ *   - arch/x86/include/asm/nospec-branch.h|536| <<indirect_branch_prediction_barrier>> alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *   - arch/x86/include/asm/nospec-branch.h|557| <<firmware_restrict_branch_speculation_start>> alternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB, \
+ *   - arch/x86/kernel/cpu/amd.c|608| <<early_init_amd>> else if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {
+ *   - arch/x86/kvm/svm/svm.c|4668| <<svm_vcpu_after_set_cpuid>> set_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,
+ *   - arch/x86/kvm/vmx/nested.c|712| <<nested_vmx_prepare_msr_bitmap>> MSR_IA32_PRED_CMD, MSR_TYPE_W);
+ *   - arch/x86/kvm/vmx/vmx.c|7957| <<vmx_vcpu_after_set_cpuid>> vmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,
+ *   - arch/x86/kvm/x86.c|3914| <<kvm_set_msr_common>> case MSR_IA32_PRED_CMD: {
+ *   - arch/x86/kvm/x86.c|3941| <<kvm_set_msr_common>> wrmsrl(MSR_IA32_PRED_CMD, data);
+ */
+
 #define MAX_IO_MSRS 256
 #define KVM_MAX_MCE_BANKS 32
 
@@ -185,6 +233,33 @@ module_param(force_emulation_prefix, int, 0644);
 int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, 0644);
 
+/*
+ * 在以下使用global的enable_pmu:
+ *   - arch/x86/kvm/cpuid.c|1063| <<__do_cpuid_func>> if (!enable_pmu || !static_cpu_has(X86_FEATURE_ARCH_PERFMON)) {
+ *   - arch/x86/kvm/cpuid.c|1399| <<__do_cpuid_func>> if (!enable_pmu || !kvm_cpu_cap_has(X86_FEATURE_PERFMON_V2)) {
+ *   - arch/x86/kvm/pmu.h|216| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|218| <<kvm_init_pmu_capability>> if (enable_pmu) {
+ *   - arch/x86/kvm/pmu.h|229| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|231| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|234| <<kvm_init_pmu_capability>> if (!enable_pmu) {
+ *   - arch/x86/kvm/svm/svm.c|5272| <<svm_set_cpu_caps>> if (enable_pmu) {
+ *   - arch/x86/kvm/svm/svm.c|5450| <<svm_hardware_setup>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|7968| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|8037| <<vmx_set_cpu_caps>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|8627| <<vmx_hardware_setup>> if (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())
+ *   - arch/x86/kvm/x86.c|4803| <<kvm_vm_ioctl_check_extension>> r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
+ *   - arch/x86/kvm/x86.c|6726| <<kvm_vm_ioctl_enable_cap>> if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
+ *   - arch/x86/kvm/x86.c|7542| <<kvm_init_msr_lists>> if (enable_pmu) {
+ *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+ *
+ * 在以下使用kvm_arch->enable_pmu:
+ *   - arch/x86/kvm/pmu.c|938| <<kvm_pmu_refresh>> if (!vcpu->kvm->arch.enable_pmu)
+ *   - arch/x86/kvm/svm/pmu.c|44| <<get_gp_pmc_amd>> if (!vcpu->kvm->arch.enable_pmu)
+ *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+ *   - arch/x86/kvm/x86.c|6731| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+ *
+ * enable_pmu不可以被动态修改
+ */
 /* Enable/disable PMU virtualization */
 bool __read_mostly enable_pmu = true;
 EXPORT_SYMBOL_GPL(enable_pmu);
@@ -226,6 +301,34 @@ static struct kvm_user_return_msrs __percpu *user_return_msrs;
 bool __read_mostly allow_smaller_maxphyaddr = 0;
 EXPORT_SYMBOL_GPL(allow_smaller_maxphyaddr);
 
+/*
+ * 在以下使用enable_apicv:
+ *   - arch/x86/kvm/vmx/vmx.c|113| <<global>> module_param(enable_apicv, bool, 0444);
+ *   - arch/x86/kvm/x86.c|256| <<global>> bool __read_mostly enable_apicv = true;
+ *   - arch/x86/kvm/hyperv.c|134| <<synic_update_vector>> if (!enable_apicv)
+ *   - arch/x86/kvm/lapic.c|2952| <<kvm_create_lapic>> if (enable_apicv) {
+ *   - arch/x86/kvm/svm/avic.c|294| <<avic_vm_destroy>> if (!enable_apicv) 
+ *   - arch/x86/kvm/svm/avic.c|330| <<avic_vm_init>> if (!enable_apicv)
+ *   - arch/x86/kvm/svm/avic.c|945| <<avic_init_vcpu>> if (!enable_apicv || !irqchip_in_kernel(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|1451| <<avic_refresh_virtual_apic_mode>> if (!lapic_in_kernel(vcpu) || !enable_apicv)
+ *   - arch/x86/kvm/svm/avic.c|1483| <<avic_refresh_apicv_exec_ctrl>> if (!enable_apicv)
+ *   - arch/x86/kvm/svm/svm.c|5057| <<svm_vm_init>> if (enable_apicv) {
+ *   - arch/x86/kvm/svm/svm.c|5454| <<svm_hardware_setup>> enable_apicv = avic = avic && avic_hardware_setup();
+ *   - arch/x86/kvm/svm/svm.c|5456| <<svm_hardware_setup>> if (!enable_apicv) {
+ *   - arch/x86/kvm/vmx/nested.c|6972| <<nested_vmx_setup_pinbased_ctls>> (enable_apicv ? PIN_BASED_POSTED_INTR : 0);
+ *   - arch/x86/kvm/vmx/posted_intr.c|66| <<vmx_vcpu_pi_load>> if (!enable_apicv || !lapic_in_kernel(vcpu))
+ *   - arch/x86/kvm/vmx/posted_intr.c|137| <<vmx_can_use_vtd_pi>> return irqchip_in_kernel(kvm) && enable_apicv &&
+ *   - arch/x86/kvm/vmx/vmx.c|4106| <<vmx_update_msr_bitmap_x2apic>> if (enable_apicv && kvm_vcpu_apicv_active(vcpu))
+ *   - arch/x86/kvm/vmx/vmx.c|4770| <<init_vmcs>> if (enable_apicv && lapic_in_kernel(&vmx->vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|6969| <<vmx_sync_pir_to_irr>> if (KVM_BUG_ON(!enable_apicv, vcpu->kvm))
+ *   - arch/x86/kvm/vmx/vmx.c|8570| <<vmx_hardware_setup>> enable_apicv = 0;
+ *   - arch/x86/kvm/vmx/vmx.c|8571| <<vmx_hardware_setup>> if (!enable_apicv)
+ *   - arch/x86/kvm/vmx/vmx.c|8574| <<vmx_hardware_setup>> if (!enable_apicv || !cpu_has_vmx_ipiv())
+ *   - arch/x86/kvm/x86.c|10140| <<kvm_apicv_init>> enum kvm_apicv_inhibit reason = enable_apicv ? APICV_INHIBIT_REASON_ABSENT :
+ *   - arch/x86/kvm/x86.c|10945| <<kvm_set_or_clear_apicv_inhibit>> if (!enable_apicv)
+ *   - arch/x86/kvm/x86.c|12439| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> if (!enable_apicv)
+ *   - arch/x86/kvm/x86.c|13978| <<kvm_arch_has_irq_bypass>> return enable_apicv && irq_remapping_cap(IRQ_POSTING_CAP);
+ */
 bool __read_mostly enable_apicv = true;
 EXPORT_SYMBOL_GPL(enable_apicv);
 
@@ -974,9 +1077,24 @@ void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1370| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5207| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	atomic_inc(&vcpu->arch.nmi_queued);
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * 处理的函数: process_nmi()
+	 */
 	kvm_make_request(KVM_REQ_NMI, vcpu);
 }
 
@@ -2282,6 +2400,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3953| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK_NEW)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|3960| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2307,6 +2430,13 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 
 	wall_nsec = kvm_get_wall_clock_epoch(kvm);
 
+	/*
+	 * struct pvclock_wall_clock {
+	 *     u32   version;
+	 *     u32   sec;
+	 *     u32   nsec;
+	 * } __attribute__((__packed__));
+	 */
 	wc.nsec = do_div(wall_nsec, NSEC_PER_SEC);
 	wc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */
 	wc.version = version;
@@ -3315,6 +3445,11 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
  * Fall back to using their values at slightly different moments by
  * calling ktime_get_real_ns() and get_kvmclock_ns() separately.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2308| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ */
 uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -4573,6 +4708,10 @@ static bool kvm_is_vm_type_supported(unsigned long type)
 	return type < 32 && (kvm_caps.supported_vm_types & BIT(type));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4888| <<kvm_vm_ioctl_check_extension_generic>> return kvm_vm_ioctl_check_extension(kvm, arg);
+ */
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r = 0;
@@ -4768,6 +4907,15 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	}
 	case KVM_CAP_PMU_CAPABILITY:
+		/*
+		 * 在以下使用KVM_CAP_PMU_CAPABILITY:
+		 *   - include/uapi/linux/kvm.h|909| <<global>> #define KVM_CAP_PMU_CAPABILITY 212
+		 *   - tools/include/uapi/linux/kvm.h|909| <<global>> #define KVM_CAP_PMU_CAPABILITY 212
+		 *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|328| <<global>> r = kvm_check_cap(KVM_CAP_PMU_CAPABILITY);
+		 *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|334| <<global>> vm_enable_cap(vm, KVM_CAP_PMU_CAPABILITY, KVM_PMU_CAP_DISABLE);
+		 *   - arch/x86/kvm/x86.c|4802| <<kvm_vm_ioctl_check_extension>> case KVM_CAP_PMU_CAPABILITY:
+		 *   - arch/x86/kvm/x86.c|6724| <<kvm_vm_ioctl_enable_cap>> case KVM_CAP_PMU_CAPABILITY:
+		 */
 		r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
 		break;
 	case KVM_CAP_DISABLE_QUIRKS2:
@@ -4870,6 +5018,14 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		if (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))
 			goto out;
 
+		/*
+		 * // for KVM_SET_CPUID2
+		 * struct kvm_cpuid2 {
+		 *     __u32 nent;
+		 *     __u32 padding;
+		 *     struct kvm_cpuid_entry2 entries[];
+		 * };
+		 */
 		r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,
 					    ioctl);
 		if (r)
@@ -4952,6 +5108,11 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|172| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|6320| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -4960,6 +5121,17 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 
 	if (vcpu->scheduled_out && pmu->version && pmu->event_count) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *
+		 * 处理的函数: kvm_pmu_handle_event()
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 
@@ -5171,6 +5343,11 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 
 static int kvm_vcpu_ioctl_nmi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|1370| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+	 *   - arch/x86/kvm/x86.c|5207| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+	 */
 	kvm_inject_nmi(vcpu);
 
 	return 0;
@@ -5463,6 +5640,16 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	if (events->flags & KVM_VCPUEVENT_VALID_NMI_PENDING) {
 		vcpu->arch.nmi_pending = 0;
 		atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+		/*
+		 * 在以下使用KVM_REQ_NMI:
+		 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *
+		 * 处理的函数: process_nmi()
+		 */
 		if (events->nmi.pending)
 			kvm_make_request(KVM_REQ_NMI, vcpu);
 	}
@@ -6443,6 +6630,10 @@ static int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)
 	return 0;
 }
 
+/*
+ * 处理KVM_REINJECT_CONTROL:
+ *   - arch/x86/kvm/x86.c|7223| <<kvm_arch_vm_ioctl(KVM_REINJECT_CONTROL)>> r = kvm_vm_ioctl_reinject(kvm, &control);
+ */
 static int kvm_vm_ioctl_reinject(struct kvm *kvm,
 				 struct kvm_reinject_control *control)
 {
@@ -9068,6 +9259,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -9272,6 +9469,12 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
@@ -9279,6 +9482,12 @@ EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction_from_buffer);
@@ -9659,6 +9868,11 @@ static void kvm_x86_check_cpu_compat(void *ret)
 	*(int *)ret = kvm_x86_check_processor_compatibility();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5487| <<svm_init>> r = kvm_x86_vendor_init(&svm_init_ops);
+ *   - arch/x86/kvm/vmx/vmx.c|8654| <<vmx_init>> r = kvm_x86_vendor_init(&vt_init_ops);
+ */
 int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	u64 host_pat;
@@ -9897,15 +10111,58 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|229| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4434| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|256| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|290| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|1164| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/nested.c|1248| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
 }
 EXPORT_SYMBOL_GPL(kvm_apicv_activated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1642| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+ *   - arch/x86/kvm/x86.c|10584| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) &&
+ *                  (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);
+ *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) !=
+ *                  kvm_vcpu_apicv_active(vcpu)) && (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
+ */
 bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	/*
+	 * 只有svm:
+	 * avic_vcpu_get_apicv_inhibit_reasons()
+	 */
 	ulong vcpu_reasons =
 			kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
 
@@ -9913,6 +10170,25 @@ bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_apicv_activated);
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+ *   - arch/x86/kvm/x86.c|10614| <<__kvm_set_or_clear_apicv_inhibit>> set_or_clear_apicv_inhibit(&new, reason, set);
+ */
 static void set_or_clear_apicv_inhibit(unsigned long *inhibits,
 				       enum kvm_apicv_inhibit reason, bool set)
 {
@@ -9925,16 +10201,47 @@ static void set_or_clear_apicv_inhibit(unsigned long *inhibits,
 	else
 		__clear_bit(reason, inhibits);
 
+	/*
+	 * 只在此处trace
+	 */
 	trace_kvm_apicv_inhibit_changed(reason, set, *inhibits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12757| <<kvm_arch_init_vm>> kvm_apicv_init(kvm);
+ */
 static void kvm_apicv_init(struct kvm *kvm)
 {
 	enum kvm_apicv_inhibit reason = enable_apicv ? APICV_INHIBIT_REASON_ABSENT :
 						       APICV_INHIBIT_REASON_DISABLED;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	init_rwsem(&kvm->arch.apicv_update_lock);
 }
 
@@ -10251,6 +10558,45 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * apic_has_interrupt_for_ppr
+ * kvm_cpu_has_injectable_intr
+ * kvm_check_and_inject_events
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * apic_has_interrupt_for_ppr
+ * kvm_cpu_has_interrupt
+ * kvm_vcpu_has_events
+ * kvm_arch_vcpu_runnable
+ * kvm_vcpu_check_block
+ * kvm_vcpu_halt
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
@@ -10522,6 +10868,19 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_read(&vcpu->kvm->arch.apicv_update_lock);
 	preempt_disable();
 
@@ -10574,16 +10933,62 @@ static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	__kvm_vcpu_update_apicv(vcpu);
 }
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|148| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+ *                  APICV_INHIBIT_REASON_HYPERV, !!hv->synic_auto_eoi_used);
+ *   - arch/x86/kvm/x86.c|10642| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+ *   - arch/x86/kvm/x86.c|12051| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+ *                  APICV_INHIBIT_REASON_BLOCKIRQ, set);
+ */
 void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				      enum kvm_apicv_inhibit reason, bool set)
 {
 	unsigned long old, new;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
 
 	if (!(kvm_x86_ops.required_apicv_inhibits & BIT(reason)))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	old = new = kvm->arch.apicv_inhibit_reasons;
 
 	set_or_clear_apicv_inhibit(&new, reason, set);
@@ -10615,12 +11020,44 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 	}
 }
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/include/asm/kvm_host.h|2175| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+ *   - arch/x86/include/asm/kvm_host.h|2181| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+ */
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set)
 {
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 	__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
 	up_write(&kvm->arch.apicv_update_lock);
@@ -10777,14 +11214,44 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 		if (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))
 			record_steal_time(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *
+		 * 处理的函数: kvm_pmu_handle_event()
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+		 *
+		 * 处理的函数: kvm_pmu_deliver_pmi()
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 #ifdef CONFIG_KVM_SMM
 		if (kvm_check_request(KVM_REQ_SMI, vcpu))
 			process_smi(vcpu);
 #endif
+		/*
+		 * 在以下使用KVM_REQ_NMI:
+		 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *
+		 * 处理的函数: process_nmi()
+		 */
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -11112,6 +11579,16 @@ static bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 	if (kvm_is_exception_pending(vcpu))
 		return true;
 
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * 处理的函数: process_nmi()
+	 */
 	if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
 	    (vcpu->arch.nmi_pending &&
 	     kvm_x86_call(nmi_allowed)(vcpu, false)))
@@ -11124,6 +11601,15 @@ static bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 		return true;
 #endif
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	if (kvm_test_request(KVM_REQ_PMI, vcpu))
 		return true;
 
@@ -11274,6 +11760,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11544| <<kvm_emulate_halt_noskip>> return __kvm_emulate_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);
+ *   - arch/x86/kvm/x86.c|11581| <<kvm_emulate_ap_reset_hold>> return __kvm_emulate_halt(vcpu, KVM_MP_STATE_AP_RESET_HOLD, KVM_EXIT_AP_RESET_HOLD) && ret;
+ */
 static int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)
 {
 	/*
@@ -11285,6 +11776,17 @@ static int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)
 	 */
 	++vcpu->stat.halt_exits;
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * 在以下使用pv_unhalted:
+		 *   - arch/x86/kvm/lapic.c|1356| <<__apic_accept_irq>> vcpu->arch.pv.pv_unhalted = 1;
+		 *   - arch/x86/kvm/svm/sev.c|3833| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/svm/sev.c|3872| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11352| <<kvm_vcpu_has_events>> if (vcpu->arch.pv.pv_unhalted)
+		 *   - arch/x86/kvm/x86.c|11453| <<vcpu_block>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11532| <<__kvm_emulate_halt>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11599| <<kvm_arch_dy_runnable>> if (READ_ONCE(vcpu->arch.pv.pv_unhalted))
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu->arch.pv.pv_unhalted)
+		 */
 		if (kvm_vcpu_has_events(vcpu))
 			vcpu->arch.pv.pv_unhalted = false;
 		else
@@ -11356,6 +11858,16 @@ bool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)
 	if (READ_ONCE(vcpu->arch.pv.pv_unhalted))
 		return true;
 
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * 处理的函数: process_nmi()
+	 */
 	if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
 #ifdef CONFIG_KVM_SMM
 		kvm_test_request(KVM_REQ_SMI, vcpu) ||
@@ -12014,6 +12526,10 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12107| <<kvm_arch_vcpu_ioctl_set_guest_debug>> kvm_arch_vcpu_guestdbg_update_apicv_inhibit(vcpu->kvm);
+ */
 static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 {
 	bool set = false;
@@ -12023,6 +12539,19 @@ static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
@@ -12225,6 +12754,10 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return kvm_x86_call(vcpu_precreate)(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4106| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -12241,6 +12774,9 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	else
 		vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
 
+	/*
+	 * 只在此处调用
+	 */
 	r = kvm_mmu_create(vcpu);
 	if (r < 0)
 		return r;
@@ -13051,6 +13587,10 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13368| <<kvm_arch_commit_memory_region>> kvm_mmu_slot_apply_flags(kvm, old, new, change);
+ */
 static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new,
@@ -13168,6 +13708,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1668| <<kvm_commit_memory_region>> kvm_arch_commit_memory_region(kvm, old, new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
diff --git a/arch/x86/kvm/x86.h b/arch/x86/kvm/x86.h
index ec623d23d..173c2608c 100644
--- a/arch/x86/kvm/x86.h
+++ b/arch/x86/kvm/x86.h
@@ -135,6 +135,15 @@ static inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)
 	vcpu->arch.exception_vmexit.pending = false;
 }
 
+/*
+ * 在以下使用kvm_queue_interrupt():
+ *   - arch/x86/kvm/svm/svm.c|4280| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, false);
+ *   - arch/x86/kvm/svm/svm.c|4283| <<svm_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, true);
+ *   - arch/x86/kvm/vmx/vmx.c|7214| <<__vmx_complete_interrupts>> kvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);
+ *   - arch/x86/kvm/x86.c|5276| <<kvm_vcpu_ioctl_interrupt>> kvm_queue_interrupt(vcpu, irq->irq, false);
+ *   - arch/x86/kvm/x86.c|10698| <<kvm_check_and_inject_events>> kvm_queue_interrupt(vcpu, irq, false);
+ *   - arch/x86/kvm/x86.c|12401| <<__set_sregs>> kvm_queue_interrupt(vcpu, pending_vec, false);
+ */
 static inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,
 	bool soft)
 {
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index a2becb85b..18a5c917d 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -319,6 +319,45 @@ void leave_mm(void)
 }
 EXPORT_SYMBOL_GPL(leave_mm);
 
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
 void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	       struct task_struct *tsk)
 {
@@ -378,6 +417,10 @@ static unsigned long mm_mangle_tif_spec_bits(struct task_struct *next)
 	return (unsigned long)next->mm | spec_bits;
 }
 
+/*
+ * called by:
+ *   - arch/x86/mm/tlb.c|607| <<switch_mm_irqs_off>> cond_mitigation(tsk);
+ */
 static void cond_mitigation(struct task_struct *next)
 {
 	unsigned long prev_mm, next_mm;
@@ -401,6 +444,14 @@ static void cond_mitigation(struct task_struct *next)
 	 * would only affect the first schedule so the theoretically
 	 * exposed data is not really interesting.
 	 */
+	/*
+	 * 在以下使用switch_mm_cond_ibpb:
+	 *   - arch/x86/include/asm/nospec-branch.h|586| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+	 *   - arch/x86/kernel/cpu/bugs.c|179| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);
+	 *   - arch/x86/kernel/cpu/bugs.c|1450| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_cond_ibpb);
+	 *   - arch/x86/kernel/cpu/bugs.c|2909| <<ibpb_state>> if (static_key_enabled(&switch_mm_cond_ibpb))
+	 *   - arch/x86/mm/tlb.c|408| <<cond_mitigation>> if (static_branch_likely(&switch_mm_cond_ibpb)) {
+	 */
 	if (static_branch_likely(&switch_mm_cond_ibpb)) {
 		/*
 		 * This is a bit more complex than the always mode because
@@ -436,11 +487,29 @@ static void cond_mitigation(struct task_struct *next)
 		 * Issue IBPB only if the mm's are different and one or
 		 * both have the IBPB bit set.
 		 */
+		/*
+		 * 在以下使用indirect_branch_prediction_barrier():
+		 *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+		 */
 		if (next_mm != prev_mm &&
 		    (next_mm | prev_mm) & LAST_USER_MM_IBPB)
 			indirect_branch_prediction_barrier();
 	}
 
+	/*
+	 * 在以下使用switch_mm_always_ibpb:
+	 *   - arch/x86/include/asm/nospec-branch.h|587| <<global>> DECLARE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+	 *   - arch/x86/kernel/cpu/bugs.c|181| <<global>> DEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);
+	 *   - arch/x86/kernel/cpu/bugs.c|1444| <<spectre_v2_user_select_mitigation>> static_branch_enable(&switch_mm_always_ibpb);
+	 *   - arch/x86/kernel/cpu/bugs.c|1455| <<spectre_v2_user_select_mitigation>> static_key_enabled(&switch_mm_always_ibpb) ?
+	 *   - arch/x86/kernel/cpu/bugs.c|2907| <<ibpb_state>> if (static_key_enabled(&switch_mm_always_ibpb))
+	 *   - arch/x86/mm/tlb.c|457| <<cond_mitigation>> if (static_branch_unlikely(&switch_mm_always_ibpb)) {
+	 */
 	if (static_branch_unlikely(&switch_mm_always_ibpb)) {
 		/*
 		 * Only flush when switching to a user space task with a
@@ -496,6 +565,15 @@ static inline void cr4_update_pce_mm(struct mm_struct *mm) { }
  * 'cpu_tlbstate.loaded_mm' instead because it does not always keep
  * 'current->active_mm' up to date.
  */
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ */
 void switch_mm_irqs_off(struct mm_struct *unused, struct mm_struct *next,
 			struct task_struct *tsk)
 {
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 3efe378f1..b9e321224 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -220,6 +220,25 @@ static int virtblk_map_data(struct blk_mq_hw_ctx *hctx, struct request *req,
 		return 0;
 
 	vbr->sg_table.sgl = vbr->sg;
+	/*
+	 * called by:
+	 *   - drivers/block/rnbd/rnbd-clt.c|1131| <<rnbd_queue_rq>> err = sg_alloc_table_chained(&iu->sgt,
+	 *   - drivers/block/virtio_blk.c|223| <<virtblk_map_data>> err = sg_alloc_table_chained(&vbr->sg_table,
+	 *   - drivers/nvme/host/fc.c|2608| <<nvme_fc_map_data>> ret = sg_alloc_table_chained(&freq->sg_table,
+	 *   - drivers/nvme/host/rdma.c|1473| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->data_sgl.sg_table,
+	 *   - drivers/nvme/host/rdma.c|1492| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->metadata_sgl->sg_table,
+	 *   - drivers/nvme/target/loop.c|157| <<nvme_loop_queue_rq>> if (sg_alloc_table_chained(&iod->sg_table,
+	 *   - drivers/scsi/scsi_lib.c|1135| <<scsi_alloc_sgtables>> if (unlikely(sg_alloc_table_chained(&cmd->sdb.table, nr_segs,
+	 *   - drivers/scsi/scsi_lib.c|1180| <<scsi_alloc_sgtables>> if (sg_alloc_table_chained(&prot_sdb->table,
+	 *   - fs/smb/server/transport_rdma.c|1422| <<smb_direct_rdma_xmit>> ret = sg_alloc_table_chained(&msg->sgt,
+	 *   - net/sunrpc/xprtrdma/svc_rdma_rw.c|78| <<svc_rdma_get_rw_ctxt>> if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges, ctxt->rw_sg_table.sgl, first_sgl_nents))
+	 *
+	 * struct sg_table {
+	 *     struct scatterlist *sgl;        // the list
+	 *     unsigned int nents;             // number of mapped entries
+	 *     unsigned int orig_nents;        // original size of list
+	 * };
+	 */
 	err = sg_alloc_table_chained(&vbr->sg_table,
 				     blk_rq_nr_phys_segments(req),
 				     vbr->sg_table.sgl,
diff --git a/drivers/idle/intel_idle.c b/drivers/idle/intel_idle.c
index ac4d8faa3..78bca86e2 100644
--- a/drivers/idle/intel_idle.c
+++ b/drivers/idle/intel_idle.c
@@ -2083,6 +2083,17 @@ static void state_update_enter_method(struct cpuidle_state *state, int cstate)
 		return;
 	}
 
+	/*
+	 * 在以下使用X86_FEATURE_KERNEL_IBRS:
+	 *   - arch/x86/entry/calling.h|306| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/entry/calling.h|335| <<global>> ALTERNATIVE "jmp .Lend_\@", "", X86_FEATURE_KERNEL_IBRS
+	 *   - arch/x86/include/asm/cpufeatures.h|204| <<global>> #define X86_FEATURE_KERNEL_IBRS ( 7*32+12)
+	 *   - arch/x86/kernel/cpu/bugs.c|220| <<update_spec_ctrl_cond>> if (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kernel/cpu/bugs.c|2154| <<spectre_v2_select_mitigation>> setup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);
+	 *   - arch/x86/kernel/smpboot.c|1404| <<native_play_dead>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))
+	 *   - arch/x86/kvm/vmx/vmx.c|7337| <<vmx_spec_ctrl_restore_host>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||
+	 *   - drivers/idle/intel_idle.c|2086| <<state_update_enter_method>> if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
+	 */
 	if (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) &&
 			((state->flags & CPUIDLE_FLAG_IBRS) || ibrs_off)) {
 		/*
diff --git a/drivers/iommu/amd/init.c b/drivers/iommu/amd/init.c
index 0e0a53104..5b2fe89bc 100644
--- a/drivers/iommu/amd/init.c
+++ b/drivers/iommu/amd/init.c
@@ -894,6 +894,10 @@ static void free_ga_log(struct amd_iommu *iommu)
 }
 
 #ifdef CONFIG_IRQ_REMAP
+/*
+ * 在以下使用iommu_ga_log_enable():
+ *   - drivers/iommu/amd/init.c|2860| <<enable_iommus_vapic>> if (iommu_init_ga_log(iommu) || iommu_ga_log_enable(iommu))
+ */
 static int iommu_ga_log_enable(struct amd_iommu *iommu)
 {
 	u32 status, i;
diff --git a/drivers/iommu/amd/iommu.c b/drivers/iommu/amd/iommu.c
index 16f40b800..4d7a24164 100644
--- a/drivers/iommu/amd/iommu.c
+++ b/drivers/iommu/amd/iommu.c
@@ -822,8 +822,19 @@ static void iommu_poll_events(struct amd_iommu *iommu)
 }
 
 #ifdef CONFIG_IRQ_REMAP
+/*
+ * 在以下使用iommu_ga_log_notifier:
+ *   - drivers/iommu/amd/iommu.c|825| <<global>> static int (*iommu_ga_log_notifier)(u32);
+ *   - drivers/iommu/amd/iommu.c|833| <<global>> iommu_ga_log_notifier = notifier;
+ *   - drivers/iommu/amd/iommu.c|865| <<iommu_poll_ga_log>> if (!iommu_ga_log_notifier)
+ *   - drivers/iommu/amd/iommu.c|872| <<iommu_poll_ga_log>> if (iommu_ga_log_notifier(GA_TAG(log_entry)) != 0)
+ */
 static int (*iommu_ga_log_notifier)(u32);
 
+/*
+ * 只在一个地方调用:
+ *   - arch/x86/kvm/svm/avic.c|1313| <<avic_hardware_setup>> amd_iommu_register_ga_log_notifier(&avic_ga_log_notifier);
+ */
 int amd_iommu_register_ga_log_notifier(int (*notifier)(u32))
 {
 	iommu_ga_log_notifier = notifier;
@@ -832,6 +843,12 @@ int amd_iommu_register_ga_log_notifier(int (*notifier)(u32))
 }
 EXPORT_SYMBOL(amd_iommu_register_ga_log_notifier);
 
+/*
+ * 在以下使用iommu_poll_ga_log():
+ *   - drivers/iommu/amd/iommu.c|958| <<amd_iommu_int_thread_galog>> amd_iommu_handle_irq(data,
+ *                   "GA", MMIO_STATUS_GALOG_INT_MASK,
+ *                   MMIO_STATUS_GALOG_OVERFLOW_MASK, iommu_poll_ga_log, amd_iommu_restart_ga_log);
+ */
 static void iommu_poll_ga_log(struct amd_iommu *iommu)
 {
 	u32 head, tail;
@@ -889,6 +906,12 @@ static inline void
 amd_iommu_set_pci_msi_domain(struct device *dev, struct amd_iommu *iommu) { }
 #endif /* !CONFIG_IRQ_REMAP */
 
+/*
+ * called by:
+ *   - drivers/iommu/amd/iommu.c|937| <<amd_iommu_int_thread_evtlog>> amd_iommu_handle_irq(data, "Evt", MMIO_STATUS_EVT_INT_MASK,
+ *   - drivers/iommu/amd/iommu.c|946| <<amd_iommu_int_thread_pprlog>> amd_iommu_handle_irq(data, "PPR", MMIO_STATUS_PPR_INT_MASK,
+ *   - drivers/iommu/amd/iommu.c|956| <<amd_iommu_int_thread_galog>> amd_iommu_handle_irq(data, "GA", MMIO_STATUS_GALOG_INT_MASK,
+ */
 static void amd_iommu_handle_irq(void *data, const char *evt_type,
 				 u32 int_mask, u32 overflow_mask,
 				 void (*int_handler)(struct amd_iommu *),
@@ -946,9 +969,21 @@ irqreturn_t amd_iommu_int_thread_pprlog(int irq, void *data)
 	return IRQ_HANDLED;
 }
 
+/*
+ * 也许就在下面使用amd_iommu_int_thread_galog():
+ *   - drivers/iommu/amd/init.c|2428| <<iommu_setup_intcapxt>> ret = __iommu_setup_intcapxt(iommu, iommu->ga_irq_name,
+ *                   MMIO_INTCAPXT_GALOG_OFFSET, amd_iommu_int_thread_galog);
+ *   - drivers/iommu/amd/iommu.c|968| <<amd_iommu_int_thread>> amd_iommu_int_thread_galog(irq, data);
+ */
 irqreturn_t amd_iommu_int_thread_galog(int irq, void *data)
 {
 #ifdef CONFIG_IRQ_REMAP
+	/*
+	 * 在以下使用iommu_poll_ga_log():
+	 *   - drivers/iommu/amd/iommu.c|958| <<amd_iommu_int_thread_galog>> amd_iommu_handle_irq(data,
+	 *                   "GA", MMIO_STATUS_GALOG_INT_MASK,
+	 *                   MMIO_STATUS_GALOG_OVERFLOW_MASK, iommu_poll_ga_log, amd_iommu_restart_ga_log);
+	 */
 	amd_iommu_handle_irq(data, "GA", MMIO_STATUS_GALOG_INT_MASK,
 			     MMIO_STATUS_GALOG_OVERFLOW_MASK,
 			     iommu_poll_ga_log, amd_iommu_restart_ga_log);
@@ -3841,6 +3876,12 @@ int amd_iommu_create_irq_domain(struct amd_iommu *iommu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/avic.c|1057| <<svm_ir_list_add>> amd_iommu_update_ga(entry & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK,
+ *                                         true, pi->ir_data);
+ *   - arch/x86/kvm/svm/avic.c|1243| <<avic_update_iommu_vcpu_affinity>> ret = amd_iommu_update_ga(cpu, r, ir->data);
+ */
 int amd_iommu_update_ga(int cpu, bool is_run, void *data)
 {
 	struct amd_ir_data *ir_data = (struct amd_ir_data *)data;
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h b/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h
index 5ec468268..3aeca9f3f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/txrx.h
@@ -276,6 +276,20 @@ static inline u16 mlx5e_icosq_get_next_pi(struct mlx5e_icosq *sq, u16 size)
 	return pi;
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h|161| <<mlx5e_xmit_xdp_doorbell>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, sq->doorbell_cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c|215| <<post_rx_param_wqes>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c|313| <<resync_post_get_progress_params>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_accel/ktls_rx.c|765| <<mlx5e_ktls_rx_handle_resync_list>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, db_cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_main.c|1878| <<mlx5e_deactivate_txqsq>> mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &nop->ctrl);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_rx.c|1122| <<mlx5e_post_rx_mpwqes>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, sq->doorbell_cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|371| <<mlx5e_tx_flush>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, &wqe->ctrl);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|423| <<mlx5e_txwqe_complete>> mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|634| <<mlx5e_sq_xmit_mpwqe>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|639| <<mlx5e_sq_xmit_mpwqe>> mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, cseg);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_txrx.c|85| <<mlx5e_trigger_irq>> mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &nopwqe->ctrl);
+ */
 static inline void
 mlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc, void __iomem *uar_map,
 		struct mlx5_wqe_ctrl_seg *ctrl)
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
index f8c7912ab..bd9d6389c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tx.c
@@ -423,6 +423,10 @@ mlx5e_txwqe_complete(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 		mlx5e_notify_hw(wq, sq->pc, sq->uar_map, cseg);
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|730| <<mlx5e_xmit>> mlx5e_sq_xmit_wqe(sq, skb, &attr, &wqe_attr, wqe, pi, netdev_xmit_more());
+ */
 static void
 mlx5e_sq_xmit_wqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 		  const struct mlx5e_tx_attr *attr, const struct mlx5e_tx_wqe_attr *wqe_attr,
@@ -598,6 +602,10 @@ static struct mlx5_wqe_ctrl_seg *mlx5e_tx_mpwqe_session_complete(struct mlx5e_tx
 	return cseg;
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_tx.c|715| <<mlx5e_xmit>> mlx5e_sq_xmit_mpwqe(sq, skb, &eseg, netdev_xmit_more());
+ */
 static void
 mlx5e_sq_xmit_mpwqe(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 		    struct mlx5_wqe_eth_seg *eseg, bool xmit_more)
@@ -672,6 +680,11 @@ static void mlx5e_txwqe_build_eseg(struct mlx5e_priv *priv, struct mlx5e_txqsq *
 		mlx5e_cqe_ts_id_eseg(sq->ptpsq, skb, eseg);
 }
 
+/*
+ * 在以下使用mlx5e_xmit():
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_main.c|5178| <<global>> .ndo_start_xmit = mlx5e_xmit,
+ *   - drivers/net/ethernet/mellanox/mlx5/core/en_rep.c|803| <<global>> .ndo_start_xmit = mlx5e_xmit,
+ */
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct mlx5e_priv *priv = netdev_priv(dev);
diff --git a/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h b/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
index b1edc71ff..250a0ba7f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/lib/eq.h
@@ -65,6 +65,13 @@ static inline struct mlx5_eqe *next_eqe_sw(struct mlx5_eq *eq)
 	return (eqe->owner ^ (eq->cons_index >> eq->fbc.log_sz)) & 1 ? NULL : eqe;
 }
 
+/*
+ * 在以下调用eq_update_ci():
+ *   - drivers/net/ethernet/mellanox/mlx5/core/eq.c|145| <<mlx5_eq_comp_int>> eq_update_ci(eq, 1);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/eq.c|228| <<mlx5_eq_async_int>> eq_update_ci(eq, 1);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/eq.c|362| <<mlx5_eq_enable>> eq_update_ci(eq, 1);
+ *   - drivers/net/ethernet/mellanox/mlx5/core/eq.c|805| <<mlx5_eq_update_ci>> eq_update_ci(eq, arm);
+ */
 static inline void eq_update_ci(struct mlx5_eq *eq, int arm)
 {
 	__be32 __iomem *addr = eq->doorbell + (arm ? 0 : 2);
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 7646ddd9b..142115a0e 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -444,6 +444,14 @@ struct virtnet_info {
 	/* The lock to synchronize the access to refill_enabled */
 	spinlock_t refill_lock;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	/* Work struct for config space updates */
 	struct work_struct config_work;
 
@@ -1059,6 +1067,13 @@ static void virtnet_rq_unmap_free_buf(struct virtqueue *vq, void *buf)
 	virtnet_rq_free_buf(vi, rq, buf);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1130| <<check_sq_full_and_disable>> free_old_xmit(sq, txq, false);
+ *   - drivers/net/virtio_net.c|2976| <<virtnet_poll_cleantx>> free_old_xmit(sq, txq, !!budget);
+ *   - drivers/net/virtio_net.c|3174| <<virtnet_poll_tx>> free_old_xmit(sq, txq, !!budget);
+ *   - drivers/net/virtio_net.c|3278| <<start_xmit>> free_old_xmit(sq, txq, false);
+ */
 static void free_old_xmit(struct send_queue *sq, struct netdev_queue *txq,
 			  bool in_napi)
 {
@@ -2789,6 +2804,15 @@ static void skb_recv_done(struct virtqueue *rvq)
 	virtqueue_napi_schedule(&rq->napi, rvq);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2828| <<virtnet_napi_tx_enable>> return virtnet_napi_enable(vq, napi);
+ *   - drivers/net/virtio_net.c|2849| <<refill_work>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|3066| <<virtnet_enable_queue_pair>> virtnet_napi_enable(vi->rq[qp_index].vq, &vi->rq[qp_index].napi);
+ *   - drivers/net/virtio_net.c|3331| <<virtnet_rx_resume>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|5991| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|6008| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ */
 static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 {
 	napi_enable(napi);
@@ -3658,6 +3682,14 @@ static int virtnet_close(struct net_device *dev)
 	/* Stop getting status/speed updates: we don't care until next
 	 * open
 	 */
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	cancel_work_sync(&vi->config_work);
 
 	for (i = 0; i < vi->max_queue_pairs; i++) {
@@ -5607,6 +5639,14 @@ static void virtnet_freeze_down(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	/* Make sure no work handler is accessing the device */
 	flush_work(&vi->config_work);
 	disable_rx_mode_work(vi);
@@ -6156,6 +6196,14 @@ static void virtnet_config_changed(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	schedule_work(&vi->config_work);
 }
 
@@ -6716,6 +6764,14 @@ static int virtnet_probe(struct virtio_device *vdev)
 	vi->vdev = vdev;
 	vdev->priv = vi;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	INIT_WORK(&vi->config_work, virtnet_config_changed_work);
 	INIT_WORK(&vi->rx_mode_work, virtnet_rx_mode_work);
 	spin_lock_init(&vi->refill_lock);
@@ -6935,6 +6991,14 @@ static int virtnet_probe(struct virtio_device *vdev)
 	   otherwise get link status from config. */
 	netif_carrier_off(dev);
 	if (virtio_has_feature(vi->vdev, VIRTIO_NET_F_STATUS)) {
+		/*
+		 * 在以下使用virtnet_info->config_work:
+		 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+		 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+		 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+		 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+		 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+		 */
 		virtnet_config_changed_work(&vi->config_work);
 	} else {
 		vi->status = VIRTIO_NET_S_LINK_UP;
@@ -7003,6 +7067,14 @@ static void virtnet_remove(struct virtio_device *vdev)
 
 	virtnet_cpu_notif_remove(vi);
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	/* Make sure no work handler is accessing the device. */
 	flush_work(&vi->config_work);
 	disable_rx_mode_work(vi);
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 63fe51d0e..3c807001d 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -267,6 +267,10 @@ static void xennet_maybe_wake_tx(struct netfront_queue *queue)
 }
 
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|316| <<xennet_alloc_rx_buffers>> skb = xennet_alloc_one_rx_buffer(queue);
+ */
 static struct sk_buff *xennet_alloc_one_rx_buffer(struct netfront_queue *queue)
 {
 	struct sk_buff *skb;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 8471f38b7..6c1f00bf1 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -42,6 +42,18 @@ module_param(virtscsi_poll_queues, uint, 0644);
 MODULE_PARM_DESC(virtscsi_poll_queues,
 		 "The number of dedicated virtqueues for polling I/O");
 
+/*
+ * 在以下使用virtio_scsi_cmd->resp:
+ *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+ *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+ *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+ *             req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+ *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+ *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+ *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+ */
 /* Command queue element */
 struct virtio_scsi_cmd {
 	struct scsi_cmnd *sc;
@@ -93,7 +105,31 @@ struct virtio_scsi {
 	struct virtio_scsi_vq req_vqs[];
 };
 
+/*
+ * 在以下使用virtscsi_cmd_cache:
+ *   - drivers/scsi/virtio_scsi.c|108| <<global>> static struct kmem_cache *virtscsi_cmd_cache;
+ *   - drivers/scsi/virtio_scsi.c|1261| <<virtio_scsi_init>> virtscsi_cmd_cache = KMEM_CACHE(virtio_scsi_cmd, 0);
+ *   - drivers/scsi/virtio_scsi.c|1262| <<virtio_scsi_init>> if (!virtscsi_cmd_cache) {
+ *   - drivers/scsi/virtio_scsi.c|1270| <<virtio_scsi_init>> virtscsi_cmd_pool = mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
+ *             virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1284| <<virtio_scsi_init>> kmem_cache_destroy(virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1285| <<virtio_scsi_init>> virtscsi_cmd_cache = NULL;
+ *   - drivers/scsi/virtio_scsi.c|1293| <<virtio_scsi_fini>> kmem_cache_destroy(virtscsi_cmd_cache);
+ */
 static struct kmem_cache *virtscsi_cmd_cache;
+/*
+ * 在以下使用virtscsi_cmd_pool:
+ *   - drivers/scsi/virtio_scsi.c|109| <<global>> static mempool_t *virtscsi_cmd_pool;
+ *   - drivers/scsi/virtio_scsi.c|818| <<virtscsi_tmf>> mempool_free(cmd, virtscsi_cmd_pool);
+ *   - drivers/scsi/virtio_scsi.c|831| <<virtscsi_device_reset>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+ *   - drivers/scsi/virtio_scsi.c|905| <<virtscsi_abort>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+ *   - drivers/scsi/virtio_scsi.c|1268| <<virtio_scsi_init>> virtscsi_cmd_pool = mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
+ *             virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1271| <<virtio_scsi_init>> if (!virtscsi_cmd_pool) {
+ *   - drivers/scsi/virtio_scsi.c|1282| <<virtio_scsi_init>> mempool_destroy(virtscsi_cmd_pool);
+ *   - drivers/scsi/virtio_scsi.c|1283| <<virtio_scsi_init>> virtscsi_cmd_pool = NULL;
+ *   - drivers/scsi/virtio_scsi.c|1292| <<virtio_scsi_fini>> mempool_destroy(virtscsi_cmd_pool);
+ */
 static mempool_t *virtscsi_cmd_pool;
 
 static inline struct Scsi_Host *virtio_scsi_host(struct virtio_device *vdev)
@@ -112,10 +148,29 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * 在以下使用virtscsi_complete_cmd():
+ *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|253| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|699| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|889| <<virtscsi_mq_poll>> virtscsi_complete_cmd(vscsi, buf);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
 	struct scsi_cmnd *sc = cmd->sc;
+	/*
+	 * 在以下使用virtio_scsi_cmd->resp:
+	 *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+	 *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+	 *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+	 *             req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+	 *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+	 *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+	 */
 	struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
 
 	dev_dbg(&sc->device->sdev_gendev,
@@ -173,6 +228,17 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 	scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+ *             req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+ *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+ *             &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+ *             &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -192,6 +258,10 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_unlock_irqrestore(&virtscsi_vq->vq_lock, flags);
 }
 
+/*
+ * 在以下使用virtscsi_req_done():
+ *   - drivers/scsi/virtio_scsi.c|919| <<virtscsi_init>> vqs_info[i].callback = virtscsi_req_done;
+ */
 static void virtscsi_req_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -199,19 +269,51 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	int index = vq->index - VIRTIO_SCSI_VQ_BASE;
 	struct virtio_scsi_vq *req_vq = &vscsi->req_vqs[index];
 
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|664| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
 
 	num_vqs = vscsi->num_queues;
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 *
+	 * 这里是&vscsi->req_vqs[i], 不是ctrl或者event!
+	 */
 	for (i = 0; i < num_vqs; i++)
 		virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
 				 virtscsi_complete_cmd);
 }
 
+/*
+ * 只在以下使用virtscsi_complete_free():
+ *   - drivers/scsi/virtio_scsi.c|289| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ */
 static void virtscsi_complete_free(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -225,6 +327,17 @@ static void virtscsi_ctrl_done(struct virtqueue *vq)
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
 	struct virtio_scsi *vscsi = shost_priv(sh);
 
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
 };
 
@@ -423,9 +536,24 @@ static void virtscsi_event_done(struct virtqueue *vq)
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
 	struct virtio_scsi *vscsi = shost_priv(sh);
 
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|505| <<virtscsi_add_cmd>> err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int __virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -456,6 +584,34 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 		sgs[out_num++] = out->sgl;
 	}
 
+	/*
+	 * 在以下使用virtio_scsi_cmd->resp:
+	 *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+	 *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+	 *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+	 *             req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+	 *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+	 *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+	 *
+	 *
+	 * struct virtio_scsi_cmd_resp {
+	 *     __virtio32 sense_len;           // Sense data length
+	 *     __virtio32 resid;               // Residual bytes in data buffer
+	 *     __virtio16 status_qualifier;    // Status qualifier
+	 *     __u8 status;            // Command completion status
+	 *     __u8 response;          // Response values
+	 *     __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+	 * } __attribute__((packed));
+	 *
+	 * 参考代码:
+	 * memset(sgl, 0, sizeof(*sgl) * nents);
+	 * sg_set_page(sg, virt_to_page(buf), buflen, offset_in_page(buf));
+	 * sg_mark_end(&sgl[nents - 1]);
+	 */
+
 	/* Response header.  */
 	sg_init_one(&resp, &cmd->resp, resp_size);
 	sgs[out_num + in_num++] = &resp;
@@ -492,6 +648,13 @@ static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
  * @resp_size	: size of the response buffer
  * @kick	: whether to kick the virtqueue immediately
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|602| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+ *             cmd, req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|620| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+ *             cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ */
 static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size,
@@ -502,6 +665,9 @@ static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 	bool needs_kick = false;
 
 	spin_lock_irqsave(&vq->vq_lock, flags);
+	/*
+	 * 只在此处调用
+	 */
 	err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
 	if (!err && kick)
 		needs_kick = virtqueue_kick_prepare(vq->vq);
@@ -598,6 +764,18 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 		req_size = sizeof(cmd->req.cmd);
 	}
 
+	/*
+	 * 在以下使用virtio_scsi_cmd->resp:
+	 *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+	 *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+	 *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+	 *             req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+	 *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+	 *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+	 */
 	kick = (sc->flags & SCMD_LAST) != 0;
 	ret = virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick);
 	if (ret == -EIO) {
@@ -611,17 +789,37 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|665| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|723| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+ */
 static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 {
 	DECLARE_COMPLETION_ONSTACK(comp);
 	int ret = FAILED;
 
 	cmd->comp = &comp;
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|602| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+	 *             cmd, req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|620| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+	 *             cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 */
 	if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
 			      sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
 		goto out;
 
 	wait_for_completion(&comp);
+	/*
+	 * struct virtio_scsi_ctrl_tmf_resp {
+	 *     __u8 response;
+	 * } __attribute__((packed));
+	 *
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> struct virtio_scsi_ctrl_tmf_resp tmf;
+	 */
 	if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
 	    cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
 		ret = SUCCESS;
@@ -635,6 +833,9 @@ static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 	 * In the abort case, scsi_done() will do nothing, because the
 	 * command timed out and hence SCMD_STATE_COMPLETE has been set.
 	 */
+	/*
+	 * 只在此处调用	
+	 */
 	virtscsi_poll_requests(vscsi);
 
 out:
@@ -642,6 +843,9 @@ static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 	return ret;
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.eh_device_reset_handler = virtscsi_device_reset()
+ */
 static int virtscsi_device_reset(struct scsi_cmnd *sc)
 {
 	struct virtio_scsi *vscsi = shost_priv(sc->device->host);
@@ -653,6 +857,17 @@ static int virtscsi_device_reset(struct scsi_cmnd *sc)
 		return FAILED;
 
 	memset(cmd, 0, sizeof(*cmd));
+	/*
+	 * struct virtio_scsi_ctrl_tmf_req {
+	 *     __virtio32 type;
+	 *     __virtio32 subtype;
+	 *     __u8 lun[8];
+	 *     __virtio64 tag;
+	 * } __attribute__((packed));
+	 *
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> struct virtio_scsi_ctrl_tmf_req tmf;
+	 */
 	cmd->req.tmf = (struct virtio_scsi_ctrl_tmf_req){
 		.type = VIRTIO_SCSI_T_TMF,
 		.subtype = cpu_to_virtio32(vscsi->vdev,
@@ -662,6 +877,11 @@ static int virtscsi_device_reset(struct scsi_cmnd *sc)
 		.lun[2] = (sc->device->lun >> 8) | 0x40,
 		.lun[3] = sc->device->lun & 0xff,
 	};
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|665| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+	 *   - drivers/scsi/virtio_scsi.c|723| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+	 */
 	return virtscsi_tmf(vscsi, cmd);
 }
 
@@ -750,6 +970,9 @@ static void virtscsi_map_queues(struct Scsi_Host *shost)
 	}
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.mq_poll = virtscsi_mq_poll()
+ */
 static int virtscsi_mq_poll(struct Scsi_Host *shost, unsigned int queue_num)
 {
 	struct virtio_scsi *vscsi = shost_priv(shost);
@@ -1066,6 +1289,11 @@ static int __init virtio_scsi_init(void)
 	}
 
 
+	/*
+	 * mempool_create_slab_pool()有意思的调用:
+	 *   - drivers/scsi/virtio_scsi.c|1269| <<virtio_scsi_init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache);
+	 *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+	 */
 	virtscsi_cmd_pool =
 		mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
 					 virtscsi_cmd_cache);
diff --git a/drivers/target/loopback/tcm_loop.c b/drivers/target/loopback/tcm_loop.c
index 761c511ae..e31c21781 100644
--- a/drivers/target/loopback/tcm_loop.c
+++ b/drivers/target/loopback/tcm_loop.c
@@ -1123,6 +1123,23 @@ static int __init tcm_loop_fabric_init(void)
 	if (ret)
 		goto out_destroy_cache;
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3961| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1662| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_ops);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1667| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_npiv_ops);
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4036| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1878| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1882| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+	 *   - drivers/target/iscsi/iscsi_target.c|695| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+	 *   - drivers/target/loopback/tcm_loop.c|1126| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+	 *   - drivers/target/sbp/sbp_target.c|2288| <<sbp_init>> return target_register_template(&sbp_ops);
+	 *   - drivers/target/tcm_fc/tfc_conf.c|448| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+	 *   - drivers/target/tcm_remote/tcm_remote.c|256| <<tcm_remote_fabric_init>> return target_register_template(&remote_ops);
+	 *   - drivers/usb/gadget/function/f_tcm.c|2289| <<tcm_init>> ret = target_register_template(&usbg_ops);
+	 *   - drivers/vhost/scsi.c|2792| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+	 *   - drivers/xen/xen-scsiback.c|1866| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+	 */
 	ret = target_register_template(&loop_ops);
 	if (ret)
 		goto out_release_core_bus;
diff --git a/drivers/target/target_core_configfs.c b/drivers/target/target_core_configfs.c
index c40217f44..c73e58f88 100644
--- a/drivers/target/target_core_configfs.c
+++ b/drivers/target/target_core_configfs.c
@@ -320,12 +320,27 @@ static struct configfs_subsystem target_core_fabrics = {
 	},
 };
 
+/*
+ * 在以下调用target_depend_item():
+ *   - drivers/target/target_core_pr.c|1414| <<core_scsi3_tpg_depend_item>> return target_depend_item(&tpg->tpg_group.cg_item);
+ *   - drivers/target/target_core_pr.c|1427| <<core_scsi3_nodeacl_depend_item>> return target_depend_item(&nacl->acl_group.cg_item);
+ *   - drivers/target/target_core_pr.c|1445| <<core_scsi3_lunacl_depend_item>> return target_depend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
+ *   - drivers/vhost/scsi.c|3025| <<vhost_scsi_set_endpoint>> ret = target_depend_item(&se_tpg->tpg_group.cg_item);
+ */
 int target_depend_item(struct config_item *item)
 {
 	return configfs_depend_item(&target_core_fabrics, item);
 }
 EXPORT_SYMBOL(target_depend_item);
 
+/*
+ * 在以下调用target_undepend_item():
+ *   - drivers/target/target_core_pr.c|1419| <<core_scsi3_tpg_undepend_item>> target_undepend_item(&tpg->tpg_group.cg_item);
+ *   - drivers/target/target_core_pr.c|1433| <<core_scsi3_nodeacl_undepend_item>> target_undepend_item(&nacl->acl_group.cg_item);
+ *   - drivers/target/target_core_pr.c|1458| <<core_scsi3_lunacl_undepend_item>> target_undepend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
+ *   - drivers/vhost/scsi.c|3164| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+ *   - drivers/vhost/scsi.c|3317| <<vhost_scsi_clear_endpoint>> target_undepend_item(&se_tpg->tpg_group.cg_item);
+ */
 void target_undepend_item(struct config_item *item)
 {
 	return configfs_undepend_item(item);
@@ -465,6 +480,23 @@ static void target_set_default_ops(struct target_core_fabric_ops *tfo)
 		tfo->get_cmd_state = target_default_get_cmd_state;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3961| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+ *   - drivers/scsi/elx/efct/efct_lio.c|1662| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_ops);
+ *   - drivers/scsi/elx/efct/efct_lio.c|1667| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_npiv_ops);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4036| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1878| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1882| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+ *   - drivers/target/iscsi/iscsi_target.c|695| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+ *   - drivers/target/loopback/tcm_loop.c|1126| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+ *   - drivers/target/sbp/sbp_target.c|2288| <<sbp_init>> return target_register_template(&sbp_ops);
+ *   - drivers/target/tcm_fc/tfc_conf.c|448| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+ *   - drivers/target/tcm_remote/tcm_remote.c|256| <<tcm_remote_fabric_init>> return target_register_template(&remote_ops);
+ *   - drivers/usb/gadget/function/f_tcm.c|2289| <<tcm_init>> ret = target_register_template(&usbg_ops);
+ *   - drivers/vhost/scsi.c|2792| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/xen/xen-scsiback.c|1866| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+ */
 int target_register_template(const struct target_core_fabric_ops *fo)
 {
 	struct target_core_fabric_ops *tfo;
diff --git a/drivers/target/target_core_file.c b/drivers/target/target_core_file.c
index 2d78ef746..297a8bb2a 100644
--- a/drivers/target/target_core_file.c
+++ b/drivers/target/target_core_file.c
@@ -167,6 +167,21 @@ static int fd_configure_device(struct se_device *dev)
 			" block_device blocks: %llu logical_block_size: %d\n",
 			dev_size, div_u64(dev_size, fd_dev->fd_block_size),
 			fd_dev->fd_block_size);
+		/*
+		 * 在以下使用se_dev_attrib->max_write_same_len:
+		 *   - drivers/target/target_core_configfs.c|595| <<global>> DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+		 *   - drivers/target/target_core_configfs.c|618| <<global>> DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+		 *   - drivers/target/target_core_configfs.c|1318| <<global>> CONFIGFS_ATTR(, max_write_same_len);
+		 *   - drivers/target/target_core_device.c|775| <<target_alloc_device>> dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
+		 *   - drivers/target/target_core_file.c|174| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+		 *   - drivers/target/target_core_file.c|192| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0x1000;
+		 *   - drivers/target/target_core_iblock.c|143| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;
+		 *   - drivers/target/target_core_iblock.c|145| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+		 *   - drivers/target/target_core_sbc.c|294| <<sbc_setup_write_same>> if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
+		 *   - drivers/target/target_core_sbc.c|296| <<sbc_setup_write_same>> pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
+		 *                    sectors, cmd->se_dev->dev_attrib.max_write_same_len);
+		 *   - drivers/target/target_core_spc.c|599| <<spc_emulate_evpd_b0>> put_unaligned_be64(dev->dev_attrib.max_write_same_len, &buf[36]);
+		 */
 		/*
 		 * Enable write same emulation for IBLOCK and use 0xFFFF as
 		 * the smaller WRITE_SAME(10) only has a two-byte block count.
@@ -248,6 +263,10 @@ struct target_core_file_cmd {
 	struct bio_vec	bvecs[];
 };
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|686| <<fd_execute_rw>> return fd_execute_rw_aio(cmd, sgl, sgl_nents, data_direction);
+ */
 static void cmd_rw_aio_complete(struct kiocb *iocb, long ret)
 {
 	struct target_core_file_cmd *cmd;
@@ -309,6 +328,13 @@ fd_execute_rw_aio(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|599| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, pfile, dev->prot_length,
+ *   - drivers/target/target_core_file.c|606| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, file, dev->dev_attrib.block_size,
+ *   - drivers/target/target_core_file.c|631| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, file, dev->dev_attrib.block_size,
+ *   - drivers/target/target_core_file.c|652| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, pfile, dev->prot_length,
+ */
 static int fd_do_rw(struct se_cmd *cmd, struct file *fd,
 		    u32 block_size, struct scatterlist *sgl,
 		    u32 sgl_nents, u32 data_length, int is_write)
@@ -934,6 +960,14 @@ static const struct target_backend_ops fileio_ops = {
 
 static int __init fileio_module_init(void)
 {
+	/*
+	 * called by:
+	 *   - drivers/target/target_core_file.c|937| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+	 *   - drivers/target/target_core_iblock.c|1189| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+	 *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+	 *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+	 *   - drivers/target/target_core_user.c|3359| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+	 */
 	return transport_backend_register(&fileio_ops);
 }
 
diff --git a/drivers/target/target_core_hba.c b/drivers/target/target_core_hba.c
index d508b343b..20f9cb8bb 100644
--- a/drivers/target/target_core_hba.c
+++ b/drivers/target/target_core_hba.c
@@ -35,6 +35,14 @@ static DEFINE_SPINLOCK(hba_lock);
 static LIST_HEAD(hba_list);
 
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|937| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+ *   - drivers/target/target_core_iblock.c|1189| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+ *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+ *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+ *   - drivers/target/target_core_user.c|3359| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+ */
 int transport_backend_register(const struct target_backend_ops *ops)
 {
 	struct target_backend *tb, *old;
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index c8dc92a7d..b141f6e33 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -381,6 +381,12 @@ static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 	return bio;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|562| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|810| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|834| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ */
 static void iblock_submit_bios(struct bio_list *list)
 {
 	struct blk_plug plug;
@@ -1186,6 +1192,14 @@ static const struct target_backend_ops iblock_ops = {
 
 static int __init iblock_module_init(void)
 {
+	/*
+	 * called by:
+	 *   - drivers/target/target_core_file.c|937| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+	 *   - drivers/target/target_core_iblock.c|1189| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+	 *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+	 *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+	 *   - drivers/target/target_core_user.c|3359| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+	 */
 	return transport_backend_register(&iblock_ops);
 }
 
diff --git a/drivers/target/target_core_pr.c b/drivers/target/target_core_pr.c
index 4f4ad6af4..6b777df84 100644
--- a/drivers/target/target_core_pr.c
+++ b/drivers/target/target_core_pr.c
@@ -1455,6 +1455,14 @@ static void core_scsi3_lunacl_undepend_item(struct se_dev_entry *se_deve)
 		return;
 	}
 
+	/*
+	 * 在以下调用target_undepend_item():
+	 *   - drivers/target/target_core_pr.c|1419| <<core_scsi3_tpg_undepend_item>> target_undepend_item(&tpg->tpg_group.cg_item);
+	 *   - drivers/target/target_core_pr.c|1433| <<core_scsi3_nodeacl_undepend_item>> target_undepend_item(&nacl->acl_group.cg_item);
+	 *   - drivers/target/target_core_pr.c|1458| <<core_scsi3_lunacl_undepend_item>> target_undepend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
+	 *   - drivers/vhost/scsi.c|3164| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+	 *   - drivers/vhost/scsi.c|3317| <<vhost_scsi_clear_endpoint>> target_undepend_item(&se_tpg->tpg_group.cg_item);
+	 */
 	target_undepend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
 	kref_put(&se_deve->pr_kref, target_pr_kref_release);
 }
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index fe8beb7db..e99f4d929 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -270,6 +270,12 @@ static inline unsigned long long transport_lba_64(unsigned char *cdb)
 	return get_unaligned_be64(&cdb[2]);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+ *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+ *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+ */
 static sense_reason_t
 sbc_setup_write_same(struct se_cmd *cmd, unsigned char flags,
 		     struct exec_cmd_ops *ops)
@@ -285,6 +291,21 @@ sbc_setup_write_same(struct se_cmd *cmd, unsigned char flags,
 			" Emulation\n");
 		return TCM_UNSUPPORTED_SCSI_OPCODE;
 	}
+	/*
+	 * 在以下使用se_dev_attrib->max_write_same_len:
+	 *   - drivers/target/target_core_configfs.c|595| <<global>> DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|618| <<global>> DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|1318| <<global>> CONFIGFS_ATTR(, max_write_same_len);
+	 *   - drivers/target/target_core_device.c|775| <<target_alloc_device>> dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
+	 *   - drivers/target/target_core_file.c|174| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_file.c|192| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0x1000;
+	 *   - drivers/target/target_core_iblock.c|143| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;
+	 *   - drivers/target/target_core_iblock.c|145| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_sbc.c|294| <<sbc_setup_write_same>> if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
+	 *   - drivers/target/target_core_sbc.c|296| <<sbc_setup_write_same>> pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
+	 *                    sectors, cmd->se_dev->dev_attrib.max_write_same_len);
+	 *   - drivers/target/target_core_spc.c|599| <<spc_emulate_evpd_b0>> put_unaligned_be64(dev->dev_attrib.max_write_same_len, &buf[36]);
+	 */
 	if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
 		pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
 			sectors, cmd->se_dev->dev_attrib.max_write_same_len);
@@ -889,6 +910,12 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 			size = sbc_get_size(cmd, 1);
 			cmd->t_task_lba = get_unaligned_be64(&cdb[12]);
 
+			/*
+			 * called by:
+			 *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+			 *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+			 *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+			 */
 			ret = sbc_setup_write_same(cmd, cdb[10], ops);
 			if (ret)
 				return ret;
@@ -988,6 +1015,12 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 		size = sbc_get_size(cmd, 1);
 		cmd->t_task_lba = get_unaligned_be64(&cdb[2]);
 
+		/*
+		 * called by:
+		 *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+		 *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 */
 		ret = sbc_setup_write_same(cmd, cdb[1], ops);
 		if (ret)
 			return ret;
@@ -1006,6 +1039,12 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 		 * Follow sbcr26 with WRITE_SAME (10) and check for the existence
 		 * of byte 1 bit 3 UNMAP instead of original reserved field
 		 */
+		/*
+		 * called by:
+		 *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+		 *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 */
 		ret = sbc_setup_write_same(cmd, cdb[1], ops);
 		if (ret)
 			return ret;
diff --git a/drivers/target/target_core_spc.c b/drivers/target/target_core_spc.c
index ea14a3835..745280030 100644
--- a/drivers/target/target_core_spc.c
+++ b/drivers/target/target_core_spc.c
@@ -1015,6 +1015,11 @@ static int spc_modesense_long_blockdesc(unsigned char *buf, u64 blocks, u32 bloc
 	return 17;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_spc.c|2298| <<spc_parse_cdb>> cmd->execute_cmd = spc_emulate_modesense;
+ *   - drivers/target/target_core_spc.c|2302| <<spc_parse_cdb>> cmd->execute_cmd = spc_emulate_modesense;
+ */
 static sense_reason_t spc_emulate_modesense(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 05d29201b..f2add00ff 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -945,6 +945,52 @@ void target_complete_cmd_with_sense(struct se_cmd *cmd, u8 scsi_status,
 }
 EXPORT_SYMBOL(target_complete_cmd_with_sense);
 
+/*
+ * called by:
+ *   - drivers/target/iscsi/iscsi_target.c|4239| <<iscsit_release_commands_from_conn>> target_complete_cmd(&cmd->se_cmd, SAM_STAT_TASK_ABORTED);
+ *   - drivers/target/target_core_alua.c|126| <<target_emulate_report_referrals>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_alua.c|430| <<target_emulate_set_target_port_groups>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|258| <<cmd_rw_aio_complete>> target_complete_cmd(cmd->cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_file.c|260| <<cmd_rw_aio_complete>> target_complete_cmd(cmd->cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|396| <<fd_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|420| <<fd_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_file.c|422| <<fd_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|478| <<fd_execute_write_same>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|663| <<fd_execute_rw_buffered>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_iblock.c|336| <<iblock_complete_cmd>> target_complete_cmd(cmd, status);
+ *   - drivers/target/target_core_iblock.c|407| <<iblock_end_io_flush>> target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_iblock.c|409| <<iblock_end_io_flush>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_iblock.c|431| <<iblock_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_iblock.c|490| <<iblock_execute_zero_out>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|237| <<target_scsi2_reservation_release>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|300| <<target_scsi2_reservation_reserve>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|3725| <<target_scsi3_emulate_pr_out>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|4161| <<target_scsi3_emulate_pr_in>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pscsi.c|1028| <<pscsi_req_done>> target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_rd.c|433| <<rd_execute_rw>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_rd.c|528| <<rd_execute_rw>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|165| <<sbc_emulate_startstop>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|205| <<sbc_execute_write_same_unmap>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|212| <<sbc_emulate_noop>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|1118| <<sbc_execute_unmap>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|1181| <<sbc_execute_unmap>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1162| <<spc_emulate_modeselect>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1205| <<spc_emulate_modeselect>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1238| <<spc_emulate_request_sense>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1313| <<spc_emulate_testunitready>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_transport.c|978| <<target_complete_cmd_with_length>> target_complete_cmd(cmd, scsi_status);
+ *   - drivers/target/target_core_user.c|1284| <<tcmu_tmr_notify>> target_complete_cmd(se_cmd, SAM_STAT_TASK_ABORTED);
+ *   - drivers/target/target_core_user.c|1390| <<tcmu_handle_completion>> target_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);
+ *   - drivers/target/target_core_user.c|1534| <<tcmu_check_expired_ring_cmd>> target_complete_cmd(se_cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_user.c|1552| <<tcmu_check_expired_queue_cmd>> target_complete_cmd(se_cmd, SAM_STAT_TASK_SET_FULL);
+ *   - drivers/target/target_core_user.c|1779| <<run_qfull_queue>> target_complete_cmd(tcmu_cmd->se_cmd, SAM_STAT_BUSY);
+ *   - drivers/target/target_core_user.c|1793| <<run_qfull_queue>> target_complete_cmd(tcmu_cmd->se_cmd,
+ *   - drivers/target/target_core_user.c|2389| <<tcmu_reset_ring>> target_complete_cmd(cmd->se_cmd, SAM_STAT_BUSY);
+ *   - drivers/target/target_core_user.c|2392| <<tcmu_reset_ring>> target_complete_cmd(cmd->se_cmd,
+ *   - drivers/target/target_core_xcopy.c|763| <<target_xcopy_do_work>> target_complete_cmd(ec_cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_xcopy.c|892| <<target_do_xcopy>> target_complete_cmd(se_cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_xcopy.c|1002| <<target_rcr_operating_parameters>> target_complete_cmd(se_cmd, SAM_STAT_GOOD);
+ */
 void target_complete_cmd(struct se_cmd *cmd, u8 scsi_status)
 {
 	target_complete_cmd_with_sense(cmd, scsi_status, scsi_status ?
@@ -1685,6 +1731,10 @@ transport_generic_map_mem_to_cmd(struct se_cmd *cmd, struct scatterlist *sgl,
  * return code and handle failures. This will never fail for other drivers,
  * and the return code can be ignored.
  */
+/*
+ * 在以下调用target_init_cmd():
+ *   - 
+ */
 int target_init_cmd(struct se_cmd *se_cmd, struct se_session *se_sess,
 		    unsigned char *sense, u64 unpacked_lun,
 		    u32 data_length, int task_attr, int data_dir, int flags)
@@ -1976,6 +2026,17 @@ static void target_complete_tmr_failure(struct work_struct *work)
  * Callable from all contexts.
  **/
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1667| <<srpt_handle_tsk_mgmt>> rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,
+ *   - drivers/scsi/elx/efct/efct_lio.c|1449| <<efct_scsi_recv_tmf>> rc = target_submit_tmr(&ocp->cmd, se_sess, NULL, lun, ocp, tmr_func,
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2799| <<ibmvscsis_parse_task>> rc = target_submit_tmr(&cmd->se_cmd, nexus->se_sess, NULL,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|617| <<tcm_qla2xxx_handle_tmr>> return target_submit_tmr(se_cmd, sess->se_sess, NULL, lun, mcmd,
+ *   - drivers/target/loopback/tcm_loop.c|216| <<tcm_loop_issue_tmr>> rc = target_submit_tmr(se_cmd, se_sess, tl_cmd->tl_sense_buf, lun,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|365| <<ft_send_tm>> rc = target_submit_tmr(&cmd->se_cmd, cmd->sess->se_sess,
+ *   - drivers/vhost/scsi.c|1701| <<vhost_scsi_handle_tmf>> if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
+ *   - drivers/xen/xen-scsiback.c|626| <<scsiback_device_action>> rc = target_submit_tmr(&pending_req->se_cmd, nexus->tvn_se_sess,
+ */
 int target_submit_tmr(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *sense, u64 unpacked_lun,
 		void *fabric_tmr_ptr, unsigned char tm_type,
diff --git a/drivers/vdpa/ifcvf/ifcvf_main.c b/drivers/vdpa/ifcvf/ifcvf_main.c
index ccf64d7bb..f016468db 100644
--- a/drivers/vdpa/ifcvf/ifcvf_main.c
+++ b/drivers/vdpa/ifcvf/ifcvf_main.c
@@ -518,6 +518,22 @@ static int ifcvf_vdpa_set_vq_address(struct vdpa_device *vdpa_dev, u16 qid,
 	return ifcvf_set_vq_address(vf, qid, desc_area, driver_area, device_area);
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void ifcvf_vdpa_kick_vq(struct vdpa_device *vdpa_dev, u16 qid)
 {
 	struct ifcvf_hw *vf = vdpa_to_vf(vdpa_dev);
diff --git a/drivers/vdpa/mlx5/net/mlx5_vnet.c b/drivers/vdpa/mlx5/net/mlx5_vnet.c
index 5f581e71e..cca3ecfd2 100644
--- a/drivers/vdpa/mlx5/net/mlx5_vnet.c
+++ b/drivers/vdpa/mlx5/net/mlx5_vnet.c
@@ -1455,6 +1455,10 @@ static irqreturn_t mlx5_vdpa_int_handler(int irq, void *priv)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|1528| <<setup_vq>> alloc_vector(ndev, mvq);
+ */
 static void alloc_vector(struct mlx5_vdpa_net *ndev,
 			 struct mlx5_vdpa_virtqueue *mvq)
 {
@@ -2425,6 +2429,22 @@ static void mlx5_cvq_kick_handler(struct work_struct *work)
 	up_write(&ndev->reslock);
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void mlx5_vdpa_kick_vq(struct vdpa_device *vdev, u16 idx)
 {
 	struct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);
@@ -2446,6 +2466,13 @@ static void mlx5_vdpa_kick_vq(struct vdpa_device *vdev, u16 idx)
 	if (unlikely(!mvq->ready))
 		return;
 
+	/*
+	 * struct mlx5_vdpa_net *ndev:
+	 * -> struct mlx5_vdpa_dev mvdev;
+	 *    -> struct vdpa_device vdev;
+	 *    -> struct mlx5_vdpa_resources res;
+	 *       -> void __iomem *kick_addr;
+	 */
 	iowrite16(idx, ndev->mvdev.res.kick_addr);
 }
 
diff --git a/drivers/vdpa/vdpa.c b/drivers/vdpa/vdpa.c
index 8a372b51c..205cbb500 100644
--- a/drivers/vdpa/vdpa.c
+++ b/drivers/vdpa/vdpa.c
@@ -16,6 +16,13 @@
 #include <linux/mod_devicetable.h>
 #include <linux/virtio_ids.h>
 
+/*
+ * 在以下使用mdev_head:
+ *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+ *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+ *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+ *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+ */
 static LIST_HEAD(mdev_head);
 /* A global mutex that protects vdpa management device and device level operations. */
 static DECLARE_RWSEM(vdpa_dev_lock);
@@ -24,6 +31,11 @@ static DEFINE_IDA(vdpa_index_ida);
 void vdpa_set_status(struct vdpa_device *vdev, u8 status)
 {
 	down_write(&vdev->cf_lock);
+	/*
+	 * struct vdpa_device *vdev:
+	 * -> const struct vdpa_config_ops *config;
+	 *    -> void (*set_status)(struct vdpa_device *vdev, u8 status);
+	 */
 	vdev->config->set_status(vdev, status);
 	up_write(&vdev->cf_lock);
 }
@@ -154,6 +166,11 @@ static void vdpa_release_dev(struct device *d)
  * Return: Returns an error when parent/config/dma_dev is not set or fail to get
  *	   ida.
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|218| <<vdpasim_create>> vdpa = __vdpa_alloc_device(NULL, ops,
+ *   - include/linux/vdpa.h|469| <<vdpa_alloc_device>> container_of((__vdpa_alloc_device( \
+ */
 struct vdpa_device *__vdpa_alloc_device(struct device *parent,
 					const struct vdpa_config_ops *config,
 					unsigned int ngroups, unsigned int nas,
@@ -245,6 +262,17 @@ static int __vdpa_register_device(struct vdpa_device *vdev, u32 nvqs)
  *
  * Return: Returns an error when fail to add device to vDPA bus
  */
+/*
+ * called by:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|737| <<ifcvf_vdpa_dev_add>> ret = _vdpa_register_device(&adapter->vdpa, vf->nr_vring);
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3991| <<mlx5_vdpa_dev_add>> err = _vdpa_register_device(&mvdev->vdev, max_vqs + 1);
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|515| <<octep_vdpa_dev_add>> ret = _vdpa_register_device(&oct_vdpa->vdpa, oct_hw->nr_vring);
+ *   - drivers/vdpa/pds/vdpa_dev.c|749| <<pds_vdpa_dev_add>> err = _vdpa_register_device(&pdsv->vdpa_dev, pdsv->num_vqs);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|448| <<vdpasim_blk_dev_add>> ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_BLK_VQ_NUM);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|510| <<vdpasim_net_dev_add>> ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_NET_VQ_NUM);
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|2057| <<vdpa_dev_add>> ret = _vdpa_register_device(&dev->vdev->vdpa, dev->vq_num);
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|562| <<vp_vdpa_dev_add>> ret = _vdpa_register_device(&vp_vdpa->vdpa, vp_vdpa->queues);
+ */
 int _vdpa_register_device(struct vdpa_device *vdev, u32 nvqs)
 {
 	if (!vdev->mdev)
@@ -334,6 +362,17 @@ EXPORT_SYMBOL_GPL(vdpa_unregister_driver);
  * Return: Returns 0 on success or failure when required callback ops are not
  *         initialized.
  */
+/*
+ * called by:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|838| <<ifcvf_probe>> ret = vdpa_mgmtdev_register(&ifcvf_mgmt_dev->mdev);
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|4108| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|604| <<octep_vdpa_setup_task>> ret = vdpa_mgmtdev_register(&mgmt_dev->mdev);
+ *   - drivers/vdpa/pds/aux_drv.c|67| <<pds_vdpa_probe>> err = vdpa_mgmtdev_register(&vdpa_aux->vdpa_mdev);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|493| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|561| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|2117| <<vduse_mgmtdev_init>> ret = vdpa_mgmtdev_register(&vduse_mgmt->mgmt_dev);
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|648| <<vp_vdpa_probe>> err = vdpa_mgmtdev_register(mgtdev);
+ */
 int vdpa_mgmtdev_register(struct vdpa_mgmt_dev *mdev)
 {
 	if (!mdev->device || !mdev->ops || !mdev->ops->dev_add || !mdev->ops->dev_del)
@@ -341,6 +380,13 @@ int vdpa_mgmtdev_register(struct vdpa_mgmt_dev *mdev)
 
 	INIT_LIST_HEAD(&mdev->list);
 	down_write(&vdpa_dev_lock);
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+	 *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_add_tail(&mdev->list, &mdev_head);
 	up_write(&vdpa_dev_lock);
 	return 0;
@@ -448,6 +494,13 @@ static struct vdpa_mgmt_dev *vdpa_mgmtdev_get_from_attr(struct nlattr **attrs)
 	if (attrs[VDPA_ATTR_MGMTDEV_BUS_NAME])
 		busname = nla_data(attrs[VDPA_ATTR_MGMTDEV_BUS_NAME]);
 
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+	 *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_for_each_entry(mdev, &mdev_head, list) {
 		if (mgmtdev_handle_match(mdev, busname, devname))
 			return mdev;
@@ -561,6 +614,13 @@ vdpa_nl_cmd_mgmtdev_get_dumpit(struct sk_buff *msg, struct netlink_callback *cb)
 	int err;
 
 	down_read(&vdpa_dev_lock);
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+	 *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_for_each_entry(mdev, &mdev_head, list) {
 		if (idx < start) {
 			idx++;
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim.c b/drivers/vdpa/vdpa_sim/vdpa_sim.c
index 8ffea8430..cdfa7d29f 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim.c
@@ -79,6 +79,10 @@ static struct vdpasim *vdpa_to_sim(struct vdpa_device *vdpa)
 	return container_of(vdpa, struct vdpasim, vdpa);
 }
 
+/*
+ * 在以下使用vdpasim_vq_notify():
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|124| <<vdpasim_queue_ready>> vq->vring.notify = vdpasim_vq_notify;
+ */
 static void vdpasim_vq_notify(struct vringh *vring)
 {
 	struct vdpasim_virtqueue *vq =
@@ -283,6 +287,12 @@ struct vdpasim *vdpasim_create(struct vdpasim_dev_attr *dev_attr,
 }
 EXPORT_SYMBOL_GPL(vdpasim_create);
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|357| <<vdpasim_kick_vq>> vdpasim_schedule_work(vdpasim);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|363| <<vdpasim_blk_work>> vdpasim_schedule_work(vdpasim);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|262| <<vdpasim_net_work>> vdpasim_schedule_work(vdpasim);
+ */
 void vdpasim_schedule_work(struct vdpasim *vdpasim)
 {
 	kthread_queue_work(vdpasim->worker, &vdpasim->work);
@@ -322,6 +332,22 @@ static u16 vdpasim_get_vq_size(struct vdpa_device *vdpa, u16 idx)
 		return VDPASIM_QUEUE_MAX;
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void vdpasim_kick_vq(struct vdpa_device *vdpa, u16 idx)
 {
 	struct vdpasim *vdpasim = vdpa_to_sim(vdpa);
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c b/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
index b137f3679..522cfde5f 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
@@ -490,6 +490,17 @@ static int __init vdpasim_blk_init(void)
 		return ret;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|838| <<ifcvf_probe>> ret = vdpa_mgmtdev_register(&ifcvf_mgmt_dev->mdev);
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|4108| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|604| <<octep_vdpa_setup_task>> ret = vdpa_mgmtdev_register(&mgmt_dev->mdev);
+	 *   - drivers/vdpa/pds/aux_drv.c|67| <<pds_vdpa_probe>> err = vdpa_mgmtdev_register(&vdpa_aux->vdpa_mdev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|493| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|561| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|2117| <<vduse_mgmtdev_init>> ret = vdpa_mgmtdev_register(&vduse_mgmt->mgmt_dev);
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|648| <<vp_vdpa_probe>> err = vdpa_mgmtdev_register(mgtdev);
+	 */
 	ret = vdpa_mgmtdev_register(&mgmt_dev);
 	if (ret)
 		goto parent_err;
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim_net.c b/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
index 6caf09a19..b00f10d0e 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
@@ -66,6 +66,13 @@ static struct vdpasim_net *sim_to_net(struct vdpasim *vdpasim)
 	return container_of(vdpasim, struct vdpasim_net, vdpasim);
 }
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|236| <<vdpasim_net_work>> vdpasim_net_complete(txq, 0);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|244| <<vdpasim_net_work>> vdpasim_net_complete(txq, 0);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|258| <<vdpasim_net_work>> vdpasim_net_complete(txq, 0);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|259| <<vdpasim_net_work>> vdpasim_net_complete(rxq, write);
+ */
 static void vdpasim_net_complete(struct vdpasim_virtqueue *vq, size_t len)
 {
 	/* Make sure data is wrote before advancing index */
@@ -237,6 +244,19 @@ static void vdpasim_net_work(struct vdpasim *vdpasim)
 			continue;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2388| <<mlx5_cvq_kick_handler>> err = vringh_getdesc_iotlb(&cvq->vring,
+		 *             &cvq->riov, &cvq->wiov, &cvq->head,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|123| <<vdpasim_blk_handle_req>> ret = vringh_getdesc_iotlb(&vq->vring,
+		 *             &vq->out_iov, &vq->in_iov,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|144| <<vdpasim_handle_cvq>> err = vringh_getdesc_iotlb(&cvq->vring,
+		 *             &cvq->in_iov,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|220| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&txq->vring,
+		 *             &txq->out_iov, NULL,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|240| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&rxq->vring,
+		 *             NULL, &rxq->in_iov,
+		 */
 		err = vringh_getdesc_iotlb(&rxq->vring, NULL, &rxq->in_iov,
 					   &rxq->head, GFP_ATOMIC);
 		if (err <= 0) {
diff --git a/drivers/vdpa/vdpa_user/vduse_dev.c b/drivers/vdpa/vdpa_user/vduse_dev.c
index 7ae99691e..e77fee971 100644
--- a/drivers/vdpa/vdpa_user/vduse_dev.c
+++ b/drivers/vdpa/vdpa_user/vduse_dev.c
@@ -511,6 +511,22 @@ static void vduse_vq_kick_work(struct work_struct *work)
 	vduse_vq_kick(vq);
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void vduse_vdpa_kick_vq(struct vdpa_device *vdpa, u16 idx)
 {
 	struct vduse_dev *dev = vdpa_to_vduse(vdpa);
diff --git a/drivers/vdpa/virtio_pci/vp_vdpa.c b/drivers/vdpa/virtio_pci/vp_vdpa.c
index 163807642..3901e5ad0 100644
--- a/drivers/vdpa/virtio_pci/vp_vdpa.c
+++ b/drivers/vdpa/virtio_pci/vp_vdpa.c
@@ -360,6 +360,22 @@ static int vp_vdpa_set_vq_address(struct vdpa_device *vdpa, u16 qid,
 	return 0;
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void vp_vdpa_kick_vq(struct vdpa_device *vdpa, u16 qid)
 {
 	struct vp_vdpa *vp_vdpa = vdpa_to_vp(vdpa);
diff --git a/drivers/vfio/pci/vfio_pci_intrs.c b/drivers/vfio/pci/vfio_pci_intrs.c
index 8382c5834..799dee48a 100644
--- a/drivers/vfio/pci/vfio_pci_intrs.c
+++ b/drivers/vfio/pci/vfio_pci_intrs.c
@@ -363,6 +363,29 @@ static void vfio_intx_disable(struct vfio_pci_core_device *vdev)
 /*
  * MSI/MSI-X
  */
+/*
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * 中断的名字:
+ * IR-PCI-MSIX-0000:21:00.0    0-edge      vfio-msix[0](0000:21:00.0)
+ * IR-PCI-MSIX-0000:21:00.0    1-edge      vfio-msix[1](0000:21:00.0)
+ * IR-PCI-MSIX-0000:21:00.0    2-edge      vfio-msix[2](0000:21:00.0)
+ * IR-PCI-MSIX-0000:21:00.0    3-edge      vfio-msix[3](0000:21:00.0)
+ * IR-PCI-MSIX-0000:21:00.0    4-edge      vfio-msix[4](0000:21:00.0)
+ */
 static irqreturn_t vfio_msihandler(int irq, void *arg)
 {
 	struct eventfd_ctx *trigger = arg;
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 9ad37c012..8a47f1c5a 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -123,6 +123,18 @@ struct vhost_net_virtqueue {
 	/* Reference counting for outstanding ubufs.
 	 * Protected by vq mutex. Writers must also take device mutex. */
 	struct vhost_net_ubuf_ref *ubufs;
+	/*
+	 * 在以下使用vhost_net_virtqueue->rx_ring:
+	 *   - drivers/vhost/net.c|180| <<vhost_net_buf_produce>> rxq->tail = ptr_ring_consume_batched(nvq->rx_ring, rxq->queue,
+	 *   - drivers/vhost/net.c|189| <<vhost_net_buf_unproduce>> if (nvq->rx_ring && !vhost_net_buf_is_empty(rxq)) {
+	 *   - drivers/vhost/net.c|190| <<vhost_net_buf_unproduce>> ptr_ring_unconsume(nvq->rx_ring, rxq->queue + rxq->head,
+	 *   - drivers/vhost/net.c|1076| <<peek_head_len>> if (rvq->rx_ring)
+	 *   - drivers/vhost/net.c|1304| <<handle_rx>> if (nvq->rx_ring)
+	 *   - drivers/vhost/net.c|1479| <<vhost_net_open>> n->vqs[i].rx_ring = NULL;
+	 *   - drivers/vhost/net.c|1526| <<vhost_net_stop_vq>> nvq->rx_ring = NULL;
+	 *   - drivers/vhost/net.c|1716| <<vhost_net_set_backend>> nvq->rx_ring = get_tap_ptr_ring(sock->file);
+	 *   - drivers/vhost/net.c|1718| <<vhost_net_set_backend>> nvq->rx_ring = NULL;
+	 */
 	struct ptr_ring *rx_ring;
 	struct vhost_net_buf rxq;
 	/* Batched XDP buffs */
@@ -205,6 +217,10 @@ static int vhost_net_buf_peek_len(void *ptr)
 	return __skb_array_len_with_tag(ptr);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1077| <<peek_head_len>> return vhost_net_buf_peek(rvq);
+ */
 static int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
@@ -373,6 +389,11 @@ static void vhost_zerocopy_signal_used(struct vhost_net *net,
 	}
 	while (j) {
 		add = min(UIO_MAXIOV - nvq->done_idx, j);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|376| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+		 *   - drivers/vhost/net.c|460| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+		 */
 		vhost_add_used_and_signal_n(vq->dev, vq,
 					    &vq->heads[nvq->done_idx], add);
 		nvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;
@@ -431,6 +452,18 @@ static void vhost_net_disable_vq(struct vhost_net *n,
 	struct vhost_poll *poll = n->poll + (nvq - n->vqs);
 	if (!vhost_vq_get_backend(vq))
 		return;
+	/*
+	 * 在以下调用vhost_poll_stop():
+	 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+	 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+	 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+	 *
+	 * 注释:
+	 * Stop polling a file. After this function returns, it becomes safe to drop the
+	 * file reference. You must also flush afterwards.
+	 */
 	vhost_poll_stop(poll);
 }
 
@@ -446,6 +479,17 @@ static int vhost_net_enable_vq(struct vhost_net *n,
 	if (!sock)
 		return 0;
 
+	/*
+	 * 在以下调用vhost_poll_start():
+	 *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+	 *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.h|56| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+	 *
+	 * 注释:
+	 * Start polling a file. We add ourselves to file's wait queue. The caller must
+	 * keep a reference to a file until after vhost_poll_stop is called.
+	 */
 	return vhost_poll_start(poll, sock->file);
 }
 
@@ -457,6 +501,11 @@ static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 	if (!nvq->done_idx)
 		return;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|376| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+	 *   - drivers/vhost/net.c|460| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+	 */
 	vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
 	nvq->done_idx = 0;
 }
@@ -580,6 +629,25 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 	struct vhost_virtqueue *rvq = &rnvq->vq;
 	struct vhost_virtqueue *tvq = &tnvq->vq;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+	 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
 				  out_num, in_num, NULL, NULL);
 
@@ -592,6 +660,25 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 
 		vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, false);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
 				      out_num, in_num, NULL, NULL);
 	}
@@ -790,6 +877,17 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 				goto done;
 			} else if (unlikely(err != -ENOSPC)) {
 				vhost_tx_batch(net, nvq, sock, &msg);
+				/*
+				 * called by:
+				 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *
+				 * 把vhost_virtqueue->last_avail_idx减少n
+				 */
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
@@ -811,6 +909,17 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			if (err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS) {
+				/*
+				 * called by:
+				 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *
+				 * 把vhost_virtqueue->last_avail_idx减少n
+				 */
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
@@ -919,6 +1028,17 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 					vq->heads[ubuf->desc].len = VHOST_DMA_DONE_LEN;
 			}
 			if (retry) {
+				/*
+				 * called by:
+				 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *
+				 * 把vhost_virtqueue->last_avail_idx减少n
+				 */
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
@@ -963,12 +1083,23 @@ static void handle_tx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1098| <<vhost_net_rx_peek_head_len>> int len = peek_head_len(rnvq, sk);
+ *   - drivers/vhost/net.c|1106| <<vhost_net_rx_peek_head_len>> len = peek_head_len(rnvq, sk);
+ */
 static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 {
 	struct sk_buff *head;
 	int len = 0;
 	unsigned long flags;
 
+	/*
+	 * struct vhost_net_virtqueue *rvq:
+	 * -> struct ptr_ring *rx_ring;
+	 *
+	 * 只在此处调用vhost_net_buf_peek()
+	 */
 	if (rvq->rx_ring)
 		return vhost_net_buf_peek(rvq);
 
@@ -984,6 +1115,10 @@ static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 	return len;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1274| <<handle_rx>> sock_len = vhost_net_rx_peek_head_len(net, sock->sk,
+ */
 static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,
 				      bool *busyloop_intr)
 {
@@ -993,6 +1128,16 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,
 	struct vhost_virtqueue *tvq = &tnvq->vq;
 	int len = peek_head_len(rnvq, sk);
 
+	/*
+	 * 在以下使用vhost_virtqueue->busyloop_timeout:
+	 *   - drivers/vhost/net.c|577| <<vhost_net_busy_poll>> busyloop_timeout = poll_rx ?
+	 *             rvq->busyloop_timeout: tvq->busyloop_timeout;
+	 *   - drivers/vhost/net.c|638| <<vhost_net_tx_get_vq_desc>> if (r == tvq->num && tvq->busyloop_timeout) {
+	 *   - drivers/vhost/net.c|1100| <<vhost_net_rx_peek_head_len>> if (!len && rvq->busyloop_timeout) {
+	 *   - drivers/vhost/vhost.c|550| <<vhost_vq_reset>> vq->busyloop_timeout = 0;
+	 *   - drivers/vhost/vhost.c|2369| <<vhost_vring_ioctl(VHOST_SET_VRING_BUSYLOOP_TIMEOUT)>> vq->busyloop_timeout = s.num;
+	 *   - drivers/vhost/vhost.c|2373| <<vhost_vring_ioctl(VHOST_GET_VRING_BUSYLOOP_TIMEOUT)>> s.num = vq->busyloop_timeout;
+	 */
 	if (!len && rvq->busyloop_timeout) {
 		/* Flush batched heads first */
 		vhost_net_signal_used(rnvq);
@@ -1015,6 +1160,12 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,
  * @quota       - headcount quota, 1 for big buffer
  *	returns number of buffer heads allocated, negative on error
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1274| <<handle_rx>> headcount = get_rx_bufs(vq,
+ *             vq->heads + nvq->done_idx, vhost_len, &in, vq_log, &log,
+ *             likely(mergeable) ? UIO_MAXIOV : 1);
+ */
 static int get_rx_bufs(struct vhost_virtqueue *vq,
 		       struct vring_used_elem *heads,
 		       int datalen,
@@ -1038,6 +1189,25 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			r = -ENOBUFS;
 			goto err;
 		}
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		r = vhost_get_vq_desc(vq, vq->iov + seg,
 				      ARRAY_SIZE(vq->iov) - seg, &out,
 				      &in, log, log_num);
@@ -1078,10 +1248,26 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	}
 	return headcount;
 err:
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+	 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+	 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+	 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+	 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+	 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+	 *
+	 * 把vhost_virtqueue->last_avail_idx减少n
+	 */
 	vhost_discard_vq_desc(vq, headcount);
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1454| <<handle_rx_kick>> handle_rx(net);
+ *   - drivers/vhost/net.c|1468| <<handle_rx_net>> handle_rx(net);
+ */
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
 static void handle_rx(struct vhost_net *net)
@@ -1131,12 +1317,18 @@ static void handle_rx(struct vhost_net *net)
 	mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
 
 	do {
+		/*
+		 * 只在此处调用
+		 */
 		sock_len = vhost_net_rx_peek_head_len(net, sock->sk,
 						      &busyloop_intr);
 		if (!sock_len)
 			break;
 		sock_len += sock_hlen;
 		vhost_len = sock_len + vhost_hlen;
+		/*
+		 * 只在此处调用
+		 */
 		headcount = get_rx_bufs(vq, vq->heads + nvq->done_idx,
 					vhost_len, &in, vq_log, &log,
 					likely(mergeable) ? UIO_MAXIOV : 1);
@@ -1185,6 +1377,17 @@ static void handle_rx(struct vhost_net *net)
 		if (unlikely(err != sock_len)) {
 			pr_debug("Discarded rx packet: "
 				 " len %d, expected %zd\n", err, sock_len);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *
+			 * 把vhost_virtqueue->last_avail_idx减少n
+			 */
 			vhost_discard_vq_desc(vq, headcount);
 			continue;
 		}
@@ -1209,6 +1412,17 @@ static void handle_rx(struct vhost_net *net)
 		    copy_to_iter(&num_buffers, sizeof num_buffers,
 				 &fixup) != sizeof num_buffers) {
 			vq_err(vq, "Failed num_buffers write");
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *
+			 * 把vhost_virtqueue->last_avail_idx减少n
+			 */
 			vhost_discard_vq_desc(vq, headcount);
 			goto out;
 		}
@@ -1314,6 +1528,22 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		n->vqs[i].rx_ring = NULL;
 		vhost_net_buf_init(&n->vqs[i].rxq);
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
 		       UIO_MAXIOV + VHOST_NET_BATCH,
 		       VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,
@@ -1514,8 +1744,32 @@ static long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)
 		}
 
 		vhost_net_disable_vq(n, vq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+		 *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+		 *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+		 *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+		 *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+		 *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+		 *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+		 */
 		vhost_vq_set_backend(vq, sock);
 		vhost_net_buf_unproduce(nvq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(vq);
 		if (r)
 			goto err_used;
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 718fa4e0b..e61483699 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -39,6 +39,150 @@
 
 #include "vhost.h"
 
+/*
+ * vhost_scsi_set_features()
+ * -> mutex_lock(&vs->dev.mutex);
+ * -> for (i = 0; i < vs->dev.nvqs; i++) {
+ *        mutex_lock(&vq->mutex);
+ *        vq->acked_features = features;
+ *        mutex_unlock(&vq->mutex);
+ *    }
+ * -> mutex_unlock(&vs->dev.mutex);
+ *
+ *
+ * vhost_scsi_set_endpoint()
+ * -> mutex_lock(&vs->dev.mutex);
+ * -> for (i = VHOST_SCSI_VQ_IO; i < vs->dev.nvqs; i++) {
+ *        vhost_scsi_setup_vq_cmds()
+ *        -> if (svq->scsi_cmds) return 0;
+ *    }
+ * -> for (i = 0; i < vs->dev.nvqs; i++) {
+ *        mutex_lock(&vq->mutex);
+ *        vhost_vq_set_backend(vq, vs_tpg);
+ *        vhost_vq_init_access(vq);
+ *    }
+ * -> vhost_scsi_flush(vs);
+ * -> mutex_unlock(&vs->dev.mutex);
+ *
+ *
+ * vhost_scsi_clear_endpoint()
+ * -> mutex_lock(&vs->dev.mutex);
+ * -> for (i = 0; i < vs->dev.nvqs; i++) {
+ *        mutex_lock(&vq->mutex);
+ *        vhost_vq_set_backend(vq, NULL);
+ *        mutex_unlock(&vq->mutex);
+ *    }
+ * -> vhost_scsi_flush(vs);
+ * -> for (i = 0; i < vs->dev.nvqs; i++) {
+ *        vhost_scsi_destroy_vq_cmds()
+ *    }
+ * -> mutex_unlock(&vs->dev.mutex);
+ *
+ *
+ * 在以下调用for_each_sgtable_sg():
+ *   - drivers/dma-buf/dma-buf.c|788| <<mangle_sg_table>> for_each_sgtable_sg(sg_table, sg, i)
+ *   - drivers/dma-buf/heaps/system_heap.c|74| <<dup_sg_table>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|292| <<system_heap_dma_buf_release>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|406| <<system_heap_allocate>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|703| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i)
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|713| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|734| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|765| <<amdgpu_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|409| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/armada/armada_gem.c|442| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|465| <<armada_gem_prime_unmap_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/msm/msm_iommu.c|124| <<msm_iommu_pagetable_map>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/core/firmware.c|272| <<nvkm_firmware_ctor>> for_each_sgtable_sg(&fw->mem.sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2195| <<nvkm_gsp_sg_free>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2215| <<nvkm_gsp_sg>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2308| <<nvkm_gsp_radix3_sg>> for_each_sgtable_sg(&rx3->lvl2, sg, i) {
+ *   - drivers/gpu/drm/rockchip/rockchip_drm_gem.c|104| <<rockchip_gem_get_pages>> for_each_sgtable_sg(rk_obj->sgt, s, i)
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|222| <<drm_gem_shmem_test_get_pages_sgt>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|257| <<drm_gem_shmem_test_get_sg_table>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_object.c|169| <<virtio_gpu_object_shmem_init>> for_each_sgtable_sg(pages, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|296| <<vmalloc_to_sgt>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|404| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i)
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|414| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|435| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|456| <<xe_ttm_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/host1x/job.c|238| <<pin_job>> for_each_sgtable_sg(map->sgt, sg, j)
+ *   - drivers/infiniband/core/umem.c|58| <<__ib_umem_release>> for_each_sgtable_sg(&umem->sgt_append.sgt, sg, i)
+ *   - drivers/media/common/videobuf2/videobuf2-vmalloc.c|234| <<vb2_vmalloc_dmabuf_ops_attach>> for_each_sgtable_sg(sgt, sg, i) {
+ *
+ * 注释: Loop over each sg element in the given sg_table object.
+ */
+/*
+ * 2796.238623 |   114)  vhost-7-7071  |  .N... |   0.086 us    |      vhost_exceeds_weight [vhost]();
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |      vhost_scsi_get_desc [vhost_scsi]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |        vhost_get_vq_desc [vhost]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |   0.106 us    |          vhost_copy_from_user.constprop.0 [vhost]();
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |          get_indirect [vhost]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |   0.085 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |   0.252 us    |            }
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.085 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.256 us    |            }
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.085 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.278 us    |            }
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.083 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.251 us    |            }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   1.660 us    |          }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   2.044 us    |        }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   2.359 us    |      }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.082 us    |      vhost_scsi_chk_size [vhost_scsi]();
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |               |      vhost_scsi_get_req [vhost_scsi]() {
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |               |        __check_object_size() {
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |               |          __check_object_size.part.0() {
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.082 us    |            check_stack_object();
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.249 us    |          }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.408 us    |        }
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |   0.719 us    |      }
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |   0.130 us    |      vhost_scsi_get_cmd.isra.0 [vhost_scsi]();
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |      vhost_scsi_mapal [vhost_scsi]() {
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |   0.085 us    |        vhost_scsi_calc_sgls.constprop.0 [vhost_scsi]();
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |        vhost_scsi_iov_to_sgl.constprop.0 [vhost_scsi]() {
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |          get_user_pages_fast() {
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |            internal_get_user_pages_fast() {
+ * 2796.238628 |   114)  vhost-7-7071  |  .N... |               |              lockless_pages_from_mm() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |               |                gup_pgd_range() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.081 us    |                  pud_huge();
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |               |                  gup_pmd_range.constprop.0() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |               |                    gup_huge_pmd() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.141 us    |                      try_grab_compound_head();
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.314 us    |                    }
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.489 us    |                  }
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.829 us    |                }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.008 us    |              }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.179 us    |            }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.337 us    |          }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.537 us    |        }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.887 us    |      }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |               |      target_init_cmd [target_core_mod]() {
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   0.083 us    |        __init_swait_queue_head();
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   0.089 us    |        target_get_sess_cmd [target_core_mod]();
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   0.434 us    |      }
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |      target_submit_prep [target_core_mod]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |   0.095 us    |        target_cmd_init_cdb [target_core_mod]();
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |   0.118 us    |        transport_lookup_cmd_lun [target_core_mod]();
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |        target_cmd_parse_cdb [target_core_mod]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |          iblock_parse_cdb [target_core_iblock]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |            sbc_parse_cdb [target_core_mod]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |   0.083 us    |              sbc_check_dpofua [target_core_mod]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.084 us    |              sbc_check_prot [target_core_mod]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.087 us    |              iblock_get_blocks [target_core_iblock]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.086 us    |              target_cmd_size_check [target_core_mod]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.770 us    |            }
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.929 us    |          }
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   1.092 us    |        }
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   1.828 us    |      } // target_submit_prep [target_core_mod]
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |               |      target_queue_submission [target_core_mod]() {
+ * 2796.238632 |   114)  vhost-7-7071  |  .N... |   0.091 us    |        queue_work_on();
+ * 2796.238632 |   114)  vhost-7-7071  |  .N... |   0.261 us    |      }
+ */
+
 #define VHOST_SCSI_VERSION  "v0.1"
 #define VHOST_SCSI_NAMELEN 256
 #define VHOST_SCSI_MAX_CDB_SIZE 32
@@ -64,12 +208,22 @@ struct vhost_scsi_cmd {
 	int tvc_vq_desc;
 	/* virtio-scsi initiator task attribute */
 	int tvc_task_attr;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_in_iovs:
+	 *   - drivers/vhost/scsi.c|855| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter,
+	 *             ITER_DEST, cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+	 *   - drivers/vhost/scsi.c|1562| <<vhost_scsi_handle_vq>> cmd->tvc_in_iovs = vc.in;
+	 */
 	/* virtio-scsi response incoming iovecs */
 	int tvc_in_iovs;
 	/* virtio-scsi initiator data direction */
 	enum dma_data_direction tvc_data_direction;
 	/* Expected data transfer length from virtio-scsi header */
 	u32 tvc_exp_data_len;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_tag:
+	 *   - drivers/vhost/scsi.c|1105| <<vhost_scsi_get_cmd>> cmd->tvc_tag = scsi_tag;
+	 */
 	/* The Tag from include/linux/virtio_scsi.h:struct virtio_scsi_cmd_req */
 	u64 tvc_tag;
 	/* The number of scatterlists associated with this cmd */
@@ -81,9 +235,63 @@ struct vhost_scsi_cmd {
 	const void *saved_iter_addr;
 	struct iov_iter saved_iter;
 	/* Pointer to the SGL formatted memory from virtio-scsi */
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_release_cmd_res>> __free_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|576| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|950| <<vhost_scsi_copy_sgl_to_iov>> struct scatterlist *sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1086| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|1400| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1403| <<vhost_scsi_mapal>> pr_debug( ... cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1405| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1408| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1412| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1448| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|2490| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|2541| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|2544| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	struct scatterlist *tvc_sgl;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_prot_sgl:
+	 *   - drivers/vhost/scsi.c|567| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
+	 *   - drivers/vhost/scsi.c|1072| <<vhost_scsi_get_cmd>> prot_sg = cmd->tvc_prot_sgl;
+	 *   - drivers/vhost/scsi.c|1087| <<vhost_scsi_get_cmd>> cmd->tvc_prot_sgl = prot_sg;
+	 *   - drivers/vhost/scsi.c|1352| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_prot_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1355| <<vhost_scsi_mapal>> pr_debug(... cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count);
+	 *   - drivers/vhost/scsi.c|1358| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, prot_iter,
+	 *             cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count, true);
+	 *   - drivers/vhost/scsi.c|1421| <<vhost_scsi_target_queue_cmd>> sg_prot_ptr = cmd->tvc_prot_sgl;
+	 *   - drivers/vhost/scsi.c|2461| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_prot_sgl);
+	 *   - drivers/vhost/scsi.c|2545| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_prot_sgl = kcalloc(VHOST_SCSI_PREALLOC_PROT_SGLS,
+	 *             sizeof(struct scatterlist), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2548| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_prot_sgl) {
+	 */
 	struct scatterlist *tvc_prot_sgl;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_upages:
+	 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+	 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+	 *                                          sizeof(struct page *), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+	 */
 	struct page **tvc_upages;
+	/*
+	 * 在以下使用vhost_scso_cmd->tvc_resp_iov:
+	 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+	 *                   cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+	 *   - drivers/vhost/scsi.c|1364| <<vhost_scsi_get_cmd>> tvc_resp_iov = cmd->tvc_resp_iov;
+	 *   - drivers/vhost/scsi.c|1377| <<vhost_scsi_get_cmd>> cmd->tvc_resp_iov = tvc_resp_iov;
+	 *   - drivers/vhost/scsi.c|2141| <<vhost_scsi_handle_vq>> cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
+	 *   - drivers/vhost/scsi.c|2805| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_resp_iov);
+	 *   - drivers/vhost/scsi.c|2898| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
+	 *                   sizeof(struct iovec), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2901| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_resp_iov) {
+	 */
 	/* Pointer to response header iovec */
 	struct iovec *tvc_resp_iov;
 	/* Pointer to vhost_scsi for our device */
@@ -110,24 +318,97 @@ struct vhost_scsi_nexus {
 };
 
 struct vhost_scsi_tpg {
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi_tpg:
+	 *   - drivers/vhost/scsi.c|753| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|2614| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|3019| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|3057| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+	 *   - drivers/vhost/scsi.c|3251| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+	 *   - drivers/vhost/scsi.c|4072| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+	 */
 	/* Vhost port target portal group tag for TCM */
 	u16 tport_tpgt;
+	/*
+	 * 在以下使用vhost_scsi_tpg->tv_tpg_port_count:
+	 *   - drivers/vhost/scsi.c|2927| <<vhost_scsi_port_link>> tpg->tv_tpg_port_count++;
+	 *   - drivers/vhost/scsi.c|2944| <<vhost_scsi_port_unlink>> tpg->tv_tpg_port_count--;
+	 *   - drivers/vhost/scsi.c|3046| <<vhost_scsi_drop_nexus>> if (tpg->tv_tpg_port_count != 0) {
+	 *   - drivers/vhost/scsi.c|3050| <<vhost_scsi_drop_nexus>> pr_err("Unable ... tpg->tv_tpg_port_count);
+	 */
 	/* Used to track number of TPG Port/Lun Links wrt to explict I_T Nexus shutdown */
 	int tv_tpg_port_count;
+	/*
+	 * 在以下使用vhost_scsi_tpg->tv_tpg_vhost_count:
+	 *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_set_endpoint>> if (tpg->tv_tpg_vhost_count != 0) {
+	 *   - drivers/vhost/scsi.c|2355| <<vhost_scsi_set_endpoint>> tpg->tv_tpg_vhost_count++;
+	 *   - drivers/vhost/scsi.c|2424| <<vhost_scsi_set_endpoint>> tpg->tv_tpg_vhost_count--;
+	 *   - drivers/vhost/scsi.c|2519| <<vhost_scsi_clear_endpoint>> tpg->tv_tpg_vhost_count--;
+	 *   - drivers/vhost/scsi.c|3054| <<vhost_scsi_drop_nexus>> if (tpg->tv_tpg_vhost_count != 0) {
+	 *   - drivers/vhost/scsi.c|3058| <<vhost_scsi_drop_nexus>> pr_err(... tpg->tv_tpg_vhost_count);
+	 */
 	/* Used for vhost_scsi device reference to tpg_nexus, protected by tv_tpg_mutex */
 	int tv_tpg_vhost_count;
 	/* Used for enabling T10-PI with legacy devices */
 	int tv_fabric_prot_type;
 	/* list for vhost_scsi_list */
 	struct list_head tv_tpg_list;
+	/*
+	 * 在以下使用vhost_scsi_tpg->tv_tpg_mutex:
+	 *   - drivers/vhost/scsi.c|2842| <<vhost_scsi_set_endpoint>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2844| <<vhost_scsi_set_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2848| <<vhost_scsi_set_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2855| <<vhost_scsi_set_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2870| <<vhost_scsi_set_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2879| <<vhost_scsi_set_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2965| <<vhost_scsi_set_endpoint>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|2968| <<vhost_scsi_set_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3096| <<vhost_scsi_clear_endpoint>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3102| <<vhost_scsi_clear_endpoint>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3526| <<vhost_scsi_port_link>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3529| <<vhost_scsi_port_link>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3543| <<vhost_scsi_port_unlink>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3546| <<vhost_scsi_port_unlink>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3600| <<vhost_scsi_make_nexus>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3602| <<vhost_scsi_make_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3609| <<vhost_scsi_make_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3622| <<vhost_scsi_make_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3628| <<vhost_scsi_make_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3637| <<vhost_scsi_drop_nexus>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3640| <<vhost_scsi_drop_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3646| <<vhost_scsi_drop_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3651| <<vhost_scsi_drop_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3659| <<vhost_scsi_drop_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3675| <<vhost_scsi_drop_nexus>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3689| <<vhost_scsi_tpg_nexus_show>> mutex_lock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3692| <<vhost_scsi_tpg_nexus_show>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3697| <<vhost_scsi_tpg_nexus_show>> mutex_unlock(&tpg->tv_tpg_mutex);
+	 *   - drivers/vhost/scsi.c|3813| <<vhost_scsi_make_tpg>> mutex_init(&tpg->tv_tpg_mutex);
+	 */
 	/* Used to protect access for tpg_nexus */
 	struct mutex tv_tpg_mutex;
 	/* Pointer to the TCM VHost I_T Nexus for this TPG endpoint */
 	struct vhost_scsi_nexus *tpg_nexus;
 	/* Pointer back to vhost_scsi_tport */
 	struct vhost_scsi_tport *tport;
+	/*
+	 * 在以下使用vhost_scsi->se_tpg (struct se_portal_group):
+	 *   - drivers/vhost/scsi.c|2924| <<vhost_scsi_set_endpoint>> se_tpg = &tpg->se_tpg;
+	 *   - drivers/vhost/scsi.c|3027| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+	 *   - drivers/vhost/scsi.c|3162| <<vhost_scsi_clear_endpoint>> se_tpg = &tpg->se_tpg;
+	 *   - drivers/vhost/scsi.c|3676| <<vhost_scsi_make_nexus>> tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
+	 *   - drivers/vhost/scsi.c|3876| <<vhost_scsi_make_tpg>> ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
+	 *   - drivers/vhost/scsi.c|3896| <<vhost_scsi_make_tpg>> return &tpg->se_tpg;
+	 */
 	/* Returned by vhost_scsi_make_tpg() */
 	struct se_portal_group se_tpg;
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi:
+	 *   - drivers/vhost/scsi.c|2349| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = vs;
+	 *   - drivers/vhost/scsi.c|2416| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2513| <<vhost_scsi_clear_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2828| <<vhost_scsi_do_plug>> struct vhost_scsi *vs = tpg->vhost_scsi;
+	 */
 	/* Pointer back to vhost_scsi, protected by tv_tpg_mutex */
 	struct vhost_scsi *vhost_scsi;
 };
@@ -178,13 +459,56 @@ struct vhost_scsi_virtqueue {
 	 * each time, one reference tracks new commands submitted, while we
 	 * wait for another one to reach 0.
 	 */
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->inflights[2]:
+	 *   - drivers/vhost/scsi.c|608| <<vhost_scsi_init_inflight>> old_inflight[i] = &vs->vqs[i].inflights[idx];
+	 *   - drivers/vhost/scsi.c|612| <<vhost_scsi_init_inflight>> new_inflight = &vs->vqs[i].inflights[idx ^ 1];
+	 *   - drivers/vhost/scsi.c|632| <<vhost_scsi_get_inflight>> inflight = &svq->inflights[svq->inflight_idx];
+	 *
+	 * 注释:
+	 * Reference counting for inflight reqs, used for flush operation. At
+	 * each time, one reference tracks new commands submitted, while we
+	 * wait for another one to reach 0.
+	 */
 	struct vhost_scsi_inflight inflights[2];
 	/*
 	 * Indicate current inflight in use, protected by vq->mutex.
 	 * Writers must also take dev mutex and flush under it.
 	 */
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->inflight_idx:
+	 *   - drivers/vhost/scsi.c|606| <<vhost_scsi_init_inflight>> idx = vs->vqs[i].inflight_idx;
+	 *   - drivers/vhost/scsi.c|611| <<vhost_scsi_init_inflight>> vs->vqs[i].inflight_idx = idx ^ 1;
+	 *   - drivers/vhost/scsi.c|632| <<vhost_scsi_get_inflight>> inflight = &svq->inflights[svq->inflight_idx];
+	 *
+	 * 注释:
+	 * Indicate current inflight in use, protected by vq->mutex.
+	 * Writers must also take dev mutex and flush under it.
+	 */
 	int inflight_idx;
+	/*
+	 * 在以下设置vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	struct vhost_scsi_cmd *scsi_cmds;
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	struct sbitmap scsi_tags;
 	int max_cmds;
 
@@ -194,27 +518,103 @@ struct vhost_scsi_virtqueue {
 
 struct vhost_scsi {
 	/* Protected by vhost_scsi->dev.mutex */
+	/*
+	 * 在以下使用vhost_scsi->vs_tpg:
+	 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+	 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_handle_vq>> if (!vs_tpg)
+	 *   - drivers/vhost/scsi.c|2762| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg)
+	 *   - drivers/vhost/scsi.c|2763| <<vhost_scsi_set_endpoint>> memcpy(vs_tpg, vs->vs_tpg, len);
+	 *   - drivers/vhost/scsi.c|2779| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|2873| <<vhost_scsi_set_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|2874| <<vhost_scsi_set_endpoint>> vs->vs_tpg = vs_tpg;
+	 *   - drivers/vhost/scsi.c|2920| <<vhost_scsi_clear_endpoint>> if (!vs->vs_tpg) {
+	 *   - drivers/vhost/scsi.c|2927| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|2993| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|3001| <<vhost_scsi_clear_endpoint>> vs->vs_tpg[target] = NULL;
+	 *   - drivers/vhost/scsi.c|3021| <<vhost_scsi_clear_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|3022| <<vhost_scsi_clear_endpoint>> vs->vs_tpg = NULL;
+	 *
+	 * vhost_scsi->vs_tpg被vhost_scsi->dev.mutex保护
+	 */
 	struct vhost_scsi_tpg **vs_tpg;
+	/*
+	 * 在以下使用vhost_scsi->vs_vhost_wwpn[TRANSPORT_IQN_LEN]:
+	 *   - drivers/vhost/scsi.c|519| <<global>> char vs_vhost_wwpn[TRANSPORT_IQN_LEN];
+	 *   - drivers/vhost/scsi.c|3055| <<vhost_scsi_set_endpoint>> memcpy(vs->vs_vhost_wwpn,
+	 *              t->vhost_wwpn, sizeof(vs->vs_vhost_wwpn));
+	 *   - drivers/vhost/scsi.c|3056| <<vhost_scsi_set_endpoint>> memcpy(vs->vs_vhost_wwpn,
+	 *              t->vhost_wwpn, sizeof(vs->vs_vhost_wwpn));
+	 *   - drivers/vhost/scsi.c|3496| <<vhost_scsi_release>> memcpy(t.vhost_wwpn,
+	 *              vs->vs_vhost_wwpn, sizeof(t.vhost_wwpn));
+	 */
 	char vs_vhost_wwpn[TRANSPORT_IQN_LEN];
 
 	struct vhost_dev dev;
 	struct vhost_scsi_virtqueue *vqs;
 	struct vhost_scsi_inflight **old_inflight;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	struct vhost_work vs_event_work; /* evt injection work item */
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|541| <<vhost_scsi_complete_events>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	struct llist_head vs_event_list; /* evt injection queue */
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 */
 	bool vs_events_missed; /* any missed events, protected by vq->mutex */
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	int vs_events_nr; /* num of pending events, protected by vq->mutex */
 };
 
 struct vhost_scsi_tmf {
+	/*
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	struct vhost_work vwork;
+	/*
+	 * 在以下使用vhost_scsi_tmf->flush_work:
+	 *   - drivers/vhost/scsi.c|457| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+	 *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	 */
 	struct work_struct flush_work;
 	struct vhost_scsi *vhost;
 	struct vhost_scsi_virtqueue *svq;
 
 	struct se_cmd se_cmd;
+	/*
+	 * 在以下使用vhost_scsi_tmf->scsi_resp:
+	 *   - drivers/vhost/scsi.c|543| <<vhost_scsi_queue_tm_rsp>> tmf->scsi_resp = se_cmd->se_tmr_req->response;
+	 *   - drivers/vhost/scsi.c|1680| <<vhost_scsi_tmf_resp_work>> if (tmf->scsi_resp == TMR_FUNCTION_COMPLETE)
+	 */
 	u8 scsi_resp;
 	struct vhost_scsi_inflight *inflight;
 	struct iovec resp_iov;
@@ -230,6 +630,15 @@ struct vhost_scsi_ctx {
 	unsigned int out, in;
 	size_t req_size, rsp_size;
 	size_t out_size, in_size;
+	/*
+	 * 在以下使用vhost_scsi_ctx->lunp:
+	 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_get_req>> } else if (unlikely(*vc->lunp != 1)) {
+	 *   - drivers/vhost/scsi.c|1275| <<vhost_scsi_get_req>> vq_err(vq, "Illegal virtio-scsi lun: %u\n", *vc->lunp);
+	 *   - drivers/vhost/scsi.c|1349| <<vhost_scsi_handle_vq>> vc.lunp = &v_req_pi.lun[0];
+	 *   - drivers/vhost/scsi.c|1354| <<vhost_scsi_handle_vq>> vc.lunp = &v_req.lun[0];
+	 *   - drivers/vhost/scsi.c|1778| <<vhost_scsi_ctl_handle_vq>> vc.lunp = &v_req.tmf.lun[0];
+	 *   - drivers/vhost/scsi.c|1786| <<vhost_scsi_ctl_handle_vq>> vc.lunp = &v_req.an.lun[0];
+	 */
 	u8 *target, *lunp;
 	void *req;
 	struct iov_iter out_iter;
@@ -239,9 +648,31 @@ struct vhost_scsi_ctx {
  * Global mutex to protect vhost_scsi TPG list for vhost IOCTLs and LIO
  * configfs management operations.
  */
+/*
+ * 在以下使用vhost_scsi_mutex:
+ *   - drivers/vhost/scsi.c|2840| <<vhost_scsi_set_endpoint>> mutex_lock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|2856| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|2871| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|2881| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|3823| <<vhost_scsi_make_tpg>> mutex_lock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|3825| <<vhost_scsi_make_tpg>> mutex_unlock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|3838| <<vhost_scsi_drop_tpg>> mutex_lock(&vhost_scsi_mutex);
+ *   - drivers/vhost/scsi.c|3840| <<vhost_scsi_drop_tpg>> mutex_unlock(&vhost_scsi_mutex);
+ */
 static DEFINE_MUTEX(vhost_scsi_mutex);
+/*
+ * 在以下使用vhost_scsi_list:
+ *   - drivers/vhost/scsi.c|308| <<global>> static LIST_HEAD(vhost_scsi_list);
+ *   - drivers/vhost/scsi.c|2322| <<vhost_scsi_set_endpoint>> list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
+ *   - drivers/vhost/scsi.c|3220| <<vhost_scsi_make_tpg>> list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
+ */
 static LIST_HEAD(vhost_scsi_list);
 
+/*
+ * 在以下使用vhost_scsi_done_inflight():
+ *   - drivers/vhost/scsi.c|645| <<vhost_scsi_put_inflight>> kref_put(&inflight->kref, vhost_scsi_done_inflight);
+ *   - drivers/vhost/scsi.c|2644| <<vhost_scsi_flush>> kref_put(&vs->old_inflight[i]->kref, vhost_scsi_done_inflight);
+ */
 static void vhost_scsi_done_inflight(struct kref *kref)
 {
 	struct vhost_scsi_inflight *inflight;
@@ -250,6 +681,11 @@ static void vhost_scsi_done_inflight(struct kref *kref)
 	complete(&inflight->comp);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2143| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+ *   - drivers/vhost/scsi.c|2618| <<vhost_scsi_open>> vhost_scsi_init_inflight(vs, NULL);
+ */
 static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 				    struct vhost_scsi_inflight *old_inflight[])
 {
@@ -277,6 +713,13 @@ static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|851| <<vhost_scsi_get_cmd>> cmd->inflight = vhost_scsi_get_inflight(vq);
+ *   - drivers/vhost/scsi.c|1571| <<vhost_scsi_handle_tmf>> tmf->inflight = vhost_scsi_get_inflight(vq);
+ *
+ * 主要调用kref_get(&inflight->kref)
+ */
 static struct vhost_scsi_inflight *
 vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 {
@@ -290,16 +733,28 @@ vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|397| <<vhost_scsi_release_cmd_res>> vhost_scsi_put_inflight(inflight);
+ *   - drivers/vhost/scsi.c|405| <<vhost_scsi_release_tmf_res>> vhost_scsi_put_inflight(inflight);
+ */
 static void vhost_scsi_put_inflight(struct vhost_scsi_inflight *inflight)
 {
 	kref_put(&inflight->kref, vhost_scsi_done_inflight);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode = vhost_scsi_check_true()
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode_cache = vhost_scsi_check_true()
+ */
 static int vhost_scsi_check_true(struct se_portal_group *se_tpg)
 {
 	return 1;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_wwn = vhost_scsi_get_fabric_wwn()
+ */
 static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -309,13 +764,28 @@ static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 	return &tport->tport_name[0];
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_tag = vhost_scsi_get_tpgt()
+ */
 static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
 				struct vhost_scsi_tpg, se_tpg);
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi_tpg:
+	 *   - drivers/vhost/scsi.c|753| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|2614| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|3019| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|3057| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+	 *   - drivers/vhost/scsi.c|3251| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+	 *   - drivers/vhost/scsi.c|4072| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+	 */
 	return tpg->tport_tpgt;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_prot_fabric_only = vhost_scsi_check_prot_fabric_only()
+ */
 static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -324,6 +794,12 @@ static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 	return tpg->tv_fabric_prot_type;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|468| <<vhost_scsi_drop_cmds>> vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
+ *   - drivers/vhost/scsi.c|882| <<vhost_scsi_complete_cmd_work>> vhost_scsi_release_cmd_res(se_cmd);
+ *   - drivers/vhost/scsi.c|1592| <<vhost_scsi_handle_vq>> vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
+ */
 static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *tv_cmd = container_of(se_cmd,
@@ -347,10 +823,25 @@ static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 			put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
 	}
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1524| <<vhost_scsi_tmf_resp_work>> vhost_scsi_release_tmf_res(tmf);
+ *   - drivers/vhost/scsi.c|1538| <<vhost_scsi_tmf_flush_work>> vhost_scsi_release_tmf_res(tmf);
+ *   - drivers/vhost/scsi.c|1577| <<vhost_scsi_handle_tmf>> vhost_scsi_release_tmf_res(tmf);
+ */
 static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 {
 	struct vhost_scsi_inflight *inflight = tmf->inflight;
@@ -359,6 +850,10 @@ static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|503| <<vhost_scsi_release_cmd>> vhost_scsi_drop_cmds(svq);
+ */
 static void vhost_scsi_drop_cmds(struct vhost_scsi_virtqueue *svq)
 {
 	struct vhost_scsi_cmd *cmd, *t;
@@ -369,12 +864,20 @@ static void vhost_scsi_drop_cmds(struct vhost_scsi_virtqueue *svq)
 		vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.release_cmd = vhost_scsi_release_cmd()
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	if (se_cmd->se_cmd_flags & SCF_SCSI_TMR_CDB) {
 		struct vhost_scsi_tmf *tmf = container_of(se_cmd,
 					struct vhost_scsi_tmf, se_cmd);
 
+		/*
+		 * 在以下使用vhost_scsi_tmf->flush_work:
+		 *   - drivers/vhost/scsi.c|457| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+		 *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+		 */
 		schedule_work(&tmf->flush_work);
 	} else {
 		struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -383,11 +886,23 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 					struct vhost_scsi_virtqueue, vq);
 
 		llist_add(&cmd->tvc_completion_list, &svq->completion_list);
+		/*
+		 * 在以下使用vhost_vq_work_queue():
+		 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+		 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+		 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+		 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+		 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+		 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+		 */
 		if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
 			vhost_scsi_drop_cmds(svq);
 	}
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.write_pending = vhost_scsi_write_pending()
+ */
 static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 {
 	/* Go ahead and process the write immediately */
@@ -395,18 +910,27 @@ static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_data_in = vhost_scsi_queue_data_in()
+ */
 static int vhost_scsi_queue_data_in(struct se_cmd *se_cmd)
 {
 	transport_generic_free_cmd(se_cmd, 0);
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_status = vhost_scsi_queue_status()
+ */
 static int vhost_scsi_queue_status(struct se_cmd *se_cmd)
 {
 	transport_generic_free_cmd(se_cmd, 0);
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_tm_rsp = vhost_scsi_queue_tm_rsp()
+ */
 static void vhost_scsi_queue_tm_rsp(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_tmf *tmf = container_of(se_cmd, struct vhost_scsi_tmf,
@@ -416,17 +940,36 @@ static void vhost_scsi_queue_tm_rsp(struct se_cmd *se_cmd)
 	transport_generic_free_cmd(&tmf->se_cmd, 0);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.aborted_task = vhost_scsi_aborted_task()
+ */
 static void vhost_scsi_aborted_task(struct se_cmd *se_cmd)
 {
 	return;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|545| <<vhost_scsi_complete_events>> vhost_scsi_free_evt(vs, evt);
+ */
 static void vhost_scsi_free_evt(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 {
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	vs->vs_events_nr--;
 	kfree(evt);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1719| <<vhost_scsi_send_evt>> evt = vhost_scsi_allocate_evt(vs, event, reason);
+ */
 static struct vhost_scsi_evt *
 vhost_scsi_allocate_evt(struct vhost_scsi *vs,
 		       u32 event, u32 reason)
@@ -434,6 +977,28 @@ vhost_scsi_allocate_evt(struct vhost_scsi *vs,
 	struct vhost_virtqueue *vq = &vs->vqs[VHOST_SCSI_VQ_EVT].vq;
 	struct vhost_scsi_evt *evt;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 *
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 */
 	if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
 		vs->vs_events_missed = true;
 		return NULL;
@@ -448,16 +1013,38 @@ vhost_scsi_allocate_evt(struct vhost_scsi *vs,
 
 	evt->event.event = cpu_to_vhost32(vq, event);
 	evt->event.reason = cpu_to_vhost32(vq, reason);
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	vs->vs_events_nr++;
 
 	return evt;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.check_stop_free = vhost_scsi_check_stop_free()
+ */
 static int vhost_scsi_check_stop_free(struct se_cmd *se_cmd)
 {
 	return target_put_sess_cmd(se_cmd);
 }
 
+/*
+ * vhost_scsi_evt_work() or vhost_scsi_send_evt()
+ * -> vhost_scsi_complete_events()
+ *    -> vhost_scsi_do_evt_work()
+ *       -> vhost_get_vq_desc()
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|521| <<vhost_scsi_complete_events>> vhost_scsi_do_evt_work(vs, evt);
+ *
+ * vq->mutex被拿着
+ */
 static void
 vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 {
@@ -467,6 +1054,21 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 	unsigned out, in;
 	int head, ret;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 */
 	if (!vhost_vq_get_backend(vq)) {
 		vs->vs_events_missed = true;
 		return;
@@ -474,6 +1076,25 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 
 again:
 	vhost_disable_notify(&vs->dev, vq);
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+	 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	head = vhost_get_vq_desc(vq, vq->iov,
 			ARRAY_SIZE(vq->iov), &out, &in,
 			NULL, NULL);
@@ -488,6 +1109,15 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 		return;
 	}
 
+	/*
+	 * struct virtio_scsi_event {
+	 *     __virtio32 event;
+	 *     __u8 lun[8];
+	 *     __virtio32 reason;
+	 * } __attribute__((packed));
+	 *
+	 * 对于event, 就一个desc, 不会是两个!
+	 */
 	if ((vq->iov[out].iov_len != sizeof(struct virtio_scsi_event))) {
 		vq_err(vq, "Expecting virtio_scsi_event, got %zu bytes\n",
 				vq->iov[out].iov_len);
@@ -500,14 +1130,25 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 		vs->vs_events_missed = false;
 	}
 
+	/*
+	 * struct virtio_scsi_event __user *eventp;
+	 */
 	eventp = vq->iov[out].iov_base;
 	ret = __copy_to_user(eventp, event, sizeof(*event));
+	/*
+	 * 这里被vq保护着, 可以log!
+	 */
 	if (!ret)
 		vhost_add_used_and_signal(&vs->dev, vq, head, 0);
 	else
 		vq_err(vq, "Faulted on vhost_scsi_send_event\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|554| <<vhost_scsi_evt_work>> vhost_scsi_complete_events(vs, false);
+ *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_send_evt>> vhost_scsi_complete_events(vs, true);
+ */
 static void vhost_scsi_complete_events(struct vhost_scsi *vs, bool drop)
 {
 	struct vhost_virtqueue *vq = &vs->vqs[VHOST_SCSI_VQ_EVT].vq;
@@ -515,25 +1156,64 @@ static void vhost_scsi_complete_events(struct vhost_scsi *vs, bool drop)
 	struct llist_node *llnode;
 
 	mutex_lock(&vq->mutex);
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|541| <<vhost_scsi_complete_events>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	llnode = llist_del_all(&vs->vs_event_list);
 	llist_for_each_entry_safe(evt, t, llnode, list) {
 		if (!drop)
 			vhost_scsi_do_evt_work(vs, evt);
+		/*
+		 * 只在此处调用
+		 */
 		vhost_scsi_free_evt(vs, evt);
 	}
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi->vs_event_work:
+ *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+ *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ */
 static void vhost_scsi_evt_work(struct vhost_work *work)
 {
 	struct vhost_scsi *vs = container_of(work, struct vhost_scsi,
 					     vs_event_work);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|554| <<vhost_scsi_evt_work>> vhost_scsi_complete_events(vs, false);
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_send_evt>> vhost_scsi_complete_events(vs, true);
+	 */
 	vhost_scsi_complete_events(vs, false);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|821| <<vhost_scsi_complete_cmd_work>> if (cmd->saved_iter_addr && vhost_scsi_copy_sgl_to_iov(cmd)) {
+ */
 static int vhost_scsi_copy_sgl_to_iov(struct vhost_scsi_cmd *cmd)
 {
 	struct iov_iter *iter = &cmd->saved_iter;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_release_cmd_res>> __free_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|576| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|950| <<vhost_scsi_copy_sgl_to_iov>> struct scatterlist *sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1086| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|1400| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1403| <<vhost_scsi_mapal>> pr_debug( ... cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1405| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1408| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1412| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1448| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|2490| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|2541| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|2544| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	struct scatterlist *sg = cmd->tvc_sgl;
 	struct page *page;
 	size_t len;
@@ -558,6 +1238,10 @@ static int vhost_scsi_copy_sgl_to_iov(struct vhost_scsi_cmd *cmd)
  * This is scheduled in the vhost work queue so we are called with the owner
  * process mm and can access the vring.
  */
+/*
+ * 在以下使用vhost_scsi_complete_cmd_work():
+ *   - vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+ */
 static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(work,
@@ -578,9 +1262,16 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 			cmd, se_cmd->residual_count, se_cmd->scsi_status);
 		memset(&v_rsp, 0, sizeof(v_rsp));
 
+		/*
+		 * 只在此处调用vhost_scsi_copy_sgl_to_iov()
+		 */
 		if (cmd->saved_iter_addr && vhost_scsi_copy_sgl_to_iov(cmd)) {
 			v_rsp.response = VIRTIO_SCSI_S_BAD_TARGET;
 		} else {
+			/*
+			 * 就算是这里也看不出是好是坏!!!
+			 * 谁会看status???
+			 */
 			v_rsp.resid = cpu_to_vhost32(cmd->tvc_vq,
 						     se_cmd->residual_count);
 			/* TODO is status_qualifier field needed? */
@@ -591,6 +1282,18 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 			       se_cmd->scsi_sense_length);
 		}
 
+		/*
+		 * 在以下使用vhost_scso_cmd->tvc_resp_iov:
+		 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+		 *                   cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+		 *   - drivers/vhost/scsi.c|1364| <<vhost_scsi_get_cmd>> tvc_resp_iov = cmd->tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|1377| <<vhost_scsi_get_cmd>> cmd->tvc_resp_iov = tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|2141| <<vhost_scsi_handle_vq>> cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
+		 *   - drivers/vhost/scsi.c|2805| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_resp_iov);
+		 *   - drivers/vhost/scsi.c|2898| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
+		 *                   sizeof(struct iovec), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2901| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_resp_iov) {
+		 */
 		iov_iter_init(&iov_iter, ITER_DEST, cmd->tvc_resp_iov,
 			      cmd->tvc_in_iovs, sizeof(v_rsp));
 		ret = copy_to_iter(&v_rsp, sizeof(v_rsp), &iov_iter);
@@ -604,10 +1307,22 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_scsi_release_cmd_res(se_cmd);
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	if (signal)
 		vhost_signal(&svq->vs->dev, &svq->vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1496| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -628,16 +1343,60 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		return ERR_PTR(-EIO);
 	}
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	tag = sbitmap_get(&svq->scsi_tags);
 	if (tag < 0) {
 		pr_err("Unable to obtain tag for vhost_scsi_cmd\n");
 		return ERR_PTR(-ENOMEM);
 	}
 
+	/*
+	 * 在以下设置vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	cmd = &svq->scsi_cmds[tag];
 	sg = cmd->tvc_sgl;
 	prot_sg = cmd->tvc_prot_sgl;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_upages:
+	 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+	 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+	 *                                          sizeof(struct page *), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+	 */
 	pages = cmd->tvc_upages;
+	/*
+	 * 在以下使用vhost_scso_cmd->tvc_resp_iov:
+	 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+	 *                   cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+	 *   - drivers/vhost/scsi.c|1364| <<vhost_scsi_get_cmd>> tvc_resp_iov = cmd->tvc_resp_iov;
+	 *   - drivers/vhost/scsi.c|1377| <<vhost_scsi_get_cmd>> cmd->tvc_resp_iov = tvc_resp_iov;
+	 *   - drivers/vhost/scsi.c|2141| <<vhost_scsi_handle_vq>> cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
+	 *   - drivers/vhost/scsi.c|2805| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_resp_iov);
+	 *   - drivers/vhost/scsi.c|2898| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
+	 *                   sizeof(struct iovec), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2901| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_resp_iov) {
+	 */
 	tvc_resp_iov = cmd->tvc_resp_iov;
 	memset(cmd, 0, sizeof(*cmd));
 	cmd->tvc_sgl = sg;
@@ -663,18 +1422,59 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
  *
  * Returns the number of scatterlist entries used or -errno on error.
  */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1231| <<vhost_scsi_map_iov_to_sgl>> ret = vhost_scsi_map_to_sgl(cmd, iter, sg, is_prot);
+ *
+ * 猜测, 似乎是把iter所对应的page地址全部存储到cmd->tvc_upages.
+ */
 static int
 vhost_scsi_map_to_sgl(struct vhost_scsi_cmd *cmd,
 		      struct iov_iter *iter,
 		      struct scatterlist *sgl,
 		      bool is_prot)
 {
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_upages:
+	 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+	 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+	 *                                          sizeof(struct page *), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+	 */
 	struct page **pages = cmd->tvc_upages;
 	struct scatterlist *sg = sgl;
 	ssize_t bytes, mapped_bytes;
 	size_t offset, mapped_offset;
 	unsigned int npages = 0;
 
+	/*
+	 * 在以下调用iov_iter_get_pages2():
+	 *   - drivers/block/ublk_drv.c|879| <<ublk_copy_user_pages>> len = iov_iter_get_pages2(uiter,
+	 *             iter.pages, iov_iter_count(uiter), UBLK_MAX_PIN_PAGES, &off);
+	 *   - drivers/vhost/scsi.c|1135| <<vhost_scsi_map_to_sgl>> bytes = iov_iter_get_pages2(iter,
+	 *             pages, LONG_MAX, VHOST_SCSI_PREALLOC_UPAGES, &offset);
+	 *   - fs/ceph/file.c|100| <<__iter_get_bvecs>> bytes = iov_iter_get_pages2(iter, pages,
+	 *             maxsize - size, ITER_GET_BVECS_PAGES, &start);
+	 *   - fs/fuse/dev.c|786| <<fuse_copy_fill>> err = iov_iter_get_pages2(cs->iter, &page,
+	 *             PAGE_SIZE, 1, &off);
+	 *   - fs/splice.c|1465| <<iter_to_pipe>> left = iov_iter_get_pages2(from, pages, ~0UL, 16, &start);
+	 *   - net/ceph/messenger.c|992| <<ceph_msg_data_iter_next>> len = iov_iter_get_pages2(&cursor->iov_iter,
+	 *             &page, PAGE_SIZE, 1, page_offset);
+	 *   - net/core/datagram.c|642| <<zerocopy_fill_skb_from_iter>> copied = iov_iter_get_pages2(from, pages,
+	 *             length, MAX_SKB_FRAGS - frag, &start);
+	 *   - net/core/skmsg.c|328| <<sk_msg_zerocopy_from_iter>> copied = iov_iter_get_pages2(from, pages,
+	 *             bytes, maxpages, &offset);
+	 *   - net/rds/message.c|393| <<rds_message_zcopy_from_user>> copied = iov_iter_get_pages2(from, &pages,
+	 *             PAGE_SIZE, 1, &start);
+	 *   - net/tls/tls_sw.c|1382| <<tls_setup_from_iter>> copied = iov_iter_get_pages2(from, pages, length,
+	 *             maxpages, &offset);
+	 *
+	 * 目前猜测, 把iter的页面的地址全部存到pages,
+	 * 也就是cmd->tvc_upages
+	 */
 	bytes = iov_iter_get_pages2(iter, pages, LONG_MAX,
 				VHOST_SCSI_PREALLOC_UPAGES, &offset);
 	/* No pages were pinned */
@@ -730,6 +1530,11 @@ vhost_scsi_map_to_sgl(struct vhost_scsi_cmd *cmd,
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1300| <<vhost_scsi_mapal>> sgl_count = vhost_scsi_calc_sgls(prot_iter, prot_bytes, VHOST_SCSI_PREALLOC_PROT_SGLS);
+ *   - drivers/vhost/scsi.c|1318| <<vhost_scsi_mapal>> sgl_count = vhost_scsi_calc_sgls(data_iter, data_bytes, VHOST_SCSI_PREALLOC_SGLS);
+ */
 static int
 vhost_scsi_calc_sgls(struct iov_iter *iter, size_t bytes, int max_sgls)
 {
@@ -750,6 +1555,10 @@ vhost_scsi_calc_sgls(struct iov_iter *iter, size_t bytes, int max_sgls)
 	return sgl_count;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1294| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl, cmd->tvc_sgl_count);
+ */
 static int
 vhost_scsi_copy_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 			   struct scatterlist *sg, int sg_count)
@@ -796,6 +1605,13 @@ vhost_scsi_copy_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1310| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd,
+ *             prot_iter, cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count, true);
+ *   - drivers/vhost/scsi.c|1328| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd,
+ *             data_iter, cmd->tvc_sgl, cmd->tvc_sgl_count, false);
+ */
 static int
 vhost_scsi_map_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 			  struct scatterlist *sg, int sg_count, bool is_prot)
@@ -805,6 +1621,9 @@ vhost_scsi_map_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 	int ret;
 
 	while (iov_iter_count(iter)) {
+		/*
+		 * 只在此处调用vhost_scsi_map_to_sgl()
+		 */
 		ret = vhost_scsi_map_to_sgl(cmd, iter, sg, is_prot);
 		if (ret < 0) {
 			revert_bytes = 0;
@@ -828,6 +1647,11 @@ vhost_scsi_map_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1771| <<vhost_scsi_handle_vq>> if (unlikely(vhost_scsi_mapal(cmd,
+ *                     prot_bytes, &prot_iter, exp_data_len, &data_iter))) {
+ */
 static int
 vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 		 size_t prot_bytes, struct iov_iter *prot_iter,
@@ -841,6 +1665,21 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 		if (sgl_count < 0)
 			return sgl_count;
 
+		/*
+		 * 在以下使用vhost_scsi_cmd->tvc_prot_sgl:
+		 *   - drivers/vhost/scsi.c|567| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
+		 *   - drivers/vhost/scsi.c|1072| <<vhost_scsi_get_cmd>> prot_sg = cmd->tvc_prot_sgl;
+		 *   - drivers/vhost/scsi.c|1087| <<vhost_scsi_get_cmd>> cmd->tvc_prot_sgl = prot_sg;
+		 *   - drivers/vhost/scsi.c|1352| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_prot_sgl, sgl_count);
+		 *   - drivers/vhost/scsi.c|1355| <<vhost_scsi_mapal>> pr_debug(... cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count);
+		 *   - drivers/vhost/scsi.c|1358| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, prot_iter,
+		 *             cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count, true);
+		 *   - drivers/vhost/scsi.c|1421| <<vhost_scsi_target_queue_cmd>> sg_prot_ptr = cmd->tvc_prot_sgl;
+		 *   - drivers/vhost/scsi.c|2461| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_prot_sgl);
+		 *   - drivers/vhost/scsi.c|2545| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_prot_sgl = kcalloc(VHOST_SCSI_PREALLOC_PROT_SGLS,
+		 *             sizeof(struct scatterlist), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2548| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_prot_sgl) {
+		 */
 		sg_init_table(cmd->tvc_prot_sgl, sgl_count);
 		cmd->tvc_prot_sgl_count = sgl_count;
 		pr_debug("%s prot_sg %p prot_sgl_count %u\n", __func__,
@@ -859,6 +1698,23 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 	if (sgl_count < 0)
 		return sgl_count;
 
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_release_cmd_res>> __free_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|576| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|950| <<vhost_scsi_copy_sgl_to_iov>> struct scatterlist *sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1086| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|1400| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1403| <<vhost_scsi_mapal>> pr_debug( ... cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1405| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1408| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1412| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1448| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|2490| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|2541| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|2544| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	sg_init_table(cmd->tvc_sgl, sgl_count);
 	cmd->tvc_sgl_count = sgl_count;
 	pr_debug("%s data_sg %p data_sgl_count %u\n", __func__,
@@ -868,6 +1724,9 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 					cmd->tvc_sgl_count, false);
 	if (ret == -EINVAL) {
 		sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+		/*
+		 * 只在此处调用
+		 */
 		ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
 						 cmd->tvc_sgl_count);
 	}
@@ -929,6 +1788,29 @@ static void vhost_scsi_target_queue_cmd(struct vhost_scsi_cmd *cmd)
 	target_submit(se_cmd);
 }
 
+/*
+ * struct virtio_scsi_ctrl_tmf_resp {
+ *     __u8 response;
+ * } __attribute__((packed));
+ * 
+ * struct virtio_scsi_ctrl_an_resp {
+ *     __virtio32 event_actual;
+ *     __u8 response;
+ * } __attribute__((packed));
+ *
+ * struct virtio_scsi_cmd_resp {
+ *     __virtio32 sense_len;           // Sense data length
+ *     __virtio32 resid;               // Residual bytes in data buffer
+ *     __virtio16 status_qualifier;    // Status qualifier
+ *     __u8 status;            // Command completion status
+ *     __u8 response;          // Response values
+ *     __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+ * } __attribute__((packed));
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|1510| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ */
 static void
 vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 			   struct vhost_virtqueue *vq,
@@ -940,6 +1822,9 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 
 	memset(&rsp, 0, sizeof(rsp));
 	rsp.response = VIRTIO_SCSI_S_BAD_TARGET;
+	/*
+	 * 怎么可以做这种假设?????!!!!
+	 */
 	resp = vq->iov[out].iov_base;
 	ret = __copy_to_user(resp, &rsp, sizeof(rsp));
 	if (!ret)
@@ -948,12 +1833,36 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1093| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+	 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -998,6 +1907,11 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1432| <<vhost_scsi_handle_vq>> ret = vhost_scsi_chk_size(vq, &vc);
+ *   - drivers/vhost/scsi.c|1937| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_chk_size(vq, &vc);
+ */
 static int
 vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 {
@@ -1016,6 +1930,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1367| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1809| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
@@ -1054,6 +1973,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1826| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1072,6 +1995,26 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	void *cdb;
 
 	mutex_lock(&vq->mutex);
+	/*
+	 * 在以下使用vhost_scsi->vs_tpg:
+	 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+	 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_handle_vq>> if (!vs_tpg)
+	 *   - drivers/vhost/scsi.c|2762| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg)
+	 *   - drivers/vhost/scsi.c|2763| <<vhost_scsi_set_endpoint>> memcpy(vs_tpg, vs->vs_tpg, len);
+	 *   - drivers/vhost/scsi.c|2779| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|2873| <<vhost_scsi_set_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|2874| <<vhost_scsi_set_endpoint>> vs->vs_tpg = vs_tpg;
+	 *   - drivers/vhost/scsi.c|2920| <<vhost_scsi_clear_endpoint>> if (!vs->vs_tpg) {
+	 *   - drivers/vhost/scsi.c|2927| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|2993| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|3001| <<vhost_scsi_clear_endpoint>> vs->vs_tpg[target] = NULL;
+	 *   - drivers/vhost/scsi.c|3021| <<vhost_scsi_clear_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|3022| <<vhost_scsi_clear_endpoint>> vs->vs_tpg = NULL;
+	 *
+	 * vhost_scsi_handle_vq()中的使用被vq->mutex保护
+	 *
+	 * vhost_scsi->vs_tpg被vhost_scsi->dev.mutex保护
+	 */
 	/*
 	 * We can handle the vq only after the endpoint is setup by calling the
 	 * VHOST_SCSI_SET_ENDPOINT ioctl.
@@ -1086,6 +2029,11 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	do {
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1093| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
@@ -1212,6 +2160,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 				scsi_command_size(cdb), VHOST_SCSI_MAX_CDB_SIZE);
 				goto err;
 		}
+		/*
+		 * 只在此处调用
+		 */
 		cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
 					 exp_data_len + prot_bytes,
 					 data_direction);
@@ -1222,6 +2173,18 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		}
 		cmd->tvc_vhost = vs;
 		cmd->tvc_vq = vq;
+		/*
+		 * 在以下使用vhost_scso_cmd->tvc_resp_iov:
+		 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+		 *                   cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+		 *   - drivers/vhost/scsi.c|1364| <<vhost_scsi_get_cmd>> tvc_resp_iov = cmd->tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|1377| <<vhost_scsi_get_cmd>> cmd->tvc_resp_iov = tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|2141| <<vhost_scsi_handle_vq>> cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
+		 *   - drivers/vhost/scsi.c|2805| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_resp_iov);
+		 *   - drivers/vhost/scsi.c|2898| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
+		 *                   sizeof(struct iovec), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2901| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_resp_iov) {
+		 */
 		for (i = 0; i < vc.in ; i++)
 			cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
 		cmd->tvc_in_iovs = vc.in;
@@ -1232,6 +2195,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			 " %d\n", cmd, exp_data_len, prot_bytes, data_direction);
 
 		if (data_direction != DMA_NONE) {
+			/*
+			 * 只在此处调用vhost_scsi_mapal()
+			 */
 			if (unlikely(vhost_scsi_mapal(cmd, prot_bytes,
 						      &prot_iter, exp_data_len,
 						      &data_iter))) {
@@ -1259,16 +2225,35 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			break;
 		else if (ret == -EIO)
 			vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		/*
+		 * 调用vhost_scsi_send_bad_target()的地方:
+		 *   - drivers/vhost/scsi.c|1510| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 */
 	} while (likely(!vhost_exceeds_weight(vq, ++c, 0)));
 out:
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_tmf_resp_work>> vhost_scsi_send_tmf_resp(tmf->vhost,
+ *                   &tmf->svq->vq, tmf->in_iovs, tmf->vq_desc, &tmf->resp_iov, resp_code);
+ *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_handle_tmf>> vhost_scsi_send_tmf_resp(vs, vq,
+ *                   vc->in, vc->head, &vq->iov[vc->out], VIRTIO_SCSI_S_FUNCTION_REJECTED);
+ */
 static void
 vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 			 int in_iovs, int vq_desc, struct iovec *resp_iov,
 			 int tmf_resp_code)
 {
+	/*
+	 * 大小一定是1个byte, 怎么都不会垮两个desc了!
+	 *
+	 * struct virtio_scsi_ctrl_tmf_resp {
+	 *     __u8 response;
+	 * } __attribute__((packed));
+	 */
 	struct virtio_scsi_ctrl_tmf_resp rsp;
 	struct iov_iter iov_iter;
 	int ret;
@@ -1279,6 +2264,9 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 
 	iov_iter_init(&iov_iter, ITER_DEST, resp_iov, in_iovs, sizeof(rsp));
 
+	/*
+	 * copy, 可以是多个desc
+	 */
 	ret = copy_to_iter(&rsp, sizeof(rsp), &iov_iter);
 	if (likely(ret == sizeof(rsp)))
 		vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
@@ -1286,6 +2274,16 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		pr_err("Faulted on virtio_scsi_ctrl_tmf_resp\n");
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf->vwork:
+ *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+ *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *
+ * 在以下使用vhost_scsi_tmf_resp_work():
+ *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *
+ * 一个work对应一个tmf cmd
+ */
 static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
@@ -1297,25 +2295,71 @@ static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 	else
 		resp_code = VIRTIO_SCSI_S_FUNCTION_REJECTED;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_tmf_resp_work>> vhost_scsi_send_tmf_resp(tmf->vhost,
+	 *                   &tmf->svq->vq, tmf->in_iovs, tmf->vq_desc, &tmf->resp_iov, resp_code);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_handle_tmf>> vhost_scsi_send_tmf_resp(vs, vq,
+	 *                   vc->in, vc->head, &vq->iov[vc->out], VIRTIO_SCSI_S_FUNCTION_REJECTED);
+	 */
 	vhost_scsi_send_tmf_resp(tmf->vhost, &tmf->svq->vq, tmf->in_iovs,
 				 tmf->vq_desc, &tmf->resp_iov, resp_code);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|1524| <<vhost_scsi_tmf_resp_work>> vhost_scsi_release_tmf_res(tmf);
+	 *   - drivers/vhost/scsi.c|1538| <<vhost_scsi_tmf_flush_work>> vhost_scsi_release_tmf_res(tmf);
+	 *   - drivers/vhost/scsi.c|1577| <<vhost_scsi_handle_tmf>> vhost_scsi_release_tmf_res(tmf);
+	 */
 	vhost_scsi_release_tmf_res(tmf);
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf_flush_work():
+ *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+ */
 static void vhost_scsi_tmf_flush_work(struct work_struct *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
 						 flush_work);
 	struct vhost_virtqueue *vq = &tmf->svq->vq;
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1432| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/net.c|1623| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/scsi.c|1639| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+	 *   - drivers/vhost/scsi.c|2044| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+	 *   - drivers/vhost/test.c|165| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/vhost.c|1050| <<vhost_dev_stop>> vhost_dev_flush(dev);
+	 *   - drivers/vhost/vhost.c|2147| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+	 *   - drivers/vhost/vhost.h|61| <<vhost_vring_ioctl>> void vhost_dev_flush(struct vhost_dev *dev);
+	 *   - drivers/vhost/vsock.c|751| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+	 */
 	/*
 	 * Make sure we have sent responses for other commands before we
 	 * send our response.
 	 */
 	vhost_dev_flush(vq->dev);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	if (!vhost_vq_work_queue(vq, &tmf->vwork))
 		vhost_scsi_release_tmf_res(tmf);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1700| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_handle_tmf(vs, tpg, vq, &v_req.tmf, &vc);
+ */
 static void
 vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 		      struct vhost_virtqueue *vq,
@@ -1335,19 +2379,57 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 		goto send_reject;
 	}
 
+	/*
+	 * struct virtio_scsi_ctrl_tmf_req {
+	 *     __virtio32 type;
+	 *     __virtio32 subtype;
+	 *     __u8 lun[8];
+	 *     __virtio64 tag;
+	 * } __attribute__((packed));
+	 */
 	tmf = kzalloc(sizeof(*tmf), GFP_KERNEL);
 	if (!tmf)
 		goto send_reject;
 
+	/*
+	 * 在以下使用vhost_scsi_tmf->flush_work:
+	 *   - drivers/vhost/scsi.c|457| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+	 *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	 */
 	INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	/*
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
 	tmf->vhost = vs;
 	tmf->svq = svq;
+	/*
+	 * 大小一定是1个byte, 怎么都不会垮两个desc了!
+	 *
+	 * struct virtio_scsi_ctrl_tmf_resp { 
+	 *     __u8 response;
+	 * } __attribute__((packed));
+	 *
+	 * 这里假设一定是一个????
+	 */
 	tmf->resp_iov = vq->iov[vc->out];
 	tmf->vq_desc = vc->head;
 	tmf->in_iovs = vc->in;
 	tmf->inflight = vhost_scsi_get_inflight(vq);
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1667| <<srpt_handle_tsk_mgmt>> rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1449| <<efct_scsi_recv_tmf>> rc = target_submit_tmr(&ocp->cmd, se_sess, NULL, lun, ocp, tmr_func,
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2799| <<ibmvscsis_parse_task>> rc = target_submit_tmr(&cmd->se_cmd, nexus->se_sess, NULL,
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|617| <<tcm_qla2xxx_handle_tmr>> return target_submit_tmr(se_cmd, sess->se_sess, NULL, lun, mcmd,
+	 *   - drivers/target/loopback/tcm_loop.c|216| <<tcm_loop_issue_tmr>> rc = target_submit_tmr(se_cmd, se_sess, tl_cmd->tl_sense_buf, lun,
+	 *   - drivers/target/tcm_fc/tfc_cmd.c|365| <<ft_send_tm>> rc = target_submit_tmr(&cmd->se_cmd, cmd->sess->se_sess,
+	 *   - drivers/vhost/scsi.c|1701| <<vhost_scsi_handle_tmf>> if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
+	 *   - drivers/xen/xen-scsiback.c|626| <<scsiback_device_action>> rc = target_submit_tmr(&pending_req->se_cmd, nexus->tvn_se_sess,
+	 */
 	if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
 			      vhost_buf_to_lun(vtmf->lun), NULL,
 			      TMR_LUN_RESET, GFP_KERNEL, 0,
@@ -1359,10 +2441,21 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 	return;
 
 send_reject:
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_tmf_resp_work>> vhost_scsi_send_tmf_resp(tmf->vhost,
+	 *                   &tmf->svq->vq, tmf->in_iovs, tmf->vq_desc, &tmf->resp_iov, resp_code);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_handle_tmf>> vhost_scsi_send_tmf_resp(vs, vq,
+	 *                   vc->in, vc->head, &vq->iov[vc->out], VIRTIO_SCSI_S_FUNCTION_REJECTED);
+	 */
 	vhost_scsi_send_tmf_resp(vs, vq, vc->in, vc->head, &vq->iov[vc->out],
 				 VIRTIO_SCSI_S_FUNCTION_REJECTED);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1702| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_an_resp(vs, vq, &vc);
+ */
 static void
 vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 			struct vhost_virtqueue *vq,
@@ -1378,6 +2471,16 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 
 	iov_iter_init(&iov_iter, ITER_DEST, &vq->iov[vc->out], vc->in, sizeof(rsp));
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|969| <<handle_tx_zerocopy>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+	 *   - drivers/vhost/scsi.c|529| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+	 *   - drivers/vhost/scsi.c|969| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+	 *   - drivers/vhost/scsi.c|1331| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+	 *   - drivers/vhost/scsi.c|1430| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+	 *   - drivers/vhost/test.c|87| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+	 */
+
 	ret = copy_to_iter(&rsp, sizeof(rsp), &iov_iter);
 	if (likely(ret == sizeof(rsp)))
 		vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
@@ -1385,6 +2488,10 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_ctrl_an_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1726| <<vhost_scsi_ctl_handle_kick>> vhost_scsi_ctl_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1411,10 +2518,28 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	do {
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1093| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *
+		 * 可以假设log只有一个吗???
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
 
+		/*
+		 * struct vhost_scsi_ctx {
+		 *     int head;
+		 *     unsigned int out, in;
+		 *     size_t req_size, rsp_size;
+		 *     size_t out_size, in_size;
+		 *     u8 *target, *lunp;
+		 *     void *req;
+		 *     struct iov_iter out_iter;
+		 * };
+		 */
 		/*
 		 * Get the request type first in order to setup
 		 * other parameters dependent on the type.
@@ -1470,10 +2595,23 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		vc.req += typ_size;
 		vc.req_size -= typ_size;
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1367| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+		 *   - drivers/vhost/scsi.c|1809| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+		 *
+		 * 有ret err立刻跳下去!!!
+		 */
 		ret = vhost_scsi_get_req(vq, &vc, &tpg);
 		if (ret)
 			goto err;
 
+		/*
+		 * TMF
+		 * - virtio_scsi_ctrl_tmf_resp
+		 * AN
+		 * - virtio_scsi_ctrl_an_resp
+		 */
 		if (v_req.type == VIRTIO_SCSI_T_TMF)
 			vhost_scsi_handle_tmf(vs, tpg, vq, &v_req.tmf, &vc);
 		else
@@ -1489,11 +2627,20 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			break;
 		else if (ret == -EIO)
 			vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		/*
+		 * 调用vhost_scsi_send_bad_target()的地方:
+		 *   - drivers/vhost/scsi.c|1510| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 */
 	} while (likely(!vhost_exceeds_weight(vq, ++c, 0)));
 out:
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_ctl_handle_kick():
+ *   - drivers/vhost/scsi.c|2271| <<vhost_scsi_open>> vs->vqs[VHOST_SCSI_VQ_CTL].vq.handle_kick = vhost_scsi_ctl_handle_kick;
+ */
 static void vhost_scsi_ctl_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1504,6 +2651,11 @@ static void vhost_scsi_ctl_handle_kick(struct vhost_work *work)
 	vhost_scsi_ctl_handle_vq(vs, vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1594| <<vhost_scsi_evt_handle_kick>> vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
+ *   - drivers/vhost/scsi.c|2222| <<vhost_scsi_do_plug>> vhost_scsi_send_evt(vs, vq, tpg, lun, VIRTIO_SCSI_T_TRANSPORT_RESET, reason);
+ */
 static void
 vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_tpg *tpg, struct se_lun *lun,
@@ -1511,6 +2663,9 @@ vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 {
 	struct vhost_scsi_evt *evt;
 
+	/*
+	 * 只在此处调用
+	 */
 	evt = vhost_scsi_allocate_evt(vs, event, reason);
 	if (!evt)
 		return;
@@ -1522,17 +2677,52 @@ vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		 * lun[4-7] need to be zero according to virtio-scsi spec.
 		 */
 		evt->event.lun[0] = 0x01;
+		/*
+		 * 在以下使用vhost_scsi_tpg->vhost_scsi_tpg:
+		 *   - drivers/vhost/scsi.c|753| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+		 *   - drivers/vhost/scsi.c|2614| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+		 *   - drivers/vhost/scsi.c|3019| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+		 *   - drivers/vhost/scsi.c|3057| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+		 *   - drivers/vhost/scsi.c|3251| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+		 *   - drivers/vhost/scsi.c|4072| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+		 */
 		evt->event.lun[1] = tpg->tport_tpgt;
 		if (lun->unpacked_lun >= 256)
 			evt->event.lun[2] = lun->unpacked_lun >> 8 | 0x40 ;
 		evt->event.lun[3] = lun->unpacked_lun & 0xFF;
 	}
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|541| <<vhost_scsi_complete_events>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	llist_add(&evt->list, &vs->vs_event_list);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *
+	 * 在以下调用vhost_scsi_complete_events():
+	 *   - drivers/vhost/scsi.c|554| <<vhost_scsi_evt_work>> vhost_scsi_complete_events(vs, false);
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_send_evt>> vhost_scsi_complete_events(vs, true);
+	 *
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
 		vhost_scsi_complete_events(vs, true);
 }
 
+/*
+ * 在以下使用vhost_scsi_evt_handle_kick():
+ *   - drivers/vhost/scsi.c|2136| <<vhost_scsi_open>> vs->vqs[VHOST_SCSI_VQ_EVT].vq.handle_kick = vhost_scsi_evt_handle_kick;
+ */
 static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1543,6 +2733,25 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	if (!vhost_vq_get_backend(vq))
 		goto out;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 *
+	 * 在以下使用vhost_scsi_send_evt():
+	 *   - drivers/vhost/scsi.c|1594| <<vhost_scsi_evt_handle_kick>> vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
+	 *   - drivers/vhost/scsi.c|2222| <<vhost_scsi_do_plug>> vhost_scsi_send_evt(vs, vq, tpg, lun,
+	 */
 	if (vs->vs_events_missed)
 		vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
 				    0);
@@ -1559,11 +2768,22 @@ static void vhost_scsi_handle_kick(struct vhost_work *work)
 	vhost_scsi_handle_vq(vs, vq);
 }
 
+/*
+ * caled by:
+ *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+ */
 /* Callers must hold dev mutex */
 static void vhost_scsi_flush(struct vhost_scsi *vs)
 {
 	int i;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2143| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+	 *   - drivers/vhost/scsi.c|2618| <<vhost_scsi_open>> vhost_scsi_init_inflight(vs, NULL);
+	 */
 	/* Init new inflight and remember the old inflight */
 	vhost_scsi_init_inflight(vs, vs->old_inflight);
 
@@ -1575,6 +2795,18 @@ static void vhost_scsi_flush(struct vhost_scsi *vs)
 	for (i = 0; i < vs->dev.nvqs; i++)
 		kref_put(&vs->old_inflight[i]->kref, vhost_scsi_done_inflight);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1432| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/net.c|1623| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/scsi.c|1639| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+	 *   - drivers/vhost/scsi.c|2044| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+	 *   - drivers/vhost/test.c|165| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/vhost.c|1050| <<vhost_dev_stop>> vhost_dev_flush(dev);
+	 *   - drivers/vhost/vhost.c|2147| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+	 *   - drivers/vhost/vhost.h|61| <<vhost_vring_ioctl>> void vhost_dev_flush(struct vhost_dev *dev);
+	 *   - drivers/vhost/vsock.c|751| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+	 */
 	/* Flush both the vhost poll and vhost work */
 	vhost_dev_flush(&vs->dev);
 
@@ -1583,6 +2815,12 @@ static void vhost_scsi_flush(struct vhost_scsi *vs)
 		wait_for_completion(&vs->old_inflight[i]->comp);
 }
 
+/*
+ * 在以下调用vhost_scsi_destroy_vq_cmds():
+ *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+ *   - drivers/vhost/scsi.c|2474| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+ *   - drivers/vhost/scsi.c|2562| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+ */
 static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1590,6 +2828,19 @@ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 	struct vhost_scsi_cmd *tv_cmd;
 	unsigned int i;
 
+	/*
+	 * 在以下设置vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	if (!svq->scsi_cmds)
 		return;
 
@@ -1599,14 +2850,39 @@ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 		kfree(tv_cmd->tvc_sgl);
 		kfree(tv_cmd->tvc_prot_sgl);
 		kfree(tv_cmd->tvc_upages);
+		/*
+		 * 在以下使用vhost_scso_cmd->tvc_resp_iov:
+		 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+		 *                   cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+		 *   - drivers/vhost/scsi.c|1364| <<vhost_scsi_get_cmd>> tvc_resp_iov = cmd->tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|1377| <<vhost_scsi_get_cmd>> cmd->tvc_resp_iov = tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|2141| <<vhost_scsi_handle_vq>> cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
+		 *   - drivers/vhost/scsi.c|2805| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_resp_iov);
+		 *   - drivers/vhost/scsi.c|2898| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
+		 *                   sizeof(struct iovec), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2901| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_resp_iov) {
+		 */
 		kfree(tv_cmd->tvc_resp_iov);
 	}
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	sbitmap_free(&svq->scsi_tags);
 	kfree(svq->scsi_cmds);
 	svq->scsi_cmds = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2419| <<vhost_scsi_set_endpoint>> ret = vhost_scsi_setup_vq_cmds(vq, vq->num);
+ */
 static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1614,9 +2890,31 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 	struct vhost_scsi_cmd *tv_cmd;
 	unsigned int i;
 
+	/*
+	 * 在以下设置vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	if (svq->scsi_cmds)
 		return 0;
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
 			      NUMA_NO_NODE, false, true))
 		return -ENOMEM;
@@ -1639,6 +2937,16 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 			goto out;
 		}
 
+		/*
+		 * 在以下使用vhost_scsi_cmd->tvc_upages:
+		 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+		 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+		 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+		 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+		 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+		 *                                          sizeof(struct page *), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+		 */
 		tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
 					     sizeof(struct page *),
 					     GFP_KERNEL);
@@ -1647,6 +2955,18 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 			goto out;
 		}
 
+		/*
+		 * 在以下使用vhost_scso_cmd->tvc_resp_iov:
+		 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+		 *                   cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+		 *   - drivers/vhost/scsi.c|1364| <<vhost_scsi_get_cmd>> tvc_resp_iov = cmd->tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|1377| <<vhost_scsi_get_cmd>> cmd->tvc_resp_iov = tvc_resp_iov;
+		 *   - drivers/vhost/scsi.c|2141| <<vhost_scsi_handle_vq>> cmd->tvc_resp_iov[i] = vq->iov[vc.out + i];
+		 *   - drivers/vhost/scsi.c|2805| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_resp_iov);
+		 *   - drivers/vhost/scsi.c|2898| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
+		 *                   sizeof(struct iovec), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2901| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_resp_iov) {
+		 */
 		tv_cmd->tvc_resp_iov = kcalloc(UIO_MAXIOV,
 					       sizeof(struct iovec),
 					       GFP_KERNEL);
@@ -1665,6 +2985,12 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 	}
 	return 0;
 out:
+	/*
+	 * 在以下调用vhost_scsi_destroy_vq_cmds():
+	 *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+	 *   - drivers/vhost/scsi.c|2474| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+	 *   - drivers/vhost/scsi.c|2562| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+	 */
 	vhost_scsi_destroy_vq_cmds(vq);
 	return -ENOMEM;
 }
@@ -1676,6 +3002,17 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
  *  The lock nesting rule is:
  *    vs->dev.mutex -> vhost_scsi_mutex -> tpg->tv_tpg_mutex -> vq->mutex
  */
+/*
+ * struct vhost_scsi_target {
+ *     int abi_version;
+ *     char vhost_wwpn[224]; // TRANSPORT_IQN_LEN
+ *     unsigned short vhost_tpgt;
+ *     unsigned short reserved;
+ * };
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|2772| <<vhost_scsi_ioctl(VHOST_SCSI_SET_ENDPOINT)>> return vhost_scsi_set_endpoint(vs, &backend);
+ */
 static int
 vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			struct vhost_scsi_target *t)
@@ -1688,10 +3025,25 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 	int index, ret, i, len;
 	bool match = false;
 
+	/*
+	 * struct vhost_scsi *vs:
+	 * -> struct vhost_dev dev;
+	 *    -> struct mutex mutex;
+	 */
 	mutex_lock(&vs->dev.mutex);
 
 	/* Verify that ring has been setup correctly. */
 	for (index = 0; index < vs->dev.nvqs; ++index) {
+		/*
+		 * 在以下调用vhost_vq_access_ok():
+		 *   - drivers/vhost/net.c|1726| <<vhost_net_set_backend>> if (!vhost_vq_access_ok(vq)) {
+		 *   - drivers/vhost/scsi.c|2788| <<vhost_scsi_set_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+		 *   - drivers/vhost/scsi.c|2976| <<vhost_scsi_clear_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+		 *   - drivers/vhost/test.c|198| <<vhost_test_run>> if (!vhost_vq_access_ok(&n->vqs[index])) {
+		 *   - drivers/vhost/test.c|304| <<vhost_test_set_backend>> if (!vhost_vq_access_ok(vq)) {
+		 *   - drivers/vhost/vhost.c|525| <<vhost_vq_is_setup>> return vq->avail && vq->desc && vq->used && vhost_vq_access_ok(vq);
+		 *   - drivers/vhost/vsock.c|645| <<vhost_vsock_start>> if (!vhost_vq_access_ok(vq)) {
+		 */
 		/* Verify that ring has been setup correctly. */
 		if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
 			ret = -EFAULT;
@@ -1699,16 +3051,57 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 		}
 	}
 
+	/*
+	 * 在以下使用vhost_scsi->vs_tpg:
+	 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+	 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_handle_vq>> if (!vs_tpg)
+	 *   - drivers/vhost/scsi.c|2762| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg)
+	 *   - drivers/vhost/scsi.c|2763| <<vhost_scsi_set_endpoint>> memcpy(vs_tpg, vs->vs_tpg, len);
+	 *   - drivers/vhost/scsi.c|2779| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|2873| <<vhost_scsi_set_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|2874| <<vhost_scsi_set_endpoint>> vs->vs_tpg = vs_tpg;
+	 *   - drivers/vhost/scsi.c|2920| <<vhost_scsi_clear_endpoint>> if (!vs->vs_tpg) {
+	 *   - drivers/vhost/scsi.c|2927| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|2993| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|3001| <<vhost_scsi_clear_endpoint>> vs->vs_tpg[target] = NULL;
+	 *   - drivers/vhost/scsi.c|3021| <<vhost_scsi_clear_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|3022| <<vhost_scsi_clear_endpoint>> vs->vs_tpg = NULL;
+	 *
+	 * vhost_scsi_handle_vq()中的使用被vq->mutex保护
+	 *
+	 * vhost_scsi->vs_tpg被vhost_scsi->dev.mutex保护
+	 */
 	len = sizeof(vs_tpg[0]) * VHOST_SCSI_MAX_TARGET;
 	vs_tpg = kzalloc(len, GFP_KERNEL);
 	if (!vs_tpg) {
 		ret = -ENOMEM;
 		goto out;
 	}
+	/*
+	 * struct vhost_scsi_tpg *tpg;
+	 * struct vhost_scsi_tpg **vs_tpg;
+	 */
 	if (vs->vs_tpg)
 		memcpy(vs_tpg, vs->vs_tpg, len);
 
+	/*
+	 * 在以下使用vhost_scsi_mutex:
+	 *   - drivers/vhost/scsi.c|2840| <<vhost_scsi_set_endpoint>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2856| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2871| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2881| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3823| <<vhost_scsi_make_tpg>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3825| <<vhost_scsi_make_tpg>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3838| <<vhost_scsi_drop_tpg>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3840| <<vhost_scsi_drop_tpg>> mutex_unlock(&vhost_scsi_mutex);
+	 */
 	mutex_lock(&vhost_scsi_mutex);
+	/*
+	 * 在以下使用vhost_scsi_list:
+	 *   - drivers/vhost/scsi.c|308| <<global>> static LIST_HEAD(vhost_scsi_list);
+	 *   - drivers/vhost/scsi.c|2322| <<vhost_scsi_set_endpoint>> list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
+	 *   - drivers/vhost/scsi.c|3220| <<vhost_scsi_make_tpg>> list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
+	 */
 	list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
 		mutex_lock(&tpg->tv_tpg_mutex);
 		if (!tpg->tpg_nexus) {
@@ -1722,6 +3115,15 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 		tv_tport = tpg->tport;
 
 		if (!strcmp(tv_tport->tport_name, t->vhost_wwpn)) {
+			/*
+			 * 在以下使用vhost_scsi_tpg->vhost_scsi_tpg:
+			 *   - drivers/vhost/scsi.c|753| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+			 *   - drivers/vhost/scsi.c|2614| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+			 *   - drivers/vhost/scsi.c|3019| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+			 *   - drivers/vhost/scsi.c|3057| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+			 *   - drivers/vhost/scsi.c|3251| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+			 *   - drivers/vhost/scsi.c|4072| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+			 */
 			if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
 				mutex_unlock(&tpg->tv_tpg_mutex);
 				mutex_unlock(&vhost_scsi_mutex);
@@ -1734,7 +3136,23 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			 * go ahead and take an explicit se_tpg->tpg_group.cg_item
 			 * dependency now.
 			 */
+			/*
+			 * 在以下使用vhost_scsi->se_tpg (struct se_portal_group):
+			 *   - drivers/vhost/scsi.c|2924| <<vhost_scsi_set_endpoint>> se_tpg = &tpg->se_tpg;
+			 *   - drivers/vhost/scsi.c|3027| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+			 *   - drivers/vhost/scsi.c|3162| <<vhost_scsi_clear_endpoint>> se_tpg = &tpg->se_tpg;
+			 *   - drivers/vhost/scsi.c|3676| <<vhost_scsi_make_nexus>> tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
+			 *   - drivers/vhost/scsi.c|3876| <<vhost_scsi_make_tpg>> ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
+			 *   - drivers/vhost/scsi.c|3896| <<vhost_scsi_make_tpg>> return &tpg->se_tpg;
+			 */
 			se_tpg = &tpg->se_tpg;
+			/*
+			 * 在以下调用target_depend_item():
+			 *   - drivers/target/target_core_pr.c|1414| <<core_scsi3_tpg_depend_item>> return target_depend_item(&tpg->tpg_group.cg_item);
+			 *   - drivers/target/target_core_pr.c|1427| <<core_scsi3_nodeacl_depend_item>> return target_depend_item(&nacl->acl_group.cg_item);
+			 *   - drivers/target/target_core_pr.c|1445| <<core_scsi3_lunacl_depend_item>> return target_depend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
+			 *   - drivers/vhost/scsi.c|3025| <<vhost_scsi_set_endpoint>> ret = target_depend_item(&se_tpg->tpg_group.cg_item);
+			 */
 			ret = target_depend_item(&se_tpg->tpg_group.cg_item);
 			if (ret) {
 				pr_warn("target_depend_item() failed: %d\n", ret);
@@ -1757,18 +3175,61 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 
 		for (i = VHOST_SCSI_VQ_IO; i < vs->dev.nvqs; i++) {
 			vq = &vs->vqs[i].vq;
+			/*
+			 * 只在此处调用vhost_vq_is_setup()
+			 * !!! !!!
+			 */
 			if (!vhost_vq_is_setup(vq))
 				continue;
 
+			/*
+			 * 在以下调用vhost_scsi_destroy_vq_cmds():
+			 *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+			 *   - drivers/vhost/scsi.c|2474| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+			 *   - drivers/vhost/scsi.c|2562| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+			 *
+			 *
+			 * 只在此处调用, 分配command的field
+			 */
 			ret = vhost_scsi_setup_vq_cmds(vq, vq->num);
 			if (ret)
 				goto destroy_vq_cmds;
 		}
 
 		for (i = 0; i < vs->dev.nvqs; i++) {
+			/*
+			 * struct vhost_scsi *vs:
+			 * -> struct vhost_scsi_virtqueue *vqs;
+			 */
 			vq = &vs->vqs[i].vq;
 			mutex_lock(&vq->mutex);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+			 *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+			 *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+			 *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+			 *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+			 *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+			 *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+			 *
+			 * 就是设置vhost_virtqueue->private_data.
+			 */
 			vhost_vq_set_backend(vq, vs_tpg);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+			 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+			 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+			 */
 			vhost_vq_init_access(vq);
 			mutex_unlock(&vq->mutex);
 		}
@@ -1781,12 +3242,26 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 	 * Act as synchronize_rcu to make sure access to
 	 * old vs->vs_tpg is finished.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *
+	 * 这里为什么要flush???
+	 */
 	vhost_scsi_flush(vs);
 	kfree(vs->vs_tpg);
 	vs->vs_tpg = vs_tpg;
 	goto out;
 
 destroy_vq_cmds:
+	/*
+	 * 在以下调用vhost_scsi_destroy_vq_cmds():
+	 *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+	 *   - drivers/vhost/scsi.c|2474| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+	 *   - drivers/vhost/scsi.c|2562| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+	 */
 	for (i--; i >= VHOST_SCSI_VQ_IO; i--) {
 		if (!vhost_vq_get_backend(&vs->vqs[i].vq))
 			vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
@@ -1799,6 +3274,22 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			tpg->vhost_scsi = NULL;
 			tpg->tv_tpg_vhost_count--;
 			mutex_unlock(&tpg->tv_tpg_mutex);
+			/*
+			 * 在以下使用vhost_scsi->se_tpg (struct se_portal_group):
+			 *   - drivers/vhost/scsi.c|2924| <<vhost_scsi_set_endpoint>> se_tpg = &tpg->se_tpg;
+			 *   - drivers/vhost/scsi.c|3027| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+			 *   - drivers/vhost/scsi.c|3162| <<vhost_scsi_clear_endpoint>> se_tpg = &tpg->se_tpg;
+			 *   - drivers/vhost/scsi.c|3676| <<vhost_scsi_make_nexus>> tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
+			 *   - drivers/vhost/scsi.c|3876| <<vhost_scsi_make_tpg>> ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
+			 *   - drivers/vhost/scsi.c|3896| <<vhost_scsi_make_tpg>> return &tpg->se_tpg;
+			 *
+			 *   在以下调用target_undepend_item():
+			 *   - drivers/target/target_core_pr.c|1419| <<core_scsi3_tpg_undepend_item>> target_undepend_item(&tpg->tpg_group.cg_item);
+			 *   - drivers/target/target_core_pr.c|1433| <<core_scsi3_nodeacl_undepend_item>> target_undepend_item(&nacl->acl_group.cg_item);
+			 *   - drivers/target/target_core_pr.c|1458| <<core_scsi3_lunacl_undepend_item>> target_undepend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
+			 *   - drivers/vhost/scsi.c|3164| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+			 *   - drivers/vhost/scsi.c|3317| <<vhost_scsi_clear_endpoint>> target_undepend_item(&se_tpg->tpg_group.cg_item);
+			 */
 			target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
 		}
 	}
@@ -1808,6 +3299,14 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 	return ret;
 }
 
+/*
+ * struct vhost_scsi_target {
+ *     int abi_version;
+ *     char vhost_wwpn[224]; // TRANSPORT_IQN_LEN
+ *     unsigned short vhost_tpgt;
+ *     unsigned short reserved;
+ * };
+ */
 static int
 vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 			  struct vhost_scsi_target *t)
@@ -1829,6 +3328,26 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 		}
 	}
 
+	/*
+	 * 在以下使用vhost_scsi->vs_tpg:
+	 *   - drivers/vhost/scsi.c|1799| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+	 *   - drivers/vhost/scsi.c|1800| <<vhost_scsi_handle_vq>> if (!vs_tpg)
+	 *   - drivers/vhost/scsi.c|2762| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg)
+	 *   - drivers/vhost/scsi.c|2763| <<vhost_scsi_set_endpoint>> memcpy(vs_tpg, vs->vs_tpg, len);
+	 *   - drivers/vhost/scsi.c|2779| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|2873| <<vhost_scsi_set_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|2874| <<vhost_scsi_set_endpoint>> vs->vs_tpg = vs_tpg;
+	 *   - drivers/vhost/scsi.c|2920| <<vhost_scsi_clear_endpoint>> if (!vs->vs_tpg) {
+	 *   - drivers/vhost/scsi.c|2927| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|2993| <<vhost_scsi_clear_endpoint>> tpg = vs->vs_tpg[target];
+	 *   - drivers/vhost/scsi.c|3001| <<vhost_scsi_clear_endpoint>> vs->vs_tpg[target] = NULL;
+	 *   - drivers/vhost/scsi.c|3021| <<vhost_scsi_clear_endpoint>> kfree(vs->vs_tpg);
+	 *   - drivers/vhost/scsi.c|3022| <<vhost_scsi_clear_endpoint>> vs->vs_tpg = NULL;
+	 *
+	 * vhost_scsi_handle_vq()中的使用被vq->mutex保护
+	 *
+	 * vhost_scsi->vs_tpg被vhost_scsi->dev.mutex保护
+	 */
 	if (!vs->vs_tpg) {
 		ret = 0;
 		goto err_dev;
@@ -1847,6 +3366,15 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 		}
 
 		if (strcmp(tv_tport->tport_name, t->vhost_wwpn)) {
+			/*
+			 * 在以下使用vhost_scsi_tpg->vhost_scsi_tpg:
+			 *   - drivers/vhost/scsi.c|753| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+			 *   - drivers/vhost/scsi.c|2614| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+			 *   - drivers/vhost/scsi.c|3019| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+			 *   - drivers/vhost/scsi.c|3057| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+			 *   - drivers/vhost/scsi.c|3251| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+			 *   - drivers/vhost/scsi.c|4072| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+			 */
 			pr_warn("tv_tport->tport_name: %s, tpg->tport_tpgt: %hu"
 				" does not match t->vhost_wwpn: %s, t->vhost_tpgt: %hu\n",
 				tv_tport->tport_name, tpg->tport_tpgt,
@@ -1863,14 +3391,44 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 	for (i = 0; i < vs->dev.nvqs; i++) {
 		vq = &vs->vqs[i].vq;
 		mutex_lock(&vq->mutex);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+		 *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+		 *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+		 *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+		 *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+		 *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+		 *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+		 */
 		vhost_vq_set_backend(vq, NULL);
 		mutex_unlock(&vq->mutex);
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *
+	 * 这里过后应该没有任何没complete的cmd了
+	 */
 	/* Make sure cmds are not running before tearing them down. */
 	vhost_scsi_flush(vs);
 
 	for (i = 0; i < vs->dev.nvqs; i++) {
 		vq = &vs->vqs[i].vq;
+		/*
+		 * 在以下调用vhost_scsi_destroy_vq_cmds():
+		 *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+		 *   - drivers/vhost/scsi.c|2474| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+		 *   - drivers/vhost/scsi.c|2562| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+		 */
 		vhost_scsi_destroy_vq_cmds(vq);
 	}
 
@@ -1892,7 +3450,24 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 
 		mutex_unlock(&tpg->tv_tpg_mutex);
 
+		/*
+		 * 在以下使用vhost_scsi->se_tpg (struct se_portal_group):
+		 *   - drivers/vhost/scsi.c|2924| <<vhost_scsi_set_endpoint>> se_tpg = &tpg->se_tpg;
+		 *   - drivers/vhost/scsi.c|3027| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+		 *   - drivers/vhost/scsi.c|3162| <<vhost_scsi_clear_endpoint>> se_tpg = &tpg->se_tpg;
+		 *   - drivers/vhost/scsi.c|3676| <<vhost_scsi_make_nexus>> tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
+		 *   - drivers/vhost/scsi.c|3876| <<vhost_scsi_make_tpg>> ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
+		 *   - drivers/vhost/scsi.c|3896| <<vhost_scsi_make_tpg>> return &tpg->se_tpg;
+		 */
 		se_tpg = &tpg->se_tpg;
+		/*
+		 * 在以下调用target_undepend_item():
+		 *   - drivers/target/target_core_pr.c|1419| <<core_scsi3_tpg_undepend_item>> target_undepend_item(&tpg->tpg_group.cg_item);
+		 *   - drivers/target/target_core_pr.c|1433| <<core_scsi3_nodeacl_undepend_item>> target_undepend_item(&nacl->acl_group.cg_item);
+		 *   - drivers/target/target_core_pr.c|1458| <<core_scsi3_lunacl_undepend_item>> target_undepend_item(&se_deve->se_lun_acl->se_lun_group.cg_item);
+		 *   - drivers/vhost/scsi.c|3164| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+		 *   - drivers/vhost/scsi.c|3317| <<vhost_scsi_clear_endpoint>> target_undepend_item(&se_tpg->tpg_group.cg_item);
+		 */
 		target_undepend_item(&se_tpg->tpg_group.cg_item);
 	}
 
@@ -1901,9 +3476,23 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 	 * Act as synchronize_rcu to make sure access to
 	 * old vs->vs_tpg is finished.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 */
 	vhost_scsi_flush(vs);
 	kfree(vs->vs_tpg);
 	vs->vs_tpg = NULL;
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	WARN_ON(vs->vs_events_nr);
 	mutex_unlock(&vs->dev.mutex);
 	return 0;
@@ -1913,6 +3502,10 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 	return ret;
 }
 
+/*
+ * 处理VHOST_SET_FEATURES:
+ *   - drivers/vhost/scsi.c|2806| <<vhost_scsi_ioctl(VHOST_SET_FEATURES)>> return vhost_scsi_set_features(vs, features);
+ */
 static int vhost_scsi_set_features(struct vhost_scsi *vs, u64 features)
 {
 	struct vhost_virtqueue *vq;
@@ -1929,8 +3522,27 @@ static int vhost_scsi_set_features(struct vhost_scsi *vs, u64 features)
 	}
 
 	for (i = 0; i < vs->dev.nvqs; i++) {
+		/*
+		 * struct vhost_scsi *vs:
+		 * -> struct vhost_scsi_virtqueue *vqs;
+		 *    -> struct vhost_scsi_cmd *scsi_cmds;
+		 *
+		 * 在以下分配:
+		 *   - drivers/vhost/scsi.c|2673| <<vhost_scsi_open>> vs->vqs = kmalloc_array(nvqs,
+		 *             sizeof(*vs->vqs), GFP_KERNEL | __GFP_ZERO);
+		 */
 		vq = &vs->vqs[i].vq;
 		mutex_lock(&vq->mutex);
+		/*
+		 * 在以下使用vhost_virtqueue->acked_features:
+		 *   - drivers/vhost/net.c|1886| <<vhost_net_set_features>> n->vqs[i].vq.acked_features = features;
+		 *   - drivers/vhost/scsi.c|3460| <<vhost_scsi_set_features>> vq->acked_features = features;
+		 *   - drivers/vhost/test.c|277| <<vhost_test_set_features>> vq->acked_features = features;
+		 *   - drivers/vhost/vdpa.c|511| <<vhost_vdpa_set_features>> vq->acked_features = actual_features;
+		 *   - drivers/vhost/vhost.c|594| <<vhost_vq_reset>> vq->acked_features = 0;
+		 *   - drivers/vhost/vhost.h|451| <<vhost_has_feature>> return vq->acked_features & (1ULL << bit);
+		 *   - drivers/vhost/vsock.c|917| <<vhost_vsock_set_features>> vq->acked_features = features;
+		 */
 		vq->acked_features = features;
 		mutex_unlock(&vq->mutex);
 	}
@@ -1973,8 +3585,21 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 	if (!vqs)
 		goto err_local_vqs;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	vs->vs_events_nr = 0;
 	vs->vs_events_missed = false;
 
@@ -1992,6 +3617,22 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 				vhost_scsi_complete_cmd_work);
 		svq->vq.handle_kick = vhost_scsi_handle_kick;
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vs->dev, vqs, nvqs, UIO_MAXIOV,
 		       VHOST_SCSI_WEIGHT, 0, true, NULL);
 
@@ -2028,6 +3669,16 @@ static int vhost_scsi_release(struct inode *inode, struct file *f)
 	return 0;
 }
 
+/*
+ * 3522 static const struct file_operations vhost_scsi_fops = {
+ * 3523         .owner          = THIS_MODULE,
+ * 3524         .release        = vhost_scsi_release,
+ * 3525         .unlocked_ioctl = vhost_scsi_ioctl,
+ * 3526         .compat_ioctl   = compat_ptr_ioctl,
+ * 3527         .open           = vhost_scsi_open,
+ * 3528         .llseek         = noop_llseek,
+ * 3529 };
+ */
 static long
 vhost_scsi_ioctl(struct file *f,
 		 unsigned int ioctl,
@@ -2045,6 +3696,14 @@ vhost_scsi_ioctl(struct file *f,
 
 	switch (ioctl) {
 	case VHOST_SCSI_SET_ENDPOINT:
+		/*
+		 * struct vhost_scsi_target {
+		 *     int abi_version;
+		 *     char vhost_wwpn[224]; // TRANSPORT_IQN_LEN
+		 *     unsigned short vhost_tpgt;
+		 *     unsigned short reserved;
+		 * };
+		 */
 		if (copy_from_user(&backend, argp, sizeof backend))
 			return -EFAULT;
 		if (backend.reserved != 0)
@@ -2145,11 +3804,51 @@ static char *vhost_scsi_dump_proto_id(struct vhost_scsi_tport *tport)
 	return "Unknown";
 }
 
+/*
+ * add:
+ *
+ * vhost_scsi_do_plug
+ * vhost_scsi_port_link
+ * target_fabric_port_link
+ * configfs_symlink
+ * vfs_symlink
+ * do_symlinkat
+ * __x64_sys_symlink
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * del:
+ *
+ * vhost_scsi_do_plug
+ * vhost_scsi_port_unlink
+ * target_fabric_port_unlink
+ * configfs_unlink
+ * vfs_unlink
+ * do_unlinkat
+ * __x64_sys_unlink
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * QEMU是下面的代码:
+ *   - hw/scsi/virtio-scsi.c|1148| <<virtio_scsi_hotplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+ *   - hw/scsi/virtio-scsi.c|1185| <<virtio_scsi_hotunplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|2230| <<vhost_scsi_hotplug>> vhost_scsi_do_plug(tpg, lun, true);
+ *   - drivers/vhost/scsi.c|2235| <<vhost_scsi_hotunplug>> vhost_scsi_do_plug(tpg, lun, false);
+ */
 static void
 vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 		  struct se_lun *lun, bool plug)
 {
 
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi:
+	 *   - drivers/vhost/scsi.c|2349| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = vs;
+	 *   - drivers/vhost/scsi.c|2416| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2513| <<vhost_scsi_clear_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2828| <<vhost_scsi_do_plug>> struct vhost_scsi *vs = tpg->vhost_scsi;
+	 */
 	struct vhost_scsi *vs = tpg->vhost_scsi;
 	struct vhost_virtqueue *vq;
 	u32 reason;
@@ -2171,6 +3870,15 @@ vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 	if (!vhost_vq_get_backend(vq))
 		goto unlock;
 
+	/*
+	 * QEMU是下面的代码:
+	 *   - hw/scsi/virtio-scsi.c|1148| <<virtio_scsi_hotplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+	 *   - hw/scsi/virtio-scsi.c|1185| <<virtio_scsi_hotunplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+	 *
+	 * 在以下使用vhost_scsi_send_evt():
+	 *   - drivers/vhost/scsi.c|1594| <<vhost_scsi_evt_handle_kick>> vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
+	 *   - drivers/vhost/scsi.c|2222| <<vhost_scsi_do_plug>> vhost_scsi_send_evt(vs, vq, tpg, lun,
+	 */
 	if (vhost_has_feature(vq, VIRTIO_SCSI_F_HOTPLUG))
 		vhost_scsi_send_evt(vs, vq, tpg, lun,
 				   VIRTIO_SCSI_T_TRANSPORT_RESET, reason);
@@ -2178,6 +3886,10 @@ vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2246| <<vhost_scsi_port_link>> vhost_scsi_hotplug(tpg, lun);
+ */
 static void vhost_scsi_hotplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 {
 	vhost_scsi_do_plug(tpg, lun, true);
@@ -2188,6 +3900,9 @@ static void vhost_scsi_hotunplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 	vhost_scsi_do_plug(tpg, lun, false);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_post_link = vhost_scsi_port_link()
+ */
 static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 			       struct se_lun *lun)
 {
@@ -2202,6 +3917,9 @@ static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_pre_unlink = vhost_scsi_port_unlink()
+ */
 static void vhost_scsi_port_unlink(struct se_portal_group *se_tpg,
 				  struct se_lun *lun)
 {
@@ -2248,11 +3966,18 @@ static ssize_t vhost_scsi_tpg_attrib_fabric_prot_type_show(
 
 CONFIGFS_ATTR(vhost_scsi_tpg_attrib_, fabric_prot_type);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_attrib_attrs = vhost_scsi_tpg_attrib_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrib_attrs[] = {
 	&vhost_scsi_tpg_attrib_attr_fabric_prot_type,
 	NULL,
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|3169| <<vhost_scsi_tpg_nexus_store>> ret = vhost_scsi_make_nexus(tpg, port_ptr);
+ */
 static int vhost_scsi_make_nexus(struct vhost_scsi_tpg *tpg,
 				const char *name)
 {
@@ -2276,6 +4001,15 @@ static int vhost_scsi_make_nexus(struct vhost_scsi_tpg *tpg,
 	 * struct se_node_acl for the vhost_scsi struct se_portal_group with
 	 * the SCSI Initiator port name of the passed configfs group 'name'.
 	 */
+	/*
+	 * 在以下使用vhost_scsi->se_tpg (struct se_portal_group):
+	 *   - drivers/vhost/scsi.c|2924| <<vhost_scsi_set_endpoint>> se_tpg = &tpg->se_tpg;
+	 *   - drivers/vhost/scsi.c|3027| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+	 *   - drivers/vhost/scsi.c|3162| <<vhost_scsi_clear_endpoint>> se_tpg = &tpg->se_tpg;
+	 *   - drivers/vhost/scsi.c|3676| <<vhost_scsi_make_nexus>> tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
+	 *   - drivers/vhost/scsi.c|3876| <<vhost_scsi_make_tpg>> ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
+	 *   - drivers/vhost/scsi.c|3896| <<vhost_scsi_make_tpg>> return &tpg->se_tpg;
+	 */
 	tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
 					TARGET_PROT_DIN_PASS | TARGET_PROT_DOUT_PASS,
 					(unsigned char *)name, tv_nexus, NULL);
@@ -2440,11 +4174,17 @@ static ssize_t vhost_scsi_tpg_nexus_store(struct config_item *item,
 
 CONFIGFS_ATTR(vhost_scsi_tpg_, nexus);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_base_attrs = vhost_scsi_tpg_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrs[] = {
 	&vhost_scsi_tpg_attr_nexus,
 	NULL,
 };
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_tpg = vhost_scsi_make_tpg()
+ */
 static struct se_portal_group *
 vhost_scsi_make_tpg(struct se_wwn *wwn, const char *name)
 {
@@ -2468,25 +4208,74 @@ vhost_scsi_make_tpg(struct se_wwn *wwn, const char *name)
 	mutex_init(&tpg->tv_tpg_mutex);
 	INIT_LIST_HEAD(&tpg->tv_tpg_list);
 	tpg->tport = tport;
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi_tpg:
+	 *   - drivers/vhost/scsi.c|753| <<vhost_scsi_get_tpgt>> return tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|2614| <<vhost_scsi_send_evt>> evt->event.lun[1] = tpg->tport_tpgt;
+	 *   - drivers/vhost/scsi.c|3019| <<vhost_scsi_set_endpoint>> if (vs->vs_tpg && vs->vs_tpg[tpg->tport_tpgt]) {
+	 *   - drivers/vhost/scsi.c|3057| <<vhost_scsi_set_endpoint>> vs_tpg[tpg->tport_tpgt] = tpg;
+	 *   - drivers/vhost/scsi.c|3251| <<vhost_scsi_clear_endpoint>> tv_tport->tport_name, tpg->tport_tpgt,
+	 *   - drivers/vhost/scsi.c|4072| <<vhost_scsi_make_tpg>> tpg->tport_tpgt = tpgt;
+	 */
 	tpg->tport_tpgt = tpgt;
 
+	/*
+	 * 在以下使用vhost_scsi->se_tpg (struct se_portal_group):
+	 *   - drivers/vhost/scsi.c|2924| <<vhost_scsi_set_endpoint>> se_tpg = &tpg->se_tpg;
+	 *   - drivers/vhost/scsi.c|3027| <<vhost_scsi_set_endpoint>> target_undepend_item(&tpg->se_tpg.tpg_group.cg_item);
+	 *   - drivers/vhost/scsi.c|3162| <<vhost_scsi_clear_endpoint>> se_tpg = &tpg->se_tpg;
+	 *   - drivers/vhost/scsi.c|3676| <<vhost_scsi_make_nexus>> tv_nexus->tvn_se_sess = target_setup_session(&tpg->se_tpg, 0, 0,
+	 *   - drivers/vhost/scsi.c|3876| <<vhost_scsi_make_tpg>> ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
+	 *   - drivers/vhost/scsi.c|3896| <<vhost_scsi_make_tpg>> return &tpg->se_tpg;
+	 */
 	ret = core_tpg_register(wwn, &tpg->se_tpg, tport->tport_proto_id);
 	if (ret < 0) {
 		kfree(tpg);
 		return NULL;
 	}
+	/*
+	 * 在以下使用vhost_scsi_mutex:
+	 *   - drivers/vhost/scsi.c|2840| <<vhost_scsi_set_endpoint>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2856| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2871| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2881| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3823| <<vhost_scsi_make_tpg>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3825| <<vhost_scsi_make_tpg>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3838| <<vhost_scsi_drop_tpg>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3840| <<vhost_scsi_drop_tpg>> mutex_unlock(&vhost_scsi_mutex);
+	 */
 	mutex_lock(&vhost_scsi_mutex);
+	/*
+	 * 在以下使用vhost_scsi_list:
+	 *   - drivers/vhost/scsi.c|308| <<global>> static LIST_HEAD(vhost_scsi_list);
+	 *   - drivers/vhost/scsi.c|2322| <<vhost_scsi_set_endpoint>> list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
+	 *   - drivers/vhost/scsi.c|3220| <<vhost_scsi_make_tpg>> list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
+	 */
 	list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
 	mutex_unlock(&vhost_scsi_mutex);
 
 	return &tpg->se_tpg;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_tpg = vhost_scsi_drop_tpg()
+ */
 static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
 				struct vhost_scsi_tpg, se_tpg);
 
+	/*
+	 * 在以下使用vhost_scsi_mutex:
+	 *   - drivers/vhost/scsi.c|2840| <<vhost_scsi_set_endpoint>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2856| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2871| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|2881| <<vhost_scsi_set_endpoint>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3823| <<vhost_scsi_make_tpg>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3825| <<vhost_scsi_make_tpg>> mutex_unlock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3838| <<vhost_scsi_drop_tpg>> mutex_lock(&vhost_scsi_mutex);
+	 *   - drivers/vhost/scsi.c|3840| <<vhost_scsi_drop_tpg>> mutex_unlock(&vhost_scsi_mutex);
+	 */
 	mutex_lock(&vhost_scsi_mutex);
 	list_del(&tpg->tv_tpg_list);
 	mutex_unlock(&vhost_scsi_mutex);
@@ -2501,6 +4290,9 @@ static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 	kfree(tpg);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_wwn = vhost_scsi_make_tport()
+ */
 static struct se_wwn *
 vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 		     struct config_group *group,
@@ -2562,6 +4354,9 @@ vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 	return &tport->tport_wwn;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_wwn = vhost_scsi_drop_tport()
+ */
 static void vhost_scsi_drop_tport(struct se_wwn *wwn)
 {
 	struct vhost_scsi_tport *tport = container_of(wwn,
@@ -2584,6 +4379,9 @@ vhost_scsi_wwn_version_show(struct config_item *item, char *page)
 
 CONFIGFS_ATTR_RO(vhost_scsi_wwn_, version);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_wwn_attrs = vhost_scsi_wwn_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_wwn_attrs[] = {
 	&vhost_scsi_wwn_attr_version,
 	NULL,
@@ -2636,6 +4434,23 @@ static int __init vhost_scsi_init(void)
 	if (ret < 0)
 		goto out;
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3961| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1662| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_ops);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1667| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_npiv_ops);
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4036| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1878| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1882| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+	 *   - drivers/target/iscsi/iscsi_target.c|695| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+	 *   - drivers/target/loopback/tcm_loop.c|1126| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+	 *   - drivers/target/sbp/sbp_target.c|2288| <<sbp_init>> return target_register_template(&sbp_ops);
+	 *   - drivers/target/tcm_fc/tfc_conf.c|448| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+	 *   - drivers/target/tcm_remote/tcm_remote.c|256| <<tcm_remote_fabric_init>> return target_register_template(&remote_ops);
+	 *   - drivers/usb/gadget/function/f_tcm.c|2289| <<tcm_init>> ret = target_register_template(&usbg_ops);
+	 *   - drivers/vhost/scsi.c|2792| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+	 *   - drivers/xen/xen-scsiback.c|1866| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+	 */
 	ret = target_register_template(&vhost_scsi_ops);
 	if (ret < 0)
 		goto out_vhost_scsi_deregister;
diff --git a/drivers/vhost/test.c b/drivers/vhost/test.c
index 42c955a5b..d5b29fb89 100644
--- a/drivers/vhost/test.c
+++ b/drivers/vhost/test.c
@@ -119,6 +119,22 @@ static int vhost_test_open(struct inode *inode, struct file *f)
 	dev = &n->dev;
 	vqs[VHOST_TEST_VQ] = &n->vqs[VHOST_TEST_VQ];
 	n->vqs[VHOST_TEST_VQ].handle_kick = handle_vq_kick;
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
 		       VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
 
@@ -194,6 +210,14 @@ static long vhost_test_run(struct vhost_test *n, int test)
 		oldpriv = vhost_vq_get_backend(vq);
 		vhost_vq_set_backend(vq, priv);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(&n->vqs[index]);
 
 		mutex_unlock(&vq->mutex);
@@ -282,12 +306,43 @@ static long vhost_test_set_backend(struct vhost_test *n, unsigned index, int fd)
 		goto err_vq;
 	}
 	if (!enable) {
+		/*
+		 * 在以下调用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 *
+		 * 注释:
+		 * Stop polling a file. After this function returns, it becomes safe to drop the
+		 * file reference. You must also flush afterwards.
+		 */
 		vhost_poll_stop(&vq->poll);
 		backend = vhost_vq_get_backend(vq);
 		vhost_vq_set_backend(vq, NULL);
 	} else {
 		vhost_vq_set_backend(vq, backend);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(vq);
+		/*
+		 * 在以下调用vhost_poll_start():
+		 *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+		 *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+		 *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+		 *   - drivers/vhost/vhost.h|56| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+		 *
+		 * 注释:
+		 * Start polling a file. We add ourselves to file's wait queue. The caller must
+		 * keep a reference to a file until after vhost_poll_stop is called.
+		 */
 		if (r == 0)
 			r = vhost_poll_start(&vq->poll, vq->kick);
 	}
diff --git a/drivers/vhost/vdpa.c b/drivers/vhost/vdpa.c
index 5a49b5a6d..ff1ab9197 100644
--- a/drivers/vhost/vdpa.c
+++ b/drivers/vhost/vdpa.c
@@ -170,9 +170,29 @@ static void handle_vq_kick(struct vhost_work *work)
 	struct vhost_vdpa *v = container_of(vq->dev, struct vhost_vdpa, vdev);
 	const struct vdpa_config_ops *ops = v->vdpa->config;
 
+	/*
+	 * 在以下调用vdpa_config_ops->kick_vq:
+	 *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+	 *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+	 * 在以下设置vdpa_config_ops->kick_vq:
+	 *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+	 *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+	 *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+	 */
 	ops->kick_vq(v->vdpa, vq - v->vqs);
 }
 
+/*
+ * 在以下使用vhost_vdpa_virtqueue_cb():
+ *   - drivers/vhost/vdpa.c|753| <<vhost_vdpa_vring_ioctl(VHOST_SET_VRING_CALL)>> cb.callback = vhost_vdpa_virtqueue_cb;
+ */
 static irqreturn_t vhost_vdpa_virtqueue_cb(void *private)
 {
 	struct vhost_virtqueue *vq = private;
@@ -989,6 +1009,12 @@ static int perm_to_iommu_flags(u32 perm)
 	return flags | IOMMU_CACHE;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vdpa.c|1075| <<vhost_vdpa_va_map>> ret = vhost_vdpa_map(v, iotlb, map_iova, map_size, uaddr,
+ *   - drivers/vhost/vdpa.c|1160| <<vhost_vdpa_pa_map>> ret = vhost_vdpa_map(v, iotlb, iova, csize,
+ *   - drivers/vhost/vdpa.c|1190| <<vhost_vdpa_pa_map>> ret = vhost_vdpa_map(v, iotlb, iova, PFN_PHYS(last_pfn - map_pfn + 1),
+ */
 static int vhost_vdpa_map(struct vhost_vdpa *v, struct vhost_iotlb *iotlb,
 			  u64 iova, u64 size, u64 pa, u32 perm, void *opaque)
 {
@@ -1430,6 +1456,22 @@ static int vhost_vdpa_open(struct inode *inode, struct file *filep)
 		vqs[i]->handle_kick = handle_vq_kick;
 		vqs[i]->call_ctx.ctx = NULL;
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,
 		       vhost_vdpa_process_iotlb_msg);
 
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 9ac25d08f..6396fc145 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -143,6 +143,10 @@ struct vhost_flush_struct {
 	struct completion wait_event;
 };
 
+/*
+ * 在以下使用vhost_flush_work():
+ *   - drivers/vhost/vhost.c|307| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ */
 static void vhost_flush_work(struct vhost_work *work)
 {
 	struct vhost_flush_struct *s;
@@ -151,6 +155,10 @@ static void vhost_flush_work(struct vhost_work *work)
 	complete(&s->wait_event);
 }
 
+/*
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|212| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt)
 {
@@ -161,6 +169,11 @@ static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|211| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|233| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -170,6 +183,23 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	if (!(key_to_poll(key) & poll->mask))
 		return 0;
 
+	/*
+	 * 在以下使用vhost_poll_queue():
+	 *   - drivers/vhost/net.c|411| <<vhost_zerocopy_complete>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|526| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|529| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|821| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|937| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|1260| <<handle_rx>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|1358| <<handle_rx>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/vhost.c|176| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+	 *   - drivers/vhost/vhost.c|531| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/vhost.c|1552| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+	 *   - drivers/vhost/vhost.h|58| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+	 *   - drivers/vhost/vsock.c|284| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+	 *   - drivers/vhost/vsock.c|347| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+	 */
+
 	if (!poll->dev->use_worker)
 		work->fn(work);
 	else
@@ -178,6 +208,15 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1547| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *   - drivers/vhost/scsi.c|2231| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/scsi.c|2254| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/vhost.c|200| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|280| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vsock.c|725| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -185,6 +224,15 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 }
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|570| <<vhost_dev_init>> vhost_poll_init(&vq->poll,
+ *                   vq->handle_kick, EPOLLIN, dev, vq);
+ *   - drivers/vhost/net.c|1379| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX,
+ *                   handle_tx_net, EPOLLOUT, dev, vqs[VHOST_NET_VQ_TX]);
+ *   - drivers/vhost/net.c|1381| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX,
+ *                   handle_rx_net, EPOLLIN, dev, vqs[VHOST_NET_VQ_RX]);
+ */
 /* Init poll structure */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev,
@@ -203,6 +251,16 @@ EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * 在以下调用vhost_poll_start():
+ *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *
+ * 注释:
+ * Start polling a file. We add ourselves to file's wait queue. The caller must
+ * keep a reference to a file until after vhost_poll_stop is called.
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	__poll_t mask;
@@ -214,6 +272,18 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
 	if (mask & EPOLLERR) {
+		/*
+		 * 在以下调用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 *
+		 * 注释:
+		 * Stop polling a file. After this function returns, it becomes safe to drop the
+		 * file reference. You must also flush afterwards.
+		 */
 		vhost_poll_stop(poll);
 		return -EINVAL;
 	}
@@ -224,6 +294,18 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * 在以下调用vhost_poll_stop():
+ *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+ *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ *
+ * 注释:
+ * Stop polling a file. After this function returns, it becomes safe to drop the
+ * file reference. You must also flush afterwards.
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
 	if (poll->wqh) {
@@ -233,6 +315,11 @@ void vhost_poll_stop(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_stop);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|285| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+ *   - drivers/vhost/vhost.c|309| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+ */
 static void vhost_worker_queue(struct vhost_worker *worker,
 			       struct vhost_work *work)
 {
@@ -246,6 +333,15 @@ static void vhost_worker_queue(struct vhost_worker *worker,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+ *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+ *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+ *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+ *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+ *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+ */
 bool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)
 {
 	struct vhost_worker *worker;
@@ -255,6 +351,11 @@ bool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)
 	worker = rcu_dereference(vq->worker);
 	if (worker) {
 		queued = true;
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|285| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+		 *   - drivers/vhost/vhost.c|309| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+		 */
 		vhost_worker_queue(worker, work);
 	}
 	rcu_read_unlock();
@@ -269,6 +370,12 @@ EXPORT_SYMBOL_GPL(vhost_vq_work_queue);
  *
  * The worker's flush_mutex must be held.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+ *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+ *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+ */
 static void __vhost_worker_flush(struct vhost_worker *worker)
 {
 	struct vhost_flush_struct flush;
@@ -279,6 +386,11 @@ static void __vhost_worker_flush(struct vhost_worker *worker)
 	init_completion(&flush.wait_event);
 	vhost_work_init(&flush.work, vhost_flush_work);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|285| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+	 *   - drivers/vhost/vhost.c|309| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+	 */
 	vhost_worker_queue(worker, &flush.work);
 	/*
 	 * Drop mutex in case our worker is killed and it needs to take the
@@ -289,23 +401,52 @@ static void __vhost_worker_flush(struct vhost_worker *worker)
 	mutex_lock(&worker->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|344| <<vhost_dev_flush>> vhost_worker_flush(worker);
+ */
 static void vhost_worker_flush(struct vhost_worker *worker)
 {
 	mutex_lock(&worker->mutex);
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	mutex_unlock(&worker->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1432| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/net.c|1623| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/scsi.c|1639| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+ *   - drivers/vhost/scsi.c|2044| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+ *   - drivers/vhost/test.c|165| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/vhost.c|1050| <<vhost_dev_stop>> vhost_dev_flush(dev);
+ *   - drivers/vhost/vhost.c|2147| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+ *   - drivers/vhost/vhost.h|61| <<vhost_vring_ioctl>> void vhost_dev_flush(struct vhost_dev *dev);
+ *   - drivers/vhost/vsock.c|751| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+ */
 void vhost_dev_flush(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	unsigned long i;
 
+	/*
+	 * 只在此处调用vhost_worker_flush()
+	 */
 	xa_for_each(&dev->worker_xa, i, worker)
 		vhost_worker_flush(worker);
 }
 EXPORT_SYMBOL_GPL(vhost_dev_flush);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|561| <<vhost_net_busy_poll>> if (vhost_vq_has_work(vq)) {
+ */
 /* A lockless hint for busy polling code to exit the loop */
 bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 {
@@ -322,8 +463,33 @@ bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_has_work);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|411| <<vhost_zerocopy_complete>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|526| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|529| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|821| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|937| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1260| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1358| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|176| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|531| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|1552| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vhost.h|58| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+ *   - drivers/vhost/vsock.c|284| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|347| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(poll->vq, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_queue);
@@ -350,12 +516,21 @@ static void vhost_vring_call_reset(struct vhost_vring_call *call_ctx)
 	memset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2335| <<vhost_scsi_set_endpoint>> if (!vhost_vq_is_setup(vq))
+ */
 bool vhost_vq_is_setup(struct vhost_virtqueue *vq)
 {
 	return vq->avail && vq->desc && vq->used && vhost_vq_access_ok(vq);
 }
 EXPORT_SYMBOL_GPL(vhost_vq_is_setup);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|772| <<vhost_dev_init()>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|1285| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -366,14 +541,81 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	vq->last_avail_idx = 0;
 	vq->avail_idx = 0;
 	vq->last_used_idx = 0;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|3622| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|3723| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|3725| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	vq->signalled_used = 0;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|545| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3102| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3623| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3724| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|3726| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	vq->signalled_used_valid = false;
+	/*
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 */
 	vq->used_flags = 0;
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	vq->log_used = false;
+	/*
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	vq->log_addr = -1ull;
 	vq->private_data = NULL;
+	/*
+	 * 在以下使用vhost_virtqueue->acked_features:
+	 *   - drivers/vhost/net.c|1886| <<vhost_net_set_features>> n->vqs[i].vq.acked_features = features;
+	 *   - drivers/vhost/scsi.c|3460| <<vhost_scsi_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/test.c|277| <<vhost_test_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/vdpa.c|511| <<vhost_vdpa_set_features>> vq->acked_features = actual_features;
+	 *   - drivers/vhost/vhost.c|594| <<vhost_vq_reset>> vq->acked_features = 0;
+	 *   - drivers/vhost/vhost.h|451| <<vhost_has_feature>> return vq->acked_features & (1ULL << bit);
+	 *   - drivers/vhost/vsock.c|917| <<vhost_vsock_set_features>> vq->acked_features = features;
+	 */
 	vq->acked_features = 0;
 	vq->acked_backend_features = 0;
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	vq->log_base = NULL;
 	vq->error_ctx = NULL;
 	vq->kick = NULL;
@@ -413,6 +655,11 @@ static bool vhost_run_work_list(void *data)
 	return !!node;
 }
 
+/*
+ * 在以下使用vhost_worker_killed():
+ *   - drivers/vhost/vhost.c|903| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+ *             vhost_worker_killed, worker, name);
+ */
 static void vhost_worker_killed(void *data)
 {
 	struct vhost_worker *worker = data;
@@ -530,6 +777,22 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+ *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+ *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+ *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+ *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+ *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+ *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+ *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+ *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+ *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+ *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+ *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -649,6 +912,11 @@ static void vhost_workers_free(struct vhost_dev *dev)
 	xa_destroy(&dev->worker_xa);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|821| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+ *   - drivers/vhost/vhost.c|979| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+ */
 static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -664,6 +932,14 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	worker->dev = dev;
 	snprintf(name, sizeof(name), "vhost-%d", current->pid);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7420| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread =
+	 *            vhost_task_create(kvm_nx_huge_page_recovery_worker,
+	 *                              kvm_nx_huge_page_recovery_worker_kill, kvm, "kvm-nx-lpage-recovery");
+	 * drivers/vhost/vhost.c|701| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+	 *            vhost_worker_killed, worker, name);
+	 */
 	vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
 				 worker, name);
 	if (!vtsk)
@@ -690,6 +966,11 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1059| <<vhost_vq_attach_worker>> __vhost_vq_attach_worker(vq, worker);
+ *   - drivers/vhost/vhost.c|1240| <<vhost_dev_set_owner>> __vhost_vq_attach_worker(dev->vqs[i], worker);
+ */
 /* Caller must have device mutex */
 static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 				     struct vhost_worker *worker)
@@ -753,6 +1034,12 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 
 	/* Make sure new vq queue/flush/poll calls see the new worker */
 	synchronize_rcu();
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	/* Make sure whatever was queued gets run */
 	__vhost_worker_flush(old_worker);
 	old_worker->attachment_cnt--;
@@ -813,6 +1100,12 @@ static int vhost_free_worker(struct vhost_dev *dev,
 	 * to zero. Make sure flushes are flushed from the queue before
 	 * freeing.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	mutex_unlock(&worker->mutex);
 
@@ -963,6 +1256,12 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1182| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+ *   - drivers/vhost/vhost.c|2117| <<vhost_set_memory>> newumem = iotlb_alloc();
+ *   - drivers/vhost/vhost.c|2411| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+ */
 static struct vhost_iotlb *iotlb_alloc(void)
 {
 	return vhost_iotlb_alloc(max_iotlb_entries,
@@ -991,11 +1290,33 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1611| <<vhost_net_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/net.c|1828| <<vhost_net_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/scsi.c|2642| <<vhost_scsi_release>> vhost_dev_stop(&vs->dev);
+ *   - drivers/vhost/test.c|175| <<vhost_test_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/test.c|258| <<vhost_test_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/vdpa.c|1486| <<vhost_vdpa_release>> vhost_dev_stop(&v->vdev);
+ *   - drivers/vhost/vsock.c|846| <<vhost_vsock_dev_release>> vhost_dev_stop(&vsock->dev);
+ */
 void vhost_dev_stop(struct vhost_dev *dev)
 {
 	int i;
 
 	for (i = 0; i < dev->nvqs; ++i) {
+		/*
+		 * 在以下调用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 *
+		 * 注释:
+		 * Stop polling a file. After this function returns, it becomes safe to drop the
+		 * file reference. You must also flush afterwards.
+		 */
 		if (dev->vqs[i]->kick && dev->vqs[i]->handle_kick)
 			vhost_poll_stop(&dev->vqs[i]->poll);
 	}
@@ -1053,6 +1374,11 @@ void vhost_dev_cleanup(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_cleanup);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1366| <<vq_memory_access_ok>> else if (log_all && !log_access_ok(log_base,
+ *   - drivers/vhost/vhost.c|2118| <<vq_log_used_access_ok>> return !log_used || log_access_ok(log_base, log_addr,
+ */
 static bool log_access_ok(void __user *log_base, u64 addr, unsigned long sz)
 {
 	u64 a = addr / VHOST_PAGE_SIZE / 8;
@@ -1066,6 +1392,11 @@ static bool log_access_ok(void __user *log_base, u64 addr, unsigned long sz)
 			 (sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1360| <<vq_memory_access_ok>> if (vhost_overflow(map->addr, map->size))
+ *   - drivers/vhost/vhost.c|1787| <<umem_access_ok>> if (vhost_overflow(uaddr, size))
+ */
 /* Make sure 64 bit math will not overflow. */
 static bool vhost_overflow(u64 uaddr, u64 size)
 {
@@ -1078,6 +1409,13 @@ static bool vhost_overflow(u64 uaddr, u64 size)
 	return uaddr > ULONG_MAX - size + 1;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1414| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+ *   - drivers/vhost/vhost.c|2150| <<vq_log_access_ok>> return vq_memory_access_ok(log_base, vq->umem,
+ *             vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
+ *             vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+ */
 /* Caller should have vq mutex and device mutex. */
 static bool vq_memory_access_ok(void __user *log_base, struct vhost_iotlb *umem,
 				int log_all)
@@ -1118,6 +1456,11 @@ static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 
 /* Can we switch to this memory table? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2104| <<vhost_log_access_ok>> return memory_access_ok(dev, dev->umem, 1);
+ *   - drivers/vhost/vhost.c|2231| <<vhost_set_memory>> if (!memory_access_ok(d, newumem, 0))
+ */
 static bool memory_access_ok(struct vhost_dev *d, struct vhost_iotlb *umem,
 			     int log_all)
 {
@@ -1129,6 +1472,19 @@ static bool memory_access_ok(struct vhost_dev *d, struct vhost_iotlb *umem,
 
 		mutex_lock(&d->vqs[i]->mutex);
 		log = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);
+		/*
+		 * 在以下设置vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+		 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+		 * 在以下使用vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+		 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+		 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+		 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+		 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 */
 		/* If ring is inactive, will check when it's enabled. */
 		if (d->vqs[i]->private_data)
 			ok = vq_memory_access_ok(d->vqs[i]->log_base,
@@ -1166,6 +1522,29 @@ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 		if (uaddr)
 			return __copy_to_user(uaddr, from, size);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
 				     ARRAY_SIZE(vq->iotlb_iov),
 				     VHOST_ACCESS_WO);
@@ -1201,6 +1580,29 @@ static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
 		if (uaddr)
 			return __copy_from_user(to, uaddr, size);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
 				     ARRAY_SIZE(vq->iotlb_iov),
 				     VHOST_ACCESS_RO);
@@ -1226,6 +1628,29 @@ static void __user *__vhost_get_user_slow(struct vhost_virtqueue *vq,
 {
 	int ret;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+	 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
 			     ARRAY_SIZE(vq->iotlb_iov),
 			     VHOST_ACCESS_RO);
@@ -1297,6 +1722,16 @@ static inline int vhost_put_used(struct vhost_virtqueue *vq,
 static inline int vhost_put_used_flags(struct vhost_virtqueue *vq)
 
 {
+	/*
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 */
 	return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
 			      &vq->used->flags);
 }
@@ -1670,6 +2105,14 @@ static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)
 	return 0;
 }
 
+/*
+ * 在以下使用vq_access_ok():
+ *   - drivers/vhost/vhost.c|2266| <<vhost_vq_access_ok>> return vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);
+ *   - drivers/vhost/vhost.c|2400| <<vhost_vring_set_addr>> if (!vq_access_ok(vq, vq->num,
+ *                            (void __user *)(unsigned long)a.desc_user_addr,
+ *                            (void __user *)(unsigned long)a.avail_user_addr,
+ *                            (void __user *)(unsigned long)a.used_user_addr))
+ */
 static bool vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 			 vring_desc_t __user *desc,
 			 vring_avail_t __user *avail,
@@ -1697,6 +2140,12 @@ static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 		vq->meta_iotlb[type] = map;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2090| <<vq_meta_prefetch>> return iotlb_access_ok(vq, VHOST_MAP_RO, (u64)(uintptr_t)vq->desc,
+ *   - drivers/vhost/vhost.c|2092| <<vq_meta_prefetch>> iotlb_access_ok(vq, VHOST_MAP_RO, (u64)(uintptr_t)vq->avail,
+ *   - drivers/vhost/vhost.c|2095| <<vq_meta_prefetch>> iotlb_access_ok(vq, VHOST_MAP_WO, (u64)(uintptr_t)vq->used,
+ */
 static bool iotlb_access_ok(struct vhost_virtqueue *vq,
 			    int access, u64 addr, u64 len, int type)
 {
@@ -1748,6 +2197,13 @@ int vq_meta_prefetch(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vq_meta_prefetch);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1860| <<vhost_net_set_features>> !vhost_log_access_ok(&n->dev))
+ *   - drivers/vhost/scsi.c|2517| <<vhost_scsi_set_features>> !vhost_log_access_ok(&vs->dev)) {
+ *   - drivers/vhost/test.c|271| <<vhost_test_set_features>> !vhost_log_access_ok(&n->dev)) {
+ *   - drivers/vhost/vsock.c|903| <<vhost_vsock_set_features>> !vhost_log_access_ok(&vsock->dev)) {
+ */
 /* Can we log writes? */
 /* Caller should have device mutex but not vq mutex */
 bool vhost_log_access_ok(struct vhost_dev *dev)
@@ -1756,6 +2212,11 @@ bool vhost_log_access_ok(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_log_access_ok);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2180| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+ *   - drivers/vhost/vhost.c|2353| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+ */
 static bool vq_log_used_access_ok(struct vhost_virtqueue *vq,
 				  void __user *log_base,
 				  bool log_used,
@@ -1772,21 +2233,84 @@ static bool vq_log_used_access_ok(struct vhost_virtqueue *vq,
 
 /* Verify access for write logging. */
 /* Caller should have vq mutex and device mutex */
+/*
+ * 在以下调用vq_log_access_ok():
+ *   - drivers/vhost/vhost.c|2137| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+ *   - drivers/vhost/vhost.c|2585| <<vhost_dev_ioctl>> if (vq->private_data && !vq_log_access_ok(vq, base))
+ *
+ * 判断log_base合法性的唯一入口!
+ */
 static bool vq_log_access_ok(struct vhost_virtqueue *vq,
 			     void __user *log_base)
 {
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	return vq_memory_access_ok(log_base, vq->umem,
 				   vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
 		vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
 }
 
+/*
+ * 在以下调用vhost_vq_access_ok():
+ *   - drivers/vhost/net.c|1726| <<vhost_net_set_backend>> if (!vhost_vq_access_ok(vq)) {
+ *   - drivers/vhost/scsi.c|2788| <<vhost_scsi_set_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+ *   - drivers/vhost/scsi.c|2976| <<vhost_scsi_clear_endpoint>> if (!vhost_vq_access_ok(&vs->vqs[index].vq)) {
+ *   - drivers/vhost/test.c|198| <<vhost_test_run>> if (!vhost_vq_access_ok(&n->vqs[index])) {
+ *   - drivers/vhost/test.c|304| <<vhost_test_set_backend>> if (!vhost_vq_access_ok(vq)) {
+ *   - drivers/vhost/vhost.c|525| <<vhost_vq_is_setup>> return vq->avail && vq->desc && vq->used && vhost_vq_access_ok(vq);
+ *   - drivers/vhost/vsock.c|645| <<vhost_vsock_start>> if (!vhost_vq_access_ok(vq)) {
+ */
 /* Can we start vq? */
 /* Caller should have vq mutex and device mutex */
 bool vhost_vq_access_ok(struct vhost_virtqueue *vq)
 {
+	/*
+	 * 在以下调用vq_log_access_ok():
+	 *   - drivers/vhost/vhost.c|2137| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2585| <<vhost_dev_ioctl>> if (vq->private_data && !vq_log_access_ok(vq, base))
+	 *
+	 * 判断log_base合法性的唯一入口!
+	 *
+	 *
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	if (!vq_log_access_ok(vq, vq->log_base))
 		return false;
 
+	/*
+	 * 在以下使用vq_access_ok():
+	 *   - drivers/vhost/vhost.c|2266| <<vhost_vq_access_ok>> return vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);
+	 *   - drivers/vhost/vhost.c|2400| <<vhost_vring_set_addr>> if (!vq_access_ok(vq, vq->num,
+	 *                            (void __user *)(unsigned long)a.desc_user_addr,
+	 *                            (void __user *)(unsigned long)a.avail_user_addr,
+	 *                            (void __user *)(unsigned long)a.used_user_addr))
+	 */
 	return vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);
 }
 EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
@@ -1794,6 +2318,14 @@ EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
 	struct vhost_memory mem, *newmem;
+	/*
+	 * struct vhost_memory_region {
+	 *     __u64 guest_phys_addr;
+	 *     __u64 memory_size; // bytes
+	 *     __u64 userspace_addr;
+	 *     __u64 flags_padding; // No flags are currently specified.
+	 * };
+	 */
 	struct vhost_memory_region *region;
 	struct vhost_iotlb *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
@@ -1879,6 +2411,10 @@ static long vhost_vring_set_num(struct vhost_dev *d,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2401| <<vhost_vring_set_num_addr(VHOST_SET_VRING_ADDR)>> r = vhost_vring_set_addr(d, vq, argp);
+ */
 static long vhost_vring_set_addr(struct vhost_dev *d,
 				 struct vhost_virtqueue *vq,
 				 void __user *argp)
@@ -1909,12 +2445,33 @@ static long vhost_vring_set_addr(struct vhost_dev *d,
 	 * If it is not, we don't as size might not have been setup.
 	 * We will verify when backend is configured. */
 	if (vq->private_data) {
+		/*
+		 * 在以下使用vq_access_ok():
+		 *   - drivers/vhost/vhost.c|2266| <<vhost_vq_access_ok>> return vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);
+		 *   - drivers/vhost/vhost.c|2400| <<vhost_vring_set_addr>> if (!vq_access_ok(vq, vq->num,
+		 *                            (void __user *)(unsigned long)a.desc_user_addr,
+		 *                            (void __user *)(unsigned long)a.avail_user_addr,
+		 *                            (void __user *)(unsigned long)a.used_user_addr))
+		 */
 		if (!vq_access_ok(vq, vq->num,
 			(void __user *)(unsigned long)a.desc_user_addr,
 			(void __user *)(unsigned long)a.avail_user_addr,
 			(void __user *)(unsigned long)a.used_user_addr))
 			return -EINVAL;
 
+		/*
+		 * 在以下设置vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+		 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+		 * 在以下使用vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+		 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+		 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+		 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+		 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 */
 		/* Also validate log access for used ring if enabled. */
 		if (!vq_log_used_access_ok(vq, vq->log_base,
 				a.flags & (0x1 << VHOST_VRING_F_LOG),
@@ -1922,9 +2479,28 @@ static long vhost_vring_set_addr(struct vhost_dev *d,
 			return -EINVAL;
 	}
 
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
 	vq->desc = (void __user *)(unsigned long)a.desc_user_addr;
 	vq->avail = (void __user *)(unsigned long)a.avail_user_addr;
+	/*
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	vq->log_addr = a.log_guest_addr;
 	vq->used = (void __user *)(unsigned long)a.used_user_addr;
 
@@ -2003,6 +2579,19 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 		vq->avail_idx = vq->last_avail_idx;
 		break;
 	case VHOST_GET_VRING_BASE:
+		/*
+		 * 在以下使用vhost_virtqueue->last_used_idx:
+		 *   - drivers/vhost/vhost.c|543| <<vhost_vq_reset>> vq->last_used_idx = 0;
+		 *   - drivers/vhost/vhost.c|1717| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx), &vq->used->idx);
+		 *   - drivers/vhost/vhost.c|2504| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+		 *   - drivers/vhost/vhost.c|2518| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+		 *   - drivers/vhost/vhost.c|3147| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+		 *   - drivers/vhost/vhost.c|3618| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+		 *   - drivers/vhost/vhost.c|3649| <<__vhost_add_used_n>> old = vq->last_used_idx;
+		 *   - drivers/vhost/vhost.c|3650| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+		 *   - drivers/vhost/vhost.c|3686| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+		 *   - drivers/vhost/vhost.c|3787| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+		 */
 		s.index = idx;
 		if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED))
 			s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
@@ -2075,6 +2664,18 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 		r = -ENOIOCTLCMD;
 	}
 
+	/*
+	 * 在以下调用vhost_poll_stop():
+	 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+	 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+	 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+	 *
+	 * 注释:
+	 * Stop polling a file. After this function returns, it becomes safe to drop the
+	 * file reference. You must also flush afterwards.
+	 */
 	if (pollstop && vq->handle_kick)
 		vhost_poll_stop(&vq->poll);
 
@@ -2083,6 +2684,17 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 	if (filep)
 		fput(filep);
 
+	/*
+	 * 在以下调用vhost_poll_start():
+	 *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+	 *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.h|56| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+	 *
+	 * 注释:
+	 * Start polling a file. We add ourselves to file's wait queue. The caller must
+	 * keep a reference to a file until after vhost_poll_stop is called.
+	 */
 	if (pollstart && vq->handle_kick)
 		r = vhost_poll_start(&vq->poll, vq->kick);
 
@@ -2121,6 +2733,14 @@ int vhost_init_device_iotlb(struct vhost_dev *d)
 }
 EXPORT_SYMBOL_GPL(vhost_init_device_iotlb);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/scsi.c|2648| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+ *   - drivers/vhost/test.c|361| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/vdpa.c|888| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+ *   - drivers/vhost/vsock.c|947| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp); 
+ */
 /* Caller must have device mutex */
 long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
@@ -2158,6 +2778,19 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 			void __user *base = (void __user *)(unsigned long)p;
 			vq = d->vqs[i];
 			mutex_lock(&vq->mutex);
+			/*
+			 * 在以下设置vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+			 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+			 * 在以下使用vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+			 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+			 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+			 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+			 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+			 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+			 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+			 */
 			/* If ring is inactive, will check when it's enabled. */
 			if (vq->private_data && !vq_log_access_ok(vq, base))
 				r = -EFAULT;
@@ -2197,6 +2830,10 @@ EXPORT_SYMBOL_GPL(vhost_dev_ioctl);
  * (instruction directly accesses the data, with an exception table entry
  * returning -EFAULT). See Documentation/arch/x86/exception-tables.rst.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2370| <<log_write>> r = set_bit_to_user(bit, (void __user *)(unsigned long )log);
+ */
 static int set_bit_to_user(int nr, void __user *addr)
 {
 	unsigned long log = (unsigned long)addr;
@@ -2216,6 +2853,12 @@ static int set_bit_to_user(int nr, void __user *addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+ *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+ *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+ */
 static int log_write(void __user *log_base,
 		     u64 write_address, u64 write_length)
 {
@@ -2242,6 +2885,11 @@ static int log_write(void __user *log_base,
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2457| <<log_used>> ret = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+ *   - drivers/vhost/vhost.c|2476| <<vhost_log_write>> r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+ */
 static int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)
 {
 	struct vhost_iotlb *umem = vq->umem;
@@ -2262,6 +2910,24 @@ static int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)
 			start = max(u->addr, hva);
 			end = min(u->addr - 1 + u->size, hva - 1 + len);
 			l = end - start + 1;
+			/*
+			 * 在以下设置vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+			 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+			 * 在以下使用vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+			 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+			 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+			 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+			 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+			 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+			 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+			 *
+			 * called by:
+			 *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+			 *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+			 *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+			 */
 			r = log_write(vq->log_base,
 				      u->start + start - u->addr,
 				      l);
@@ -2281,20 +2947,80 @@ static int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+ *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+ *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+ *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+ */
 static int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)
 {
 	struct iovec *iov = vq->log_iov;
 	int i, ret;
 
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 *
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	if (!vq->iotlb)
 		return log_write(vq->log_base, vq->log_addr + used_offset, len);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+	 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, (uintptr_t)vq->used + used_offset,
 			     len, iov, 64, VHOST_ACCESS_WO);
 	if (ret < 0)
 		return ret;
 
 	for (i = 0; i < ret; i++) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2457| <<log_used>> ret = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+		 *   - drivers/vhost/vhost.c|2476| <<vhost_log_write>> r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+		 */
 		ret = log_write_hva(vq,	(uintptr_t)iov[i].iov_base,
 				    iov[i].iov_len);
 		if (ret)
@@ -2304,6 +3030,10 @@ static int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1352| <<handle_rx>> vhost_log_write(vq, vq_log, log, vhost_len, vq->iov, in);
+ */
 int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 		    unsigned int log_num, u64 len, struct iovec *iov, int count)
 {
@@ -2314,6 +3044,11 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 
 	if (vq->iotlb) {
 		for (i = 0; i < count; i++) {
+			/*
+			 * called by:
+			 *   - drivers/vhost/vhost.c|2457| <<log_used>> ret = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+			 *   - drivers/vhost/vhost.c|2476| <<vhost_log_write>> r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+			 */
 			r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
 					  iov[i].iov_len);
 			if (r < 0)
@@ -2324,6 +3059,24 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 
 	for (i = 0; i < log_num; ++i) {
 		u64 l = min(log[i].len, len);
+		/*
+		 * 在以下设置vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+		 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+		 * 在以下使用vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+		 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+		 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+		 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+		 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 *
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 */
 		r = log_write(vq->log_base, log[i].addr, l);
 		if (r < 0)
 			return r;
@@ -2340,16 +3093,40 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 }
 EXPORT_SYMBOL_GPL(vhost_log_write);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+ */
 static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 {
 	void __user *used;
 	if (vhost_put_used_flags(vq))
 		return -EFAULT;
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		/* Make sure the flag is seen before log. */
 		smp_wmb();
 		/* Log used flag write. */
 		used = &vq->used->flags;
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		log_used(vq, (used - (void __user *)vq->used),
 			 sizeof vq->used->flags);
 		if (vq->log_ctx)
@@ -2358,16 +3135,38 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|3387| <<vhost_enable_notify>> r = vhost_update_avail_event(vq);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 {
 	if (vhost_put_avail_event(vq))
 		return -EFAULT;
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		void __user *used;
 		/* Make sure the event is seen before log. */
 		smp_wmb();
 		/* Log avail event write */
 		used = vhost_avail_event(vq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		log_used(vq, (used - (void __user *)vq->used),
 			 sizeof *vhost_avail_event(vq));
 		if (vq->log_ctx)
@@ -2376,6 +3175,14 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -2387,9 +3194,23 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 
 	vhost_init_is_le(vq);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+	 *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+	 *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+	 */
 	r = vhost_update_used_flags(vq);
 	if (r)
 		goto err;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|545| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3102| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3623| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3724| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|3726| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	vq->signalled_used_valid = false;
 	if (!vq->iotlb &&
 	    !access_ok(&vq->used->idx, sizeof vq->used->idx)) {
@@ -2411,6 +3232,29 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+ *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+ *             VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+ *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+ *             VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+ *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+ *             VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+ *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+ *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+ *             VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+ *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+ *             iov + iov_count, iov_size - iov_count, access);
+ *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+ *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+ *             iov + iov_count, iov_size - iov_count, access);
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
@@ -2459,6 +3303,11 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 /* Each buffer in the virtqueues is actually a chain of descriptors.  This
  * function returns the next descriptor in the chain,
  * or -1U if we're at the end. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2796| <<get_indirect>> } while ((i = next_desc(vq, &desc)) != -1);
+ *   - drivers/vhost/vhost.c|2961| <<vhost_get_vq_desc>> } while ((i = next_desc(vq, &desc)) != -1);
+ */
 static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 {
 	unsigned int next;
@@ -2472,6 +3321,10 @@ static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2841| <<vhost_get_vq_desc>> ret = get_indirect(vq, iov, iov_size,
+ */
 static int get_indirect(struct vhost_virtqueue *vq,
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
@@ -2493,6 +3346,29 @@ static int get_indirect(struct vhost_virtqueue *vq,
 		return -EINVAL;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+	 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
 			     UIO_MAXIOV, VHOST_ACCESS_RO);
 	if (unlikely(ret < 0)) {
@@ -2534,6 +3410,29 @@ static int get_indirect(struct vhost_virtqueue *vq,
 		else
 			access = VHOST_ACCESS_RO;
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2573,6 +3472,25 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+ *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+ *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+ *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+ *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2637,6 +3555,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			return -EFAULT;
 		}
 		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {
+			/*
+			 * 只在此处调用
+			 */
 			ret = get_indirect(vq, iov, iov_size,
 					   out_num, in_num,
 					   log, log_num, &desc);
@@ -2653,6 +3574,29 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2688,11 +3632,32 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 
 	/* Assume notifications from guest are disabled at this point,
 	 * if they aren't we would need to update avail_event index. */
+	/*
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 */
 	BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
 	return head;
 }
 EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *
+ * 把vhost_virtqueue->last_avail_idx减少n
+ */
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
@@ -2702,6 +3667,14 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|851| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|3122| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vhost.h|214| <<vhost_add_used_and_signal>> int vhost_add_used(struct vhost_virtqueue *, unsigned int head, int len);
+ *   - drivers/vhost/vsock.c|235| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(*hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|581| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2709,10 +3682,20 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 		cpu_to_vhost32(vq, len)
 	};
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2997| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+	 *   - drivers/vhost/vhost.c|3140| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+	 */
 	return vhost_add_used_n(vq, &heads, 1);
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|3043| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|3049| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2721,15 +3704,46 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	u16 old, new;
 	int start;
 
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1717| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx), &vq->used->idx);
+	 *   - drivers/vhost/vhost.c|2504| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|2518| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|3147| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|3618| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3649| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|3650| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|3686| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3787| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	start = vq->last_used_idx & (vq->num - 1);
 	used = vq->used->ring + start;
 	if (vhost_put_used(vq, heads, start, count)) {
 		vq_err(vq, "Failed to write used");
 		return -EFAULT;
 	}
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		/* Make sure data is seen before log. */
 		smp_wmb();
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		/* Log used ring entry write. */
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
@@ -2740,11 +3754,30 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	 * used index might wrap around. If that happens, invalidate
 	 * signalled_used index we stored. TODO: make sure driver
 	 * signals at least once in 2^16 and remove this. */
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|3622| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|3723| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|3725| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 *
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|545| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3102| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3623| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3724| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|3726| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
 		vq->signalled_used_valid = false;
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2997| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|3140| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
@@ -2752,15 +3785,38 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 {
 	int start, n, r;
 
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1717| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx), &vq->used->idx);
+	 *   - drivers/vhost/vhost.c|2504| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|2518| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|3147| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|3618| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3649| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|3650| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|3686| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3787| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	start = vq->last_used_idx & (vq->num - 1);
 	n = vq->num - start;
 	if (n < count) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|3043| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+		 *   - drivers/vhost/vhost.c|3049| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+		 */
 		r = __vhost_add_used_n(vq, heads, n);
 		if (r < 0)
 			return r;
 		heads += n;
 		count -= n;
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|3043| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+	 *   - drivers/vhost/vhost.c|3049| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+	 */
 	r = __vhost_add_used_n(vq, heads, count);
 
 	/* Make sure buffer is written before we update index. */
@@ -2769,9 +3825,27 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		vq_err(vq, "Failed to increment used idx");
 		return -EFAULT;
 	}
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		/* Make sure used idx is seen before log. */
 		smp_wmb();
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		/* Log used index update. */
 		log_used(vq, offsetof(struct vring_used, idx),
 			 sizeof vq->used->idx);
@@ -2782,6 +3856,12 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|3147| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ *
+ * 返回true的话caller才会signal
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2804,11 +3884,42 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		}
 		return !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));
 	}
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|3622| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|3723| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|3725| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	old = vq->signalled_used;
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|545| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3102| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3623| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3724| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|3726| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	v = vq->signalled_used_valid;
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1717| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx), &vq->used->idx);
+	 *   - drivers/vhost/vhost.c|2504| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|2518| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|3147| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|3618| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3649| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|3650| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|3686| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3787| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	new = vq->signalled_used = vq->last_used_idx;
 	vq->signalled_used_valid = true;
 
+	/*
+	 * 如果vq->signalled_used_valid失效了, 一定发signal
+	 */
 	if (unlikely(!v))
 		return true;
 
@@ -2816,38 +3927,108 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		vq_err(vq, "Failed to get used event idx");
 		return true;
 	}
+	/*
+	 * 在以下调用vring_need_event():
+	 *   - drivers/vhost/vhost.c|3869| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+	 *   - drivers/vhost/vringh.c|535| <<vhost_notify>> notify = vring_need_event(used_event,
+	 *             vrh->last_used_idx + vrh->completed, vrh->last_used_idx);
+	 *   - drivers/virtio/virtio_ring.c|735| <<virtqueue_kick_prepare_split>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,
+	 *             vring_avail_event(&vq->split.vring)), new, old);
+	 *   - drivers/virtio/virtio_ring.c|1646| <<virtqueue_kick_prepare_packed>> needs_kick = vring_need_event(event_idx, new, old);
+	 *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+	 *             guest.avail_idx, guest.kicked_avail_idx);
+	 *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+	 *             host.used_idx, host.called_used_idx);
+	 *
+	 * 如果new距离event_idx的距离更短(比距离old), 发signal.
+	 * old是上次理论上发signal的index.
+	 * 如果还没处理到old, 说明这次也不用发.
+	 */
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+ *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 /* This actually signals the guest, using eventfd. */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
+	/*
+	 * 只在此处调用vhost_notify()
+	 */
 	/* Signal the Guest tell them we used something up. */
 	if (vq->call_ctx.ctx && vhost_notify(dev, vq))
 		eventfd_signal(vq->call_ctx.ctx);
 }
 EXPORT_SYMBOL_GPL(vhost_signal);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|969| <<handle_tx_zerocopy>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|529| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|969| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1331| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+ *   - drivers/vhost/scsi.c|1430| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|87| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 /* And here's the combo meal deal.  Supersize me! */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
 {
 	vhost_add_used(vq, head, len);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	vhost_signal(dev, vq);
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|376| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|460| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+ */
 /* multi-buffer version of vhost_add_used_and_signal */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
 {
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2997| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+	 *   - drivers/vhost/vhost.c|3140| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+	 */
 	vhost_add_used_n(vq, heads, count);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	vhost_signal(dev, vq);
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|515| <<vhost_net_busy_poll_try_queue>> if (!vhost_vq_avail_empty(&net->dev, vq)) {
+ *   - drivers/vhost/net.c|557| <<vhost_net_busy_poll>> !vhost_vq_avail_empty(&net->dev, rvq)) ||
+ *   - drivers/vhost/net.c|558| <<vhost_net_busy_poll>> !vhost_vq_avail_empty(&net->dev, tvq))
+ *   - drivers/vhost/net.c|695| <<tx_can_batch>> !vhost_vq_avail_empty(vq->dev, vq);
+ */
 /* return true if we're sure that avaiable ring is empty */
 bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
@@ -2863,22 +4044,65 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|517| <<vhost_net_busy_poll_try_queue>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|569| <<vhost_net_busy_poll>> vhost_enable_notify(&net->dev, rvq);
+ *   - drivers/vhost/net.c|812| <<handle_tx_copy>> } else if (unlikely(vhost_enable_notify(&net->dev,
+ *   - drivers/vhost/net.c|928| <<handle_tx_zerocopy>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1251| <<handle_rx>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|683| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|1265| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|70| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|123| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|158| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|561| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 /* OK, now we need to know about added descriptors. */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
 
+	/*
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 */
 	if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
 		return false;
 	vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+		 */
 		r = vhost_update_used_flags(vq);
 		if (r) {
+			/*
+			 * 在以下使用vhost_virtqueue->used_flags:
+			 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+			 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+			 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+			 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+			 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+			 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+			 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+			 */
 			vq_err(vq, "Failed to enable notification at %p: %d\n",
 			       &vq->used->flags, r);
 			return false;
 		}
 	} else {
+		/*
+		 * 只在此处调用
+		 */
 		r = vhost_update_avail_event(vq);
 		if (r) {
 			vq_err(vq, "Failed to update avail event index at %p: %d\n",
@@ -2899,15 +4123,51 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|518| <<vhost_net_busy_poll_try_queue>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|541| <<vhost_net_busy_poll>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|814| <<handle_tx_copy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|929| <<handle_tx_zerocopy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1025| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1224| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1254| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|655| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1266| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1390| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1845| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|58| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|71| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|107| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|159| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|524| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|562| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 /* We don't need to be notified again. */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	int r;
 
+	/*
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 */
 	if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
 		return;
 	vq->used_flags |= VRING_USED_F_NO_NOTIFY;
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+		 */
 		r = vhost_update_used_flags(vq);
 		if (r)
 			vq_err(vq, "Failed to disable notification at %p: %d\n",
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index bb75a292d..c36d6e949 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -110,19 +110,76 @@ struct vhost_virtqueue {
 	/* Last index we used.
 	 * Values are limited to 0x7fff, and the high bit is used as
 	 * a wrap counter when using VIRTIO_F_RING_PACKED. */
+	/*
+	 * 在以下使用vhost_virtqueue->last_used_idx:
+	 *   - drivers/vhost/vhost.c|543| <<vhost_vq_reset>> vq->last_used_idx = 0;
+	 *   - drivers/vhost/vhost.c|1717| <<vhost_put_used_idx>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx), &vq->used->idx);
+	 *   - drivers/vhost/vhost.c|2504| <<vhost_vring_ioctl(VHOST_SET_VRING_BASE)>> vq->last_used_idx = (s.num >> 16) & 0xffff;
+	 *   - drivers/vhost/vhost.c|2518| <<vhost_vring_ioctl(VHOST_GET_VRING_BASE)>> s.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);
+	 *   - drivers/vhost/vhost.c|3147| <<vhost_vq_init_access>> vq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);
+	 *   - drivers/vhost/vhost.c|3618| <<__vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3649| <<__vhost_add_used_n>> old = vq->last_used_idx;
+	 *   - drivers/vhost/vhost.c|3650| <<__vhost_add_used_n>> new = (vq->last_used_idx += count);
+	 *   - drivers/vhost/vhost.c|3686| <<vhost_add_used_n>> start = vq->last_used_idx & (vq->num - 1);
+	 *   - drivers/vhost/vhost.c|3787| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	u16 last_used_idx;
 
+	/*
+	 * 在以下使用vhost_virtqueue->used_flags:
+	 *   - drivers/vhost/vhost.c|546| <<vhost_vq_reset>> vq->used_flags = 0;
+	 *   - drivers/vhost/vhost.c|1685| <<vhost_put_used_flags>> return vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),
+	 *   - drivers/vhost/vhost.c|3523| <<vhost_get_vq_desc>> BUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));
+	 *   - drivers/vhost/vhost.c|3854| <<vhost_enable_notify>> if (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))
+	 *   - drivers/vhost/vhost.c|3856| <<vhost_enable_notify>> vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
+	 *   - drivers/vhost/vhost.c|3919| <<vhost_disable_notify>> if (vq->used_flags & VRING_USED_F_NO_NOTIFY)
+	 *   - drivers/vhost/vhost.c|3921| <<vhost_disable_notify>> vq->used_flags |= VRING_USED_F_NO_NOTIFY;
+	 */
 	/* Used flags */
 	u16 used_flags;
 
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->signalled_used = 0;
+	 *   - drivers/vhost/vhost.c|3622| <<__vhost_add_used_n>> if (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))
+	 *   - drivers/vhost/vhost.c|3723| <<vhost_notify>> old = vq->signalled_used;
+	 *   - drivers/vhost/vhost.c|3725| <<vhost_notify>> new = vq->signalled_used = vq->last_used_idx;
+	 */
 	/* Last used index value we have signalled on */
 	u16 signalled_used;
 
+	/*
+	 * 在以下使用vhost_virtqueue->signalled_used_valid:
+	 *   - drivers/vhost/vhost.c|545| <<vhost_vq_reset>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3102| <<vhost_vq_init_access>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3623| <<__vhost_add_used_n>> vq->signalled_used_valid = false;
+	 *   - drivers/vhost/vhost.c|3724| <<vhost_notify>> v = vq->signalled_used_valid;
+	 *   - drivers/vhost/vhost.c|3726| <<vhost_notify>> vq->signalled_used_valid = true;
+	 */
 	/* Last used index value we have signalled on */
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	bool log_used;
+	/*
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	u64 log_addr;
 
 	struct iovec iov[UIO_MAXIOV];
@@ -133,9 +190,32 @@ struct vhost_virtqueue {
 	struct vhost_iotlb *umem;
 	struct vhost_iotlb *iotlb;
 	void *private_data;
+	/*
+	 * 在以下使用vhost_virtqueue->acked_features:
+	 *   - drivers/vhost/net.c|1886| <<vhost_net_set_features>> n->vqs[i].vq.acked_features = features;
+	 *   - drivers/vhost/scsi.c|3460| <<vhost_scsi_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/test.c|277| <<vhost_test_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/vdpa.c|511| <<vhost_vdpa_set_features>> vq->acked_features = actual_features;
+	 *   - drivers/vhost/vhost.c|594| <<vhost_vq_reset>> vq->acked_features = 0;
+	 *   - drivers/vhost/vhost.h|451| <<vhost_has_feature>> return vq->acked_features & (1ULL << bit);
+	 *   - drivers/vhost/vsock.c|917| <<vhost_vsock_set_features>> vq->acked_features = features;
+	 */
 	u64 acked_features;
 	u64 acked_backend_features;
 	/* Log write descriptors */
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	void __user *log_base;
 	struct vhost_log *log;
 	struct iovec log_iov[64];
@@ -147,6 +227,16 @@ struct vhost_virtqueue {
 	/* Ring endianness requested by userspace for cross-endian support. */
 	bool user_be;
 #endif
+	/*
+	 * 在以下使用vhost_virtqueue->busyloop_timeout:
+	 *   - drivers/vhost/net.c|577| <<vhost_net_busy_poll>> busyloop_timeout = poll_rx ?
+	 *             rvq->busyloop_timeout: tvq->busyloop_timeout;
+	 *   - drivers/vhost/net.c|638| <<vhost_net_tx_get_vq_desc>> if (r == tvq->num && tvq->busyloop_timeout) {
+	 *   - drivers/vhost/net.c|1100| <<vhost_net_rx_peek_head_len>> if (!len && rvq->busyloop_timeout) {
+	 *   - drivers/vhost/vhost.c|550| <<vhost_vq_reset>> vq->busyloop_timeout = 0;
+	 *   - drivers/vhost/vhost.c|2369| <<vhost_vring_ioctl(VHOST_SET_VRING_BUSYLOOP_TIMEOUT)>> vq->busyloop_timeout = s.num;
+	 *   - drivers/vhost/vhost.c|2373| <<vhost_vring_ioctl(VHOST_GET_VRING_BUSYLOOP_TIMEOUT)>> s.num = vq->busyloop_timeout;
+	 */
 	u32 busyloop_timeout;
 };
 
@@ -262,6 +352,24 @@ enum {
 			 (1ULL << VIRTIO_F_VERSION_1)
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+ *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+ *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+ *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+ *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+ *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+ *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+ *
+ * 就是设置vhost_virtqueue->private_data.
+ */
 /**
  * vhost_vq_set_backend - Set backend.
  *
@@ -273,9 +381,47 @@ enum {
 static inline void vhost_vq_set_backend(struct vhost_virtqueue *vq,
 					void *private_data)
 {
+	/*
+	 * vhost-scsi对vhost_vq_get_backend()的使用:
+	 *   - drivers/vhost/scsi.c|725| <<vhost_scsi_do_evt_work>> if (!vhost_vq_get_backend(vq)) {
+	 *   - drivers/vhost/scsi.c|1447| <<vhost_scsi_get_req>> vs_tpg = vhost_vq_get_backend(vq);
+	 *   - drivers/vhost/scsi.c|1494| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+	 *   - drivers/vhost/scsi.c|1970| <<vhost_scsi_ctl_handle_vq>> if (!vhost_vq_get_backend(vq))
+	 *   - drivers/vhost/scsi.c|2181| <<vhost_scsi_evt_handle_kick>> if (!vhost_vq_get_backend(vq))
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_set_endpoint>> if (!vhost_vq_get_backend(&vs->vqs[i].vq))
+	 *   - drivers/vhost/scsi.c|3009| <<vhost_scsi_do_plug>> if (!vhost_vq_get_backend(vq))
+	 */
 	vq->private_data = private_data;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|453| <<vhost_net_disable_vq>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/net.c|478| <<vhost_net_enable_vq>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|591| <<vhost_net_busy_poll>> sock = vhost_vq_get_backend(rvq);
+ *   - drivers/vhost/net.c|656| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|658| <<vhost_net_tx_get_vq_desc>> vhost_vq_get_backend(tvq),
+ *   - drivers/vhost/net.c|755| <<vhost_net_build_xdp>> struct socket *sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1067| <<handle_tx>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1302| <<handle_rx>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1571| <<vhost_net_stop_vq>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1737| <<vhost_net_set_backend>> oldsock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|725| <<vhost_scsi_do_evt_work>> if (!vhost_vq_get_backend(vq)) {
+ *   - drivers/vhost/scsi.c|1447| <<vhost_scsi_get_req>> vs_tpg = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|1494| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|1970| <<vhost_scsi_ctl_handle_vq>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/scsi.c|2181| <<vhost_scsi_evt_handle_kick>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_set_endpoint>> if (!vhost_vq_get_backend(&vs->vqs[i].vq))
+ *   - drivers/vhost/scsi.c|3009| <<vhost_scsi_do_plug>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/test.c|52| <<handle_vq>> private = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|152| <<vhost_test_stop_vq>> private = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|210| <<vhost_test_run>> oldpriv = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|322| <<vhost_test_set_backend>> backend = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/vhost.c|979| <<__vhost_vq_attach_worker>> if (!vhost_vq_get_backend(vq) && !vq->kick) {
+ *   - drivers/vhost/vsock.c|100| <<vhost_transport_do_send_pkt>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/vsock.c|527| <<vhost_vsock_handle_tx_kick>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/vsock.c|650| <<vhost_vsock_start>> if (!vhost_vq_get_backend(vq)) {
+ */
 /**
  * vhost_vq_get_backend - Get backend.
  *
@@ -289,8 +435,39 @@ static inline void *vhost_vq_get_backend(struct vhost_virtqueue *vq)
 	return vq->private_data;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1186| <<handle_rx>> vq_log = unlikely(vhost_has_feature(vq, VHOST_F_LOG_ALL)) ? 
+ *   - drivers/vhost/net.c|1188| <<handle_rx>> mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
+ *   - drivers/vhost/scsi.c|1275| <<vhost_scsi_handle_vq>> bool t10_pi = vhost_has_feature(vq, VIRTIO_SCSI_F_T10_PI);
+ *   - drivers/vhost/scsi.c|2447| <<vhost_scsi_do_plug>> if (vhost_has_feature(vq, VIRTIO_SCSI_F_HOTPLUG))
+ *   - drivers/vhost/vdpa.c|701| <<vhost_vdpa_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {
+ *   - drivers/vhost/vdpa.c|740| <<vhost_vdpa_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {
+ *   - drivers/vhost/vhost.c|111| <<vhost_init_is_le>> vq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1) || !vq->user_be;
+ *   - drivers/vhost/vhost.c|131| <<vhost_init_is_le>> vq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1)
+ *   - drivers/vhost/vhost.c|513| <<vhost_get_avail_size>> vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|522| <<vhost_get_used_size>> vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|1131| <<memory_access_ok>> log = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);
+ *   - drivers/vhost/vhost.c|1779| <<vq_log_access_ok>> vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/vhost.c|1992| <<vhost_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {
+ *   - drivers/vhost/vhost.c|2007| <<vhost_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED))
+ *   - drivers/vhost/vhost.c|2814| <<vhost_notify>> if (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&
+ *   - drivers/vhost/vhost.c|2818| <<vhost_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vhost.c|2902| <<vhost_enable_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vhost.c|2938| <<vhost_disable_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ */
 static inline bool vhost_has_feature(struct vhost_virtqueue *vq, int bit)
 {
+	/*
+	 * 在以下使用vhost_virtqueue->acked_features:
+	 *   - drivers/vhost/net.c|1886| <<vhost_net_set_features>> n->vqs[i].vq.acked_features = features;
+	 *   - drivers/vhost/scsi.c|3460| <<vhost_scsi_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/test.c|277| <<vhost_test_set_features>> vq->acked_features = features;
+	 *   - drivers/vhost/vdpa.c|511| <<vhost_vdpa_set_features>> vq->acked_features = actual_features;
+	 *   - drivers/vhost/vhost.c|594| <<vhost_vq_reset>> vq->acked_features = 0;
+	 *   - drivers/vhost/vhost.h|451| <<vhost_has_feature>> return vq->acked_features & (1ULL << bit);
+	 *   - drivers/vhost/vsock.c|917| <<vhost_vsock_set_features>> vq->acked_features = features;
+	 */
 	return vq->acked_features & (1ULL << bit);
 }
 
diff --git a/drivers/vhost/vringh.c b/drivers/vhost/vringh.c
index 73e153f9b..11b9f8e39 100644
--- a/drivers/vhost/vringh.c
+++ b/drivers/vhost/vringh.c
@@ -1477,6 +1477,14 @@ EXPORT_SYMBOL(vringh_set_iotlb);
  * When you don't have to use riov and wiov anymore, you should clean up them
  * calling vringh_kiov_cleanup() to release the memory, even on error!
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2388| <<mlx5_cvq_kick_handler>> err = vringh_getdesc_iotlb(&cvq->vring, &cvq->riov, &cvq->wiov, &cvq->head,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|123| <<vdpasim_blk_handle_req>> ret = vringh_getdesc_iotlb(&vq->vring, &vq->out_iov, &vq->in_iov,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|144| <<vdpasim_handle_cvq>> err = vringh_getdesc_iotlb(&cvq->vring, &cvq->in_iov,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|220| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&txq->vring, &txq->out_iov, NULL,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|240| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&rxq->vring, NULL, &rxq->in_iov,
+ */
 int vringh_getdesc_iotlb(struct vringh *vrh,
 			 struct vringh_kiov *riov,
 			 struct vringh_kiov *wiov,
diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
index 802153e23..c16c091d9 100644
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@ -124,6 +124,25 @@ vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 			break;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
 					 &out, &in, NULL, NULL);
 		if (head < 0) {
@@ -247,6 +266,14 @@ vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 			virtio_transport_consume_skb_sent(skb, true);
 		}
 	} while(likely(!vhost_exceeds_weight(vq, ++pkts, total_len)));
+	/*
+	 * 在以下调用vhost_signal():
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	if (added)
 		vhost_signal(&vsock->dev, vq);
 
@@ -289,6 +316,15 @@ vhost_transport_send_pkt(struct sk_buff *skb)
 		atomic_inc(&vsock->queued_replies);
 
 	virtio_vsock_skb_queue_tail(&vsock->send_pkt_queue, skb);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
 
 	rcu_read_unlock();
@@ -506,6 +542,25 @@ static void vhost_vsock_handle_tx_kick(struct vhost_work *work)
 			goto no_more_replies;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
 					 &out, &in, NULL, NULL);
 		if (head < 0)
@@ -545,6 +600,14 @@ static void vhost_vsock_handle_tx_kick(struct vhost_work *work)
 	} while(likely(!vhost_exceeds_weight(vq, ++pkts, total_len)));
 
 no_more_replies:
+	/*
+	 * 在以下使用vhost_signal():
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	if (added)
 		vhost_signal(&vsock->dev, vq);
 
@@ -586,6 +649,14 @@ static int vhost_vsock_start(struct vhost_vsock *vsock)
 
 		if (!vhost_vq_get_backend(vq)) {
 			vhost_vq_set_backend(vq, vsock);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+			 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+			 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+			 */
 			ret = vhost_vq_init_access(vq);
 			if (ret)
 				goto err_vq;
@@ -597,6 +668,15 @@ static int vhost_vsock_start(struct vhost_vsock *vsock)
 	/* Some packets may have been queued before the device was started,
 	 * let's kick the send worker to send them.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
 
 	mutex_unlock(&vsock->dev.mutex);
@@ -678,6 +758,22 @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)
 	vsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;
 	vsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
 		       UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT,
 		       VHOST_VSOCK_WEIGHT, true, NULL);
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index b9095751e..0f89d713c 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -123,10 +123,22 @@ void virtio_check_driver_offered_feature(const struct virtio_device *vdev,
 }
 EXPORT_SYMBOL_GPL(virtio_check_driver_offered_feature);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|149| <<virtio_config_changed>> __virtio_config_changed(dev);
+ *   - drivers/virtio/virtio.c|198| <<virtio_config_driver_enable>> __virtio_config_changed(dev);
+ *   - drivers/virtio/virtio.c|215| <<virtio_config_core_enable>> __virtio_config_changed(dev);
+ */
 static void __virtio_config_changed(struct virtio_device *dev)
 {
 	struct virtio_driver *drv = drv_to_virtio(dev->dev.driver);
 
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	if (!dev->config_core_enabled || dev->config_driver_disabled)
 		dev->config_change_pending = true;
 	else if (drv && drv->config_changed) {
@@ -135,6 +147,14 @@ static void __virtio_config_changed(struct virtio_device *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|451| <<vu_req_interrupt>> virtio_config_changed(&vu_dev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1217| <<virtio_ccw_int_handler>> virtio_config_changed(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|313| <<vm_interrupt>> virtio_config_changed(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|77| <<vp_config_changed>> virtio_config_changed(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|131| <<virtio_vdpa_config_cb>> virtio_config_changed(&vd_dev->vdev);
+ */
 void virtio_config_changed(struct virtio_device *dev)
 {
 	unsigned long flags;
@@ -152,9 +172,20 @@ EXPORT_SYMBOL_GPL(virtio_config_changed);
  * This is only allowed to be called by a driver and disabling can't
  * be nested.
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3657| <<virtnet_close>> virtio_config_driver_disable(vi->vdev);
+ *   - drivers/net/virtio_net.c|6874| <<virtnet_probe>> virtio_config_driver_disable(vi->vdev);
+ */
 void virtio_config_driver_disable(struct virtio_device *dev)
 {
 	spin_lock_irq(&dev->config_lock);
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	dev->config_driver_disabled = true;
 	spin_unlock_irq(&dev->config_lock);
 }
@@ -170,6 +201,12 @@ EXPORT_SYMBOL_GPL(virtio_config_driver_disable);
 void virtio_config_driver_enable(struct virtio_device *dev)
 {
 	spin_lock_irq(&dev->config_lock);
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	dev->config_driver_disabled = false;
 	if (dev->config_change_pending)
 		__virtio_config_changed(dev);
@@ -454,6 +491,16 @@ static int virtio_device_of_init(struct virtio_device *dev)
  *
  * Returns: 0 on suceess, -error on failure
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1252| <<virtio_uml_probe>> rc = register_virtio_device(&vu_dev->vdev);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1233| <<mlxbf_tmfifo_create_vdev>> ret = register_virtio_device(&tm_vdev->vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|446| <<rproc_add_virtio_dev>> ret = register_virtio_device(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1401| <<virtio_ccw_online>> ret = register_virtio_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|688| <<virtio_mmio_probe>> rc = register_virtio_device(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|723| <<virtio_pci_probe>> rc = register_virtio_device(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|512| <<virtio_vdpa_probe>> ret = register_virtio_device(&vd_dev->vdev);
+ */
 int register_virtio_device(struct virtio_device *dev)
 {
 	int err;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index fdd2d2b07..c3d30eb6f 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -2297,6 +2297,37 @@ static inline int virtqueue_add(struct virtqueue *_vq,
  *
  * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virt-pci.c|135| <<um_pci_send_cmd>> ret = virtqueue_add_sgs(dev->cmd_vq, sgs_list,
+ *   - drivers/block/virtio_blk.c|158| <<virtblk_add_req>> return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_akcipher_algs.c|256| <<__virtio_crypto_akcipher_do_req>> ret = virtqueue_add_sgs(data_vq->vq, sgs, num_out, num_in, vc_req, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|59| <<virtio_crypto_ctrl_vq_request>> err = virtqueue_add_sgs(vcrypto->ctrl_vq, sgs, out_sgs, in_sgs, vc_ctrl_req, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_skcipher_algs.c|449| <<__virtio_crypto_skcipher_do_req>> err = virtqueue_add_sgs(data_vq->vq, sgs, num_out,
+ *   - drivers/firmware/arm_scmi/transports/virtio.c|524| <<virtio_send_message>> rc = virtqueue_add_sgs(vioch->vqueue, sgs, 1, 1, msg, GFP_ATOMIC);
+ *   - drivers/gpio/gpio-virtio.c|93| <<_virtio_gpio_req>> ret = virtqueue_add_sgs(vgpio->request_vq, sgs, 1, 1, line, GFP_KERNEL);
+ *   - drivers/gpio/gpio-virtio.c|222| <<virtio_gpio_irq_prepare>> ret = virtqueue_add_sgs(vgpio->event_vq, sgs, 1, 1, irq_line, GFP_ATOMIC);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|357| <<virtio_gpu_queue_ctrl_sgs>> ret = virtqueue_add_sgs(vq, sgs, outcnt, incnt, vbuf, GFP_ATOMIC);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|466| <<virtio_gpu_queue_cursor>> ret = virtqueue_add_sgs(vq, sgs, outcnt, 0, vbuf, GFP_ATOMIC);
+ *   - drivers/i2c/busses/i2c-virtio.c|100| <<virtio_i2c_prepare_reqs>> if (virtqueue_add_sgs(vq, sgs, outcnt, incnt, &reqs[i], GFP_KERNEL)) {
+ *   - drivers/iommu/virtio-iommu.c|247| <<__viommu_add_req>> ret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);
+ *   - drivers/iommu/virtio-iommu.c|251| <<__viommu_add_req>> ret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);
+ *   - drivers/net/virtio_net.c|3461| <<virtnet_send_command_reply>> ret = virtqueue_add_sgs(vi->cvq, sgs, out_num, in_num, vi, GFP_ATOMIC);
+ *   - drivers/nvdimm/nd_virtio.c|78| <<virtio_pmem_flush>> while ((err = virtqueue_add_sgs(vpmem->req_vq, sgs, 1, 1, req_data,
+ *   - drivers/scsi/virtio_scsi.c|471| <<__virtscsi_add_cmd>> return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
+ *   - drivers/virtio/virtio_mem.c|1405| <<virtio_mem_send_request>> rc = virtqueue_add_sgs(vm->vq, sgs, 1, 1, vm, GFP_KERNEL);
+ *   - drivers/virtio/virtio_pci_modern.c|93| <<virtqueue_exec_admin_cmd>> ret = virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_KERNEL);
+ *   - fs/fuse/virtio_fs.c|1441| <<virtio_fs_enqueue_req>> ret = virtqueue_add_sgs(vq, sgs, out_sgs, in_sgs, req, GFP_ATOMIC);
+ *   - net/9p/trans_virtio.c|281| <<p9_virtio_request>> err = virtqueue_add_sgs(chan->vq, sgs, out_sgs, in_sgs, req,
+ *   - net/9p/trans_virtio.c|509| <<p9_virtio_zc_request>> err = virtqueue_add_sgs(chan->vq, sgs, out_sgs, in_sgs, req,
+ *   - net/vmw_vsock/virtio_transport.c|143| <<virtio_transport_send_skb>> ret = virtqueue_add_sgs(vq, sgs, out_sg, in_sg, skb, gfp);
+ *   - net/vmw_vsock/virtio_transport.c|326| <<virtio_vsock_rx_fill>> ret = virtqueue_add_sgs(vq, &p, 0, 1, skb, GFP_KERNEL);
+ *   - sound/virtio/virtio_card.c|41| <<virtsnd_event_send>> if (virtqueue_add_sgs(vqueue, psgs, 0, 1, event, gfp) || !notify)
+ *   - sound/virtio/virtio_ctl_msg.c|151| <<virtsnd_ctl_msg_send>> rc = virtqueue_add_sgs(queue->vqueue, psgs, nouts, nins, msg,
+ *   - sound/virtio/virtio_pcm_msg.c|234| <<virtsnd_pcm_msg_send>> rc = virtqueue_add_sgs(vqueue, psgs, 2, 1, msg,
+ *   - sound/virtio/virtio_pcm_msg.c|237| <<virtsnd_pcm_msg_send>> rc = virtqueue_add_sgs(vqueue, psgs, 1, 2, msg,
+ *   - tools/virtio/vringh_test.c|517| <<main>> err = virtqueue_add_sgs(vq, sgs, 1, 1, &err, GFP_KERNEL);
+ */
 int virtqueue_add_sgs(struct virtqueue *_vq,
 		      struct scatterlist *sgs[],
 		      unsigned int out_sgs,
diff --git a/drivers/virtio/virtio_vdpa.c b/drivers/virtio/virtio_vdpa.c
index 1f60c9d5c..85fa1133d 100644
--- a/drivers/virtio/virtio_vdpa.c
+++ b/drivers/virtio/virtio_vdpa.c
@@ -108,6 +108,22 @@ static bool virtio_vdpa_notify(struct virtqueue *vq)
 	struct vdpa_device *vdpa = vd_get_vdpa(vq->vdev);
 	const struct vdpa_config_ops *ops = vdpa->config;
 
+	/*
+	 * 在以下调用vdpa_config_ops->kick_vq:
+	 *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+	 *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+	 * 在以下设置vdpa_config_ops->kick_vq:
+	 *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+	 *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+	 *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+	 */
 	ops->kick_vq(vdpa, vq->index);
 
 	return true;
@@ -461,6 +477,10 @@ virtio_vdpa_get_vq_affinity(struct virtio_device *vdev, int index)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_vdpa.c|502| <<virtio_vdpa_probe>> vd_dev->vdev.config = &virtio_vdpa_config_ops;
+ */
 static const struct virtio_config_ops virtio_vdpa_config_ops = {
 	.get		= virtio_vdpa_get,
 	.set		= virtio_vdpa_set,
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index 401439bb2..11ad72819 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -974,6 +974,38 @@ static inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)
 	xa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \
 			  (atomic_read(&kvm->online_vcpus) - 1))
 
+/*
+ * called by:
+ *   - arch/arm64/kvm/arm.c|1360| <<kvm_vm_ioctl_irq_line>> vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
+ *   - arch/arm64/kvm/arm.c|1372| <<kvm_vm_ioctl_irq_line>> vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
+ *   - arch/arm64/kvm/vgic/vgic-init.c|437| <<__kvm_vgic_vcpu_destroy>> if (kvm_get_vcpu_by_id(vcpu->kvm, vcpu->vcpu_id) != vcpu)
+ *   - arch/arm64/kvm/vgic/vgic-its.c|377| <<collection_to_vcpu>> return kvm_get_vcpu_by_id(kvm, col->target_addr);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1243| <<vgic_its_cmd_handle_mapc>> vcpu = kvm_get_vcpu_by_id(kvm, its_cmd_get_target_addr(its_cmd));
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1391| <<vgic_its_cmd_handle_movall>> vcpu1 = kvm_get_vcpu_by_id(kvm, its_cmd_get_target_addr(its_cmd));
+ *   - arch/arm64/kvm/vgic/vgic-its.c|1392| <<vgic_its_cmd_handle_movall>> vcpu2 = kvm_get_vcpu_by_id(kvm, its_cmd_mask_field(its_cmd, 3, 16, 32));
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2202| <<vgic_its_restore_ite>> vcpu = kvm_get_vcpu_by_id(kvm, collection->target_addr);
+ *   - arch/arm64/kvm/vgic/vgic-its.c|2515| <<vgic_its_restore_cte>> if (target_addr != COLLECTION_NOT_MAPPED && !kvm_get_vcpu_by_id(kvm, target_addr))
+ *   - arch/arm64/kvm/vgic/vgic-kvm-device.c|349| <<vgic_v2_parse_attr>> reg_attr->vcpu = kvm_get_vcpu_by_id(dev->kvm, cpuid);
+ *   - 11 arch/powerpc/kvm/book3s_hv.c|521| <<kvmppc_find_vcpu>> return kvm_get_vcpu_by_id(kvm, id);
+ *   - arch/riscv/kvm/aia_imsic.c|894| <<kvm_riscv_aia_imsic_rw_attr>> vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
+ *   - arch/riscv/kvm/aia_imsic.c|935| <<kvm_riscv_aia_imsic_has_attr>> vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
+ *   - arch/riscv/kvm/vcpu_sbi_hsm.c|23| <<kvm_sbi_hsm_vcpu_start>> target_vcpu = kvm_get_vcpu_by_id(vcpu->kvm, target_vcpuid);
+ *   - arch/riscv/kvm/vcpu_sbi_hsm.c|79| <<kvm_sbi_hsm_vcpu_get_status>> target_vcpu = kvm_get_vcpu_by_id(vcpu->kvm, target_vcpuid);
+ *   - arch/riscv/kvm/vcpu_sbi_v01.c|57| <<kvm_sbi_ext_v01_handler>> rvcpu = kvm_get_vcpu_by_id(vcpu->kvm, i);
+ *   - arch/s390/kvm/diag.c|180| <<__diag_time_slice_end_directed>> tcpu = kvm_get_vcpu_by_id(vcpu->kvm, tid);
+ *   - arch/s390/kvm/interrupt.c|1547| <<__inject_extcall>> if (kvm_get_vcpu_by_id(vcpu->kvm, src_id) == NULL)
+ *   - arch/s390/kvm/interrupt.c|1630| <<__inject_sigp_emergency>> if (kvm_get_vcpu_by_id(vcpu->kvm, irq->u.emerg.code) == NULL)
+ *   - arch/s390/kvm/sigp.c|274| <<handle_sigp_dst>> struct kvm_vcpu *dst_vcpu = kvm_get_vcpu_by_id(vcpu->kvm, cpu_addr);
+ *   - arch/s390/kvm/sigp.c|486| <<kvm_s390_handle_sigp_pei>> dest_vcpu = kvm_get_vcpu_by_id(vcpu->kvm, cpu_addr);
+ *   - arch/x86/kvm/svm/avic.c|259| <<avic_ga_log_notifier>> vcpu = kvm_get_vcpu_by_id(&kvm_svm->kvm, vcpu_id);
+ *   - arch/x86/kvm/svm/avic.c|538| <<avic_kick_vcpu_by_physical_id>> struct kvm_vcpu *target_vcpu = kvm_get_vcpu_by_id(kvm, physical_id);
+ *   - arch/x86/kvm/svm/avic.c|1024| <<svm_ir_list_add>> struct kvm_vcpu *prev_vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);
+ *   - arch/x86/kvm/svm/avic.c|1207| <<avic_pi_update_irte>> vcpu = kvm_get_vcpu_by_id(kvm, id);
+ *   - arch/x86/kvm/svm/sev.c|3935| <<sev_snp_ap_creation>> target_vcpu = kvm_get_vcpu_by_id(vcpu->kvm, apic_id);
+ *   - arch/x86/kvm/xen.c|1742| <<kvm_xen_set_evtchn_fast>> vcpu = kvm_get_vcpu_by_id(kvm, xe->vcpu_id);
+ *   - arch/x86/kvm/xen.c|1924| <<kvm_xen_setup_evtchn>> vcpu = kvm_get_vcpu_by_id(kvm, ue->u.xen_evtchn.vcpu);
+ *   - virt/kvm/kvm_main.c|4214| <<kvm_vm_ioctl_create_vcpu>> if (kvm_get_vcpu_by_id(kvm, id)) {
+ */
 static inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)
 {
 	struct kvm_vcpu *vcpu = NULL;
diff --git a/include/linux/mempool.h b/include/linux/mempool.h
index 7b1514413..39f8546fa 100644
--- a/include/linux/mempool.h
+++ b/include/linux/mempool.h
@@ -56,6 +56,25 @@ extern mempool_t *mempool_create_node_noprof(int min_nr, mempool_alloc_t *alloc_
 #define mempool_create_node(...)					\
 	alloc_hooks(mempool_create_node_noprof(__VA_ARGS__))
 
+/*
+ * 在以下调用mempool_create():
+ *   - drivers/block/aoe/aoeblk.c|357| <<aoeblk_gdalloc>> mp = mempool_create(MIN_BUFS, mempool_alloc_slab, mempool_free_slab,
+ *   - drivers/md/dm-vdo/flush.c|150| <<vdo_make_flusher>> vdo->flusher->flush_pool = mempool_create(1, allocate_flush, free_flush,
+ *   - drivers/nvme/host/auth.c|1022| <<nvme_init_auth>> nvme_chap_buf_pool = mempool_create(16, mempool_alloc_slab,
+ *   - drivers/nvme/target/io-cmd-file.c|59| <<nvmet_file_ns_enable>> ns->bvec_pool = mempool_create(NVMET_MIN_MPOOL_OBJ, mempool_alloc_slab,
+ *   - drivers/scsi/qla2xxx/qla_target.c|7294| <<qlt_init>> qla_tgt_mgmt_cmd_mempool = mempool_create(25, mempool_alloc_slab,
+ *   - drivers/scsi/qla4xxx/ql4_os.c|4279| <<qla4xxx_mem_alloc>> ha->srb_mempool = mempool_create(SRB_MIN_REQ, mempool_alloc_slab,
+ *   - fs/smb/client/smbdirect.c|1441| <<allocate_caches_and_workqueue>> mempool_create(info->send_credit_target, mempool_alloc_slab,
+ *   - fs/smb/client/smbdirect.c|1457| <<allocate_caches_and_workqueue>> mempool_create(info->receive_credit_max, mempool_alloc_slab,
+ *   - fs/smb/server/transport_rdma.c|1832| <<smb_direct_create_pools>> t->sendmsg_mempool = mempool_create(t->send_credit_target,
+ *   - fs/smb/server/transport_rdma.c|1847| <<smb_direct_create_pools>> mempool_create(t->recv_credit_max, mempool_alloc_slab,
+ *   - include/linux/mempool.h|148| <<mempool_create_slab_pool>> mempool_create((_min_nr), mempool_alloc_slab, mempool_free_slab, (void *)(_kc))
+ *   - include/linux/mempool.h|161| <<mempool_create_kmalloc_pool>> mempool_create((_min_nr), mempool_kmalloc, mempool_kfree, \
+ *   - include/linux/mempool.h|174| <<mempool_create_kvmalloc_pool>> return mempool_create(min_nr, mempool_kvmalloc, mempool_kvfree, (void *) size);
+ *   - include/linux/mempool.h|188| <<mempool_create_page_pool>> mempool_create((_min_nr), mempool_alloc_pages, \
+ *   - lib/btree.c|191| <<btree_init>> head->mempool = mempool_create(0, btree_alloc, btree_free, NULL);
+ *   - net/ceph/msgpool.c|46| <<ceph_msgpool_init>> pool->pool = mempool_create(size, msgpool_alloc, msgpool_free, pool);
+ */
 #define mempool_create(_min_nr, _alloc_fn, _free_fn, _pool_data)	\
 	mempool_create_node(_min_nr, _alloc_fn, _free_fn, _pool_data,	\
 			    GFP_KERNEL, NUMA_NO_NODE)
@@ -78,8 +97,72 @@ extern void mempool_free(void *element, mempool_t *pool);
 void *mempool_alloc_slab(gfp_t gfp_mask, void *pool_data);
 void mempool_free_slab(void *element, void *pool_data);
 
+/*
+ * 在以下使用mempool_init_slab_pool():
+ *   - block/bio-integrity.c|572| <<bioset_integrity_create>> if (mempool_init_slab_pool(&bs->bio_integrity_pool,
+ *   - block/bio.c|1734| <<biovec_init_pool>> return mempool_init_slab_pool(pool, pool_entries, bp->slab); 
+ *   - block/bio.c|1800| <<bioset_init>> if (mempool_init_slab_pool(&bs->bio_pool, pool_size, bs->bio_slab))
+ *   - drivers/block/drbd/drbd_main.c|2128| <<drbd_create_mempools>> ret = mempool_init_slab_pool(&drbd_request_mempool, number,
+ *   - drivers/block/drbd/drbd_main.c|2133| <<drbd_create_mempools>> ret = mempool_init_slab_pool(&drbd_ee_mempool, number, drbd_ee_cache);
+ *   - drivers/md/bcache/super.c|1917| <<bch_cache_set_alloc>> if (mempool_init_slab_pool(&c->search, 32, bch_search_cache))
+ *   - drivers/md/dm-bio-prison-v1.c|57| <<dm_bio_prison_create>> ret = mempool_init_slab_pool(&prison->cell_pool, MIN_CELLS, _cell_cache);
+ *   - drivers/md/dm-bio-prison-v2.c|48| <<dm_bio_prison_create_v2>> ret = mempool_init_slab_pool(&prison->cell_pool, MIN_CELLS, _cell_cache);
+ *   - drivers/md/dm-cache-target.c|2535| <<cache_create>> r = mempool_init_slab_pool(&cache->migration_pool, MIGRATION_POOL_SIZE,
+ *   - drivers/md/dm-clone-target.c|1896| <<clone_ctr>> r = mempool_init_slab_pool(&clone->hydration_pool, MIN_HYDRATIONS,
+ *   - drivers/md/dm-integrity.c|4793| <<dm_integrity_ctr>> r = mempool_init_slab_pool(&ic->journal_io_mempool, JOURNAL_IO_MEMPOOL, journal_io_cache);
+ *   - drivers/md/dm-io.c|59| <<dm_io_client_create>> ret = mempool_init_slab_pool(&client->pool, min_ios, _dm_io_cache);
+ *   - drivers/md/dm-kcopyd.c|932| <<dm_kcopyd_client_create>> r = mempool_init_slab_pool(&kc->job_pool, MIN_JOBS, _job_cache);
+ *   - drivers/md/dm-log-userspace-base.c|253| <<userspace_ctr>> r = mempool_init_slab_pool(&lc->flush_entry_pool, FLUSH_ENTRY_POOL_SIZE,
+ *   - drivers/md/dm-snap.c|1343| <<snapshot_ctr>> r = mempool_init_slab_pool(&s->pending_pool, MIN_IOS, pending_cache);
+ *   - drivers/md/dm-thin.c|3025| <<pool_create>> r = mempool_init_slab_pool(&pool->mapping_pool, MAPPING_POOL_SIZE,
+ *   - drivers/md/dm-verity-fec.c|779| <<verity_fec_ctr>> ret = mempool_init_slab_pool(&f->prealloc_pool, num_online_cpus() *
+ *   - drivers/md/dm-verity-fec.c|787| <<verity_fec_ctr>> ret = mempool_init_slab_pool(&f->extra_pool, 0, f->cache);
+ *   - drivers/md/raid5-cache.c|3099| <<r5l_init_log>> ret = mempool_init_slab_pool(&log->io_pool, R5L_POOL_SIZE, log->io_kc);
+ *   - fs/netfs/main.c|115| <<netfs_init>> if (mempool_init_slab_pool(&netfs_request_pool, 100, netfs_request_slab) < 0)
+ *   - fs/netfs/main.c|125| <<netfs_init>> if (mempool_init_slab_pool(&netfs_subrequest_pool, 100, netfs_subrequest_slab) < 0)
+ *   - fs/smb/client/cifsfs.c|1791| <<cifs_init_netfs>> if (mempool_init_slab_pool(&cifs_io_request_pool, 100, cifs_io_request_cachep) < 0)
+ *   - fs/smb/client/cifsfs.c|1801| <<cifs_init_netfs>> if (mempool_init_slab_pool(&cifs_io_subrequest_pool, 100, cifs_io_subrequest_cachep) < 0)
+ *   - mm/kasan/kasan_test_c.c|1173| <<mempool_prepare_slab>> ret = mempool_init_slab_pool(pool, pool_size, cache);
+ */
 #define mempool_init_slab_pool(_pool, _min_nr, _kc)			\
 	mempool_init(_pool, (_min_nr), mempool_alloc_slab, mempool_free_slab, (void *)(_kc))
+/*
+ * 在以下调用mempool_create_slab_pool():
+ *   - arch/sh/kernel/dwarf.c|1179| <<dwarf_unwinder_init>> dwarf_frame_pool = mempool_create_slab_pool(DWARF_FRAME_MIN_REQ, dwarf_frame_cachep);
+ *   - arch/sh/kernel/dwarf.c|1184| <<dwarf_unwinder_init>> dwarf_reg_pool = mempool_create_slab_pool(DWARF_REG_MIN_REQ, dwarf_reg_cachep);
+ *   - block/blk-crypto-fallback.c|592| <<blk_crypto_fallback_init>> mempool_create_slab_pool(num_prealloc_fallback_crypt_ctxs, bio_fallback_crypt_ctx_cache);
+ *   - block/blk-crypto.c|71| <<bio_crypt_ctx_init>> bio_crypt_ctx_pool = mempool_create_slab_pool(num_prealloc_crypt_ctxs, bio_crypt_ctx_cache);
+ *   - drivers/dma/dmaengine.c|1413| <<dmaengine_init_unmap_pool>> p->pool = mempool_create_slab_pool(1, p->cache);
+ *   - drivers/s390/scsi/zfcp_aux.c|236| <<zfcp_allocate_low_mem_buffers>> mempool_create_slab_pool(4, zfcp_fsf_qtcb_cache);
+ *   - drivers/s390/scsi/zfcp_aux.c|247| <<zfcp_allocate_low_mem_buffers>> mempool_create_slab_pool(1, zfcp_fc_req_cache);
+ *   - drivers/scsi/fnic/fnic_main.c|798| <<fnic_probe>> fnic->io_req_pool = mempool_create_slab_pool(2, fnic_io_req_cache);
+ *   - drivers/scsi/fnic/fnic_main.c|802| <<fnic_probe>> pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_DFLT]);
+ *   - drivers/scsi/fnic/fnic_main.c|807| <<fnic_probe>> pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_MAX]);
+ *   - drivers/scsi/libfc/fc_exch.c|2504| <<bool>> mp->ep_pool = mempool_create_slab_pool(2, fc_em_cachep);
+ *   - drivers/scsi/libfc/fc_fcp.c|2308| <<fc_fcp_init>> si->scsi_pkt_pool = mempool_create_slab_pool(2, scsi_pkt_cachep);
+ *   - drivers/scsi/qedf/qedf_main.c|3367| <<__qedf_probe>> qedf->io_mempool = mempool_create_slab_pool(QEDF_IO_WORK_MIN, qedf_io_work_cache);
+ *   - drivers/scsi/qla2xxx/qla_init.c|9768| <<qla2xxx_create_qpair>> qpair->srb_mempool = mempool_create_slab_pool(SRB_MIN_REQ, srb_cachep);
+ *   - drivers/scsi/qla2xxx/qla_os.c|4182| <<qla2x00_mem_alloc>> ha->srb_mempool = mempool_create_slab_pool(SRB_MIN_REQ, srb_cachep);
+ *   - drivers/scsi/qla2xxx/qla_os.c|4195| <<qla2x00_mem_alloc>> ha->ctx_mempool = mempool_create_slab_pool(SRB_MIN_REQ, ctx_cachep);
+ *   - drivers/scsi/snic/snic_main.c|548| <<snic_probe>> pool = mempool_create_slab_pool(2, snic_glob->req_cache[SNIC_REQ_CACHE_DFLT_SGL]);
+ *   - drivers/scsi/snic/snic_main.c|559| <<snic_probe>> pool = mempool_create_slab_pool(2, snic_glob->req_cache[SNIC_REQ_CACHE_MAX_SGL]);
+ *   - drivers/scsi/snic/snic_main.c|570| <<snic_probe>> pool = mempool_create_slab_pool(2, snic_glob->req_cache[SNIC_REQ_TM_CACHE]);
+ *   - drivers/scsi/virtio_scsi.c|1269| <<virtio_scsi_init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache);
+ *   - drivers/tty/hvc/hvc_iucv.c|1355| <<hvc_iucv_init>> hvc_iucv_mempool = mempool_create_slab_pool(MEMPOOL_MIN_NR, hvc_iucv_buffer_cache);
+ *   - fs/ext4/readpage.c|400| <<ext4_init_post_read_processing>> mempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS, bio_post_read_ctx_cache);
+ *   - fs/f2fs/data.c|4149| <<f2fs_init_post_read_processing>> mempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS, bio_post_read_ctx_cache);
+ *   - fs/f2fs/iostat.c|279| <<f2fs_init_iostat_processing>> mempool_create_slab_pool(NUM_PREALLOC_IOSTAT_CTXS, bio_iostat_ctx_cache);
+ *   - fs/jfs/jfs_metapage.c|210| <<metapage_init>> metapage_mempool = mempool_create_slab_pool(METAPOOL_MIN_PAGES, metapage_cache);
+ *   - fs/nfs/write.c|2151| <<nfs_init_writepagecache>> nfs_wdata_mempool = mempool_create_slab_pool(MIN_POOL_WRITE, nfs_wdata_cachep);
+ *   - fs/nfs/write.c|2163| <<nfs_init_writepagecache>> nfs_commit_mempool = mempool_create_slab_pool(MIN_POOL_COMMIT, nfs_cdata_cachep);
+ *   - fs/smb/client/cifsfs.c|1705| <<cifs_init_request_bufs>> cifs_req_poolp = mempool_create_slab_pool(cifs_min_rcv, cifs_req_cachep);
+ *   - fs/smb/client/cifsfs.c|1736| <<cifs_init_request_bufs>> cifs_sm_req_poolp = mempool_create_slab_pool(cifs_min_small, cifs_sm_req_cachep);
+ *   - fs/smb/client/cifsfs.c|1767| <<init_mids>> cifs_mid_poolp = mempool_create_slab_pool(3, cifs_mid_cachep);
+ *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+ *   - net/ceph/osd_client.c|5240| <<ceph_osdc_init>> osdc->req_mempool = mempool_create_slab_pool(10, ceph_osd_request_cache);
+ *   - net/sunrpc/sched.c|1351| <<rpc_init_mempool>> rpc_task_mempool = mempool_create_slab_pool(RPC_TASK_POOLSIZE, rpc_task_slabp);
+ *   - net/sunrpc/sched.c|1355| <<rpc_init_mempool>> rpc_buffer_mempool = mempool_create_slab_pool(RPC_BUFFER_POOLSIZE, rpc_buffer_slabp);
+ */
 #define mempool_create_slab_pool(_min_nr, _kc)			\
 	mempool_create((_min_nr), mempool_alloc_slab, mempool_free_slab, (void *)(_kc))
 
diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h
index e2dd57ca3..71ed724fc 100644
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@ -428,6 +428,43 @@ static inline int mmu_notifier_test_young(struct mm_struct *mm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/proc/task_mmu.c|1593| <<clear_refs_write>> mmu_notifier_invalidate_range_start(&range);
+ *   - fs/proc/task_mmu.c|2774| <<do_pagemap_scan>> mmu_notifier_invalidate_range_start(&range);
+ *   - kernel/events/uprobes.c|189| <<__replace_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|1815| <<do_huge_zero_wp_pmd>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|2517| <<move_pages_huge_pmd>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|2675| <<__split_huge_pud>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|2983| <<__split_huge_pmd>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5232| <<copy_hugetlb_page_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5464| <<move_hugetlb_page_tables>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5721| <<unmap_hugepage_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5949| <<hugetlb_wp>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|6798| <<hugetlb_change_protection>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|7531| <<hugetlb_unshare_pmds>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/khugepaged.c|1176| <<collapse_huge_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/khugepaged.c|1598| <<collapse_pte_mapped_thp>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/khugepaged.c|1758| <<retract_page_tables>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/ksm.c|1266| <<write_protect_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/ksm.c|1372| <<replace_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/madvise.c|821| <<madvise_free_single_vma>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/mapping_dirty_helpers.c|179| <<wp_clean_pre_vma>> mmu_notifier_invalidate_range_start(&wpwalk->range);
+ *   - mm/memory.c|1401| <<copy_page_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|1909| <<unmap_vmas>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|1944| <<zap_page_range_single>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|3398| <<wp_page_copy>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|3919| <<remove_device_exclusive_entry>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/migrate_device.c|308| <<migrate_vma_collect>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/migrate_device.c|723| <<__migrate_device_pages>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/mprotect.c|460| <<change_pud_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/mremap.c|620| <<move_page_tables>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|1037| <<page_vma_mkclean_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|1690| <<try_to_unmap_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|2062| <<try_to_migrate_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|2403| <<page_make_device_exclusive_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/userfaultfd.c|1147| <<move_pages_pte>> mmu_notifier_invalidate_range_start(&range);
+ */
 static inline void
 mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
 {
diff --git a/include/linux/scatterlist.h b/include/linux/scatterlist.h
index d836e7440..437f37fa2 100644
--- a/include/linux/scatterlist.h
+++ b/include/linux/scatterlist.h
@@ -198,6 +198,39 @@ static inline void sg_set_buf(struct scatterlist *sg, const void *buf,
 /*
  * Loop over each sg element in the given sg_table object.
  */
+/*
+ * 在以下调用for_each_sgtable_sg():
+ *   - drivers/dma-buf/dma-buf.c|788| <<mangle_sg_table>> for_each_sgtable_sg(sg_table, sg, i)
+ *   - drivers/dma-buf/heaps/system_heap.c|74| <<dup_sg_table>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|292| <<system_heap_dma_buf_release>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|406| <<system_heap_allocate>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|703| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) 
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|713| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|734| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|765| <<amdgpu_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|409| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/armada/armada_gem.c|442| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|465| <<armada_gem_prime_unmap_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/msm/msm_iommu.c|124| <<msm_iommu_pagetable_map>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/core/firmware.c|272| <<nvkm_firmware_ctor>> for_each_sgtable_sg(&fw->mem.sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2195| <<nvkm_gsp_sg_free>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2215| <<nvkm_gsp_sg>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2308| <<nvkm_gsp_radix3_sg>> for_each_sgtable_sg(&rx3->lvl2, sg, i) {
+ *   - drivers/gpu/drm/rockchip/rockchip_drm_gem.c|104| <<rockchip_gem_get_pages>> for_each_sgtable_sg(rk_obj->sgt, s, i)
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|222| <<drm_gem_shmem_test_get_pages_sgt>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|257| <<drm_gem_shmem_test_get_sg_table>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_object.c|169| <<virtio_gpu_object_shmem_init>> for_each_sgtable_sg(pages, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|296| <<vmalloc_to_sgt>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|404| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i)
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|414| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|435| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|456| <<xe_ttm_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/host1x/job.c|238| <<pin_job>> for_each_sgtable_sg(map->sgt, sg, j)
+ *   - drivers/infiniband/core/umem.c|58| <<__ib_umem_release>> for_each_sgtable_sg(&umem->sgt_append.sgt, sg, i)
+ *   - drivers/media/common/videobuf2/videobuf2-vmalloc.c|234| <<vb2_vmalloc_dmabuf_ops_attach>> for_each_sgtable_sg(sgt, sg, i) {
+ *
+ * 注释: Loop over each sg element in the given sg_table object.
+ */
 #define for_each_sgtable_sg(sgt, sg, i)		\
 	for_each_sg((sgt)->sgl, sg, (sgt)->orig_nents, i)
 
@@ -510,6 +543,31 @@ size_t sg_zero_buffer(struct scatterlist *sgl, unsigned int nents,
  */
 #define SG_MAX_SINGLE_ALLOC		(PAGE_SIZE / sizeof(struct scatterlist))
 
+/*
+ * 在以下使用SG_CHUNK_SIZE:
+ *   - drivers/scsi/esas2r/esas2r_main.c|248| <<global>> .sg_tablesize = SG_CHUNK_SIZE,
+ *   - drivers/scsi/esas2r/esas2r_main.c|269| <<global>> int sg_tablesize = SG_CHUNK_SIZE;
+ *   - lib/sg_pool.c|27| <<global>> #if (SG_CHUNK_SIZE < 32)
+ *   - lib/sg_pool.c|28| <<global>> #error SG_CHUNK_SIZE is too small (must be 32 or greater)
+ *   - lib/sg_pool.c|41| <<global>> #if (SG_CHUNK_SIZE > 32)
+ *   - lib/sg_pool.c|43| <<global>> #if (SG_CHUNK_SIZE > 64)
+ *   - lib/sg_pool.c|45| <<global>> #if (SG_CHUNK_SIZE > 128)
+ *   - lib/sg_pool.c|47| <<global>> #if (SG_CHUNK_SIZE > 256)
+ *   - lib/sg_pool.c|48| <<global>> #error SG_CHUNK_SIZE is too large (256 MAX)
+ *   - lib/sg_pool.c|48| <<global>> #error SG_CHUNK_SIZE is too large (256 MAX)
+ *   - lib/sg_pool.c|53| <<global>> SP(SG_CHUNK_SIZE)
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.h|109| <<MPT_MAX_PHYS_SEGMENTS>> #define MPT_MAX_PHYS_SEGMENTS SG_CHUNK_SIZE
+ *   - fs/smb/server/transport_rdma.c|1322| <<smb_direct_free_rdma_rw_msg>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1408| <<smb_direct_rdma_xmit>> msg = kzalloc(struct_size(msg, sg_list, SG_CHUNK_SIZE),
+ *   - fs/smb/server/transport_rdma.c|1424| <<smb_direct_rdma_xmit>> msg->sg_list, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1434| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1448| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - include/linux/scatterlist.h|527| <<SG_MAX_SEGMENTS>> #define SG_MAX_SEGMENTS SG_CHUNK_SIZE
+ *   - include/scsi/scsi_host.h|24| <<SG_ALL>> #define SG_ALL SG_CHUNK_SIZE
+ *   - lib/sg_pool.c|66| <<sg_pool_index>> BUG_ON(nents > SG_CHUNK_SIZE);
+ *   - lib/sg_pool.c|141| <<sg_free_table_chained>> __sg_free_table(table, SG_CHUNK_SIZE, nents_first_chunk, sg_pool_free,
+ *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents, SG_CHUNK_SIZE,
+ */
 /*
  * The maximum number of SG segments that we will put inside a
  * scatterlist (unless chaining is used). Should ideally fit inside a
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 64934e083..afadae270 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -548,6 +548,49 @@ struct sched_entity {
 
 	struct list_head		group_node;
 	unsigned char			on_rq;
+	/*
+	 * 在以下设置sched_entity->sched_delayed:
+	 *   - kernel/sched/fair.c|5374| <<set_delayed>> se->sched_delayed = 1;
+	 *   - kernel/sched/fair.c|5386| <<clear_delayed>> se->sched_delayed = 0;
+	 * 在以下使用sched_entity->sched_delayed:
+	 *   - include/linux/sched.h|2164| <<task_is_runnable>> return p->on_rq && !p->se.sched_delayed;
+	 *   - kernel/sched/core.c|270| <<sched_core_enqueue>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|283| <<sched_core_dequeue>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|1757| <<uclamp_rq_inc>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|1784| <<uclamp_rq_dec>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|3784| <<ttwu_runnable>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|4190| <<try_to_wake_up>> SCHED_WARN_ON(p->se.sched_delayed);
+	 *   - kernel/sched/core.c|4484| <<__sched_fork>> SCHED_WARN_ON(p->se.sched_delayed);
+	 *   - kernel/sched/core.c|7213| <<rt_mutex_setprio>> if (prev_class != next_class && p->se.sched_delayed)
+	 *   - kernel/sched/ext.c|4969| <<scx_ops_disable_workfn>> if (old_class != new_class && p->se.sched_delayed)
+	 *   - kernel/sched/ext.c|5687| <<scx_ops_enable>> if (old_class != new_class && p->se.sched_delayed)
+	 *   - kernel/sched/fair.c|5412| <<dequeue_entity>> SCHED_WARN_ON(!se->sched_delayed);
+	 *   - kernel/sched/fair.c|5422| <<dequeue_entity>> SCHED_WARN_ON(delay && se->sched_delayed);
+	 *   - kernel/sched/fair.c|5546| <<pick_next_entity>> SCHED_WARN_ON(cfs_rq->next->sched_delayed);
+	 *   - kernel/sched/fair.c|5551| <<pick_next_entity>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|5887| <<throttle_cfs_rq>> if (se->sched_delayed)
+	 *   - kernel/sched/fair.c|5986| <<unthrottle_cfs_rq>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|6853| <<requeue_delayed_entity>> SCHED_WARN_ON(!se->sched_delayed);
+	 *   - kernel/sched/fair.c|6896| <<enqueue_task_fair>> if (!(p->se.sched_delayed &&
+	 *            (task_on_rq_migrating(p) || (flags & ENQUEUE_RESTORE))))
+	 *   - kernel/sched/fair.c|6913| <<enqueue_task_fair>> h_nr_delayed = !!se->sched_delayed;
+	 *   - kernel/sched/fair.c|6917| <<enqueue_task_fair>> if (se->sched_delayed)
+	 *   - kernel/sched/fair.c|7033| <<dequeue_entities>> h_nr_delayed = !!se->sched_delayed;
+	 *   - kernel/sched/fair.c|7134| <<dequeue_task_fair>> if (!(p->se.sched_delayed &&
+	 *            (task_on_rq_migrating(p) || (flags & DEQUEUE_SAVE))))
+	 *   - kernel/sched/fair.c|8629| <<task_dead_fair>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|8634| <<task_dead_fair>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|8719| <<check_preempt_wakeup_fair>> if (sched_feat(NEXT_BUDDY) &&
+	 *            !(wake_flags & WF_FORK) && !pse->sched_delayed) {
+	 *   - kernel/sched/fair.c|13127| <<switched_to_fair>> SCHED_WARN_ON(p->se.sched_delayed);
+	 *   - kernel/sched/fair.c|13162| <<__set_next_task_fair>> SCHED_WARN_ON(se->sched_delayed);
+	 *   - kernel/sched/fair.c|13307| <<unregister_fair_sched_group>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|13309| <<unregister_fair_sched_group>> if (se->sched_delayed) {
+	 *   - kernel/sched/sched.h|916| <<se_runnable>> if (se->sched_delayed)
+	 *   - kernel/sched/sched.h|933| <<se_runnable>> if (se->sched_delayed)
+	 *   - kernel/sched/stats.h|141| <<psi_enqueue>> if (p->se.sched_delayed) {
+	 *   - kernel/sched/syscalls.c|712| <<__sched_setscheduler>> if (prev_class != next_class && p->se.sched_delayed)
+	 */
 	unsigned char			sched_delayed;
 	unsigned char			rel_deadline;
 	unsigned char			custom_slice;
@@ -1317,6 +1360,28 @@ struct task_struct {
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 	int				numa_scan_seq;
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	unsigned int			numa_scan_period;
 	unsigned int			numa_scan_period_max;
 	int				numa_preferred_nid;
@@ -1325,6 +1390,13 @@ struct task_struct {
 	u64				node_stamp;
 	u64				last_task_numa_placement;
 	u64				last_sum_exec_runtime;
+	/*
+	 * 在以下使用task_struct->numa_work:
+	 *   - kernel/sched/fair.c|3305| <<task_numa_work>> SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
+	 *   - kernel/sched/fair.c|3560| <<init_numa_balancing>> p->numa_work.next = &p->numa_work;
+	 *   - kernel/sched/fair.c|3568| <<init_numa_balancing>> init_task_work(&p->numa_work, task_numa_work);
+	 *   - kernel/sched/fair.c|3595| <<task_tick_numa>> struct callback_head *work = &curr->numa_work;
+	 */
 	struct callback_head		numa_work;
 
 	/*
diff --git a/include/linux/sched/smt.h b/include/linux/sched/smt.h
index fb1e295e7..a5d4ff2bd 100644
--- a/include/linux/sched/smt.h
+++ b/include/linux/sched/smt.h
@@ -7,6 +7,37 @@
 #ifdef CONFIG_SCHED_SMT
 extern struct static_key_false sched_smt_present;
 
+/*
+ * called by:
+ *   - arch/x86/kernel/cpu/bugs.c|1513| <<unpriv_ebpf_notify>> if (sched_smt_active())
+ *   - arch/x86/kernel/cpu/bugs.c|2406| <<update_stibp_strict>> if (sched_smt_active())
+ *   - arch/x86/kernel/cpu/bugs.c|2431| <<update_indir_branch_cond>> if (sched_smt_active())
+ *   - arch/x86/kernel/cpu/bugs.c|2454| <<update_mds_branch_idle>> if (sched_smt_active()) {
+ *   - arch/x86/kernel/cpu/bugs.c|2470| <<cpu_bugs_smt_update>> if (sched_smt_active() && unprivileged_ebpf_enabled() &&
+ *   - arch/x86/kernel/cpu/bugs.c|2490| <<cpu_bugs_smt_update>> if (sched_smt_active() && !boot_cpu_has(X86_BUG_MSBDS_ONLY))
+ *   - arch/x86/kernel/cpu/bugs.c|2501| <<cpu_bugs_smt_update>> if (sched_smt_active())
+ *   - arch/x86/kernel/cpu/bugs.c|2512| <<cpu_bugs_smt_update>> if (sched_smt_active())
+ *   - arch/x86/kernel/cpu/bugs.c|3236| <<l1tf_show_state>> if (... sched_smt_active())) {
+ *   - arch/x86/kernel/cpu/bugs.c|3243| <<l1tf_show_state>> return sysfs_emit( ... sched_smt_active() ? "vulnerable" : "disabled");
+ *   - 11 arch/x86/kernel/cpu/bugs.c|3280| <<mds_show_state>> return sysfs_emit(...sched_smt_active() ? "mitigated" : "disabled"));
+ *   - arch/x86/kernel/cpu/bugs.c|3284| <<mds_show_state>> return sysfs_emit(... sched_smt_active() ? "vulnerable" : "disabled");
+ *   - arch/x86/kernel/cpu/bugs.c|3299| <<tsx_async_abort_show_state>> return sysfs_emit(...sched_smt_active() ? "vulnerable" : "disabled");
+ *   - arch/x86/kernel/cpu/bugs.c|3316| <<mmio_stale_data_show_state>> return sysfs_emit(...sched_smt_active() ? "vulnerable" : "disabled");
+ *   - arch/x86/kernel/cpu/bugs.c|3423| <<spectre_v2_show_state>> if (sched_smt_active() && unprivileged_ebpf_enabled() &&
+ *   - arch/x86/kernel/cpu/bugs.c|3453| <<retbleed_show_state>> return sysfs_emit(...!sched_smt_active() ? "disabled" :
+ *   - arch/x86/kvm/vmx/vmx.c|7786| <<vmx_vm_init>> if (sched_smt_active())
+ *   - drivers/cpufreq/intel_pstate.c|1061| <<hybrid_init_cpu_capacity_scaling>> if (hwp_is_hybrid && !sched_smt_active() && arch_enable_hybrid_capacity_scale()) {
+ *   - drivers/idle/intel_idle.c|181| <<intel_idle_ibrs>> bool smt_active = sched_smt_active();
+ *   - kernel/cpu.c|3008| <<active_show>> return sysfs_emit(buf, "%d\n", sched_smt_active());
+ *   - kernel/sched/ext.c|3179| <<test_and_clear_cpu_idle>> if (sched_smt_active()) {
+ *   - kernel/sched/ext.c|3202| <<scx_pick_idle_cpu>> if (sched_smt_active()) {
+ *   - kernel/sched/ext.c|3438| <<scx_select_cpu_dfl>> if (sched_smt_active()) {
+ *   - kernel/sched/ext.c|3601| <<update_builtin_idle>> if (sched_smt_active()) {
+ *   - kernel/sched/ext.c|7427| <<scx_bpf_get_idle_smtmask>> if (sched_smt_active())
+ *   - kernel/sched/fair.c|8131| <<select_idle_sibling>> if (sched_smt_active()) {
+ *   - kernel/sched/fair.c|10369| <<sched_use_asym_prio>> if (!sched_smt_active())
+ *   - kernel/sched/topology.c|238| <<sched_is_eas_possible>> if (sched_smt_active()) {
+ */
 static __always_inline bool sched_smt_active(void)
 {
 	return static_branch_likely(&sched_smt_present);
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 5a64582b0..933bf20a0 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -19,6 +19,27 @@ enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_END,
 };
 
+/*
+ * 在以下使用NUMA_BALANCING_DISABLED:
+ *   - kernel/sched/core.c|4566| <<set_numabalancing_state>> sysctl_numa_balancing_mode = NUMA_BALANCING_DISABLED;
+ *
+ * 在以下使用NUMA_BALANCING_NORMAL:
+ *   - kernel/sched/core.c|4564| <<set_numabalancing_state>> sysctl_numa_balancing_mode = NUMA_BALANCING_NORMAL;
+ *   - mm/huge_memory.c|2351| <<change_huge_pmd>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_NORMAL) &&
+ *   - mm/mprotect.c|161| <<change_pte_range>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_NORMAL) &&
+ *
+ * 在以下使用NUMA_BALANCING_MEMORY_TIERING:
+ *   - kernel/sched/core.c|4598| <<sysctl_numa_balancing>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
+ *   - kernel/sched/core.c|4599| <<sysctl_numa_balancing>> (state & NUMA_BALANCING_MEMORY_TIERING))
+ *   - kernel/sched/fair.c|1951| <<should_numa_migrate_memory>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
+ *   - kernel/sched/fair.c|3170| <<task_numa_fault>> (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING ||
+ *   - mm/memory-tiers.c|67| <<folio_use_access_time>> return (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
+ *   - mm/migrate.c|709| <<folio_migrate_flags>> if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) {
+ *   - mm/migrate.c|2652| <<migrate_misplaced_folio_prepare>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING))
+ *   - mm/migrate.c|2705| <<migrate_misplaced_folio>> if ((sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)
+ *   - mm/vmscan.c|4771| <<should_abort_scan>> mark = sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING ?
+ *   - mm/vmscan.c|6699| <<pgdat_balanced>> if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)
+ */
 #define NUMA_BALANCING_DISABLED		0x0
 #define NUMA_BALANCING_NORMAL		0x1
 #define NUMA_BALANCING_MEMORY_TIERING	0x2
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 58009fa66..0d1e4e600 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -938,6 +938,16 @@ struct sk_buff {
 				nohdr:1,
 				fclone:2,
 				peeked:1,
+				/*
+				 * 在以下设置sk_buff->head_frag:
+				 *   - net/core/skbuff.c|494| <<build_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|516| <<build_skb_around>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|562| <<napi_build_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|780| <<__netdev_alloc_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|868| <<napi_alloc_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|6696| <<pskb_carve_inside_header>> skb->head_frag = 0;
+				 *   - net/core/skbuff.c|6829| <<pskb_carve_inside_nonlinear>> skb->head_frag = 0;
+				 */
 				head_frag:1,
 				pfmemalloc:1,
 				pp_recycle:1; /* page_pool recycle indicator */
diff --git a/include/linux/vdpa.h b/include/linux/vdpa.h
index 2e7a30fe6..0531202e8 100644
--- a/include/linux/vdpa.h
+++ b/include/linux/vdpa.h
@@ -374,6 +374,22 @@ struct vdpa_config_ops {
 			      u16 idx, u64 desc_area, u64 driver_area,
 			      u64 device_area);
 	void (*set_vq_num)(struct vdpa_device *vdev, u16 idx, u32 num);
+	/*
+	 * 在以下调用vdpa_config_ops->kick_vq:
+	 *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+	 *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+	 * 在以下设置vdpa_config_ops->kick_vq:
+	 *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+	 *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+	 *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+	 */
 	void (*kick_vq)(struct vdpa_device *vdev, u16 idx);
 	void (*kick_vq_with_data)(struct vdpa_device *vdev, u32 data);
 	void (*set_vq_cb)(struct vdpa_device *vdev, u16 idx,
@@ -464,6 +480,17 @@ struct vdpa_device *__vdpa_alloc_device(struct device *parent,
  *
  * Return allocated data structure or ERR_PTR upon error
  */
+/*
+ * called by:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|480| <<eni_vdpa_probe>> eni_vdpa = vdpa_alloc_device(struct eni_vdpa, vdpa,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|707| <<ifcvf_vdpa_dev_add>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3879| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mgtdev->vdpa_ops,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|475| <<octep_vdpa_dev_add>> oct_vdpa = vdpa_alloc_device(struct octep_vdpa, vdpa, &pdev->dev, &octep_vdpa_ops, 1, 1,
+ *   - drivers/vdpa/pds/vdpa_dev.c|634| <<pds_vdpa_dev_add>> pdsv = vdpa_alloc_device(struct pds_vdpa_device, vdpa_dev,
+ *   - drivers/vdpa/solidrun/snet_main.c|1012| <<snet_vdpa_probe_vf>> snet = vdpa_alloc_device(struct snet, vdpa, &pdev->dev, &snet_config_ops, 1, 1, NULL,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|2010| <<vduse_dev_init_vdpa>> vdev = vdpa_alloc_device(struct vduse_vdpa, vdpa, dev->dev,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|504| <<vp_vdpa_dev_add>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+ */
 #define vdpa_alloc_device(dev_struct, member, parent, config, ngroups, nas, \
 			  name, use_va) \
 			  container_of((__vdpa_alloc_device( \
diff --git a/include/linux/virtio.h b/include/linux/virtio.h
index dd88682e2..c5e2f38c8 100644
--- a/include/linux/virtio.h
+++ b/include/linux/virtio.h
@@ -150,6 +150,12 @@ struct virtio_device {
 	int index;
 	bool failed;
 	bool config_core_enabled;
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	bool config_driver_disabled;
 	bool config_change_pending;
 	spinlock_t config_lock;
diff --git a/include/target/target_core_base.h b/include/target/target_core_base.h
index 97099a5e3..d75090871 100644
--- a/include/target/target_core_base.h
+++ b/include/target/target_core_base.h
@@ -725,6 +725,21 @@ struct se_dev_attrib {
 	u32		max_unmap_block_desc_count;
 	u32		unmap_granularity;
 	u32		unmap_granularity_alignment;
+	/*
+	 * 在以下使用se_dev_attrib->max_write_same_len:
+	 *   - drivers/target/target_core_configfs.c|595| <<global>> DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|618| <<global>> DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|1318| <<global>> CONFIGFS_ATTR(, max_write_same_len);
+	 *   - drivers/target/target_core_device.c|775| <<target_alloc_device>> dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
+	 *   - drivers/target/target_core_file.c|174| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_file.c|192| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0x1000;
+	 *   - drivers/target/target_core_iblock.c|143| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;
+	 *   - drivers/target/target_core_iblock.c|145| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_sbc.c|294| <<sbc_setup_write_same>> if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
+	 *   - drivers/target/target_core_sbc.c|296| <<sbc_setup_write_same>> pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
+	 *                    sectors, cmd->se_dev->dev_attrib.max_write_same_len);
+	 *   - drivers/target/target_core_spc.c|599| <<spc_emulate_evpd_b0>> put_unaligned_be64(dev->dev_attrib.max_write_same_len, &buf[36]);
+	 */
 	u32		max_write_same_len;
 	u8		submit_type;
 	struct se_device *da_dev;
diff --git a/include/uapi/linux/vhost_types.h b/include/uapi/linux/vhost_types.h
index d7656908f..16964ba19 100644
--- a/include/uapi/linux/vhost_types.h
+++ b/include/uapi/linux/vhost_types.h
@@ -164,6 +164,17 @@ struct vhost_vdpa_iova_range {
 };
 
 /* Feature bits */
+/*
+ * 在以下使用VHOST_F_LOG_ALL:
+ *   - drivers/vhost/vhost.h|302| <<global>> (1ULL << VHOST_F_LOG_ALL) |
+ *   - drivers/vhost/net.c|1315| <<handle_rx>> vq_log = unlikely(vhost_has_feature(vq, VHOST_F_LOG_ALL)) ?
+ *   - drivers/vhost/net.c|1859| <<vhost_net_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/scsi.c|2516| <<vhost_scsi_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/test.c|270| <<vhost_test_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/vhost.c|1398| <<memory_access_ok>> log = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);
+ *   - drivers/vhost/vhost.c|2151| <<vq_log_access_ok>> vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/vsock.c|902| <<vhost_vsock_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ */
 /* Log all write descriptors. Can be changed while device is active. */
 #define VHOST_F_LOG_ALL 26
 /* vhost-net should add virtio_net_hdr for RX, and strip for TX packets. */
diff --git a/include/uapi/linux/virtio_ring.h b/include/uapi/linux/virtio_ring.h
index f8c20d3de..3202cf33a 100644
--- a/include/uapi/linux/virtio_ring.h
+++ b/include/uapi/linux/virtio_ring.h
@@ -219,6 +219,23 @@ static inline unsigned vring_size(unsigned int num, unsigned long align)
 /* Assuming a given event_idx value from the other side, if
  * we have just incremented index from old to new_idx,
  * should we trigger an event? */
+/*
+ * 在以下调用vring_need_event():
+ *   - drivers/vhost/vhost.c|3869| <<vhost_notify>> return vring_need_event(vhost16_to_cpu(vq, event), new, old);
+ *   - drivers/vhost/vringh.c|535| <<vhost_notify>> notify = vring_need_event(used_event,
+ *             vrh->last_used_idx + vrh->completed, vrh->last_used_idx);
+ *   - drivers/virtio/virtio_ring.c|735| <<virtqueue_kick_prepare_split>> needs_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,
+ *             vring_avail_event(&vq->split.vring)), new, old);
+ *   - drivers/virtio/virtio_ring.c|1646| <<virtqueue_kick_prepare_packed>> needs_kick = vring_need_event(event_idx, new, old);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|233| <<kick_available>> need = vring_need_event(vring_avail_event(&ring),
+ *             guest.avail_idx, guest.kicked_avail_idx);
+ *   - tools/virtio/ringtest/virtio_ring_0_9.c|326| <<call_used>> need = vring_need_event(vring_used_event(&ring),
+ *             host.used_idx, host.called_used_idx);
+ *
+ * 如果new距离event_idx的距离更短(比距离old), 发signal.
+ * old是上次理论上发signal的index.
+ * 如果还没处理到old, 说明这次也不用发.
+ */
 static inline int vring_need_event(__u16 event_idx, __u16 new_idx, __u16 old)
 {
 	/* Note: Xen has similar logic for notification hold-off
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 3e5a6bf58..231e558fb 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2074,6 +2074,22 @@ void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 		sched_core_enqueue(rq, p);
 }
 
+/*
+ * 在以下使用dequeue_task():
+ *   - kernel/sched/core.c|2126| <<deactivate_task>> dequeue_task(rq, p, flags);
+ *   - kernel/sched/core.c|2131| <<block_task>> if (dequeue_task(rq, p, DEQUEUE_SLEEP | flags))
+ *   - kernel/sched/core.c|2734| <<__do_set_cpus_allowed>> dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/core.c|7214| <<rt_mutex_setprio>> dequeue_task(rq, p, DEQUEUE_SLEEP | DEQUEUE_DELAYED | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/core.c|7219| <<rt_mutex_setprio>> dequeue_task(rq, p, queue_flag);
+ *   - kernel/sched/core.c|7917| <<sched_setnuma>> dequeue_task(rq, p, DEQUEUE_SAVE);
+ *   - kernel/sched/core.c|9069| <<sched_move_task>> dequeue_task(rq, tsk, queue_flags);
+ *   - kernel/sched/core.c|10687| <<sched_deq_and_put_task>> dequeue_task(rq, p, queue_flags | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/ext.c|4970| <<scx_ops_disable_workfn>> dequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/ext.c|5688| <<scx_ops_enable>> dequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/syscalls.c|96| <<set_user_nice>> dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/syscalls.c|713| <<__sched_setscheduler>> dequeue_task(rq, p, DEQUEUE_SLEEP | DEQUEUE_DELAYED | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/syscalls.c|718| <<__sched_setscheduler>> dequeue_task(rq, p, queue_flags);
+ */
 /*
  * Must only return false when DEQUEUE_SLEEP.
  */
@@ -2095,6 +2111,15 @@ inline bool dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 	 * and mark the task ->sched_delayed.
 	 */
 	uclamp_rq_dec(rq, p);
+	/*
+	 * 在以下使用dequeue_task方法:
+	 *   - kernel/sched/deadline.c|3120| <<global>> .dequeue_task = dequeue_task_dl,
+	 *   - kernel/sched/ext.c|4369| <<global>> .dequeue_task = dequeue_task_scx,
+	 *   - kernel/sched/fair.c|13565| <<global>> .dequeue_task = dequeue_task_fair,
+	 *   - kernel/sched/idle.c|521| <<global>> .dequeue_task = dequeue_task_idle,
+	 *   - kernel/sched/rt.c|2620| <<global>> .dequeue_task = dequeue_task_rt,
+	 *   - kernel/sched/stop_task.c|100| <<global>> .dequeue_task = dequeue_task_stop,
+	 */
 	return p->sched_class->dequeue_task(rq, p, flags);
 }
 
@@ -5306,6 +5331,45 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 	calculate_sigpending();
 }
 
+/*
+ * 在以下调用switch_mm_irqs_off():
+ *   - arch/x86/kernel/alternative.c|1851| <<use_temporary_mm>> switch_mm_irqs_off(NULL, mm, current);
+ *   - arch/x86/kernel/alternative.c|1873| <<unuse_temporary_mm>> switch_mm_irqs_off(NULL, prev_state.mm, current);
+ *   - arch/x86/mm/tlb.c|328| <<switch_mm>> switch_mm_irqs_off(NULL, next, tsk);
+ *   - arch/x86/mm/tlb.c|815| <<flush_tlb_func>> switch_mm_irqs_off(NULL, &init_mm, NULL);
+ *   - kernel/kthread.c|1461| <<kthread_use_mm>> switch_mm_irqs_off(active_mm, mm, tsk);
+ *   - kernel/sched/core.c|5378| <<context_switch>> switch_mm_irqs_off(prev->active_mm, next->mm, next);
+ *
+ * 在以下使用indirect_branch_prediction_barrier():
+ *   - arch/x86/kernel/cpu/bugs.c|2311| <<ib_prctl_set>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/svm/svm.c|1610| <<svm_vcpu_load>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/nested.c|5023| <<nested_vmx_vmexit>> indirect_branch_prediction_barrier();
+ *   - arch/x86/kvm/vmx/vmx.c|1481| <<vmx_vcpu_load_vmcs>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|441| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *   - arch/x86/mm/tlb.c|452| <<cond_mitigation>> indirect_branch_prediction_barrier();
+ *
+ * 在以下使用X86_FEATURE_USE_IBPB:
+ *   - arch/x86/include/asm/nospec-branch.h|558| <<indirect_branch_prediction_barrier>>
+ *          alternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);
+ *
+ * 部分例子.
+ *
+ * context_switch()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ *
+ * switch_mm()
+ * -> switch_mm_irqs_off()
+ *    -> cond_mitigation()
+ *       -> indirect_branch_prediction_barrier()
+ *          -> alternative_msr_write(MSR_IA32_PRED_CMD,
+ *                                   x86_pred_cmd,
+ *                                   X86_FEATURE_USE_IBPB);
+ */
 /*
  * context_switch - switch to the new MM and the new thread's register state.
  */
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 26958431d..32ae1b368 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2628,6 +2628,28 @@ static void numa_migrate_preferred(struct task_struct *p)
 	if (unlikely(p->numa_preferred_nid == NUMA_NO_NODE || !p->numa_faults))
 		return;
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	/* Periodically retry migrating the task to the preferred node */
 	interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
 	p->numa_migrate_retry = jiffies + interval;
@@ -2701,6 +2723,28 @@ static void update_task_scan_period(struct task_struct *p,
 	 * node is overloaded. In either case, scan slower
 	 */
 	if (local + shared == 0 || p->numa_faults_locality[2]) {
+		/*
+		 * 在以下设置task_struct->numa_scan_period:
+		 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+		 *            p->numa_scan_period << 1);
+		 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+		 *            task_scan_min(p), task_scan_max(p));
+		 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+		 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+		 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+		 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+		 * 在以下使用task_struct->numa_scan_period:
+		 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+		 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+		 *            p->numa_scan_period << 1);
+		 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+		 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+		 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+		 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+		 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+		 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+		 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+		 */
 		p->numa_scan_period = min(p->numa_scan_period_max,
 			p->numa_scan_period << 1);
 
@@ -3284,6 +3328,36 @@ static bool vma_is_accessed(struct mm_struct *mm, struct vm_area_struct *vma)
  * The expensive part of numa migration is done from task_work context.
  * Triggered from task_tick_numa().
  */
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用task_struct->numa_work:
+ *   - kernel/sched/fair.c|3305| <<task_numa_work>> SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
+ *   - kernel/sched/fair.c|3560| <<init_numa_balancing>> p->numa_work.next = &p->numa_work;
+ *   - kernel/sched/fair.c|3568| <<init_numa_balancing>> init_task_work(&p->numa_work, task_numa_work);
+ *   - kernel/sched/fair.c|3595| <<task_tick_numa>> struct callback_head *work = &curr->numa_work;
+ *
+ * 在以下使用task_numa_work():
+ *   - kernel/sched/fair.c|3564| <<init_numa_balancing>> init_task_work(&p->numa_work, task_numa_work);
+ */
 static void task_numa_work(struct callback_head *work)
 {
 	unsigned long migrate, next_scan, now = jiffies;
@@ -3324,6 +3398,28 @@ static void task_numa_work(struct callback_head *work)
 	if (time_before(now, migrate))
 		return;
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	if (p->numa_scan_period == 0) {
 		p->numa_scan_period_max = task_scan_max(p);
 		p->numa_scan_period = task_scan_start(p);
@@ -3368,6 +3464,10 @@ static void task_numa_work(struct callback_head *work)
 	}
 
 	for (; vma; vma = vma_next(&vmi)) {
+		/*
+		 * 只在这里调用vma_policy_mof().
+		 * 似乎返回false会更好?
+		 */
 		if (!vma_migratable(vma) || !vma_policy_mof(vma) ||
 			is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {
 			trace_sched_skip_vma_numa(mm, vma, NUMAB_SKIP_UNSUITABLE);
@@ -3544,6 +3644,28 @@ void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 			mm->numa_scan_seq = 0;
 		}
 	}
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	p->node_stamp			= 0;
 	p->numa_scan_seq		= mm ? mm->numa_scan_seq : 0;
 	p->numa_scan_period		= sysctl_numa_balancing_scan_delay;
@@ -3582,6 +3704,30 @@ void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 /*
  * Drive the periodic memory faults..
  */
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - kernel/sched/fair.c|13067| <<task_tick_fair>> task_tick_numa(rq, curr);
+ */
 static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 {
 	struct callback_head *work = &curr->numa_work;
@@ -3593,6 +3739,28 @@ static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 	if (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work)
 		return;
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	/*
 	 * Using runtime rather than walltime has the dual advantage that
 	 * we (mostly) drive the selection from busy threads and that the
@@ -3643,6 +3811,28 @@ static void update_scan_period(struct task_struct *p, int new_cpu)
 			return;
 	}
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	p->numa_scan_period = task_scan_start(p);
 }
 
@@ -5369,6 +5559,10 @@ static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)
 
 static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5427| <<dequeue_entity>> set_delayed(se);
+ */
 static void set_delayed(struct sched_entity *se)
 {
 	se->sched_delayed = 1;
@@ -5381,6 +5575,11 @@ static void set_delayed(struct sched_entity *se)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5398| <<finish_delayed_dequeue_entity>> clear_delayed(se);
+ *   - kernel/sched/fair.c|6871| <<requeue_delayed_entity>> clear_delayed(se);
+ */
 static void clear_delayed(struct sched_entity *se)
 {
 	se->sched_delayed = 0;
@@ -5400,6 +5599,12 @@ static inline void finish_delayed_dequeue_entity(struct sched_entity *se)
 		se->vlag = 0;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+ *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+ *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+ */
 static bool
 dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
@@ -5421,9 +5626,22 @@ dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 
 		SCHED_WARN_ON(delay && se->sched_delayed);
 
+		/*
+		 * 注释:
+		 * Delay dequeueing tasks until they get selected or woken.
+		 *
+		 * By delaying the dequeue for non-eligible tasks, they remain in the
+		 * competition and can burn off their negative lag. When they get selected
+		 * they'll have positive lag by definition.
+		 *
+		 * DELAY_ZERO clips the lag on dequeue (or wakeup) to 0.
+		 */
 		if (sched_feat(DELAY_DEQUEUE) && delay &&
 		    !entity_eligible(cfs_rq, se)) {
 			update_load_avg(cfs_rq, se, 0);
+			/*
+			 * 只在此处调用
+			 */
 			set_delayed(se);
 			return false;
 		}
@@ -5831,6 +6049,11 @@ static int tg_throttle_down(struct task_group *tg, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|6414| <<check_enqueue_throttle>> throttle_cfs_rq(cfs_rq);
+ *   - kernel/sched/fair.c|6450| <<check_cfs_rq_runtime>> return throttle_cfs_rq(cfs_rq);
+ */
 static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 {
 	struct rq *rq = rq_of(cfs_rq);
@@ -5886,6 +6109,12 @@ static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 		flags = DEQUEUE_SLEEP | DEQUEUE_SPECIAL;
 		if (se->sched_delayed)
 			flags |= DEQUEUE_DELAYED;
+		/*
+		 * called by:
+		 *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+		 */
 		dequeue_entity(qcfs_rq, se, flags);
 
 		if (cfs_rq_is_idle(group_cfs_rq(se)))
@@ -5986,6 +6215,12 @@ void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)
 		if (se->sched_delayed) {
 			int flags = DEQUEUE_SLEEP | DEQUEUE_DELAYED;
 
+			/*
+			 * called by:
+			 *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+			 *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+			 *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+			 */
 			dequeue_entity(qcfs_rq, se, flags);
 		} else if (se->on_rq)
 			break;
@@ -6409,6 +6644,11 @@ static void sync_throttle(struct task_group *tg, int cpu)
 	cfs_rq->throttled_clock_pelt = rq_clock_pelt(cpu_rq(cpu));
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5595| <<put_prev_entity>> check_cfs_rq_runtime(cfs_rq);
+ *   - kernel/sched/fair.c|8821| <<pick_task_fair>> if (unlikely(check_cfs_rq_runtime(cfs_rq)))
+ */
 /* conditionally throttle active cfs_rq's from put_prev_entity() */
 static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 {
@@ -7012,6 +7252,13 @@ static void set_next_buddy(struct sched_entity *se);
  *  0 - dequeue throttled
  *  1 - dequeue complete
  */
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5580| <<pick_next_entity>> dequeue_entities(rq, se, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/fair.c|7194| <<dequeue_task_fair>> if (dequeue_entities(rq, &p->se, flags) < 0)
+ *   - kernel/sched/fair.c|8692| <<task_dead_fair>> dequeue_entities(rq, se, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/fair.c|13367| <<unregister_fair_sched_group>> dequeue_entities(rq, se, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ */
 static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 {
 	bool was_sched_idle = sched_idle_rq(rq);
@@ -7039,6 +7286,12 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 	for_each_sched_entity(se) {
 		cfs_rq = cfs_rq_of(se);
 
+		/*
+		 * called by:
+		 *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+		 */
 		if (!dequeue_entity(cfs_rq, se, flags)) {
 			if (p && &p->se == se)
 				return -1;
diff --git a/kernel/vhost_task.c b/kernel/vhost_task.c
index 8800f5acc..02c226e5b 100644
--- a/kernel/vhost_task.c
+++ b/kernel/vhost_task.c
@@ -115,6 +115,14 @@ EXPORT_SYMBOL_GPL(vhost_task_stop);
  * failure. The returned task is inactive, and the caller must fire it up
  * through vhost_task_start().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7420| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread =
+ *            vhost_task_create(kvm_nx_huge_page_recovery_worker,
+ *                              kvm_nx_huge_page_recovery_worker_kill, kvm, "kvm-nx-lpage-recovery");
+ * drivers/vhost/vhost.c|701| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+ *            vhost_worker_killed, worker, name);
+ */
 struct vhost_task *vhost_task_create(bool (*fn)(void *),
 				     void (*handle_sigkill)(void *), void *arg,
 				     const char *name)
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index 9ec806f98..caca66327 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -159,6 +159,41 @@ size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t size)
 }
 EXPORT_SYMBOL(fault_in_iov_iter_writeable);
 
+/*
+ * called by:
+ *   - drivers/acpi/pfr_update.c|458| <<pfru_write>> iov_iter_init(&iter, ITER_SOURCE, &iov, 1, len);
+ *   - drivers/fsi/fsi-sbefifo.c|845| <<sbefifo_user_read>> iov_iter_init(&resp_iter, ITER_DEST, &resp_iov, 1, len); 
+ *   - drivers/net/ppp/ppp_generic.c|486| <<ppp_read>> iov_iter_init(&to, ITER_DEST, &iov, 1, count); 
+ *   - drivers/vhost/net.c|704| <<init_iov_iter>> iov_iter_init(iter, ITER_SOURCE, vq->iov, out, len);
+ *   - drivers/vhost/net.c|1357| <<handle_rx>> iov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, 1, 1);
+ *   - drivers/vhost/net.c|1364| <<handle_rx>> iov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, in, vhost_len);
+ *   - drivers/vhost/scsi.c|854| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+ *             cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+ *   - drivers/vhost/scsi.c|1307| <<vhost_scsi_get_desc>> iov_iter_init(&vc->out_iter, ITER_SOURCE,
+ *             vq->iov, vc->out, vc->out_size);
+ *   - drivers/vhost/scsi.c|1481| <<vhost_scsi_handle_vq>> iov_iter_init(&in_iter, ITER_DEST,
+ *             &vq->iov[vc.out], vc.in, vc.rsp_size + exp_data_len);
+ *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_send_tmf_resp>> iov_iter_init(&iov_iter, ITER_DEST,
+ *             resp_iov, in_iovs, sizeof(rsp));
+ *   - drivers/vhost/scsi.c|1841| <<vhost_scsi_send_an_resp>> iov_iter_init(&iov_iter, ITER_DEST,
+ *             &vq->iov[vc->out], vc->in, sizeof(rsp));
+ *   - drivers/vhost/vhost.c|1423| <<vhost_copy_to_user>> iov_iter_init(&t, ITER_DEST,
+ *             vq->iotlb_iov, ret, size);
+ *   - drivers/vhost/vhost.c|1485| <<vhost_copy_from_user>> iov_iter_init(&f, ITER_SOURCE,
+ *             vq->iotlb_iov, ret, size);
+ *   - drivers/vhost/vhost.c|3007| <<get_indirect>> iov_iter_init(&from, ITER_SOURCE,
+ *             vq->indirect, ret, len);
+ *   - drivers/vhost/vringh.c|1204| <<copy_from_iotlb>> iov_iter_init(&iter, ITER_SOURCE, ivec.iov.iovec, ret, translated);
+ *   - drivers/vhost/vringh.c|1250| <<copy_to_iotlb>> iov_iter_init(&iter, ITER_DEST, ivec.iov.iovec, ret, translated);
+ *   - drivers/vhost/vsock.c|178| <<vhost_transport_do_send_pkt>> iov_iter_init(&iov_iter, ITER_DEST, &vq->iov[out], in, iov_len);
+ *   - drivers/vhost/vsock.c|388| <<vhost_vsock_alloc_skb>> iov_iter_init(&iov_iter, ITER_SOURCE, vq->iov, out, len);
+ *   - fs/fuse/ioctl.c|328| <<fuse_do_ioctl>> iov_iter_init(&ii, ITER_SOURCE, in_iov, in_iovs, in_size);
+ *   - fs/fuse/ioctl.c|395| <<fuse_do_ioctl>> iov_iter_init(&ii, ITER_DEST, out_iov, out_iovs, transferred);
+ *   - fs/seq_file.c|159| <<seq_read>> iov_iter_init(&iter, ITER_DEST, &iov, 1, size);
+ *   - io_uring/net.c|639| <<io_send>> iov_iter_init(&kmsg->msg.msg_iter, ITER_SOURCE, arg.iovs, ret, arg.out_len);
+ *   - io_uring/net.c|1108| <<io_recv_buf_select>> iov_iter_init(&kmsg->msg.msg_iter, ITER_DEST, arg.iovs, ret, arg.out_len);
+ *   - lib/iov_iter.c|1487| <<__import_iovec>> iov_iter_init(i, type, iov, nr_segs, total_len);
+ */
 void iov_iter_init(struct iov_iter *i, unsigned int direction,
 			const struct iovec *iov, unsigned long nr_segs,
 			size_t count)
@@ -1209,6 +1244,28 @@ static ssize_t __iov_iter_get_pages_alloc(struct iov_iter *i,
 	return -EFAULT;
 }
 
+/*
+ * 在以下调用iov_iter_get_pages2():
+ *   - drivers/block/ublk_drv.c|879| <<ublk_copy_user_pages>> len = iov_iter_get_pages2(uiter,
+ *             iter.pages, iov_iter_count(uiter), UBLK_MAX_PIN_PAGES, &off);
+ *   - drivers/vhost/scsi.c|1135| <<vhost_scsi_map_to_sgl>> bytes = iov_iter_get_pages2(iter,
+ *             pages, LONG_MAX, VHOST_SCSI_PREALLOC_UPAGES, &offset);
+ *   - fs/ceph/file.c|100| <<__iter_get_bvecs>> bytes = iov_iter_get_pages2(iter, pages,
+ *             maxsize - size, ITER_GET_BVECS_PAGES, &start);
+ *   - fs/fuse/dev.c|786| <<fuse_copy_fill>> err = iov_iter_get_pages2(cs->iter, &page,
+ *             PAGE_SIZE, 1, &off);
+ *   - fs/splice.c|1465| <<iter_to_pipe>> left = iov_iter_get_pages2(from, pages, ~0UL, 16, &start);
+ *   - net/ceph/messenger.c|992| <<ceph_msg_data_iter_next>> len = iov_iter_get_pages2(&cursor->iov_iter,
+ *             &page, PAGE_SIZE, 1, page_offset);
+ *   - net/core/datagram.c|642| <<zerocopy_fill_skb_from_iter>> copied = iov_iter_get_pages2(from, pages,
+ *             length, MAX_SKB_FRAGS - frag, &start);
+ *   - net/core/skmsg.c|328| <<sk_msg_zerocopy_from_iter>> copied = iov_iter_get_pages2(from, pages,
+ *             bytes, maxpages, &offset);
+ *   - net/rds/message.c|393| <<rds_message_zcopy_from_user>> copied = iov_iter_get_pages2(from, &pages,
+ *             PAGE_SIZE, 1, &start);
+ *   - net/tls/tls_sw.c|1382| <<tls_setup_from_iter>> copied = iov_iter_get_pages2(from, pages, length,
+ *             maxpages, &offset);
+ */
 ssize_t iov_iter_get_pages2(struct iov_iter *i, struct page **pages,
 		size_t maxsize, unsigned maxpages, size_t *start)
 {
diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 5bb6b8aff..ba66a73d6 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -193,6 +193,12 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
  *    that previously used with __sg_alloc_table().
  *
  **/
+/*
+ * called by:
+ *   - lib/scatterlist.c|246| <<sg_free_append_table>> __sg_free_table(&table->sgt, SG_MAX_SINGLE_ALLOC, 0, sg_kfree, table->total_nents);
+ *   - lib/scatterlist.c|259| <<sg_free_table>> __sg_free_table(table, SG_MAX_SINGLE_ALLOC, 0, sg_kfree, table->orig_nents);
+ *   - lib/sg_pool.c|141| <<sg_free_table_chained>> __sg_free_table(table, SG_CHUNK_SIZE, nents_first_chunk, sg_pool_free, table->orig_nents);
+ */
 void __sg_free_table(struct sg_table *table, unsigned int max_ents,
 		     unsigned int nents_first_chunk, sg_free_fn *free_fn,
 		     unsigned int num_ents)
@@ -283,6 +289,13 @@ EXPORT_SYMBOL(sg_free_table);
  *   __sg_free_table() to cleanup any leftover allocations.
  *
  **/
+/*
+ * called by:
+ *   - lib/scatterlist.c|385| <<sg_alloc_table>> ret = __sg_alloc_table(table, nents,
+ *         SG_MAX_SINGLE_ALLOC, NULL, 0, gfp_mask, sg_kmalloc);
+ *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents,
+ *         SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+ */
 int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		     unsigned int max_ents, struct scatterlist *first_chunk,
 		     unsigned int nents_first_chunk, gfp_t gfp_mask,
@@ -376,6 +389,13 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 {
 	int ret;
 
+	/*
+	 * called by:
+	 *   - lib/scatterlist.c|385| <<sg_alloc_table>> ret = __sg_alloc_table(table, nents,
+	 *         SG_MAX_SINGLE_ALLOC, NULL, 0, gfp_mask, sg_kmalloc);
+	 *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents,
+	 *         SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+	 */
 	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
 			       NULL, 0, gfp_mask, sg_kmalloc);
 	if (unlikely(ret))
diff --git a/lib/sg_pool.c b/lib/sg_pool.c
index 9bfe60ca3..55e4f8eb0 100644
--- a/lib/sg_pool.c
+++ b/lib/sg_pool.c
@@ -4,7 +4,16 @@
 #include <linux/mempool.h>
 #include <linux/slab.h>
 
+/*
+ * 在以下使用SG_MEMPOOL_NR:
+ *   - lib/sg_pool.c|187| <<sg_pool_init>> for (i = 0; i < SG_MEMPOOL_NR; i++) {
+ *   - lib/sg_pool.c|226| <<sg_pool_init>> for (i = 0; i < SG_MEMPOOL_NR; i++) {
+ */
 #define SG_MEMPOOL_NR		ARRAY_SIZE(sg_pools)
+/*
+ * 在以下使用SG_MEMPOOL_SIZE:
+ *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+ */
 #define SG_MEMPOOL_SIZE		2
 
 struct sg_pool {
@@ -18,6 +27,14 @@ struct sg_pool {
 #if (SG_CHUNK_SIZE < 32)
 #error SG_CHUNK_SIZE is too small (must be 32 or greater)
 #endif
+/*
+ * 在以下使用sg_pools[]:
+ *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+ *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+ *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+ *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+ *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+ */
 static struct sg_pool sg_pools[] = {
 	SP(8),
 	SP(16),
@@ -37,6 +54,11 @@ static struct sg_pool sg_pools[] = {
 };
 #undef SP
 
+/*
+ * called by:
+ *   - lib/sg_pool.c|83| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+ *   - lib/sg_pool.c|99| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+ */
 static inline unsigned int sg_pool_index(unsigned short nents)
 {
 	unsigned int index;
@@ -51,18 +73,44 @@ static inline unsigned int sg_pool_index(unsigned short nents)
 	return index;
 }
 
+/*
+ * called by:
+ *   - lib/sg_pool.c|126| <<sg_free_table_chained>> __sg_free_table(table, SG_CHUNK_SIZE,
+ *      nents_first_chunk, sg_pool_free, table->orig_nents);
+ */
 static void sg_pool_free(struct scatterlist *sgl, unsigned int nents)
 {
 	struct sg_pool *sgp;
 
+	/*
+	 * 在以下使用sg_pools[]:
+	 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+	 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 */
 	sgp = sg_pools + sg_pool_index(nents);
 	mempool_free(sgl, sgp->pool);
 }
 
+/*
+ * called by:
+ *   - lib/sg_pool.c|181| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table,
+ *     nents, SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+ */
 static struct scatterlist *sg_pool_alloc(unsigned int nents, gfp_t gfp_mask)
 {
 	struct sg_pool *sgp;
 
+	/*
+	 * 在以下使用sg_pools[]:
+	 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+	 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 */
 	sgp = sg_pools + sg_pool_index(nents);
 	return mempool_alloc(sgp->pool, gfp_mask);
 }
@@ -81,6 +129,26 @@ static struct scatterlist *sg_pool_alloc(unsigned int nents, gfp_t gfp_mask)
  *    to sg_alloc_table_chained().
  *
  **/
+/*
+ * called by:
+ *   - drivers/block/rnbd/rnbd-clt.c|373| <<rnbd_softirq_done_fn>> sg_free_table_chained(&iu->sgt, RNBD_INLINE_SG_CNT);
+ *   - drivers/block/rnbd/rnbd-clt.c|1153| <<rnbd_queue_rq>> sg_free_table_chained(&iu->sgt, RNBD_INLINE_SG_CNT);
+ *   - drivers/block/virtio_blk.c|210| <<virtblk_unmap_data>> sg_free_table_chained(&vbr->sg_table, VIRTIO_BLK_INLINE_SG_CNT);
+ *   - drivers/nvme/host/fc.c|2619| <<nvme_fc_map_data>> sg_free_table_chained(&freq->sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/host/fc.c|2642| <<nvme_fc_unmap_data>> sg_free_table_chained(&freq->sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1222| <<nvme_rdma_dma_unmap_req>> sg_free_table_chained(&req->metadata_sgl->sg_table, NVME_INLINE_METADATA_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1228| <<nvme_rdma_dma_unmap_req>> sg_free_table_chained(&req->data_sgl.sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1516| <<nvme_rdma_dma_map_req>> sg_free_table_chained(&req->metadata_sgl->sg_table, NVME_INLINE_METADATA_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1522| <<nvme_rdma_dma_map_req>> sg_free_table_chained(&req->data_sgl.sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/target/loop.c|78| <<nvme_loop_complete_rq>> sg_free_table_chained(&iod->sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/scsi/scsi_lib.c|583| <<scsi_free_sgtables>> sg_free_table_chained(&cmd->sdb.table, SCSI_INLINE_SG_CNT);
+ *   - drivers/scsi/scsi_lib.c|586| <<scsi_free_sgtables>> sg_free_table_chained(&cmd->prot_sdb->table, SCSI_INLINE_PROT_SG_CNT);
+ *   - fs/smb/server/transport_rdma.c|1322| <<smb_direct_free_rdma_rw_msg>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1434| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1448| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - lib/sg_pool.c|204| <<sg_alloc_table_chained>> sg_free_table_chained(table, nents_first_chunk);
+ *   - net/sunrpc/xprtrdma/svc_rdma_rw.c|94| <<__svc_rdma_put_rw_ctxt>> sg_free_table_chained(&ctxt->rw_sg_table, ctxt->rw_first_sgl_nents);
+ */
 void sg_free_table_chained(struct sg_table *table,
 		unsigned nents_first_chunk)
 {
@@ -109,6 +177,25 @@ EXPORT_SYMBOL_GPL(sg_free_table_chained);
  *    non-chain SGL.
  *
  **/
+/*
+ * called by:
+ *   - drivers/block/rnbd/rnbd-clt.c|1131| <<rnbd_queue_rq>> err = sg_alloc_table_chained(&iu->sgt,
+ *   - drivers/block/virtio_blk.c|223| <<virtblk_map_data>> err = sg_alloc_table_chained(&vbr->sg_table,
+ *   - drivers/nvme/host/fc.c|2608| <<nvme_fc_map_data>> ret = sg_alloc_table_chained(&freq->sg_table,
+ *   - drivers/nvme/host/rdma.c|1473| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->data_sgl.sg_table,
+ *   - drivers/nvme/host/rdma.c|1492| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->metadata_sgl->sg_table,
+ *   - drivers/nvme/target/loop.c|157| <<nvme_loop_queue_rq>> if (sg_alloc_table_chained(&iod->sg_table,
+ *   - drivers/scsi/scsi_lib.c|1135| <<scsi_alloc_sgtables>> if (unlikely(sg_alloc_table_chained(&cmd->sdb.table, nr_segs,
+ *   - drivers/scsi/scsi_lib.c|1180| <<scsi_alloc_sgtables>> if (sg_alloc_table_chained(&prot_sdb->table,
+ *   - fs/smb/server/transport_rdma.c|1422| <<smb_direct_rdma_xmit>> ret = sg_alloc_table_chained(&msg->sgt,
+ *   - net/sunrpc/xprtrdma/svc_rdma_rw.c|78| <<svc_rdma_get_rw_ctxt>> if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges, ctxt->rw_sg_table.sgl, first_sgl_nents))
+ *
+ * struct sg_table {
+ *     struct scatterlist *sgl;        // the list
+ *     unsigned int nents;             // number of mapped entries
+ *     unsigned int orig_nents;        // original size of list
+ * };
+ */
 int sg_alloc_table_chained(struct sg_table *table, int nents,
 		struct scatterlist *first_chunk, unsigned nents_first_chunk)
 {
@@ -130,6 +217,13 @@ int sg_alloc_table_chained(struct sg_table *table, int nents,
 		nents_first_chunk = 0;
 	}
 
+	/*
+	 * called by:
+	 *   - lib/scatterlist.c|385| <<sg_alloc_table>> ret = __sg_alloc_table(table, nents,
+	 *         SG_MAX_SINGLE_ALLOC, NULL, 0, gfp_mask, sg_kmalloc);
+	 *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents,
+	 *         SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+	 */
 	ret = __sg_alloc_table(table, nents, SG_CHUNK_SIZE,
 			       first_chunk, nents_first_chunk,
 			       GFP_ATOMIC, sg_pool_alloc);
@@ -139,11 +233,30 @@ int sg_alloc_table_chained(struct sg_table *table, int nents,
 }
 EXPORT_SYMBOL_GPL(sg_alloc_table_chained);
 
+/*
+ * 在以下使用sg_pool_init():
+ *   - lib/sg_pool.c|193| <<global>> subsys_initcall(sg_pool_init);
+ */
 static __init int sg_pool_init(void)
 {
 	int i;
 
 	for (i = 0; i < SG_MEMPOOL_NR; i++) {
+		/*
+		 * struct sg_pool {
+		 *     size_t          size;
+		 *     char            *name;
+		 *     struct kmem_cache       *slab;
+		 *     mempool_t       *pool;
+		 * };
+		 *
+		 * 在以下使用sg_pools[]:
+		 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+		 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 */
 		struct sg_pool *sgp = sg_pools + i;
 		int size = sgp->size * sizeof(struct scatterlist);
 
@@ -155,6 +268,11 @@ static __init int sg_pool_init(void)
 			goto cleanup_sdb;
 		}
 
+		/*
+		 * mempool_create_slab_pool()有意思的调用:
+		 *   - drivers/scsi/virtio_scsi.c|1269| <<virtio_scsi_init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache);
+		 *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+		 */
 		sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE,
 						     sgp->slab);
 		if (!sgp->pool) {
@@ -168,6 +286,14 @@ static __init int sg_pool_init(void)
 
 cleanup_sdb:
 	for (i = 0; i < SG_MEMPOOL_NR; i++) {
+		/*
+		 * 在以下使用sg_pools[]:
+		 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+		 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 */
 		struct sg_pool *sgp = sg_pools + i;
 
 		mempool_destroy(sgp->pool);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index db64116a4..74163c13a 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2274,6 +2274,10 @@ bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,
  *      or if prot_numa but THP migration is not supported
  *  - HPAGE_PMD_NR if protections changed and TLB flush necessary
  */
+/*
+ * called by:
+ *   - mm/mprotect.c|402| <<change_pmd_range>> ret = change_huge_pmd(tlb, vma, pmd,
+ */
 int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		    pmd_t *pmd, unsigned long addr, pgprot_t newprot,
 		    unsigned long cp_flags)
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 162407fbf..6d75c7d34 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -667,6 +667,29 @@ static int queue_folios_hugetlb(pte_t *pte, unsigned long hmask,
  * an architecture makes a different choice, it will need further
  * changes to the core.
  */
+/*
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl        
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - kernel/sched/fair.c|3472| <<task_numa_work>> nr_pte_updates = change_prot_numa(vma, start, end);
+ */
 unsigned long change_prot_numa(struct vm_area_struct *vma,
 			unsigned long addr, unsigned long end)
 {
@@ -1835,6 +1858,10 @@ struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
 	return pol;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|3371| <<task_numa_work>> if (!vma_migratable(vma) || !vma_policy_mof(vma) ||
+ */
 bool vma_policy_mof(struct vm_area_struct *vma)
 {
 	struct mempolicy *pol;
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index fc18fe274..7de75c6c0 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -347,6 +347,10 @@ static void mn_hlist_release(struct mmu_notifier_subscriptions *subscriptions,
 	synchronize_srcu(&srcu);
 }
 
+/*
+ * 在以下使用__mmu_notifier_release():
+ *   - include/linux/mmu_notifier.h|402| <<mmu_notifier_release>> __mmu_notifier_release(mm);
+ */
 void __mmu_notifier_release(struct mm_struct *mm)
 {
 	struct mmu_notifier_subscriptions *subscriptions =
@@ -518,6 +522,40 @@ static int mn_hlist_invalidate_range_start(
 	return ret;
 }
 
+/*
+ * 5.15例子.
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * struct mmu_notifier_range {
+ *     struct mm_struct *mm;
+ *     unsigned long start;
+ *     unsigned long end;
+ *     unsigned flags;
+ *     enum mmu_notifier_event event;
+ *     void *owner;
+ * };
+ *
+ * called by:
+ *   - include/linux/mmu_notifier.h|439| <<mmu_notifier_invalidate_range_start>> __mmu_notifier_invalidate_range_start(range);
+ *   - include/linux/mmu_notifier.h|459| <<mmu_notifier_invalidate_range_start_nonblock>> ret = __mmu_notifier_invalidate_range_start(range);
+ */
 int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
 {
 	struct mmu_notifier_subscriptions *subscriptions =
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 516b1d847..4bda4b5ab 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -358,6 +358,26 @@ pgtable_populate_needed(struct vm_area_struct *vma, unsigned long cp_flags)
 		err;							\
 	})
 
+/*
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static inline long change_pmd_range(struct mmu_gather *tlb,
 		struct vm_area_struct *vma, pud_t *pud, unsigned long addr,
 		unsigned long end, pgprot_t newprot, unsigned long cp_flags)
@@ -510,6 +530,26 @@ static inline long change_p4d_range(struct mmu_gather *tlb,
 	return pages;
 }
 
+/*
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static long change_protection_range(struct mmu_gather *tlb,
 		struct vm_area_struct *vma, unsigned long addr,
 		unsigned long end, pgprot_t newprot, unsigned long cp_flags)
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 6841e61a6..cd6523271 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -4688,6 +4688,16 @@ EXPORT_SYMBOL_GPL(skb_segment_list);
  *	a pointer to the first in a list of new skbs for the segments.
  *	In case of error it returns ERR_PTR(err).
  */
+/*
+ * called by:
+ *   - lib/test_bpf.c|15096| <<test_skb_segment_single>> segs = skb_segment(skb, test->features);
+ *   - net/core/net_test.c|229| <<gso_test_func>> segs = skb_segment(skb, features);
+ *   - net/ipv4/tcp_offload.c|176| <<tcp_gso_segment>> segs = skb_segment(skb, features);
+ *   - net/ipv4/udp_offload.c|327| <<__udp_gso_segment>> segs = skb_segment(gso_skb, features);
+ *   - net/ipv4/udp_offload.c|468| <<udp4_ufo_fragment>> segs = skb_segment(skb, features);
+ *   - net/ipv6/udp_offload.c|109| <<udp6_ufo_fragment>> segs = skb_segment(skb, features);
+ *   - net/sctp/offload.c|72| <<sctp_gso_segment>> segs = skb_segment(skb, (features | NETIF_F_HW_CSUM) & ~NETIF_F_SG);
+ */
 struct sk_buff *skb_segment(struct sk_buff *head_skb,
 			    netdev_features_t features)
 {
diff --git a/net/netfilter/nf_tables_core.c b/net/netfilter/nf_tables_core.c
index 75598520b..8c8681dad 100644
--- a/net/netfilter/nf_tables_core.c
+++ b/net/netfilter/nf_tables_core.c
@@ -32,6 +32,19 @@ static bool nf_skip_indirect_calls(void)
 
 static void __init nf_skip_indirect_calls_enable(void)
 {
+	/*
+	 * 在以下使用X86_FEATURE_RETPOLINE:
+	 *   - arch/x86/include/asm/cpufeatures.h|298| <<global>> #define X86_FEATURE_RETPOLINE (11*32+12)
+	 *   - arch/x86/include/asm/disabled-features.h|56| <<DISABLE_RETPOLINE>> #define DISABLE_RETPOLINE ((1 << (X86_FEATURE_RETPOLINE & 31)) | \
+	 *   - arch/x86/include/asm/nospec-branch.h|450| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+	 *   - arch/x86/include/asm/nospec-branch.h|480| <<CALL_NOSPEC>> X86_FEATURE_RETPOLINE, \
+	 *   - arch/x86/kernel/alternative.c|668| <<patch_retpoline>> if (cpu_feature_enabled(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|1993| <<bhi_select_mitigation>> if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/kernel/cpu/bugs.c|3327| <<spectre_bhi_state>> else if (boot_cpu_has(X86_FEATURE_RETPOLINE) &&
+	 *   - arch/x86/net/bpf_jit_comp.c|659| <<emit_indirect_jump>> } else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {
+	 *   - include/net/tc_wrapper.h|205| <<tc_wrapper_init>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 *   - net/netfilter/nf_tables_core.c|35| <<nf_skip_indirect_calls_enable>> if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
+	 */
 	if (!cpu_feature_enabled(X86_FEATURE_RETPOLINE))
 		static_branch_enable(&nf_tables_skip_direct_calls);
 }
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 7bc74969a..d80634a5e 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -69,6 +69,18 @@ static void kvm_reset_dirty_gfn(struct kvm *kvm, u32 slot, u64 offset, u64 mask)
 	if (!memslot || (offset + __fls(mask)) >= memslot->npages)
 		return;
 
+	/*
+	 * 在以下使用KVM_MMU_LOCK():
+	 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+	 */
 	KVM_MMU_LOCK(kvm);
 	kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
 	KVM_MMU_UNLOCK(kvm);
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 249ba5b72..c0bd1d2fb 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -30,14 +30,33 @@
 
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 
+/*
+ * 在以下使用irqfd_cleanup_wq:
+ *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+ *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+ *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+ *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+ *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+ *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+ *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+ */
 static struct workqueue_struct *irqfd_cleanup_wq;
 
+/*
+ * 使用kvm_arch_irqfd_allowed()的地方:
+ *   - arch/x86/kvm/irq.c|167| <<kvm_arch_irqfd_allowed>> bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
+ *   - virt/kvm/eventfd.c|36| <<kvm_arch_irqfd_allowed>> kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
+ *   - virt/kvm/eventfd.c|315| <<kvm_irqfd_assign>> if (!kvm_arch_irqfd_allowed(kvm, args))
+ */
 bool __attribute__((weak))
 kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 {
 	return true;
 }
 
+/*
+ * AMD avic=0上等了好久没调用
+ */
 static void
 irqfd_inject(struct work_struct *work)
 {
@@ -169,6 +188,12 @@ irqfd_is_active(struct kvm_kernel_irqfd *irqfd)
  *
  * assumes kvm->irqfds.lock is held
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|240| <<irqfd_wakeup>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|548| <<kvm_irqfd_deassign>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|589| <<kvm_irqfd_release>> irqfd_deactivate(irqfd);
+ */
 static void
 irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 {
@@ -176,6 +201,16 @@ irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 
 	list_del_init(&irqfd->list);
 
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
 }
 
@@ -191,6 +226,28 @@ int __attribute__((weak)) kvm_arch_set_irq_inatomic(
 /*
  * Called with wqh->lock held and interrupts disabled
  */
+/*
+ * irqfd_wakeup
+ * __wake_up_common
+ * eventfd_signal_mask
+ * vfio_msihandler
+ * __handle_irq_event_percpu
+ * handle_irq_event
+ * handle_edge_irq
+ * __common_interrupt
+ * common_interrupt
+ * asm_common_interrupt
+ * cpuidle_enter_state
+ * cpuidle_enter
+ * cpuidle_idle_call
+ * do_idle
+ * cpu_startup_entry
+ * start_secondary
+ * common_startup_64
+ *
+ * 在以下使用irqfd_wakeup:
+ *   - virt/kvm/eventfd.c|428| <<kvm_irqfd_assign>> init_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);
+ */
 static int
 irqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
@@ -557,6 +614,16 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	 * so that we guarantee there will not be any more interrupts on this
 	 * gsi once this deassign function returns.
 	 */
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	flush_workqueue(irqfd_cleanup_wq);
 
 	return 0;
@@ -594,6 +661,16 @@ kvm_irqfd_release(struct kvm *kvm)
 	 * Block until we know all outstanding shutdown jobs have completed
 	 * since we do not take a kvm* reference.
 	 */
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	flush_workqueue(irqfd_cleanup_wq);
 
 }
@@ -660,8 +737,22 @@ bool kvm_notify_irqfd_resampler(struct kvm *kvm,
  * aggregated from all vm* instances. We need our own isolated
  * queue to ease flushing work items when a VM exits.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6439| <<kvm_init>> r = kvm_irqfd_init();
+ */
 int kvm_irqfd_init(void)
 {
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
 	if (!irqfd_cleanup_wq)
 		return -ENOMEM;
@@ -671,6 +762,16 @@ int kvm_irqfd_init(void)
 
 void kvm_irqfd_exit(void)
 {
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	destroy_workqueue(irqfd_cleanup_wq);
 }
 #endif
diff --git a/virt/kvm/guest_memfd.c b/virt/kvm/guest_memfd.c
index 47a9f68f7..1ecb7f93a 100644
--- a/virt/kvm/guest_memfd.c
+++ b/virt/kvm/guest_memfd.c
@@ -123,6 +123,18 @@ static void kvm_gmem_invalidate_begin(struct kvm_gmem *gmem, pgoff_t start,
 		if (!found_memslot) {
 			found_memslot = true;
 
+			/*
+			 * 在以下使用KVM_MMU_LOCK():
+			 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+			 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+			 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+			 */
 			KVM_MMU_LOCK(kvm);
 			kvm_mmu_invalidate_begin(kvm);
 		}
@@ -143,6 +155,18 @@ static void kvm_gmem_invalidate_end(struct kvm_gmem *gmem, pgoff_t start,
 	struct kvm *kvm = gmem->kvm;
 
 	if (xa_find(&gmem->bindings, &start, end - 1, XA_PRESENT)) {
+		/*
+		 * 在以下使用KVM_MMU_LOCK():
+		 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+		 */
 		KVM_MMU_LOCK(kvm);
 		kvm_mmu_invalidate_end(kvm);
 		KVM_MMU_UNLOCK(kvm);
@@ -452,6 +476,10 @@ static int __kvm_gmem_create(struct kvm *kvm, loff_t size, u64 flags)
 	return err;
 }
 
+/*
+ * 处理KVM_CREATE_GUEST_MEMFD:
+ *   - virt/kvm/kvm_main.c|5325| <<kvm_vm_ioctl(KVM_CREATE_GUEST_MEMFD)>> r = kvm_gmem_create(kvm, &guest_memfd);
+ */
 int kvm_gmem_create(struct kvm *kvm, struct kvm_create_guest_memfd *args)
 {
 	loff_t size = args->size;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index de2c11dae..2ec86b77e 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -605,6 +605,18 @@ static __always_inline kvm_mn_ret_t __kvm_handle_hva_range(struct kvm *kvm,
 
 			if (!r.found_memslot) {
 				r.found_memslot = true;
+				/*
+				 * 在以下使用KVM_MMU_LOCK():
+				 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+				 */
 				KVM_MMU_LOCK(kvm);
 				if (!IS_KVM_NULL_FN(range->on_lock))
 					range->on_lock(kvm);
@@ -671,6 +683,12 @@ void kvm_mmu_invalidate_begin(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7511| <<kvm_zap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, gfn_start, gfn_end);
+ *   - virt/kvm/kvm_main.c|719| <<kvm_mmu_unmap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+ *   - virt/kvm/kvm_main.c|2508| <<kvm_pre_set_memory_attributes>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+ */
 void kvm_mmu_invalidate_range_add(struct kvm *kvm, gfn_t start, gfn_t end)
 {
 	lockdep_assert_held_write(&kvm->mmu_lock);
@@ -697,9 +715,26 @@ void kvm_mmu_invalidate_range_add(struct kvm *kvm, gfn_t start, gfn_t end)
 	}
 }
 
+/*
+ * 在以下使用kvm_mmu_unmap_gfn_range():
+ *   - virt/kvm/guest_memfd.c|130| <<kvm_gmem_invalidate_begin>> flush |= kvm_mmu_unmap_gfn_range(kvm, &gfn_range);
+ *   - virt/kvm/kvm_main.c|713| <<kvm_mmu_notifier_invalidate_range_start>> .handler = kvm_mmu_unmap_gfn_range,
+ */
 bool kvm_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7511| <<kvm_zap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, gfn_start, gfn_end);
+	 *   - virt/kvm/kvm_main.c|719| <<kvm_mmu_unmap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+	 *   - virt/kvm/kvm_main.c|2508| <<kvm_pre_set_memory_attributes>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+	 */
 	kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+	/*
+	 * x86的调用:
+	 *   - arch/x86/kvm/mmu/mmu.c|7773| <<kvm_mmu_zap_memslot>> flush = kvm_unmap_gfn_range(kvm, &range);
+	 *   - arch/x86/kvm/mmu/mmu.c|8276| <<kvm_arch_pre_set_memory_attributes>> return kvm_unmap_gfn_range(kvm, range);
+	 *   - virt/kvm/kvm_main.c|703| <<kvm_mmu_unmap_gfn_range>> return kvm_unmap_gfn_range(kvm, range);
+	 */
 	return kvm_unmap_gfn_range(kvm, range);
 }
 
@@ -2194,6 +2229,18 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 		dirty_bitmap_buffer = kvm_second_dirty_bitmap(memslot);
 		memset(dirty_bitmap_buffer, 0, n);
 
+		/*
+		 * 在以下使用KVM_MMU_LOCK():
+		 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+		 */
 		KVM_MMU_LOCK(kvm);
 		for (i = 0; i < n / sizeof(long); i++) {
 			unsigned long mask;
@@ -2305,6 +2352,18 @@ static int kvm_clear_dirty_log_protect(struct kvm *kvm,
 	if (copy_from_user(dirty_bitmap_buffer, log->dirty_bitmap, n))
 		return -EFAULT;
 
+	/*
+	 * 在以下使用KVM_MMU_LOCK():
+	 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+	 */
 	KVM_MMU_LOCK(kvm);
 	for (offset = log->first_page, i = offset / BITS_PER_LONG,
 		 n = DIV_ROUND_UP(log->num_pages, BITS_PER_LONG); n--;
@@ -2422,6 +2481,18 @@ static __always_inline void kvm_handle_gfn_range(struct kvm *kvm,
 
 			if (!found_memslot) {
 				found_memslot = true;
+				/*
+				 * 在以下使用KVM_MMU_LOCK():
+				 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+				 */
 				KVM_MMU_LOCK(kvm);
 				if (!IS_KVM_NULL_FN(range->on_lock))
 					range->on_lock(kvm);
@@ -2765,6 +2836,10 @@ static kvm_pfn_t kvm_resolve_pfn(struct kvm_follow_pfn *kfp, struct page *page,
  * The fast path to get the writable pfn which will be stored in @pfn,
  * true indicates success, otherwise false is returned.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2913| <<hva_to_pfn>> if (hva_to_pfn_fast(kfp, &pfn))
+ */
 static bool hva_to_pfn_fast(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
 {
 	struct page *page;
@@ -2797,6 +2872,10 @@ static bool hva_to_pfn_fast(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
  * The slow path to get the pfn of the specified host virtual address,
  * 1 indicates success, -errno is returned if error is detected.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2916| <<hva_to_pfn>> npages = hva_to_pfn_slow(kfp, &pfn);
+ */
 static int hva_to_pfn_slow(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
 {
 	/*
@@ -2899,6 +2978,11 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2963| <<kvm_follow_pfn>> return hva_to_pfn(kfp);
+ *   - virt/kvm/pfncache.c|209| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(&kfp);
+ */
 kvm_pfn_t hva_to_pfn(struct kvm_follow_pfn *kfp)
 {
 	struct vm_area_struct *vma;
@@ -2960,6 +3044,11 @@ static kvm_pfn_t kvm_follow_pfn(struct kvm_follow_pfn *kfp)
 		kfp->map_writable = NULL;
 	}
 
+	/*
+	 * called by:
+	 *   - virt/kvm/kvm_main.c|2963| <<kvm_follow_pfn>> return hva_to_pfn(kfp);
+	 *   - virt/kvm/pfncache.c|209| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(&kfp);
+	 */
 	return hva_to_pfn(kfp);
 }
 
@@ -3363,6 +3452,34 @@ int kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 }
 EXPORT_SYMBOL_GPL(kvm_write_guest_offset_cached);
 
+/*
+ * kvm_write_guest_cached
+ * apic_sync_pv_eoi_from_guest
+ * kvm_lapic_sync_from_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * kvm_write_guest_cached
+ * kvm_lapic_sync_to_vapic
+ * vcpu_enter_guest.constprop.0
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/lapic.c|921| <<pv_eoi_put_user>> return kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, &val,
+ *   - arch/x86/kvm/lapic.c|3374| <<kvm_lapic_sync_to_vapic>> kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
+ *   - arch/x86/kvm/vmx/nested.c|758| <<nested_flush_cached_shadow_vmcs12>> kvm_write_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),
+ *   - arch/x86/kvm/x86.c|13813| <<apf_put_user_notpresent>> return kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apf.data, &reason,
+ */
 int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,
 			   void *data, unsigned long len)
 {
diff --git a/virt/kvm/kvm_mm.h b/virt/kvm/kvm_mm.h
index acef3f5c5..d3122117d 100644
--- a/virt/kvm/kvm_mm.h
+++ b/virt/kvm/kvm_mm.h
@@ -10,6 +10,18 @@
  * multiple architectures.
  */
 
+/*
+ * 在以下使用KVM_MMU_LOCK():
+ *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+ *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+ *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+ */
 #ifdef KVM_HAVE_MMU_RWLOCK
 #define KVM_MMU_LOCK_INIT(kvm)		rwlock_init(&(kvm)->mmu_lock)
 #define KVM_MMU_LOCK(kvm)		write_lock(&(kvm)->mmu_lock)
diff --git a/virt/kvm/pfncache.c b/virt/kvm/pfncache.c
index 728d2c1b4..3be3a3729 100644
--- a/virt/kvm/pfncache.c
+++ b/virt/kvm/pfncache.c
@@ -206,6 +206,11 @@ static kvm_pfn_t hva_to_pfn_retry(struct gfn_to_pfn_cache *gpc)
 			cond_resched();
 		}
 
+		/*
+		 * called by:
+		 *   - virt/kvm/kvm_main.c|2963| <<kvm_follow_pfn>> return hva_to_pfn(kfp);
+		 *   - virt/kvm/pfncache.c|209| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(&kfp);
+		 */
 		new_pfn = hva_to_pfn(&kfp);
 		if (is_error_noslot_pfn(new_pfn))
 			goto out_error;
diff --git a/virt/lib/irqbypass.c b/virt/lib/irqbypass.c
index 28fda42e4..5e7bfcef8 100644
--- a/virt/lib/irqbypass.c
+++ b/virt/lib/irqbypass.c
@@ -22,6 +22,29 @@
 MODULE_LICENSE("GPL v2");
 MODULE_DESCRIPTION("IRQ bypass manager utility module");
 
+/*
+ * 注释:
+ * When a physical I/O device is assigned to a virtual machine through
+ * facilities like VFIO and KVM, the interrupt for the device generally
+ * bounces through the host system before being injected into the VM.
+ * However, hardware technologies exist that often allow the host to be
+ * bypassed for some of these scenarios.  Intel Posted Interrupts allow
+ * the specified physical edge interrupts to be directly injected into a
+ * guest when delivered to a physical processor while the vCPU is
+ * running.  ARM IRQ Forwarding allows forwarded physical interrupts to
+ * be directly deactivated by the guest.
+ *
+ * The IRQ bypass manager here is meant to provide the shim to connect
+ * interrupt producers, generally the host physical device driver, with
+ * interrupt consumers, generally the hypervisor, in order to configure
+ * these bypass mechanism.  To do this, we base the connection on a
+ * shared, opaque token.  For KVM-VFIO this is expected to be an
+ * eventfd_ctx since this is the connection we already use to connect an
+ * eventfd to an irqfd on the in-kernel path.  When a producer and
+ * consumer with matching tokens is found, callbacks via both registered
+ * participants allow the bypass facilities to be automatically enabled.
+ */
+
 static LIST_HEAD(producers);
 static LIST_HEAD(consumers);
 static DEFINE_MUTEX(lock);
-- 
2.39.5 (Apple Git-154)

