From 601c451aacbfcf7e9986b5602c962843a1928d52 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang0129@gmail.com>
Date: Sun, 2 Mar 2025 13:20:19 -0800
Subject: [PATCH 1/1] linux-v6.13

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
---
 arch/x86/events/core.c                 |    9 +
 arch/x86/include/asm/debugreg.h        |   16 +
 arch/x86/include/asm/kvm_host.h        |  218 ++++
 arch/x86/kernel/kvmclock.c             |   32 +
 arch/x86/kvm/cpuid.c                   |    8 +
 arch/x86/kvm/cpuid.h                   |   10 +
 arch/x86/kvm/hyperv.c                  |   13 +
 arch/x86/kvm/i8254.c                   |    7 +
 arch/x86/kvm/ioapic.c                  |   21 +
 arch/x86/kvm/irq_comm.c                |    7 +
 arch/x86/kvm/lapic.c                   |   11 +
 arch/x86/kvm/lapic.h                   |   20 +
 arch/x86/kvm/mmu.h                     |   45 +
 arch/x86/kvm/mmu/mmu.c                 | 1255 +++++++++++++++++++++++
 arch/x86/kvm/mmu/mmu_internal.h        |   74 ++
 arch/x86/kvm/mmu/paging_tmpl.h         |   86 ++
 arch/x86/kvm/mmu/spte.c                |   32 +
 arch/x86/kvm/mmu/spte.h                |   30 +
 arch/x86/kvm/mmu/tdp_mmu.c             |  228 +++++
 arch/x86/kvm/pmu.c                     |  183 ++++
 arch/x86/kvm/pmu.h                     |   43 +
 arch/x86/kvm/svm/pmu.c                 |   15 +
 arch/x86/kvm/svm/svm.c                 |   51 +
 arch/x86/kvm/vmx/nested.c              |    4 +
 arch/x86/kvm/vmx/vmx.c                 |   72 ++
 arch/x86/kvm/vmx/vmx.h                 |   18 +
 arch/x86/kvm/x86.c                     |  444 +++++++++
 drivers/block/virtio_blk.c             |   19 +
 drivers/net/virtio_net.c               |   72 ++
 drivers/net/xen-netfront.c             |    4 +
 drivers/scsi/virtio_scsi.c             |  228 +++++
 drivers/target/loopback/tcm_loop.c     |   17 +
 drivers/target/target_core_configfs.c  |   17 +
 drivers/target/target_core_file.c      |   34 +
 drivers/target/target_core_hba.c       |    8 +
 drivers/target/target_core_iblock.c    |   14 +
 drivers/target/target_core_sbc.c       |   39 +
 drivers/target/target_core_spc.c       |    5 +
 drivers/target/target_core_transport.c |   57 ++
 drivers/vdpa/ifcvf/ifcvf_main.c        |   16 +
 drivers/vdpa/mlx5/net/mlx5_vnet.c      |   27 +
 drivers/vdpa/vdpa.c                    |   60 ++
 drivers/vdpa/vdpa_sim/vdpa_sim.c       |   26 +
 drivers/vdpa/vdpa_sim/vdpa_sim_blk.c   |   11 +
 drivers/vdpa/vdpa_sim/vdpa_sim_net.c   |   20 +
 drivers/vdpa/vdpa_user/vduse_dev.c     |   16 +
 drivers/vdpa/virtio_pci/vp_vdpa.c      |   16 +
 drivers/vhost/net.c                    |  254 +++++
 drivers/vhost/scsi.c                   | 1267 ++++++++++++++++++++++++
 drivers/vhost/test.c                   |   55 +
 drivers/vhost/vdpa.c                   |   42 +
 drivers/vhost/vhost.c                  | 1018 +++++++++++++++++++
 drivers/vhost/vhost.h                  |  107 ++
 drivers/vhost/vringh.c                 |    8 +
 drivers/vhost/vsock.c                  |   96 ++
 drivers/virtio/virtio.c                |   47 +
 drivers/virtio/virtio_ring.c           |   31 +
 drivers/virtio/virtio_vdpa.c           |   20 +
 include/linux/mempool.h                |   83 ++
 include/linux/mmu_notifier.h           |   37 +
 include/linux/scatterlist.h            |   58 ++
 include/linux/sched.h                  |   72 ++
 include/linux/sched/sysctl.h           |   21 +
 include/linux/skbuff.h                 |   10 +
 include/linux/vdpa.h                   |   27 +
 include/linux/virtio.h                 |    6 +
 include/target/target_core_base.h      |   15 +
 include/uapi/linux/vhost_types.h       |   11 +
 kernel/sched/core.c                    |   25 +
 kernel/sched/fair.c                    |  253 +++++
 kernel/vhost_task.c                    |    8 +
 lib/iov_iter.c                         |   57 ++
 lib/scatterlist.c                      |   20 +
 lib/sg_pool.c                          |  126 +++
 mm/huge_memory.c                       |    4 +
 mm/mempolicy.c                         |   27 +
 mm/mmu_notifier.c                      |   38 +
 mm/mprotect.c                          |   40 +
 net/core/skbuff.c                      |   10 +
 virt/kvm/dirty_ring.c                  |   12 +
 virt/kvm/eventfd.c                     |   76 ++
 virt/kvm/guest_memfd.c                 |   28 +
 virt/kvm/kvm_main.c                    |   89 ++
 virt/kvm/kvm_mm.h                      |   12 +
 virt/kvm/pfncache.c                    |    5 +
 virt/lib/irqbypass.c                   |   23 +
 86 files changed, 7796 insertions(+)

diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index c75c482d4..5854a4e58 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -3056,6 +3056,15 @@ unsigned long perf_arch_misc_flags(struct pt_regs *regs)
 	return flags;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.h|198| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_init_pmu_capability()
+ *       -> perf_get_x86_pmu_capability()
+ */
 void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)
 {
 	/* This API doesn't currently support enumerating hybrid PMUs. */
diff --git a/arch/x86/include/asm/debugreg.h b/arch/x86/include/asm/debugreg.h
index fdbbbfec7..e9f4e0d72 100644
--- a/arch/x86/include/asm/debugreg.h
+++ b/arch/x86/include/asm/debugreg.h
@@ -161,6 +161,14 @@ static inline unsigned long amd_get_dr_addr_mask(unsigned int dr)
 }
 #endif
 
+/*
+ * 在以下调用get_debugctlmsr():
+ *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+ *   - arch/x86/events/intel/ds.c|832| <<intel_pmu_enable_bts>> debugctlmsr = get_debugctlmsr();
+ *   - arch/x86/events/intel/ds.c|856| <<intel_pmu_disable_bts>> debugctlmsr = get_debugctlmsr();
+ *   - arch/x86/kernel/step.c|188| <<set_task_blockstep>> debugctl = get_debugctlmsr();
+ *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+ */
 static inline unsigned long get_debugctlmsr(void)
 {
 	unsigned long debugctlmsr = 0;
@@ -174,6 +182,14 @@ static inline unsigned long get_debugctlmsr(void)
 	return debugctlmsr;
 }
 
+/*
+ * 在以下调用update_debugctlmsr():
+ *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+ *   - arch/x86/events/intel/ds.c|845| <<intel_pmu_enable_bts>> update_debugctlmsr(debugctlmsr);
+ *   - arch/x86/events/intel/ds.c|862| <<intel_pmu_disable_bts>> update_debugctlmsr(debugctlmsr);
+ *   - arch/x86/kernel/step.c|197| <<set_task_blockstep>> update_debugctlmsr(debugctl);
+ *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+ */
 static inline void update_debugctlmsr(unsigned long debugctlmsr)
 {
 #ifndef CONFIG_X86_DEBUGCTLMSR
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index e159e44a6..0621203ba 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -89,8 +89,38 @@
 #define KVM_REQ_EVENT			KVM_ARCH_REQ(6)
 #define KVM_REQ_APF_HALT		KVM_ARCH_REQ(7)
 #define KVM_REQ_STEAL_UPDATE		KVM_ARCH_REQ(8)
+/*
+ * 在以下使用KVM_REQ_NMI:
+ *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+ *
+ * 处理的函数: process_nmi()
+ */
 #define KVM_REQ_NMI			KVM_ARCH_REQ(9)
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *
+ * 处理的函数: kvm_pmu_handle_event()
+ */
 #define KVM_REQ_PMU			KVM_ARCH_REQ(10)
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理的函数: kvm_pmu_deliver_pmi()
+ */
 #define KVM_REQ_PMI			KVM_ARCH_REQ(11)
 #ifdef CONFIG_KVM_SMM
 #define KVM_REQ_SMI			KVM_ARCH_REQ(12)
@@ -280,6 +310,19 @@ enum x86_intercept_stage;
  * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
  * when the guest was accessing private memory.
  */
+/*
+ * 在以下使用PFERR_PRIVATE_ACCESS:
+ *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+ *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+ *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+ *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+ *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+ *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+ *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+ *
+ * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+ * when the guest was accessing private memory.
+ */
 #define PFERR_PRIVATE_ACCESS   BIT_ULL(49)
 #define PFERR_SYNTHETIC_MASK   (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
 
@@ -519,6 +562,18 @@ struct kvm_pmc {
 	 * PMC events triggered by KVM emulation that haven't been fully
 	 * processed, i.e. haven't undergone overflow detection.
 	 */
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	u64 emulated_counter;
 	u64 eventsel;
 	struct perf_event *perf_event;
@@ -996,6 +1051,17 @@ struct kvm_vcpu_arch {
 
 	/* pv related host specific info */
 	struct {
+		/*
+		 * 在以下使用pv_unhalted:
+		 *   - arch/x86/kvm/lapic.c|1356| <<__apic_accept_irq>> vcpu->arch.pv.pv_unhalted = 1;
+		 *   - arch/x86/kvm/svm/sev.c|3833| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/svm/sev.c|3872| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11352| <<kvm_vcpu_has_events>> if (vcpu->arch.pv.pv_unhalted)
+		 *   - arch/x86/kvm/x86.c|11453| <<vcpu_block>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11532| <<__kvm_emulate_halt>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11599| <<kvm_arch_dy_runnable>> if (READ_ONCE(vcpu->arch.pv.pv_unhalted))
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu->arch.pv.pv_unhalted)
+		 */
 		bool pv_unhalted;
 	} pv;
 
@@ -1194,6 +1260,70 @@ struct kvm_x86_pmu_event_filter {
 	__u64 events[];
 };
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ *
+ * 在以下设置APICV_INHIBIT_REASON_DISABLED:
+ *   - arch/x86/kvm/x86.c|10028| <<kvm_apicv_init>> enum kvm_apicv_inhibit reason = enable_apicv ?
+ *              APICV_INHIBIT_REASON_ABSENT : APICV_INHIBIT_REASON_DISABLED;
+ *
+ * 在以下设置APICV_INHIBIT_REASON_HYPERV:
+ *   - arch/x86/kvm/hyperv.c|162| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+ *              APICV_INHIBIT_REASON_HYPERV, !!hv->synic_auto_eoi_used); 
+ *
+ * 在以下设置APICV_INHIBIT_REASON_ABSENT:
+ *   - arch/x86/kvm/x86.c|6546| <<kvm_vm_ioctl_enable_cap(KVM_CAP_SPLIT_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|7094| <<kvm_arch_vm_ioctl(KVM_CREATE_IRQCHIP)>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_ABSENT);
+ *   - arch/x86/kvm/x86.c|10027| <<kvm_apicv_init>> enum kvm_apicv_inhibit reason = enable_apicv ?
+ *              APICV_INHIBIT_REASON_ABSENT : APICV_INHIBIT_REASON_DISABLED;
+ *
+ * 在以下设置APICV_INHIBIT_REASON_BLOCKIRQ:
+ *   - arch/x86/kvm/x86.c|12232| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+ *              APICV_INHIBIT_REASON_BLOCKIRQ, set);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED:
+ *   - arch/x86/kvm/lapic.c|466| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|468| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_APIC_ID_MODIFIED:
+ *   - arch/x86/kvm/lapic.c|476| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/lapic.c|478| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_APIC_BASE_MODIFIED:
+ *   - arch/x86/kvm/lapic.c|2628| <<__kvm_apic_set_base>> APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_NESTED:
+ *   - arch/x86/kvm/svm/avic.c|545| <<avic_vcpu_get_apicv_inhibit_reasons>> return APICV_INHIBIT_REASON_NESTED;
+ *
+ * 在以下设置APICV_INHIBIT_REASON_IRQWIN:
+ *   - arch/x86/kvm/svm/svm.c|3262| <<interrupt_window_interception>> kvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ *   - arch/x86/kvm/svm/svm.c|3907| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_PIT_REINJ:
+ *   - arch/x86/kvm/i8254.c|315| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/i8254.c|321| <<kvm_pit_set_reinject>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_SEV:
+ *   - arch/x86/kvm/svm/sev.c|463| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+ *
+ * 在以下设置APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED:
+ *   - arch/x86/kvm/lapic.c|471| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|473| <<kvm_recalculate_apic_map>> kvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ */
+
 enum kvm_apicv_inhibit {
 
 	/********************************************************************/
@@ -1318,6 +1448,15 @@ struct kvm_arch {
 	 * guest attempts to execute from the region then KVM obviously can't
 	 * create an NX huge page (without hanging the guest).
 	 */
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+	 *              &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+	 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	struct list_head possible_nx_huge_pages;
 #ifdef CONFIG_KVM_EXTERNAL_WRITE_TRACKING
 	struct kvm_page_track_notifier_head track_notifier_head;
@@ -1349,8 +1488,32 @@ struct kvm_arch {
 	bool apic_access_memslot_enabled;
 	bool apic_access_memslot_inhibited;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	/* Protects apicv_inhibit_reasons */
 	struct rw_semaphore apicv_update_lock;
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	unsigned long apicv_inhibit_reasons;
 
 	gpa_t wall_clock;
@@ -1422,6 +1585,31 @@ struct kvm_arch {
 	bool triple_fault_event;
 
 	bool bus_lock_detection_enabled;
+	/*
+	 * 在以下使用global的enable_pmu:
+	 *   - arch/x86/kvm/cpuid.c|1063| <<__do_cpuid_func>> if (!enable_pmu || !static_cpu_has(X86_FEATURE_ARCH_PERFMON)) {
+	 *   - arch/x86/kvm/cpuid.c|1399| <<__do_cpuid_func>> if (!enable_pmu || !kvm_cpu_cap_has(X86_FEATURE_PERFMON_V2)) {
+	 *   - arch/x86/kvm/pmu.h|216| <<kvm_init_pmu_capability>> enable_pmu = false;
+	 *   - arch/x86/kvm/pmu.h|218| <<kvm_init_pmu_capability>> if (enable_pmu) {
+	 *   - arch/x86/kvm/pmu.h|229| <<kvm_init_pmu_capability>> enable_pmu = false;
+	 *   - arch/x86/kvm/pmu.h|231| <<kvm_init_pmu_capability>> enable_pmu = false;
+	 *   - arch/x86/kvm/pmu.h|234| <<kvm_init_pmu_capability>> if (!enable_pmu) {
+	 *   - arch/x86/kvm/svm/svm.c|5272| <<svm_set_cpu_caps>> if (enable_pmu) {
+	 *   - arch/x86/kvm/svm/svm.c|5450| <<svm_hardware_setup>> if (!enable_pmu)
+	 *   - arch/x86/kvm/vmx/vmx.c|7968| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+	 *   - arch/x86/kvm/vmx/vmx.c|8037| <<vmx_set_cpu_caps>> if (!enable_pmu)
+	 *   - arch/x86/kvm/vmx/vmx.c|8627| <<vmx_hardware_setup>> if (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())
+	 *   - arch/x86/kvm/x86.c|4803| <<kvm_vm_ioctl_check_extension>> r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
+	 *   - arch/x86/kvm/x86.c|6726| <<kvm_vm_ioctl_enable_cap>> if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
+	 *   - arch/x86/kvm/x86.c|7542| <<kvm_init_msr_lists>> if (enable_pmu) {
+	 *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+	 *
+	 * 在以下使用kvm_arch->enable_pmu:
+	 *   - arch/x86/kvm/pmu.c|938| <<kvm_pmu_refresh>> if (!vcpu->kvm->arch.enable_pmu)
+	 *   - arch/x86/kvm/svm/pmu.c|44| <<get_gp_pmc_amd>> if (!vcpu->kvm->arch.enable_pmu)
+	 *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+	 *   - arch/x86/kvm/x86.c|6731| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+	 */
 	bool enable_pmu;
 
 	u32 notify_window;
@@ -1443,6 +1631,16 @@ struct kvm_arch {
 	bool sgx_provisioning_allowed;
 
 	struct kvm_x86_pmu_event_filter __rcu *pmu_event_filter;
+	/*
+	 * 在以下使用kvm_arch->nx_huge_page_recovery_thread:
+	 *   - arch/x86/kvm/mmu/mmu.c|7778| <<set_nx_huge_pages>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7907| <<set_nx_huge_pages_recovery_param>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8071| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread = vhost_task_create(
+	 *   - arch/x86/kvm/mmu/mmu.c|8075| <<kvm_mmu_post_init_vm>> if (!kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8078| <<kvm_mmu_post_init_vm>> vhost_task_start(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8084| <<kvm_mmu_pre_destroy_vm>> if (kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8085| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 */
 	struct vhost_task *nx_huge_page_recovery_thread;
 	u64 nx_huge_page_last;
 
@@ -2169,15 +2367,35 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|308| <<kvm_pit_set_reinject>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PIT_REINJ);
+ *   - arch/x86/kvm/lapic.c|466| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|471| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);
+ *   - arch/x86/kvm/lapic.c|476| <<kvm_recalculate_apic_map>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);
+ *   - arch/x86/kvm/lapic.c|2623| <<__kvm_apic_set_base>> kvm_set_apicv_inhibit(apic->vcpu->kvm, APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);
+ *   - arch/x86/kvm/svm/sev.c|463| <<__sev_guest_init>> kvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);
+ *   - arch/x86/kvm/svm/svm.c|3883| <<svm_enable_irq_window>> kvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);
+ */
 static inline void kvm_set_apicv_inhibit(struct kvm *kvm,
 					 enum kvm_apicv_inhibit reason)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/include/asm/kvm_host.h|2175| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+	 *   - arch/x86/include/asm/kvm_host.h|2181| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+	 */
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
 }
 
 static inline void kvm_clear_apicv_inhibit(struct kvm *kvm,
 					   enum kvm_apicv_inhibit reason)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/include/asm/kvm_host.h|2175| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+	 *   - arch/x86/include/asm/kvm_host.h|2181| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+	 */
 	kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
 }
 
diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c
index 5b2c15214..2786cef9d 100644
--- a/arch/x86/kernel/kvmclock.c
+++ b/arch/x86/kernel/kvmclock.c
@@ -25,6 +25,14 @@
 static int kvmclock __initdata = 1;
 static int kvmclock_vsyscall __initdata = 1;
 static int msr_kvm_system_time __ro_after_init;
+/*
+ * 在以下使用msr_kvm_wall_clock:
+ *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+ *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+ *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+ *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+ *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+ */
 static int msr_kvm_wall_clock __ro_after_init;
 static u64 kvm_sched_clock_offset __ro_after_init;
 
@@ -60,6 +68,14 @@ EXPORT_PER_CPU_SYMBOL_GPL(hv_clock_per_cpu);
  */
 static void kvm_get_wallclock(struct timespec64 *now)
 {
+	/*
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+	 *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
 	preempt_disable();
 	pvclock_read_wallclock(&wall_clock, this_cpu_pvti(), now);
@@ -292,6 +308,14 @@ void __init kvmclock_init(void)
 	if (!kvm_para_available() || !kvmclock)
 		return;
 
+	/*
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+	 *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	if (kvm_para_has_feature(KVM_FEATURE_CLOCKSOURCE2)) {
 		msr_kvm_system_time = MSR_KVM_SYSTEM_TIME_NEW;
 		msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
@@ -307,6 +331,14 @@ void __init kvmclock_init(void)
 		return;
 	}
 
+	/*
+	 * 在以下使用msr_kvm_wall_clock:
+	 *   - arch/x86/kernel/kvmclock.c|63| <<kvm_get_wallclock>> wrmsrl(msr_kvm_wall_clock, slow_virt_to_phys(&wall_clock));
+	 *   - arch/x86/kernel/kvmclock.c|297| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK_NEW;
+	 *   - arch/x86/kernel/kvmclock.c|300| <<kvmclock_init>> msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
+	 *   - arch/x86/kernel/kvmclock.c|311| <<kvmclock_init>> pr_info("kvm-clock: Using msrs %x and %x",
+	 *                                       msr_kvm_system_time, msr_kvm_wall_clock);
+	 */
 	pr_info("kvm-clock: Using msrs %x and %x",
 		msr_kvm_system_time, msr_kvm_wall_clock);
 
diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index ae0b438a2..3590218fd 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -963,6 +963,10 @@ static int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.c|1444| <<do_cpuid_func>> return __do_cpuid_func(array, func);
+ */
 static inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)
 {
 	struct kvm_cpuid_entry2 *entry;
@@ -1497,6 +1501,10 @@ static bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,
 	return false;
 }
 
+/*
+ * 处理KVM_GET_SUPPORTED_CPUID和KVM_GET_EMULATED_CPUID:
+ *   - arch/x86/kvm/x86.c|4905| <<kvm_arch_dev_ioctl>> r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries, ioctl);
+ */
 int kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,
 			    struct kvm_cpuid_entry2 __user *entries,
 			    unsigned int type)
diff --git a/arch/x86/kvm/cpuid.h b/arch/x86/kvm/cpuid.h
index f16a7b2c2..47b7ed554 100644
--- a/arch/x86/kvm/cpuid.h
+++ b/arch/x86/kvm/cpuid.h
@@ -42,6 +42,16 @@ static inline int cpuid_maxphyaddr(struct kvm_vcpu *vcpu)
 	return vcpu->arch.maxphyaddr;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/cpuid.h|53| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+ *   - arch/x86/kvm/cpuid.h|274| <<kvm_vcpu_is_legal_cr3>> return kvm_vcpu_is_legal_gpa(vcpu, cr3);
+ *   - arch/x86/kvm/svm/nested.c|256| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+ *   - arch/x86/kvm/svm/nested.c|257| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+ *   - arch/x86/kvm/vmx/nested.c|834| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+ *   - arch/x86/kvm/vmx/nested.c|2823| <<nested_vmx_check_eptp>> if (CC(!kvm_vcpu_is_legal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+ *   - arch/x86/kvm/vmx/vmx.c|5829| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && !kvm_vcpu_is_legal_gpa(vcpu, gpa)))
+ */
 static inline bool kvm_vcpu_is_legal_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)
 {
 	return !(gpa & vcpu->arch.reserved_gpa_bits);
diff --git a/arch/x86/kvm/hyperv.c b/arch/x86/kvm/hyperv.c
index 4f0a94346..5bbb81eab 100644
--- a/arch/x86/kvm/hyperv.c
+++ b/arch/x86/kvm/hyperv.c
@@ -134,6 +134,19 @@ static void synic_update_vector(struct kvm_vcpu_hv_synic *synic,
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&vcpu->kvm->arch.apicv_update_lock);
 
 	if (auto_eoi_new)
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index cd57a517d..62f4c2730 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -288,6 +288,13 @@ static inline void kvm_pit_reset_reinject(struct kvm_pit *pit)
 	atomic_set(&pit->pit_state.irq_ack, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/i8254.c|702| <<kvm_create_pit>> kvm_pit_set_reinject(pit, true);
+ *   - arch/x86/kvm/i8254.c|727| <<kvm_create_pit>> kvm_pit_set_reinject(pit, false);
+ *   - arch/x86/kvm/i8254.c|745| <<kvm_free_pit>> kvm_pit_set_reinject(pit, false);
+ *   - arch/x86/kvm/x86.c|6473| <<kvm_vm_ioctl_reinject>> kvm_pit_set_reinject(pit, control->pit_reinject);
+ */
 void kvm_pit_set_reinject(struct kvm_pit *pit, bool reinject)
 {
 	struct kvm_kpit_state *ps = &pit->pit_state;
diff --git a/arch/x86/kvm/ioapic.c b/arch/x86/kvm/ioapic.c
index 995eb5054..9f528d1e2 100644
--- a/arch/x86/kvm/ioapic.c
+++ b/arch/x86/kvm/ioapic.c
@@ -119,6 +119,13 @@ static void __rtc_irq_eoi_tracking_restore_one(struct kvm_vcpu *vcpu)
 				 kvm_lapic_irq_dest_mode(!!e->fields.dest_mode)))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+	 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+	 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+	 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+	 */
 	new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
 	old_val = test_bit(vcpu->vcpu_id, dest_map->map);
 
@@ -188,6 +195,13 @@ static void ioapic_lazy_update_eoi(struct kvm_ioapic *ioapic, int irq)
 	union kvm_ioapic_redirect_entry *entry = &ioapic->redirtbl[irq];
 
 	kvm_for_each_vcpu(i, vcpu, ioapic->kvm) {
+		/*
+		 * 在以下使用kvm_apic_pending_eoi():
+		 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+		 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+		 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+		 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+		 */
 		if (!kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 					 entry->fields.dest_id,
 					 entry->fields.dest_mode) ||
@@ -296,6 +310,13 @@ void kvm_ioapic_scan_entry(struct kvm_vcpu *vcpu, ulong *ioapic_handled_vectors)
 		    index == RTC_GSI) {
 			u16 dm = kvm_lapic_irq_dest_mode(!!e->fields.dest_mode);
 
+			/*
+			 * 在以下使用kvm_apic_pending_eoi():
+			 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+			 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+			 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+			 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+			 */
 			if (kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 						e->fields.dest_id, dm) ||
 			    kvm_apic_pending_eoi(vcpu, e->fields.vector))
diff --git a/arch/x86/kvm/irq_comm.c b/arch/x86/kvm/irq_comm.c
index 8136695f7..710733ca4 100644
--- a/arch/x86/kvm/irq_comm.c
+++ b/arch/x86/kvm/irq_comm.c
@@ -424,6 +424,13 @@ void kvm_scan_ioapic_routes(struct kvm_vcpu *vcpu,
 
 			kvm_set_msi_irq(vcpu->kvm, entry, &irq);
 
+			/*
+			 * 在以下调用kvm_apic_pending_eoi():
+			 *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+			 *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+			 *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+			 *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+			 */
 			if (irq.trig_mode &&
 			    (kvm_apic_match_dest(vcpu, NULL, APIC_DEST_NOSHORT,
 						 irq.dest_id, irq.dest_mode) ||
diff --git a/arch/x86/kvm/lapic.c b/arch/x86/kvm/lapic.c
index 3c83951c6..b19419be4 100644
--- a/arch/x86/kvm/lapic.c
+++ b/arch/x86/kvm/lapic.c
@@ -117,6 +117,13 @@ static inline int apic_test_vector(int vec, void *bitmap)
 	return test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|122| <<__rtc_irq_eoi_tracking_restore_one>> new_val = kvm_apic_pending_eoi(vcpu, e->fields.vector);
+ *   - arch/x86/kvm/ioapic.c|194| <<ioapic_lazy_update_eoi>> kvm_apic_pending_eoi(vcpu, entry->fields.vector))
+ *   - arch/x86/kvm/ioapic.c|301| <<kvm_ioapic_scan_entry>> kvm_apic_pending_eoi(vcpu, e->fields.vector))
+ *   - arch/x86/kvm/irq_comm.c|430| <<kvm_scan_ioapic_routes>> kvm_apic_pending_eoi(vcpu, irq.vector)))
+ */
 bool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
@@ -698,6 +705,10 @@ bool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)
 }
 EXPORT_SYMBOL_GPL(__kvm_apic_update_irr);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|6930| <<vmx_sync_pir_to_irr>> kvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);
+ */
 bool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)
 {
 	struct kvm_lapic *apic = vcpu->arch.apic;
diff --git a/arch/x86/kvm/lapic.h b/arch/x86/kvm/lapic.h
index 24add38be..ab6d87ca5 100644
--- a/arch/x86/kvm/lapic.h
+++ b/arch/x86/kvm/lapic.h
@@ -61,6 +61,26 @@ struct kvm_lapic {
 	struct kvm_timer lapic_timer;
 	u32 divide_count;
 	struct kvm_vcpu *vcpu;
+	/*
+	 * 在以下使用kvm_lapic->apicv_active:
+	 *   - arch/x86/kvm/lapic.c|706| <<kvm_apic_update_irr>> if (unlikely(!apic->apicv_active && irr_updated))
+	 *   - arch/x86/kvm/lapic.c|736| <<apic_clear_irr>> if (unlikely(apic->apicv_active)) {
+	 *   - arch/x86/kvm/lapic.c|765| <<apic_set_isr>> if (unlikely(apic->apicv_active))
+	 *   - arch/x86/kvm/lapic.c|810| <<apic_clear_isr>> if (unlikely(apic->apicv_active))
+	 *   - arch/x86/kvm/lapic.c|1798| <<lapic_timer_int_injected>> if (apic->apicv_active)
+	 *   - arch/x86/kvm/lapic.c|1913| <<apic_timer_expired>> if (!from_timer_fn && apic->apicv_active) {
+	 *   - arch/x86/kvm/lapic.c|2672| <<kvm_apic_update_apicv>> if (apic->apicv_active)
+	 *   - arch/x86/kvm/lapic.c|2806| <<kvm_lapic_reset>> if (apic->apicv_active) {
+	 *   - arch/x86/kvm/lapic.c|2942| <<kvm_create_lapic>> apic->apicv_active = true;
+	 *   - arch/x86/kvm/lapic.c|3122| <<kvm_apic_set_state>> if (apic->apicv_active) {
+	 *   - arch/x86/kvm/lapic.h|222| <<kvm_vcpu_apicv_active>> return lapic_in_kernel(vcpu) && vcpu->arch.apic->apicv_active;
+	 *   - arch/x86/kvm/svm/svm.c|3689| <<svm_complete_interrupt_delivery>> if (!READ_ONCE(vcpu->arch.apic->apicv_active)) {
+	 *   - arch/x86/kvm/vmx/vmx.c|4280| <<vmx_deliver_posted_interrupt>> if (!vcpu->arch.apic->apicv_active)
+	 *   - arch/x86/kvm/x86.c|10265| <<update_cr8_intercept>> if (vcpu->arch.apic->apicv_active)
+	 *   - arch/x86/kvm/x86.c|10641| <<__kvm_vcpu_update_apicv>> if (apic->apicv_active == activate)
+	 *   - arch/x86/kvm/x86.c|10644| <<__kvm_vcpu_update_apicv>> apic->apicv_active = activate;
+	 *   - arch/x86/kvm/x86.c|10654| <<__kvm_vcpu_update_apicv>> if (!apic->apicv_active)
+	 */
 	bool apicv_active;
 	bool sw_enabled;
 	bool irr_pending;
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index e93223586..f2e3443f2 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -244,6 +244,24 @@ extern bool tdp_mmu_enabled;
 #define tdp_mmu_enabled false
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/debugfs.c|100| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1467| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1490| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1577| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|1776| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1894| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|1907| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4387| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+ *   - arch/x86/kvm/mmu/mmu.c|7475| <<kvm_rmap_zap_gfn_range>> if (!kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|7537| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7810| <<kvm_mmu_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|7835| <<kvm_mmu_slot_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7908| <<kvm_mmu_recover_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/mmu/mmu.c|7924| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+ *   - arch/x86/kvm/x86.c|13204| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+ */
 static inline bool kvm_memslots_have_rmaps(struct kvm *kvm)
 {
 	return !tdp_mmu_enabled || kvm_shadow_root_allocated(kvm);
@@ -270,6 +288,33 @@ kvm_mmu_slot_lpages(struct kvm_memory_slot *slot, int level)
 	return __kvm_mmu_slot_lpages(slot, slot->npages, level);
 }
 
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|535| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+ *   - arch/x86/kvm/mmu/mmu.c|1526| <<__rmap_add>> kvm_update_page_stats(kvm, sp->role.level, 1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|512| <<handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1341| <<tdp_mmu_split_huge_page>> kvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);
+ */
 static inline void kvm_update_page_stats(struct kvm *kvm, int level, int count)
 {
 	atomic64_add(count, &kvm->stat.pages[level - 1]);
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c
index 2401606db..d28ed13d7 100644
--- a/arch/x86/kvm/mmu/mmu.c
+++ b/arch/x86/kvm/mmu/mmu.c
@@ -59,10 +59,65 @@
 
 #include "trace.h"
 
+/*
+ * https://docs.kernel.org/virt/kvm/locking.html
+ *
+ * 在NUMA系统上,不同CPU和内存之间的距离不同,为了提高系统性能,
+ * Linux内核里会进行所谓的NUMA balance,它的目的是动态的调整程序
+ * 运行的CPU或者程序使用的内存,使得CPU和内存尽量在一个NUMA域里.
+ *
+ * 对于内存的调整,内核会周期性的断开VA到PA的页表映射,这样当访问
+ * 内存时就会触发缺页异常,内核在处理缺页异常时进行必要的内存迁移.
+ * 
+ * 需要注意的是,打开NUMA balance后,内核一定会周期性的断开VA到PA的映射,
+ * 对于不需要做内存迁移的情况,内核把页表重新配置好,这个开销相对来
+ * 说是比较小的.
+ */
+
+/*
+ * 在以下使用nx_hugepage_mitigation_hard_disabled:
+ *   - arch/x86/kvm/mmu/mmu.c|62| <<global>> static bool nx_hugepage_mitigation_hard_disabled;
+ *   - arch/x86/kvm/mmu/mmu.c|7720| <<get_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7742| <<set_nx_huge_pages>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|7760| <<set_nx_huge_pages>> nx_hugepage_mitigation_hard_disabled = true;
+ *   - arch/x86/kvm/mmu/mmu.c|7889| <<set_nx_huge_pages_recovery_param>> if (nx_hugepage_mitigation_hard_disabled)
+ *   - arch/x86/kvm/mmu/mmu.c|8059| <<kvm_mmu_post_init_vm>> if (nx_hugepage_mitigation_hard_disabled)
+ *
+ * 默认是0
+ */
 static bool nx_hugepage_mitigation_hard_disabled;
 
+/*
+ * 在以下使用nx_huge_pages:
+ *   - arch/x86/kvm/mmu/mmu.c|64| <<global>> int __read_mostly nx_huge_pages = -1;
+ *   - arch/x86/kvm/mmu/mmu.c|87| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|88| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+ *   - arch/x86/kvm/mmu/mmu.c|7235| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+ *   - arch/x86/kvm/mmu/mmu.c|7240| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+ *   - arch/x86/kvm/mmu/mmu.c|7295| <<kvm_mmu_x86_module_init>> if (nx_huge_pages == -1)
+ *   - arch/x86/kvm/mmu/mmu.c|7369| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+ *   - arch/x86/kvm/mmu/mmu_internal.h|187| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
+ */
 int __read_mostly nx_huge_pages = -1;
+/*
+ * 在以下使用nx_huge_pages_recovery_period_ms:
+ *   - arch/x86/kvm/mmu/mmu.c|76| <<global>> static uint __read_mostly nx_huge_pages_recovery_period_ms;
+ *   - arch/x86/kvm/mmu/mmu.c|103| <<global>> module_param_cb(nx_huge_pages_recovery_period_ms,
+ *              &nx_huge_pages_recovery_param_ops, &nx_huge_pages_recovery_period_ms, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|105| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
+ *   - arch/x86/kvm/mmu/mmu.c|7874| <<calc_nx_huge_pages_recovery_period>> *period = READ_ONCE(nx_huge_pages_recovery_period_ms);
+ */
 static uint __read_mostly nx_huge_pages_recovery_period_ms;
+/*
+ * 在以下使用nx_huge_pages_recovery_ratio:
+ *   - arch/x86/kvm/mmu/mmu.c|79| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
+ *   - arch/x86/kvm/mmu/mmu.c|81| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 60;
+ *   - arch/x86/kvm/mmu/mmu.c|100| <<global>> module_param_cb(nx_huge_pages_recovery_ratio, &nx_huge_pages_recovery_param_ops,
+ *              &nx_huge_pages_recovery_ratio, 0644);
+ *   - arch/x86/kvm/mmu/mmu.c|102| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, "uint");
+ *   - arch/x86/kvm/mmu/mmu.c|7869| <<calc_nx_huge_pages_recovery_period>> uint ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+ *   - arch/x86/kvm/mmu/mmu.c|7936| <<kvm_recover_nx_huge_pages>> ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+ */
 #ifdef CONFIG_PREEMPT_RT
 /* Recovery can cause latency spikes, disable it for PREEMPT_RT.  */
 static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
@@ -105,6 +160,12 @@ module_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);
  */
 bool tdp_enabled = false;
 
+/*
+ * 在以下使用tdp_mmu_allowed:
+ *   - arch/x86/kvm/mmu/mmu.c|148| <<global>> static bool __ro_after_init tdp_mmu_allowed;
+ *   - arch/x86/kvm/mmu/mmu.c|6954| <<kvm_configure_mmu>> tdp_mmu_enabled = tdp_mmu_allowed && tdp_enabled;
+ *   - arch/x86/kvm/mmu/mmu.c|7948| <<kvm_mmu_x86_module_init>> tdp_mmu_allowed = tdp_mmu_enabled;
+ */
 static bool __ro_after_init tdp_mmu_allowed;
 
 #ifdef CONFIG_X86_64
@@ -113,7 +174,23 @@ module_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0444);
 #endif
 
 static int max_huge_page_level __read_mostly;
+/*
+ * 在以下使用tdp_root_level:
+ *   - arch/x86/kvm/mmu/mmu.c|156| <<global>> static int tdp_root_level __read_mostly;
+ *   - arch/x86/kvm/mmu/mmu.c|6067| <<kvm_mmu_get_tdp_level>> if (tdp_root_level)
+ *   - arch/x86/kvm/mmu/mmu.c|6068| <<kvm_mmu_get_tdp_level>> return tdp_root_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6079| <<kvm_mmu_get_max_tdp_level>> return tdp_root_level ? tdp_root_level : max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6950| <<kvm_configure_mmu>> tdp_root_level = tdp_forced_root_level;
+ */
 static int tdp_root_level __read_mostly;
+/*
+ * 在以下使用max_tdp_level:
+ *   - arch/x86/kvm/mmu/mmu.c|157| <<global>> static int max_tdp_level __read_mostly;
+ *   - arch/x86/kvm/mmu/mmu.c|6071| <<kvm_mmu_get_tdp_level>> if (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)
+ *   - arch/x86/kvm/mmu/mmu.c|6074| <<kvm_mmu_get_tdp_level>> return max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6079| <<kvm_mmu_get_max_tdp_level>> return tdp_root_level ? tdp_root_level : max_tdp_level;
+ *   - arch/x86/kvm/mmu/mmu.c|6951| <<kvm_configure_mmu>> max_tdp_level = tdp_max_root_level;
+ */
 static int max_tdp_level __read_mostly;
 
 #define PTE_PREFETCH_NUM		8
@@ -171,6 +248,11 @@ struct kvm_shadow_walk_iterator {
 	     shadow_walk_okay(&(_walker));			\
 	     shadow_walk_next(&(_walker)))
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3857| <<fast_pf_get_last_sptep>> for_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {
+ *   - arch/x86/kvm/mmu/mmu.c|4694| <<shadow_page_table_clear_flood>> for_each_shadow_entry_lockless(vcpu, addr, iterator, spte)
+ */
 #define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)	\
 	for (shadow_walk_init(&(_walker), _vcpu, _addr);		\
 	     shadow_walk_okay(&(_walker)) &&				\
@@ -288,6 +370,11 @@ static void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)
 	kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3133| <<mmu_set_spte>> mark_mmio_spte(vcpu, sptep, gfn, pte_access);
+ *   - arch/x86/kvm/mmu/mmu.c|5573| <<sync_mmio_spte>> mark_mmio_spte(vcpu, sptep, gfn, access);
+ */
 static void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,
 			   unsigned int access)
 {
@@ -512,6 +599,33 @@ static bool mmu_spte_update(u64 *sptep, u64 new_spte)
 	return leaf_spte_change_needs_tlb_flush(old_spte, new_spte);
 }
 
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+ *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+ *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+ *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+ */
 /*
  * Rules for using mmu_spte_clear_track_bits:
  * It sets the sptep from present to nonpresent, and track the
@@ -590,6 +704,13 @@ static void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4632| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|4758| <<kvm_tdp_mmu_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+ *   - arch/x86/kvm/mmu/mmu.c|5887| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|800| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+ */
 static int mmu_topup_memory_caches(struct kvm_vcpu *vcpu, bool maybe_indirect)
 {
 	int r;
@@ -621,6 +742,11 @@ static void mmu_free_memory_caches(struct kvm_vcpu *vcpu)
 	kvm_mmu_free_memory_cache(&vcpu->arch.mmu_page_header_cache);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1162| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(head_desc);
+ *   - arch/x86/kvm/mmu/mmu.c|1224| <<kvm_zap_all_rmap_sptes>> mmu_free_pte_list_desc(desc);
+ */
 static void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)
 {
 	kmem_cache_free(pte_list_desc_cache, pte_list_desc);
@@ -684,6 +810,11 @@ static void kvm_mmu_page_set_translation(struct kvm_mmu_page *sp, int index,
 	          sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2926| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|965| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+ */
 static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
 				    unsigned int access)
 {
@@ -696,6 +827,14 @@ static void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,
  * Return the pointer to the large page information for a given gfn,
  * handling slots that are not large page aligned.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|746| <<update_gfn_disallow_lpage_count>> linfo = lpage_info_slot(gfn, slot, i);
+ *   - arch/x86/kvm/mmu/mmu.c|3191| <<__kvm_mmu_max_mapping_level>> linfo = lpage_info_slot(gfn, slot, max_level);
+ *   - arch/x86/kvm/mmu/mmu.c|7971| <<hugepage_test_mixed>> return lpage_info_slot(gfn, slot, level)->disallow_lpage & KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7977| <<hugepage_clear_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage &= ~KVM_LPAGE_MIXED_FLAG;
+ *   - arch/x86/kvm/mmu/mmu.c|7983| <<hugepage_set_mixed>> lpage_info_slot(gfn, slot, level)->disallow_lpage |= KVM_LPAGE_MIXED_FLAG;
+ */
 static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
 		const struct kvm_memory_slot *slot, int level)
 {
@@ -713,6 +852,11 @@ static struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,
  */
 #define KVM_LPAGE_MIXED_FLAG	BIT(31)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|756| <<kvm_mmu_gfn_disallow_lpage>> update_gfn_disallow_lpage_count(slot, gfn, 1);
+ *   - arch/x86/kvm/mmu/mmu.c|761| <<kvm_mmu_gfn_allow_lpage>> update_gfn_disallow_lpage_count(slot, gfn, -1);
+ */
 static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 					    gfn_t gfn, int count)
 {
@@ -728,16 +872,30 @@ static void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|788| <<account_shadowed>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|107| <<__kvm_write_track_add_gfn>> kvm_mmu_gfn_disallow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|834| <<unaccount_shadowed>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ *   - arch/x86/kvm/mmu/page_track.c|130| <<__kvm_write_track_remove_gfn>> kvm_mmu_gfn_allow_lpage(slot, gfn);
+ */
 void kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)
 {
 	update_gfn_disallow_lpage_count(slot, gfn, -1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2209| <<kvm_mmu_alloc_shadow_page>> account_shadowed(kvm, sp);
+ */
 static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -768,6 +926,11 @@ static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 		kvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|818| <<account_nx_huge_page>> track_possible_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1195| <<kvm_tdp_mmu_map>> track_possible_nx_huge_page(kvm, sp);
+ */
 void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	/*
@@ -782,19 +945,65 @@ void track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 		return;
 
 	++kvm->stat.nx_lpage_splits;
+	/*
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+	 *              &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+	 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+	 *
+	 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+	 * replaced by an NX huge page.  A shadow page is on this list if its
+	 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+	 * and there are no other conditions that prevent a huge page, e.g.
+	 * the backing host page is huge, dirtly logging is not enabled for its
+	 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+	 * guarantee an NX huge page will be created in its stead, e.g. if the
+	 * guest attempts to execute from the region then KVM obviously can't
+	 * create an NX huge page (without hanging the guest).
+	 */
 	list_add_tail(&sp->possible_nx_huge_page_link,
 		      &kvm->arch.possible_nx_huge_pages);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3599| <<direct_map>> account_nx_huge_page(vcpu->kvm, sp,
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|792| <<FNAME(fetch)>> account_nx_huge_page(vcpu->kvm, sp,
+ */
 static void account_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp,
 				 bool nx_huge_page_possible)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->nx_huge_page_disallowed = true;
 
 	if (nx_huge_page_possible)
 		track_possible_nx_huge_page(kvm, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2684| <<__kvm_mmu_prepare_zap_page>> unaccount_shadowed(kvm, sp);
+ */
 static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	struct kvm_memslots *slots;
@@ -811,6 +1020,11 @@ static void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
 	kvm_mmu_gfn_allow_lpage(slot, gfn);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|850| <<unaccount_nx_huge_page>> untrack_possible_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|321| <<tdp_mmu_unlink_sp>> untrack_possible_nx_huge_page(kvm, sp);
+ */
 void untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	if (list_empty(&sp->possible_nx_huge_page_link))
@@ -820,13 +1034,40 @@ void untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 	list_del_init(&sp->possible_nx_huge_page_link);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2576| <<__kvm_mmu_prepare_zap_page>> unaccount_nx_huge_page(kvm, sp);
+ *   - arch/x86/kvm/mmu/mmu.c|7862| <<kvm_recover_nx_huge_pages>> unaccount_nx_huge_page(kvm, sp);
+ */
 static void unaccount_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	sp->nx_huge_page_disallowed = false;
 
 	untrack_possible_nx_huge_page(kvm, sp);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2985| <<kvm_mmu_prefetch_sptes>> slot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK);
+ */
 static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu,
 							   gfn_t gfn,
 							   bool no_dirty_log)
@@ -851,6 +1092,11 @@ static struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu
  */
 #define KVM_RMAP_MANY	BIT(0)
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1564| <<__rmap_add>> rmap_count = pte_list_add(cache, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1702| <<mmu_page_add_parent_pte>> pte_list_add(cache, parent_pte, &sp->parent_ptes);
+ */
 /*
  * Returns the number of pointers in the rmap chain, not counting the new one.
  */
@@ -924,9 +1170,20 @@ static void pte_list_desc_remove_entry(struct kvm *kvm,
 		rmap_head->val = 0;
 	else
 		rmap_head->val = (unsigned long)head_desc->more | KVM_RMAP_MANY;
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1162| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(head_desc);
+	 *   - arch/x86/kvm/mmu/mmu.c|1224| <<kvm_zap_all_rmap_sptes>> mmu_free_pte_list_desc(desc);
+	 */
 	mmu_free_pte_list_desc(head_desc);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+ */
 static void pte_list_remove(struct kvm *kvm, u64 *spte,
 			    struct kvm_rmap_head *rmap_head)
 {
@@ -958,13 +1215,39 @@ static void pte_list_remove(struct kvm *kvm, u64 *spte,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7907| <<kvm_mmu_zap_collapsible_spte>> kvm_zap_one_rmap_spte(kvm, rmap_head, sptep);
+ */
 static void kvm_zap_one_rmap_spte(struct kvm *kvm,
 				  struct kvm_rmap_head *rmap_head, u64 *sptep)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+	 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+	 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+	 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+	 */
 	mmu_spte_clear_track_bits(kvm, sptep);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(kvm, sptep, rmap_head);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+ *   - arch/x86/kvm/mmu/mmu.c|1853| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+ *
+ * struct kvm_rmap_head {
+ *     unsigned long val;
+ * };
+ */
 /* Return true if at least one SPTE was zapped, false otherwise */
 static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 				   struct kvm_rmap_head *rmap_head)
@@ -976,6 +1259,13 @@ static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 		return false;
 
 	if (!(rmap_head->val & KVM_RMAP_MANY)) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+		 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+		 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+		 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+		 */
 		mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
 		goto out;
 	}
@@ -983,9 +1273,21 @@ static bool kvm_zap_all_rmap_sptes(struct kvm *kvm,
 	desc = (struct pte_list_desc *)(rmap_head->val & ~KVM_RMAP_MANY);
 
 	for (; desc; desc = next) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+		 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+		 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+		 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+		 */
 		for (i = 0; i < desc->spte_count; i++)
 			mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
 		next = desc->more;
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1162| <<pte_list_desc_remove_entry>> mmu_free_pte_list_desc(head_desc);
+		 *   - arch/x86/kvm/mmu/mmu.c|1224| <<kvm_zap_all_rmap_sptes>> mmu_free_pte_list_desc(desc);
+		 */
 		mmu_free_pte_list_desc(desc);
 	}
 out:
@@ -1037,6 +1339,12 @@ static void rmap_remove(struct kvm *kvm, u64 *spte)
 	slot = __gfn_to_memslot(slots, gfn);
 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(kvm, spte, rmap_head);
 }
 
@@ -1112,12 +1420,28 @@ static u64 *rmap_get_next(struct rmap_iterator *iter)
 	return sptep;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1399| <<rmap_write_protect>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1427| <<__rmap_clear_dirty>> for_each_rmap_spte(rmap_head, &iter, sptep)
+ *   - arch/x86/kvm/mmu/mmu.c|1822| <<kvm_rmap_age_gfn_range>> for_each_rmap_spte(iterator.rmap, &iter, sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|1953| <<kvm_mmu_mark_parents_unsync>> for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|7691| <<shadow_mmu_try_split_huge_pages>> for_each_rmap_spte(rmap_head, &iter, huge_sptep) {
+ *   - arch/x86/kvm/mmu/mmu.c|7815| <<kvm_mmu_zap_collapsible_spte>> for_each_rmap_spte(rmap_head, &iter, sptep) {
+ */
 #define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
 	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
 	     _spte_; _spte_ = rmap_get_next(_iter_))
 
 static void drop_spte(struct kvm *kvm, u64 *sptep)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1199| <<kvm_zap_one_rmap_spte>> mmu_spte_clear_track_bits(kvm, sptep);
+	 *   - arch/x86/kvm/mmu/mmu.c|1214| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);
+	 *   - arch/x86/kvm/mmu/mmu.c|1222| <<kvm_zap_all_rmap_sptes>> mmu_spte_clear_track_bits(kvm, desc->sptes[i]);
+	 *   - arch/x86/kvm/mmu/mmu.c|1365| <<drop_spte>> u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
+	 */
 	u64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);
 
 	if (is_shadow_present_pte(old_spte))
@@ -1256,6 +1580,12 @@ static void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/dirty_ring.c|73| <<kvm_reset_dirty_gfn>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2215| <<kvm_get_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ *   - virt/kvm/kvm_main.c|2332| <<kvm_clear_dirty_log_protect>> kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
+ */
 void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,
 				struct kvm_memory_slot *slot,
 				gfn_t gfn_offset, unsigned long mask)
@@ -1308,6 +1638,14 @@ int kvm_cpu_dirty_log_size(void)
 	return kvm_x86_ops.cpu_dirty_log_size;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|899| <<account_shadowed>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ *   - arch/x86/kvm/mmu/mmu.c|1507| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1512| <<kvm_arch_mmu_enable_log_dirty_pt_masked>> kvm_mmu_slot_gfn_write_protect(kvm, slot, end, PG_LEVEL_2M);
+ *   - arch/x86/kvm/mmu/mmu.c|1564| <<kvm_vcpu_write_protect_gfn>> return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
+ *   - arch/x86/kvm/mmu/page_track.c|109| <<__kvm_write_track_add_gfn>> if (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))
+ */
 bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
 				    struct kvm_memory_slot *slot, u64 gfn,
 				    int min_level)
@@ -1338,9 +1676,23 @@ static bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)
 	return kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);
 }
 
+/*
+ * 在以下使用kvm_zap_rmap():
+ *   - arch/x86/kvm/mmu/mmu.c|1761| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+ *          kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1, can_yield, true, flush);
+ */
 static bool kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,
 			 const struct kvm_memory_slot *slot)
 {
+	/*
+	 * struct kvm_rmap_head {
+	 *     unsigned long val;
+	 * };
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1853| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+	 */
 	return kvm_zap_all_rmap_sptes(kvm, rmap_head);
 }
 
@@ -1406,6 +1758,13 @@ static void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)
 	rmap_walk_init_level(iterator, iterator->level);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1671| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level,
+ *                      start_gfn, end_gfn, &iterator) {
+ *   - arch/x86/kvm/mmu/mmu.c|1820| <<kvm_rmap_age_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K,
+ *                      KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator) {
+ */
 #define for_each_slot_rmap_range(_slot_, _start_level_, _end_level_,	\
 	   _start_gfn, _end_gfn, _iter_)				\
 	for (slot_rmap_walk_init(_iter_, _slot_, _start_level_,		\
@@ -1418,6 +1777,17 @@ typedef bool (*slot_rmaps_handler) (struct kvm *kvm,
 				    struct kvm_rmap_head *rmap_head,
 				    const struct kvm_memory_slot *slot);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+ *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+ *              true, flush_on_yield, false); 
+ *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+ *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+ *              can_yield, true, flush);
+ *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+ *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+ */
 static __always_inline bool __walk_slot_rmaps(struct kvm *kvm,
 					      const struct kvm_memory_slot *slot,
 					      slot_rmaps_handler fn,
@@ -1430,6 +1800,13 @@ static __always_inline bool __walk_slot_rmaps(struct kvm *kvm,
 
 	lockdep_assert_held_write(&kvm->mmu_lock);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1671| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level,
+	 *                      start_gfn, end_gfn, &iterator) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1820| <<kvm_rmap_age_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K,
+	 *                      KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator) {
+	 */
 	for_each_slot_rmap_range(slot, start_level, end_level, start_gfn,
 			end_gfn, &iterator) {
 		if (iterator.rmap)
@@ -1457,6 +1834,17 @@ static __always_inline bool walk_slot_rmaps(struct kvm *kvm,
 					    int start_level, int end_level,
 					    bool flush_on_yield)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+	 *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+	 *              true, flush_on_yield, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+	 *              can_yield, true, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+	 *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+	 */
 	return __walk_slot_rmaps(kvm, slot, fn, start_level, end_level,
 				 slot->base_gfn, slot->base_gfn + slot->npages - 1,
 				 true, flush_on_yield, false);
@@ -1470,16 +1858,43 @@ static __always_inline bool walk_slot_rmaps_4k(struct kvm *kvm,
 	return walk_slot_rmaps(kvm, slot, fn, PG_LEVEL_4K, PG_LEVEL_4K, flush_on_yield);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1777| <<kvm_unmap_gfn_range>> flush = __kvm_rmap_zap_gfn_range(kvm, range->slot,
+ *   - arch/x86/kvm/mmu/mmu.c|7488| <<kvm_rmap_zap_gfn_range>> flush = __kvm_rmap_zap_gfn_range(kvm, memslot, start,
+ */
 static bool __kvm_rmap_zap_gfn_range(struct kvm *kvm,
 				     const struct kvm_memory_slot *slot,
 				     gfn_t start, gfn_t end, bool can_yield,
 				     bool flush)
 {
+	/*
+	 * 在以下使用kvm_zap_rmap():
+	 *   - arch/x86/kvm/mmu/mmu.c|1761| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *          kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1, can_yield, true, flush);
+	 *
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+	 *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+	 *              true, flush_on_yield, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+	 *              can_yield, true, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+	 *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+	 */
 	return __walk_slot_rmaps(kvm, slot, kvm_zap_rmap,
 				 PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
 				 start, end - 1, can_yield, true, flush);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7773| <<kvm_mmu_zap_memslot>> flush = kvm_unmap_gfn_range(kvm, &range);
+ *   - arch/x86/kvm/mmu/mmu.c|8276| <<kvm_arch_pre_set_memory_attributes>> return kvm_unmap_gfn_range(kvm, range);
+ *   - virt/kvm/kvm_main.c|703| <<kvm_mmu_unmap_gfn_range>> return kvm_unmap_gfn_range(kvm, range);
+ */
 bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 {
 	bool flush = false;
@@ -1495,6 +1910,24 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 	lockdep_assert_once(kvm->mmu_invalidate_in_progress ||
 			    lockdep_is_held(&kvm->slots_lock));
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/debugfs.c|100| <<kvm_mmu_rmaps_stat_show>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1467| <<kvm_mmu_write_protect_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1490| <<kvm_mmu_clear_dirty_pt_masked>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1577| <<kvm_mmu_slot_gfn_write_protect>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1776| <<kvm_unmap_gfn_range>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1894| <<kvm_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|1907| <<kvm_test_age_gfn>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|4387| <<mmu_first_shadow_root_alloc>> if (kvm_memslots_have_rmaps(kvm) &&
+	 *   - arch/x86/kvm/mmu/mmu.c|7475| <<kvm_rmap_zap_gfn_range>> if (!kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|7537| <<kvm_mmu_slot_remove_write_access>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7810| <<kvm_mmu_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm))
+	 *   - arch/x86/kvm/mmu/mmu.c|7835| <<kvm_mmu_slot_try_split_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7908| <<kvm_mmu_recover_huge_pages>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7924| <<kvm_mmu_slot_leaf_clear_dirty>> if (kvm_memslots_have_rmaps(kvm)) {
+	 *   - arch/x86/kvm/x86.c|13204| <<kvm_alloc_memslot_metadata>> if (kvm_memslots_have_rmaps(kvm)) {
+	 */
 	if (kvm_memslots_have_rmaps(kvm))
 		flush = __kvm_rmap_zap_gfn_range(kvm, range->slot,
 						 range->start, range->end,
@@ -1512,6 +1945,11 @@ bool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 
 #define RMAP_RECYCLE_THRESHOLD 1000
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|1544| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+ *   - arch/x86/kvm/mmu/mmu.c|6730| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+ */
 static void __rmap_add(struct kvm *kvm,
 		       struct kvm_mmu_memory_cache *cache,
 		       const struct kvm_memory_slot *slot,
@@ -1523,6 +1961,13 @@ static void __rmap_add(struct kvm *kvm,
 
 	sp = sptep_to_sp(spte);
 	kvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|535| <<mmu_spte_clear_track_bits>> kvm_update_page_stats(kvm, level, -1);
+	 *   - arch/x86/kvm/mmu/mmu.c|1526| <<__rmap_add>> kvm_update_page_stats(kvm, sp->role.level, 1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|512| <<handle_changed_spte>> kvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1341| <<tdp_mmu_split_huge_page>> kvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);
+	 */
 	kvm_update_page_stats(kvm, sp->role.level, 1);
 
 	rmap_head = gfn_to_rmap(gfn, sp->role.level, slot);
@@ -1531,16 +1976,30 @@ static void __rmap_add(struct kvm *kvm,
 	if (rmap_count > kvm->stat.max_mmu_rmap_size)
 		kvm->stat.max_mmu_rmap_size = rmap_count;
 	if (rmap_count > RMAP_RECYCLE_THRESHOLD) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|1602| <<kvm_zap_rmap>> return kvm_zap_all_rmap_sptes(kvm, rmap_head);
+		 *   - arch/x86/kvm/mmu/mmu.c|1853| <<__rmap_add>> kvm_zap_all_rmap_sptes(kvm, rmap_head);
+		 */
 		kvm_zap_all_rmap_sptes(kvm, rmap_head);
 		kvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2923| <<mmu_set_spte>> rmap_add(vcpu, slot, sptep, gfn, pte_access);
+ */
 static void rmap_add(struct kvm_vcpu *vcpu, const struct kvm_memory_slot *slot,
 		     u64 *spte, gfn_t gfn, unsigned int access)
 {
 	struct kvm_mmu_memory_cache *cache = &vcpu->arch.mmu_pte_list_desc_cache;
 
+	/*
+	 * called by: 
+	 *   - arch/x86/kvm/mmu/mmu.c|1544| <<rmap_add>> __rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
+	 *   - arch/x86/kvm/mmu/mmu.c|6730| <<shadow_mmu_split_huge_page>> __rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);
+	 */
 	__rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);
 }
 
@@ -1552,6 +2011,13 @@ static bool kvm_rmap_age_gfn_range(struct kvm *kvm,
 	bool young = false;
 	u64 *sptep;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1671| <<__walk_slot_rmaps>> for_each_slot_rmap_range(slot, start_level, end_level,
+	 *                      start_gfn, end_gfn, &iterator) {
+	 *   - arch/x86/kvm/mmu/mmu.c|1820| <<kvm_rmap_age_gfn_range>> for_each_slot_rmap_range(range->slot, PG_LEVEL_4K,
+	 *                      KVM_MAX_HUGEPAGE_LEVEL, range->start, range->end - 1, &iterator) {
+	 */
 	for_each_slot_rmap_range(range->slot, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,
 				 range->start, range->end - 1, &iterator) {
 		for_each_rmap_spte(iterator.rmap, &iter, sptep) {
@@ -1621,12 +2087,20 @@ static void kvm_mmu_check_sptes_at_free(struct kvm_mmu_page *sp)
 #endif
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2416| <<kvm_mmu_alloc_shadow_page>> kvm_account_mmu_page(kvm, sp);
+ */
 static void kvm_account_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	kvm->arch.n_used_mmu_pages++;
 	kvm_account_pgtable_pages((void *)sp->spt, +1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2784| <<__kvm_mmu_prepare_zap_page>> kvm_unaccount_mmu_page(kvm, sp);
+ */
 static void kvm_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	kvm->arch.n_used_mmu_pages--;
@@ -1661,6 +2135,12 @@ static void mmu_page_add_parent_pte(struct kvm_mmu_memory_cache *cache,
 static void mmu_page_remove_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 				       u64 *parent_pte)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1200| <<kvm_zap_one_rmap_spte>> pte_list_remove(kvm, sptep, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|1275| <<rmap_remove>> pte_list_remove(kvm, spte, rmap_head);
+	 *   - arch/x86/kvm/mmu/mmu.c|2007| <<mmu_page_remove_parent_pte>> pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
+	 */
 	pte_list_remove(kvm, parent_pte, &sp->parent_ptes);
 }
 
@@ -2047,6 +2527,10 @@ static void clear_sp_write_flooding_count(u64 *spte)
  * order to read guest page tables.  Direct shadow pages are never
  * unsync, thus @vcpu can be NULL if @role.direct is true.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2371| <<__kvm_mmu_get_shadow_page>> sp = kvm_mmu_find_shadow_page(kvm, vcpu, gfn, sp_list, role);
+ */
 static struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,
 						     struct kvm_vcpu *vcpu,
 						     gfn_t gfn,
@@ -2167,6 +2651,11 @@ static struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,
 	return sp;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2391| <<kvm_mmu_get_shadow_page>> return __kvm_mmu_get_shadow_page(vcpu->kvm, vcpu, &caches, gfn, role);
+ *   - arch/x86/kvm/mmu/mmu.c|7379| <<shadow_mmu_get_sp_for_split>> return __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);
+ */
 /* Note, @vcpu may be NULL if @role.direct is true; see kvm_mmu_find_shadow_page. */
 static struct kvm_mmu_page *__kvm_mmu_get_shadow_page(struct kvm *kvm,
 						      struct kvm_vcpu *vcpu,
@@ -2249,6 +2738,12 @@ static union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,
 	return role;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+ */
 static struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,
 						 u64 *sptep, gfn_t gfn,
 						 bool direct, unsigned int access)
@@ -2360,6 +2855,12 @@ static void __link_shadow_page(struct kvm *kvm,
 		mark_unsync(sptep);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+ */
 static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 			     struct kvm_mmu_page *sp)
 {
@@ -2465,6 +2966,13 @@ static int mmu_zap_unsync_children(struct kvm *kvm,
 	return zapped;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2699| <<kvm_mmu_prepare_zap_page>> __kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);
+ *   - arch/x86/kvm/mmu/mmu.c|2749| <<kvm_mmu_zap_oldest_mmu_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,
+ *   - arch/x86/kvm/mmu/mmu.c|6997| <<kvm_zap_obsolete_pages>> unstable = __kvm_mmu_prepare_zap_page(kvm, sp,
+ *   - arch/x86/kvm/mmu/mmu.c|7602| <<kvm_mmu_zap_all>> if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
+ */
 static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 				       struct kvm_mmu_page *sp,
 				       struct list_head *invalid_list,
@@ -2516,6 +3024,24 @@ static bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,
 		zapped_root = !is_obsolete_sp(kvm, sp);
 	}
 
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (sp->nx_huge_page_disallowed)
 		unaccount_nx_huge_page(kvm, sp);
 
@@ -2564,6 +3090,13 @@ static void kvm_mmu_commit_zap_page(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2852| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm,
+ *              KVM_REFILL_PAGES - avail);
+ *   - arch/x86/kvm/mmu/mmu.c|2877| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm,
+ *              kvm->arch.n_used_mmu_pages - goal_nr_mmu_pages);
+ */
 static unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,
 						  unsigned long nr_to_zap)
 {
@@ -2610,6 +3143,13 @@ static inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3703| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|3835| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/mmu.c|4646| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|833| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+ */
 static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
 {
 	unsigned long avail = kvm_mmu_available_pages(vcpu->kvm);
@@ -2637,11 +3177,23 @@ static int make_mmu_pages_available(struct kvm_vcpu *vcpu)
  * Changing the number of mmu pages allocated to the vm
  * Note: if goal_nr_mmu_pages is too small, you will get dead lock
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|6343| <<kvm_vm_ioctl_set_nr_mmu_pages>> kvm_mmu_change_mmu_pages(kvm, kvm_nr_mmu_pages);
+ *   - arch/x86/kvm/x86.c|13398| <<kvm_arch_commit_memory_region>> kvm_mmu_change_mmu_pages(kvm, nr_mmu_pages);
+ */
 void kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)
 {
 	write_lock(&kvm->mmu_lock);
 
 	if (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|2852| <<make_mmu_pages_available>> kvm_mmu_zap_oldest_mmu_pages(vcpu->kvm,
+		 *              KVM_REFILL_PAGES - avail);
+		 *   - arch/x86/kvm/mmu/mmu.c|2877| <<kvm_mmu_change_mmu_pages>> kvm_mmu_zap_oldest_mmu_pages(kvm,
+		 *              kvm->arch.n_used_mmu_pages - goal_nr_mmu_pages);
+		 */
 		kvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -
 						  goal_nr_mmu_pages);
 
@@ -2815,6 +3367,15 @@ int mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+ *              gfn, page_to_pfn(pages[i]), NULL);
+ *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+ *              base_gfn, fault->pfn, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+ *              gw->pte_access, base_gfn, fault->pfn, fault);
+ */
 static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			u64 *sptep, unsigned int pte_access, gfn_t gfn,
 			kvm_pfn_t pfn, struct kvm_page_fault *fault)
@@ -2860,6 +3421,16 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 			was_rmapped = 1;
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+	 *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+	 *                  fault->prefetch, false, fault->map_writable, &new_spte);
+	 */
 	wrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,
 			   false, host_writable, &spte);
 
@@ -2878,8 +3449,16 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 
 	if (!was_rmapped) {
 		WARN_ON_ONCE(ret == RET_PF_SPURIOUS);
+		/*
+		 * 只在此处调用
+		 */
 		rmap_add(vcpu, slot, sptep, gfn, pte_access);
 	} else {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|2926| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|965| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+		 */
 		/* Already rmapped but the pte_access bits may have changed. */
 		kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
 	}
@@ -2887,6 +3466,11 @@ static int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|2976| <<direct_pte_prefetch_many>> return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
+ */
 static bool kvm_mmu_prefetch_sptes(struct kvm_vcpu *vcpu, gfn_t gfn, u64 *sptep,
 				   int nr_pages, unsigned int access)
 {
@@ -2906,6 +3490,15 @@ static bool kvm_mmu_prefetch_sptes(struct kvm_vcpu *vcpu, gfn_t gfn, u64 *sptep,
 		return false;
 
 	for (i = 0; i < nr_pages; i++, gfn++, sptep++) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+		 *              gfn, page_to_pfn(pages[i]), NULL);
+		 *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+		 *              base_gfn, fault->pfn, fault);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+		 *              gw->pte_access, base_gfn, fault->pfn, fault);
+		 */
 		mmu_set_spte(vcpu, slot, sptep, access, gfn,
 			     page_to_pfn(pages[i]), NULL);
 
@@ -2931,6 +3524,11 @@ static bool direct_pte_prefetch_many(struct kvm_vcpu *vcpu,
 	gfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(start));
 	unsigned int access = sp->role.access;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<direct_pte_prefetch_many>> return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
+	 */
 	return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
 }
 
@@ -3012,6 +3610,10 @@ static void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)
  * the above "rules" ensure KVM will not _consume_ the result of the walk if a
  * race with the primary MMU occurs.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3115| <<__kvm_mmu_max_mapping_level>> host_level = host_pfn_mapping_level(kvm, gfn, slot);
+ */
 static int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,
 				  const struct kvm_memory_slot *slot)
 {
@@ -3109,11 +3711,37 @@ int kvm_mmu_max_mapping_level(struct kvm *kvm,
 	return __kvm_mmu_max_mapping_level(kvm, slot, gfn, PG_LEVEL_NUM, is_private);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+ */
 void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_memory_slot *slot = fault->slot;
 	kvm_pfn_t mask;
 
+	/*
+	 * 在以下设置kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 *
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 *
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
 
 	if (unlikely(fault->max_level == PG_LEVEL_4K))
@@ -3132,9 +3760,27 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->req_level = __kvm_mmu_max_mapping_level(vcpu->kvm, slot,
 						       fault->gfn, fault->max_level,
 						       fault->is_private);
+	/*
+	 * 在以下设置kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 *
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 */
 	if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
 		return;
 
+	/*
+	 * 注释:
+	 * Page size that will be created based on the req_level and
+	 * huge_page_disallowed.
+	 */
 	/*
 	 * mmu_invalidate_retry() was successful and mmu_lock is held, so
 	 * the pmd can't be split from under us.
@@ -3145,8 +3791,32 @@ void kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	fault->pfn &= ~mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3225| <<direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|721| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1111| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+ */
 void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)
 {
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (cur_level > PG_LEVEL_4K &&
 	    cur_level == fault->goal_level &&
 	    is_shadow_present_pte(spte) &&
@@ -3166,6 +3836,33 @@ void disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_
 	}
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4650| <<direct_page_fault>> r = direct_map(vcpu, fault);
+ */
 static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_shadow_walk_iterator it;
@@ -3173,14 +3870,39 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	int ret;
 	gfn_t base_gfn = fault->gfn;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在3个地方trace这个trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3664| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|736| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1127| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 	for_each_shadow_entry(vcpu, fault->addr, it) {
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
 		 */
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3225| <<direct_map>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|721| <<FNAME(fetch)>> disallowed_hugepage_adjust(fault, *it.sptep, it.level);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1111| <<kvm_tdp_mmu_map>> disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, *it.sptep, it.level);
 
@@ -3188,11 +3910,36 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		if (it.level == fault->goal_level)
 			break;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
 		if (sp == ERR_PTR(-EEXIST))
 			continue;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 */
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		if (fault->huge_page_disallowed)
 			account_nx_huge_page(vcpu->kvm, sp,
 					     fault->req_level >= it.level);
@@ -3201,6 +3948,15 @@ static int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+	 *              gfn, page_to_pfn(pages[i]), NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+	 *              base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+	 *              gw->pte_access, base_gfn, fault->pfn, fault);
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -3390,6 +4146,39 @@ static u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)
 /*
  * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.
  */
+/*
+ * 注释:
+ * Fast page fault:
+ *
+ * Fast page fault is the fast path which fixes the guest page fault out of the
+ * mmu-lock on x86. Currently, the page fault can be fast in one of the
+ * following two cases:
+ *
+ * Access Tracking: The SPTE is not present, but it is marked for access
+ * tracking. That means we need to restore the saved R/X bits. This is
+ * described in more detail later below.
+ *
+ * Write-Protection: The SPTE is present and the fault is caused by
+ * write-protect. That means we just need to change the W bit of the spte.
+ *
+ * What we use to avoid all the races is the Host-writable bit and MMU-writable
+ * bit on the spte:
+ *
+ * Host-writable means the gfn is writable in the host kernel page tables and
+ * in its KVM memslot.
+ *
+ * MMU-writable means the gfn is writable in the guest’s mmu and it is not
+ * write-protected by shadow page write-protection.
+ *
+ * On fast page fault path, we will use cmpxchg to atomically set the spte W
+ * bit if spte.HOST_WRITEABLE = 1 and spte.WRITE_PROTECT = 1, to restore the
+ * saved R/X bits if for an access-traced spte, or both. This is safe because
+ * whenever changing these bits can be detected by cmpxchg.
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4628| <<direct_page_fault>> r = fast_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4754| <<kvm_tdp_mmu_page_fault>> r = fast_page_fault(vcpu, fault);
+ */
 static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	struct kvm_mmu_page *sp;
@@ -3411,6 +4200,28 @@ static int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		else
 			sptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);
 
+		/*
+		 * 在以下使用FROZEN_SPTE:
+		 *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+		 *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+		 *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+		 *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+		 *                                                    level, shared);
+		 *
+		 * If a thread running without exclusive control of the MMU lock must perform a
+		 * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+		 * non-present intermediate value. Other threads which encounter this value
+		 * should not modify the SPTE.
+		 *
+		 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+		 * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+		 * vulnerability.
+		 *
+		 * Only used by the TDP MMU.
+		 */
 		/*
 		 * It's entirely possible for the mapping to have been zapped
 		 * by a different task, but the root page should always be
@@ -3630,6 +4441,13 @@ void kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_free_guest_mode_roots);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+ *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+ *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+ *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+ */
 static hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,
 			    u8 level)
 {
@@ -3665,6 +4483,13 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 		goto out_unlock;
 
 	if (shadow_root_level >= PT64_ROOT_4LEVEL) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
 		mmu->root.hpa = root;
 	} else if (shadow_root_level == PT32E_ROOT_LEVEL) {
@@ -3676,6 +4501,13 @@ static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
 		for (i = 0; i < 4; ++i) {
 			WARN_ON_ONCE(IS_VALID_PAE_ROOT(mmu->pae_root[i]));
 
+			/*
+			 * called by:
+			 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+			 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+			 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+			 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+			 */
 			root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0,
 					      PT32_ROOT_LEVEL);
 			mmu->pae_root[i] = root | PT_PRESENT_MASK |
@@ -3801,6 +4633,13 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 	 * write-protect the guests page table root.
 	 */
 	if (mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL) {
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, root_gfn, 0,
 				      mmu->root_role.level);
 		mmu->root.hpa = root;
@@ -3855,6 +4694,13 @@ static int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)
 		 */
 		quadrant = (mmu->cpu_role.base.level == PT32_ROOT_LEVEL) ? i : 0;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4186| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4197| <<mmu_alloc_direct_roots>> root = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0, PT32_ROOT_LEVEL);
+		 *   - arch/x86/kvm/mmu/mmu.c|4322| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, 0, mmu->root_role.level);
+		 *   - arch/x86/kvm/mmu/mmu.c|4376| <<mmu_alloc_shadow_roots>> root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
+		 */
 		root = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);
 		mmu->pae_root[i] = root | pm_mask;
 	}
@@ -3981,6 +4827,12 @@ static bool is_unsync_root(hpa_t root)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6464| <<kvm_mmu_load>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|3628| <<kvm_vcpu_flush_tlb_guest>> kvm_mmu_sync_roots(vcpu);
+ *   - arch/x86/kvm/x86.c|10911| <<vcpu_enter_guest>> kvm_mmu_sync_roots(vcpu);
+ */
 void kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)
 {
 	int i;
@@ -4142,6 +4994,10 @@ static bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)
 	return reserved;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6158| <<kvm_mmu_page_fault>> r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
+ */
 static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 {
 	u64 spte;
@@ -4176,6 +5032,12 @@ static int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)
 	return RET_PF_RETRY;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4625| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/mmu.c|4751| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|795| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+ */
 static bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,
 					 struct kvm_page_fault *fault)
 {
@@ -4236,6 +5098,19 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 {
 	int r;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
 		return;
 
@@ -4251,6 +5126,12 @@ void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)
 	      work->arch.cr3 != kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu))
 		return;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+	 *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+	 *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+	 */
 	r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
 				  true, NULL, NULL);
 
@@ -4375,6 +5256,12 @@ static int __kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 	return RET_PF_CONTINUE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4539| <<direct_page_fault>> r = kvm_mmu_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/mmu.c|4630| <<kvm_tdp_mmu_page_fault>> r = kvm_mmu_faultin_pfn(vcpu, fault, ACC_ALL);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|804| <<FNAME(page_fault)>> r = kvm_mmu_faultin_pfn(vcpu, fault, walker.pte_access);
+ */
 static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
 			       struct kvm_page_fault *fault, unsigned int access)
 {
@@ -4488,6 +5375,12 @@ static int kvm_mmu_faultin_pfn(struct kvm_vcpu *vcpu,
  * Returns true if the page fault is stale and needs to be retried, i.e. if the
  * root was invalidated by a memslot update or a relevant mmu_notifier fired.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4643| <<direct_page_fault>> if (is_page_fault_stale(vcpu, fault))
+ *   - arch/x86/kvm/mmu/mmu.c|4769| <<kvm_tdp_mmu_page_fault>> if (is_page_fault_stale(vcpu, fault))
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|830| <<FNAME(page_fault)>> if (is_page_fault_stale(vcpu, fault))
+ */
 static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 				struct kvm_page_fault *fault)
 {
@@ -4517,6 +5410,34 @@ static bool is_page_fault_stale(struct kvm_vcpu *vcpu,
 	       mmu_invalidate_retry_gfn(vcpu->kvm, fault->mmu_seq, fault->gfn);
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4566| <<nonpaging_page_fault>> return direct_page_fault(vcpu, fault);
+ *   - arch/x86/kvm/mmu/mmu.c|4669| <<kvm_tdp_page_fault>> return direct_page_fault(vcpu, fault);
+ */
 static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	int r;
@@ -4525,6 +5446,12 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (WARN_ON_ONCE(kvm_mmu_is_dummy_root(vcpu->arch.mmu->root.hpa)))
 		return RET_PF_RETRY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4625| <<direct_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/mmu.c|4751| <<kvm_tdp_mmu_page_fault>> if (page_fault_handle_page_track(vcpu, fault))
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|795| <<FNAME(page_fault)>> if (page_fault_handle_page_track(vcpu, fault)) {
+	 */
 	if (page_fault_handle_page_track(vcpu, fault))
 		return RET_PF_WRITE_PROTECTED;
 
@@ -4532,6 +5459,13 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (r != RET_PF_INVALID)
 		return r;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4632| <<direct_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|4758| <<kvm_tdp_mmu_page_fault>> r = mmu_topup_memory_caches(vcpu, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|5887| <<kvm_mmu_load>> r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|800| <<FNAME(page_fault)>> r = mmu_topup_memory_caches(vcpu, true);
+	 */
 	r = mmu_topup_memory_caches(vcpu, false);
 	if (r)
 		return r;
@@ -4546,10 +5480,20 @@ static int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault
 	if (is_page_fault_stale(vcpu, fault))
 		goto out_unlock;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3703| <<mmu_alloc_direct_roots>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|3835| <<mmu_alloc_shadow_roots>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/mmu.c|4646| <<direct_page_fault>> r = make_mmu_pages_available(vcpu);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|833| <<FNAME(page_fault)>> r = make_mmu_pages_available(vcpu);
+	 */
 	r = make_mmu_pages_available(vcpu);
 	if (r)
 		goto out_unlock;
 
+	/*
+	 * 只在此处调用
+	 */
 	r = direct_map(vcpu, fault);
 
 out_unlock:
@@ -4595,6 +5539,17 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 	if (!flags) {
 		trace_kvm_page_fault(vcpu, fault_address, error_code);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+		 *                      insn_len);
+		 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+		 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+		 *                      svm->vmcb->control.insn_bytes : NULL,
+		 *                      svm->vmcb->control.insn_len);
+		 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+		 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+		 */
 		r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
 				insn_len);
 	} else if (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {
@@ -4611,6 +5566,30 @@ int kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,
 EXPORT_SYMBOL_GPL(kvm_handle_page_fault);
 
 #ifdef CONFIG_X86_64
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ */
 static int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,
 				  struct kvm_page_fault *fault)
 {
@@ -4659,6 +5638,36 @@ bool kvm_mmu_may_ignore_guest_pat(void)
 	return shadow_memtype_mask;
 }
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ * 在以下使用kvm_tdp_page_fault():
+ *  - arch/x86/kvm/mmu/mmu.c|4681| <<kvm_tdp_map_page>> if (vcpu->arch.mmu->page_fault != kvm_tdp_page_fault)
+ *  - arch/x86/kvm/mmu/mmu.c|5450| <<init_kvm_tdp_mmu>> context->page_fault = kvm_tdp_page_fault;
+ *  - arch/x86/kvm/mmu/mmu_internal.h|306| <<kvm_mmu_do_page_fault>> .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ *  - arch/x86/kvm/mmu/mmu_internal.h|325| <<kvm_mmu_do_page_fault>> r = kvm_tdp_page_fault(vcpu, &fault);
+ */
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 #ifdef CONFIG_X86_64
@@ -4669,6 +5678,10 @@ int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	return direct_page_fault(vcpu, fault);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4799| <<kvm_arch_vcpu_pre_fault_memory>> r = kvm_tdp_map_page(vcpu, range->gpa, error_code, &level);
+ */
 static int kvm_tdp_map_page(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code,
 			    u8 *level)
 {
@@ -4685,6 +5698,12 @@ static int kvm_tdp_map_page(struct kvm_vcpu *vcpu, gpa_t gpa, u64 error_code,
 		if (signal_pending(current))
 			return -EINTR;
 		cond_resched();
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+		 *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+		 *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
 	} while (r == RET_PF_RETRY);
 
@@ -4728,6 +5747,19 @@ long kvm_arch_vcpu_pre_fault_memory(struct kvm_vcpu *vcpu,
 	if (r)
 		return r;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	if (kvm_arch_has_private_mem(vcpu->kvm) &&
 	    kvm_mem_is_private(vcpu->kvm, gpa_to_gfn(range->gpa)))
 		error_code |= PFERR_PRIVATE_ACCESS;
@@ -4736,6 +5768,9 @@ long kvm_arch_vcpu_pre_fault_memory(struct kvm_vcpu *vcpu,
 	 * Shadow paging uses GVA for kvm page fault, so restrict to
 	 * two-dimensional paging.
 	 */
+	/*
+	 * 只在此处调用
+	 */
 	r = kvm_tdp_map_page(vcpu, range->gpa, error_code, &level);
 	if (r < 0)
 		return r;
@@ -5885,6 +6920,10 @@ static u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)
 	return spte;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/page_track.h|55| <<kvm_page_track_write>> kvm_mmu_track_write(vcpu, gpa, new, bytes);
+ */
 void kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,
 			 int bytes)
 {
@@ -5946,6 +6985,10 @@ static bool is_write_to_guest_page_table(u64 error_code)
 	return (error_code & mask) == mask;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6111| <<kvm_mmu_page_fault>> r = kvm_mmu_write_protect_fault(vcpu, cr2_or_gpa, error_code,
+ */
 static int kvm_mmu_write_protect_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 				       u64 error_code, int *emulation_type)
 {
@@ -6039,6 +7082,17 @@ static int kvm_mmu_write_protect_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 	return RET_PF_EMULATE;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+ *                      insn_len);
+ *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+ *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+ *                      svm->vmcb->control.insn_bytes : NULL, 
+ *                      svm->vmcb->control.insn_len);
+ *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+ *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+ */
 int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
 		       void *insn, int insn_len)
 {
@@ -6048,6 +7102,19 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (WARN_ON_ONCE(!VALID_PAGE(vcpu->arch.mmu->root.hpa)))
 		return RET_PF_RETRY;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	/*
 	 * Except for reserved faults (emulated MMIO is shared-only), set the
 	 * PFERR_PRIVATE_ACCESS flag for software-protected VMs based on the gfn's
@@ -6067,6 +7134,9 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
 			return -EFAULT;
 
+		/*
+		 * 只在此处调用
+		 */
 		r = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);
 		if (r == RET_PF_EMULATE)
 			goto emulate;
@@ -6075,6 +7145,12 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (r == RET_PF_INVALID) {
 		vcpu->stat.pf_taken++;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+		 *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+		 *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+		 */
 		r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
 					  &emulation_type, NULL);
 		if (KVM_BUG_ON(r == RET_PF_INVALID, vcpu->kvm))
@@ -6084,6 +7160,9 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 	if (r < 0)
 		return r;
 
+	/*
+	 * 只在此处调用kvm_mmu_write_protect_fault()
+	 */
 	if (r == RET_PF_WRITE_PROTECTED)
 		r = kvm_mmu_write_protect_fault(vcpu, cr2_or_gpa, error_code,
 						&emulation_type);
@@ -6099,6 +7178,12 @@ int noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 err
 		return 1;
 
 emulate:
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
 				       insn_len);
 }
@@ -6230,6 +7315,13 @@ void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5353| <<svm_hardware_setup>> kvm_configure_mmu(npt_enabled, get_npt_level(),
+ *              get_npt_level(), PG_LEVEL_1G);
+ *   - arch/x86/kvm/vmx/vmx.c|8542| <<vmx_hardware_setup>> kvm_configure_mmu(enable_ept, 0, vmx_get_max_ept_level(),
+ *              ept_caps_to_lpage_level(vmx_capability.ept));
+ */
 void kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,
 		       int tdp_max_root_level, int tdp_huge_page_level)
 {
@@ -6265,6 +7357,11 @@ static void free_mmu_pages(struct kvm_mmu *mmu)
 	free_page((unsigned long)mmu->pml5_root);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6362| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+ *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+ */
 static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 {
 	struct page *page;
@@ -6318,6 +7415,10 @@ static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12424| <<kvm_arch_vcpu_create>> r = kvm_mmu_create(vcpu);
+ */
 int kvm_mmu_create(struct kvm_vcpu *vcpu)
 {
 	int ret;
@@ -6336,6 +7437,11 @@ int kvm_mmu_create(struct kvm_vcpu *vcpu)
 	vcpu->arch.mmu = &vcpu->arch.root_mmu;
 	vcpu->arch.walk_mmu = &vcpu->arch.root_mmu;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6362| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
+	 *   - arch/x86/kvm/mmu/mmu.c|6366| <<kvm_mmu_create>> ret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);
+	 */
 	ret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);
 	if (ret)
 		return ret;
@@ -6474,6 +7580,25 @@ void kvm_mmu_init_vm(struct kvm *kvm)
 {
 	kvm->arch.shadow_mmio_value = shadow_mmio_value;
 	INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
+	/*
+	 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+	 * replaced by an NX huge page.  A shadow page is on this list if its
+	 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+	 * and there are no other conditions that prevent a huge page, e.g.
+	 * the backing host page is huge, dirtly logging is not enabled for its
+	 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+	 * guarantee an NX huge page will be created in its stead, e.g. if the
+	 * guest attempts to execute from the region then KVM obviously can't
+	 * create an NX huge page (without hanging the guest).
+	 *
+	 * 在以下使用kvm_arch->possible_nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+	 *              &kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+	 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+	 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+	 */
 	INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
 	spin_lock_init(&kvm->arch.mmu_unsync_pages_lock);
 
@@ -6642,6 +7767,10 @@ static int topup_split_caches(struct kvm *kvm)
 	return kvm_mmu_topup_memory_cache(&kvm->arch.split_shadow_page_cache, 1);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7399| <<shadow_mmu_split_huge_page>> sp = shadow_mmu_get_sp_for_split(kvm, huge_sptep);
+ */
 static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *huge_sptep)
 {
 	struct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);
@@ -6669,6 +7798,10 @@ static struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *hu
 	return __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6773| <<shadow_mmu_try_split_huge_page>> shadow_mmu_split_huge_page(kvm, slot, huge_sptep);
+ */
 static void shadow_mmu_split_huge_page(struct kvm *kvm,
 				       const struct kvm_memory_slot *slot,
 				       u64 *huge_sptep)
@@ -6715,6 +7848,10 @@ static void shadow_mmu_split_huge_page(struct kvm *kvm,
 	__link_shadow_page(kvm, cache, huge_sptep, sp, flush);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6809| <<shadow_mmu_try_split_huge_pages>> r = shadow_mmu_try_split_huge_page(kvm, slot, huge_sptep);
+ */
 static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
 					  const struct kvm_memory_slot *slot,
 					  u64 *huge_sptep)
@@ -6754,6 +7891,10 @@ static int shadow_mmu_try_split_huge_page(struct kvm *kvm,
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6840| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot, shadow_mmu_try_split_huge_pages,
+ */
 static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 					    struct kvm_rmap_head *rmap_head,
 					    const struct kvm_memory_slot *slot)
@@ -6800,6 +7941,11 @@ static bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6854| <<kvm_mmu_try_split_huge_pages>> kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
+ *   - arch/x86/kvm/mmu/mmu.c|6876| <<kvm_mmu_slot_try_split_huge_pages>> kvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);
+ */
 static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 						const struct kvm_memory_slot *slot,
 						gfn_t start, gfn_t end,
@@ -6807,6 +7953,17 @@ static void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,
 {
 	int level;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|1732| <<walk_slot_rmaps>> return __walk_slot_rmaps(kvm, slot,
+	 *              fn, start_level, end_level, slot->base_gfn, slot->base_gfn + slot->npages - 1,
+	 *              true, flush_on_yield, false);
+	 *   - arch/x86/kvm/mmu/mmu.c|1750| <<__kvm_rmap_zap_gfn_range>> return __walk_slot_rmaps(kvm, slot,
+	 *              kvm_zap_rmap, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL, start, end - 1,
+	 *              can_yield, true, flush);
+	 *   - arch/x86/kvm/mmu/mmu.c|7797| <<kvm_shadow_mmu_try_split_huge_pages>> __walk_slot_rmaps(kvm, slot,
+	 *              shadow_mmu_try_split_huge_pages, level, level, start, end - 1, true, true, false);
+	 */
 	/*
 	 * Split huge pages starting with KVM_MAX_HUGEPAGE_LEVEL and working
 	 * down to the target level. This ensures pages are recursively split
@@ -6838,6 +7995,10 @@ void kvm_mmu_try_split_huge_pages(struct kvm *kvm,
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13296| <<kvm_mmu_slot_apply_flags>> kvm_mmu_slot_try_split_huge_pages(kvm, new, PG_LEVEL_4K);
+ */
 void kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,
 					const struct kvm_memory_slot *memslot,
 					int target_level)
@@ -6963,6 +8124,10 @@ void kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,
 	 */
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7860| <<kvm_arch_flush_shadow_all>> kvm_mmu_zap_all(kvm);
+ */
 static void kvm_mmu_zap_all(struct kvm *kvm)
 {
 	struct kvm_mmu_page *sp, *node;
@@ -7234,6 +8399,12 @@ void kvm_mmu_vendor_module_exit(void)
  * Calculate the effective recovery period, accounting for '0' meaning "let KVM
  * select a halving time of 1 hour".  Returns true if recovery is enabled.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7892| <<set_nx_huge_pages_recovery_param>> was_recovery_enabled = calc_nx_huge_pages_recovery_period(&old_period);
+ *   - arch/x86/kvm/mmu/mmu.c|7898| <<set_nx_huge_pages_recovery_param>> is_recovery_enabled = calc_nx_huge_pages_recovery_period(&new_period);
+ *   - arch/x86/kvm/mmu/mmu.c|8039| <<kvm_nx_huge_page_recovery_worker>> enabled = calc_nx_huge_pages_recovery_period(&period);
+ */
 static bool calc_nx_huge_pages_recovery_period(uint *period)
 {
 	/*
@@ -7246,6 +8417,14 @@ static bool calc_nx_huge_pages_recovery_period(uint *period)
 	if (!enabled || !ratio)
 		return false;
 
+	/*
+	 * 在以下使用nx_huge_pages_recovery_period_ms:
+	 *   - arch/x86/kvm/mmu/mmu.c|76| <<global>> static uint __read_mostly nx_huge_pages_recovery_period_ms;
+	 *   - arch/x86/kvm/mmu/mmu.c|103| <<global>> module_param_cb(nx_huge_pages_recovery_period_ms,
+	 *              &nx_huge_pages_recovery_param_ops, &nx_huge_pages_recovery_period_ms, 0644);
+	 *   - arch/x86/kvm/mmu/mmu.c|105| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, "uint");
+	 *   - arch/x86/kvm/mmu/mmu.c|7874| <<calc_nx_huge_pages_recovery_period>> *period = READ_ONCE(nx_huge_pages_recovery_period_ms);
+	 */
 	*period = READ_ONCE(nx_huge_pages_recovery_period_ms);
 	if (!*period) {
 		/* Make sure the period is not less than one second.  */
@@ -7287,8 +8466,20 @@ static int set_nx_huge_pages_recovery_param(const char *val, const struct kernel
 	return err;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|8052| <<kvm_nx_huge_page_recovery_worker>> kvm_recover_nx_huge_pages(kvm);
+ */
 static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_vm_stat->nx_lpage_splits:
+	 *   - arch/x86/kvm/x86.c|244| <<global>> STATS_DESC_ICOUNTER(VM, nx_lpage_splits),
+	 *   - arch/x86/kvm/mmu/mmu.c|839| <<track_possible_nx_huge_page>> ++kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|897| <<untrack_possible_nx_huge_page>> --kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7917| <<kvm_recover_nx_huge_pages>> unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
+	 *   - arch/x86/kvm/mmu/mmu.c|7937| <<kvm_recover_nx_huge_pages>> to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
+	 */
 	unsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;
 	struct kvm_memory_slot *slot;
 	int rcu_idx;
@@ -7308,9 +8499,38 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 	 */
 	rcu_read_lock();
 
+	/*
+	 * 在以下使用nx_huge_pages_recovery_ratio:
+	 *   - arch/x86/kvm/mmu/mmu.c|79| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 0;
+	 *   - arch/x86/kvm/mmu/mmu.c|81| <<global>> static uint __read_mostly nx_huge_pages_recovery_ratio = 60;
+	 *   - arch/x86/kvm/mmu/mmu.c|100| <<global>> module_param_cb(nx_huge_pages_recovery_ratio, &nx_huge_pages_recovery_param_ops,
+	 *              &nx_huge_pages_recovery_ratio, 0644);
+	 *   - arch/x86/kvm/mmu/mmu.c|102| <<global>> __MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, "uint");
+	 *   - arch/x86/kvm/mmu/mmu.c|7869| <<calc_nx_huge_pages_recovery_period>> uint ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+	 *   - arch/x86/kvm/mmu/mmu.c|7936| <<kvm_recover_nx_huge_pages>> ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
+	 */
 	ratio = READ_ONCE(nx_huge_pages_recovery_ratio);
 	to_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;
 	for ( ; to_zap; --to_zap) {
+		/*
+		 * A list of kvm_mmu_page structs that, if zapped, could possibly be
+		 * replaced by an NX huge page.  A shadow page is on this list if its
+		 * existence disallows an NX huge page (nx_huge_page_disallowed is set)
+		 * and there are no other conditions that prevent a huge page, e.g.
+		 * the backing host page is huge, dirtly logging is not enabled for its
+		 * memslot, etc...  Note, zapping shadow pages on this list doesn't
+		 * guarantee an NX huge page will be created in its stead, e.g. if the
+		 * guest attempts to execute from the region then KVM obviously can't
+		 * create an NX huge page (without hanging the guest).
+		 *
+		 * 在以下使用kvm_arch->possible_nx_huge_pages:
+		 *   - arch/x86/kvm/mmu/mmu.c|841| <<track_possible_nx_huge_page>> list_add_tail(&sp->possible_nx_huge_page_link,
+		 *              &kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|7081| <<kvm_mmu_init_vm>> INIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);
+		 *   - arch/x86/kvm/mmu/mmu.c|7939| <<kvm_recover_nx_huge_pages>> if (list_empty(&kvm->arch.possible_nx_huge_pages))
+		 *   - arch/x86/kvm/mmu/mmu.c|7949| <<kvm_recover_nx_huge_pages>> sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
+		 *              struct kvm_mmu_page, possible_nx_huge_page_link);
+		 */
 		if (list_empty(&kvm->arch.possible_nx_huge_pages))
 			break;
 
@@ -7324,6 +8544,24 @@ static void kvm_recover_nx_huge_pages(struct kvm *kvm)
 		sp = list_first_entry(&kvm->arch.possible_nx_huge_pages,
 				      struct kvm_mmu_page,
 				      possible_nx_huge_page_link);
+		/*
+		 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+		 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+		 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+		 *
+		 * The shadow page can't be replaced by an equivalent huge page
+		 * because it is being used to map an executable page in the guest
+		 * and the NX huge page mitigation is enabled.
+		 */
 		WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
 		WARN_ON_ONCE(!sp->role.direct);
 
@@ -7417,6 +8655,23 @@ int kvm_mmu_post_init_vm(struct kvm *kvm)
 		return 0;
 
 	kvm->arch.nx_huge_page_last = get_jiffies_64();
+	/*
+	 * 在以下使用kvm_arch->nx_huge_page_recovery_thread:
+	 *   - arch/x86/kvm/mmu/mmu.c|7778| <<set_nx_huge_pages>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|7907| <<set_nx_huge_pages_recovery_param>> vhost_task_wake(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8071| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread = vhost_task_create(
+	 *   - arch/x86/kvm/mmu/mmu.c|8075| <<kvm_mmu_post_init_vm>> if (!kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8078| <<kvm_mmu_post_init_vm>> vhost_task_start(kvm->arch.nx_huge_page_recovery_thread);
+	 *   - arch/x86/kvm/mmu/mmu.c|8084| <<kvm_mmu_pre_destroy_vm>> if (kvm->arch.nx_huge_page_recovery_thread)
+	 *   - arch/x86/kvm/mmu/mmu.c|8085| <<kvm_mmu_pre_destroy_vm>> vhost_task_stop(kvm->arch.nx_huge_page_recovery_thread);
+	 *
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7420| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread =
+	 *            vhost_task_create(kvm_nx_huge_page_recovery_worker,
+	 *                              kvm_nx_huge_page_recovery_worker_kill, kvm, "kvm-nx-lpage-recovery");
+	 * drivers/vhost/vhost.c|701| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+	 *            vhost_worker_killed, worker, name);
+	 */
 	kvm->arch.nx_huge_page_recovery_thread = vhost_task_create(
 		kvm_nx_huge_page_recovery_worker, kvm_nx_huge_page_recovery_worker_kill,
 		kvm, "kvm-nx-lpage-recovery");
diff --git a/arch/x86/kvm/mmu/mmu_internal.h b/arch/x86/kvm/mmu/mmu_internal.h
index b00abbe3f..83a5e0e59 100644
--- a/arch/x86/kvm/mmu/mmu_internal.h
+++ b/arch/x86/kvm/mmu/mmu_internal.h
@@ -71,6 +71,24 @@ struct kvm_mmu_page {
 	  * because it is being used to map an executable page in the guest
 	  * and the NX huge page mitigation is enabled.
 	  */
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	bool nx_huge_page_disallowed;
 
 	/*
@@ -78,6 +96,11 @@ struct kvm_mmu_page {
 	 * hash table.
 	 */
 	union kvm_mmu_page_role role;
+	/*
+	 * 似乎对direct是管理地址范围的起始地址对应的gfn
+	 * 对于shadow, 是当前sp的对应的guest的gfn
+	 * 这个值就是当前shadow页表对应的虚机页表在guest os中的gfn
+	 */
 	gfn_t gfn;
 
 	u64 *spt;
@@ -184,6 +207,17 @@ unsigned int pte_list_count(struct kvm_rmap_head *rmap_head);
 extern int nx_huge_pages;
 static inline bool is_nx_huge_page_enabled(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用nx_huge_pages:
+	 *   - arch/x86/kvm/mmu/mmu.c|64| <<global>> int __read_mostly nx_huge_pages = -1;
+	 *   - arch/x86/kvm/mmu/mmu.c|87| <<global>> module_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);
+	 *   - arch/x86/kvm/mmu/mmu.c|88| <<global>> __MODULE_PARM_TYPE(nx_huge_pages, "bool");
+	 *   - arch/x86/kvm/mmu/mmu.c|7235| <<__set_nx_huge_pages>> nx_huge_pages = itlb_multihit_kvm_mitigation = val;
+	 *   - arch/x86/kvm/mmu/mmu.c|7240| <<set_nx_huge_pages>> bool old_val = nx_huge_pages;
+	 *   - arch/x86/kvm/mmu/mmu.c|7295| <<kvm_mmu_x86_module_init>> if (nx_huge_pages == -1)
+	 *   - arch/x86/kvm/mmu/mmu.c|7369| <<calc_nx_huge_pages_recovery_period>> bool enabled = READ_ONCE(nx_huge_pages);
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|187| <<is_nx_huge_page_enabled>> return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
+	 */
 	return READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;
 }
 
@@ -203,12 +237,33 @@ struct kvm_page_fault {
 	/* Derived from mmu and global state.  */
 	const bool is_tdp;
 	const bool is_private;
+	/*
+	 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+	 */
 	const bool nx_huge_page_workaround_enabled;
 
 	/*
 	 * Whether a >4KB mapping can be created or is forbidden due to NX
 	 * hugepages.
 	 */
+	/*
+	 * 在以下设置kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+	 * 在以下使用kvm_page_fault->huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+	 *
+	 * Whether a >4KB mapping can be created or is forbidden due to NX
+	 * hugepages.
+	 */
 	bool huge_page_disallowed;
 
 	/*
@@ -290,10 +345,29 @@ static inline void kvm_mmu_prepare_memory_fault_exit(struct kvm_vcpu *vcpu,
 				      fault->is_private);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|4254| <<kvm_arch_async_page_ready>> r = kvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, work->arch.error_code,
+ *   - arch/x86/kvm/mmu/mmu.c|4688| <<kvm_tdp_map_page>> r = kvm_mmu_do_page_fault(vcpu, gpa, error_code, true, NULL, level);
+ *   - arch/x86/kvm/mmu/mmu.c|6078| <<kvm_mmu_page_fault>> r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa, error_code, false,
+ */
 static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 					u64 err, bool prefetch,
 					int *emulation_type, u8 *level)
 {
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	struct kvm_page_fault fault = {
 		.addr = cr2_or_gpa,
 		.error_code = err,
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h
index f4711674c..125b5e0bd 100644
--- a/arch/x86/kvm/mmu/paging_tmpl.h
+++ b/arch/x86/kvm/mmu/paging_tmpl.h
@@ -543,6 +543,11 @@ FNAME(prefetch_gpte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	pte_access = sp->role.access & FNAME(gpte_access)(gpte);
 	FNAME(protect_clean_gpte)(vcpu->arch.mmu, &pte_access, gpte);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2976| <<direct_pte_prefetch_many>> return kvm_mmu_prefetch_sptes(vcpu, gfn, start, end - start, access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|546| <<FNAME(prefetch_gpte)>> return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
+	 */
 	return kvm_mmu_prefetch_sptes(vcpu, gfn, spte, 1, pte_access);
 }
 
@@ -659,6 +664,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 		table_gfn = gw->table_gfn[it.level - 2];
 		access = gw->pt_access[it.level - 2];
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn,
 					  false, access);
 
@@ -695,6 +706,12 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 		if (FNAME(gpte_changed)(vcpu, gw, it.level - 1))
 			return RET_PF_RETRY;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 */
 		if (sp != ERR_PTR(-EEXIST))
 			link_shadow_page(vcpu, it.sptep, sp);
 
@@ -708,11 +725,31 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	 * are being shadowed by KVM, i.e. allocating a new shadow page may
 	 * affect the allowed hugepage size.
 	 */
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在3个地方trace这个trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3664| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|736| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1127| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	for (; shadow_walk_okay(&it); shadow_walk_next(&it)) {
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 */
 		/*
 		 * We cannot overwrite existing page tables with an NX
 		 * large page, as the leaf could be executable.
@@ -726,12 +763,37 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 
 		validate_direct_spte(vcpu, it.sptep, direct_access);
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3231| <<direct_map>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|662| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, table_gfn, false, access
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|729| <<FNAME(fetch)>> sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, direct_access);
+		 */
 		sp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn,
 					  true, direct_access);
 		if (sp == ERR_PTR(-EEXIST))
 			continue;
 
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/mmu.c|3235| <<direct_map>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|699| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|734| <<FNAME(fetch)>> link_shadow_page(vcpu, it.sptep, sp);
+		 */
 		link_shadow_page(vcpu, it.sptep, sp);
+		/*
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		if (fault->huge_page_disallowed)
 			account_nx_huge_page(vcpu->kvm, sp,
 					     fault->req_level >= it.level);
@@ -740,6 +802,15 @@ static int FNAME(fetch)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,
 	if (WARN_ON_ONCE(it.level != fault->goal_level))
 		return -EFAULT;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2951| <<kvm_mmu_prefetch_sptes>> mmu_set_spte(vcpu, slot, sptep, access,
+	 *              gfn, page_to_pfn(pages[i]), NULL);
+	 *   - arch/x86/kvm/mmu/mmu.c|3312| <<direct_map>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,
+	 *              base_gfn, fault->pfn, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|761| <<FNAME(fetch)>> ret = mmu_set_spte(vcpu, fault->slot, it.sptep,
+	 *              gw->pte_access, base_gfn, fault->pfn, fault);
+	 */
 	ret = mmu_set_spte(vcpu, fault->slot, it.sptep, gw->pte_access,
 			   base_gfn, fault->pfn, fault);
 	if (ret == RET_PF_SPURIOUS)
@@ -943,6 +1014,11 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	if (kvm_mmu_page_get_access(sp, i) == pte_access)
 		return 0;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|2926| <<mmu_set_spte>> kvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|965| <<FNAME(sync_spte)>> kvm_mmu_page_set_access(sp, i, pte_access);
+	 */
 	/* Update the shadowed access bits in case they changed. */
 	kvm_mmu_page_set_access(sp, i, pte_access);
 
@@ -950,6 +1026,16 @@ static int FNAME(sync_spte)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int
 	spte = *sptep;
 	host_writable = spte & shadow_host_writable_mask;
 	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+	 *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+	 *                  fault->prefetch, false, fault->map_writable, &new_spte);
+	 */
 	make_spte(vcpu, sp, slot, pte_access, gfn,
 		  spte_to_pfn(spte), spte, true, true,
 		  host_writable, &spte);
diff --git a/arch/x86/kvm/mmu/spte.c b/arch/x86/kvm/mmu/spte.c
index 22551e2f1..e7c13f311 100644
--- a/arch/x86/kvm/mmu/spte.c
+++ b/arch/x86/kvm/mmu/spte.c
@@ -150,6 +150,16 @@ bool spte_has_volatile_bits(u64 spte)
 	return false;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+ *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+ *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+ *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+ *                  fault->prefetch, false, fault->map_writable, &new_spte);
+ */
 bool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,
 	       const struct kvm_memory_slot *slot,
 	       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,
@@ -414,6 +424,28 @@ void kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)
 				  SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))
 		mmio_value = 0;
 
+	/*
+	 * 在以下使用FROZEN_SPTE:
+	 *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+	 *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+	 *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+	 *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+	 *                                                    level, shared);
+	 *
+	 * If a thread running without exclusive control of the MMU lock must perform a
+	 * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+	 * non-present intermediate value. Other threads which encounter this value
+	 * should not modify the SPTE.
+	 *
+	 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+	 * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+	 * vulnerability.
+	 *
+	 * Only used by the TDP MMU.
+	 */
 	/*
 	 * The masked MMIO value must obviously match itself and a frozen SPTE
 	 * must not get a false positive.  Frozen SPTEs and MMIO SPTEs should
diff --git a/arch/x86/kvm/mmu/spte.h b/arch/x86/kvm/mmu/spte.h
index af10bc038..27d02909f 100644
--- a/arch/x86/kvm/mmu/spte.h
+++ b/arch/x86/kvm/mmu/spte.h
@@ -221,11 +221,41 @@ extern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;
  *
  * Only used by the TDP MMU.
  */
+/*
+ * 在以下使用FROZEN_SPTE:
+ *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+ *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+ *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+ *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+ *                                                    level, shared);
+ *
+ * If a thread running without exclusive control of the MMU lock must perform a
+ * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+ * non-present intermediate value. Other threads which encounter this value
+ * should not modify the SPTE.
+ *
+ * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+ * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+ * vulnerability.
+ *
+ * Only used by the TDP MMU.
+ */
 #define FROZEN_SPTE	(SHADOW_NONPRESENT_VALUE | 0x5a0ULL)
 
 /* Frozen SPTEs must not be misconstrued as shadow present PTEs. */
 static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|387| <<handle_removed_pt>> if (!is_frozen_spte(old_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|518| <<handle_changed_spte>> !is_frozen_spte(new_spte)))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|554| <<__tdp_mmu_set_spte_atomic>> WARN_ON_ONCE(iter->yielded || is_frozen_spte(iter->old_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|629| <<tdp_mmu_set_spte>> WARN_ON_ONCE(is_frozen_spte(old_spte) || is_frozen_spte(new_spte));
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> if (is_frozen_spte(iter.old_spte))
+ */
 static inline bool is_frozen_spte(u64 spte)
 {
 	return spte == FROZEN_SPTE;
diff --git a/arch/x86/kvm/mmu/tdp_mmu.c b/arch/x86/kvm/mmu/tdp_mmu.c
index 2f15e0e33..6f33b6739 100644
--- a/arch/x86/kvm/mmu/tdp_mmu.c
+++ b/arch/x86/kvm/mmu/tdp_mmu.c
@@ -313,6 +313,24 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 {
 	tdp_unaccount_mmu_page(kvm, sp);
 
+	/*
+	 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+	 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+	 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+	 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+	 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+	 *
+	 * The shadow page can't be replaced by an equivalent huge page
+	 * because it is being used to map an executable page in the guest
+	 * and the NX huge page mitigation is enabled.
+	 */
 	if (!sp->nx_huge_page_disallowed)
 		return;
 
@@ -339,6 +357,10 @@ static void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
  * this thread will be responsible for ensuring the page is freed. Hence the
  * early rcu_dereferences in the function.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|540| <<handle_changed_spte>> handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
+ */
 static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 {
 	struct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));
@@ -365,6 +387,28 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			 * value to the frozen SPTE value.
 			 */
 			for (;;) {
+				/*
+				 * 在以下使用FROZEN_SPTE:
+				 *   - arch/x86/kvm/mmu/spte.h|227| <<global>> static_assert(!(FROZEN_SPTE & SPTE_MMU_PRESENT_MASK));
+				 *   - arch/x86/kvm/mmu/mmu.c|3986| <<fast_page_fault>> spte = FROZEN_SPTE;
+				 *   - arch/x86/kvm/mmu/spte.c|424| <<kvm_mmu_set_mmio_spte_mask>> if(...WARN_ON(mmio_value && (FROZEN_SPTE & mmio_mask) == mmio_value))
+				 *   - arch/x86/kvm/mmu/spte.h|231| <<is_frozen_spte>> return spte == FROZEN_SPTE;
+				 *   - arch/x86/kvm/mmu/tdp_mmu.c|386| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
+				 *   - arch/x86/kvm/mmu/tdp_mmu.c|434| <<handle_removed_pt>> old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, FROZEN_SPTE, level);
+				 *   - arch/x86/kvm/mmu/tdp_mmu.c|437| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn, old_spte, FROZEN_SPTE,
+				 *                                                    level, shared);
+				 *
+				 * If a thread running without exclusive control of the MMU lock must perform a
+				 * multi-part operation on an SPTE, it can set the SPTE to FROZEN_SPTE as a
+				 * non-present intermediate value. Other threads which encounter this value
+				 * should not modify the SPTE.
+				 *
+				 * Use a semi-arbitrary value that doesn't set RWX bits, i.e. is not-present on
+				 * both AMD and Intel CPUs, and doesn't set PFN bits, i.e. doesn't create a L1TF
+				 * vulnerability.
+				 *
+				 * Only used by the TDP MMU.
+				 */
 				old_spte = kvm_tdp_mmu_write_spte_atomic(sptep, FROZEN_SPTE);
 				if (!is_frozen_spte(old_spte))
 					break;
@@ -415,6 +459,16 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
 			old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte,
 							  FROZEN_SPTE, level);
 		}
+		/*
+		 * called by:
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+		 *
+		 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+		 * dirty logging updates are handled in common code, not here (see make_spte()
+		 * and fast_pf_fix_direct_spte()).
+		 */
 		handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
 				    old_spte, FROZEN_SPTE, level, shared);
 	}
@@ -438,6 +492,16 @@ static void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)
  * dirty logging updates are handled in common code, not here (see make_spte()
  * and fast_pf_fix_direct_spte()).
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+ *
+ * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+ * dirty logging updates are handled in common code, not here (see make_spte()
+ * and fast_pf_fix_direct_spte()).
+ */
 static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 				u64 old_spte, u64 new_spte, int level,
 				bool shared)
@@ -522,6 +586,10 @@ static void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,
 		handle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|649| <<tdp_mmu_set_spte_atomic>> ret = __tdp_mmu_set_spte_atomic(iter, new_spte);
+ */
 static inline int __must_check __tdp_mmu_set_spte_atomic(struct tdp_iter *iter,
 							 u64 new_spte)
 {
@@ -565,6 +633,15 @@ static inline int __must_check __tdp_mmu_set_spte_atomic(struct tdp_iter *iter,
  *            no side-effects other than setting iter->old_spte to the last
  *            known value of the spte.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|822| <<__tdp_mmu_zap_root>> else if (tdp_mmu_set_spte_atomic(kvm, &iter, SHADOW_NONPRESENT_VALUE))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1100| <<tdp_mmu_map_handle_target_level>> else if (tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1148| <<tdp_mmu_link_sp>> ret = tdp_mmu_set_spte_atomic(kvm, iter, spte);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1451| <<wrprot_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, new_spte))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1671| <<clear_dirty_gfn_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, iter.old_spte & ~dbit))
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|1831| <<recover_huge_pages_range>> if (tdp_mmu_set_spte_atomic(kvm, &iter, huge_spte))
+ */
 static inline int __must_check tdp_mmu_set_spte_atomic(struct kvm *kvm,
 						       struct tdp_iter *iter,
 						       u64 new_spte)
@@ -577,6 +654,16 @@ static inline int __must_check tdp_mmu_set_spte_atomic(struct kvm *kvm,
 	if (ret)
 		return ret;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 *
+	 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+	 * dirty logging updates are handled in common code, not here (see make_spte()
+	 * and fast_pf_fix_direct_spte()).
+	 */
 	handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
 			    new_spte, iter->level, true);
 
@@ -596,6 +683,16 @@ static inline int __must_check tdp_mmu_set_spte_atomic(struct kvm *kvm,
  * Returns the old SPTE value, which _may_ be different than @old_spte if the
  * SPTE had voldatile bits.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|641| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id,
+ *              iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+ *   - arch/x86/kvm/mmu/tdp_mmu.c|809| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp),
+ *              sp->ptep, old_spte, SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
+ *
+ * Returns the old SPTE value, which _may_ be different than @old_spte if the
+ * SPTE had voldatile bits.
+ */
 static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 			    u64 old_spte, u64 new_spte, gfn_t gfn, int level)
 {
@@ -612,6 +709,16 @@ static u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,
 
 	old_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|436| <<handle_removed_pt>> handle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|598| <<tdp_mmu_set_spte_atomic>> handle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|633| <<tdp_mmu_set_spte>> handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
+	 *
+	 * Handle bookkeeping that might result from the modification of a SPTE.  Note,
+	 * dirty logging updates are handled in common code, not here (see make_spte()
+	 * and fast_pf_fix_direct_spte()).
+	 */
 	handle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);
 	return old_spte;
 }
@@ -620,6 +727,16 @@ static inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,
 					 u64 new_spte)
 {
 	WARN_ON_ONCE(iter->yielded);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|641| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id,
+	 *              iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|809| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp),
+	 *              sp->ptep, old_spte, SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
+	 *
+	 * Returns the old SPTE value, which _may_ be different than @old_spte if the
+	 * SPTE had voldatile bits.
+	 */
 	iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep,
 					  iter->old_spte, new_spte,
 					  iter->gfn, iter->level);
@@ -788,6 +905,16 @@ bool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
 	if (WARN_ON_ONCE(!is_shadow_present_pte(old_spte)))
 		return false;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|641| <<tdp_mmu_iter_set_spte>> iter->old_spte = tdp_mmu_set_spte(kvm, iter->as_id,
+	 *              iter->sptep, iter->old_spte, new_spte, iter->gfn, iter->level);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|809| <<kvm_tdp_mmu_zap_sp>> tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp),
+	 *              sp->ptep, old_spte, SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
+	 *
+	 * Returns the old SPTE value, which _may_ be different than @old_spte if the
+	 * SPTE had voldatile bits.
+	 */
 	tdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte,
 			 SHADOW_NONPRESENT_VALUE, sp->gfn, sp->role.level + 1);
 
@@ -990,6 +1117,16 @@ static int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,
 	    is_last_spte(iter->old_spte, iter->level))
 		return RET_PF_SPURIOUS;
 
+	/*
+	 * 在以下调用make_spte():
+	 *   - arch/x86/kvm/mmu/mmu.c|3258| <<mmu_set_spte>> wrprot = make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, pfn, *sptep, prefetch, false, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|1029| <<FNAME(sync_spte)>> make_spte(vcpu, sp, slot, pte_access,
+	 *                  gfn, spte_to_pfn(spte), spte, true, true, host_writable, &spte);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1123| <<tdp_mmu_map_handle_target_level>> wrprot = make_spte(vcpu,
+	 *                  sp, fault->slot, ACC_ALL, iter->gfn, fault->pfn, iter->old_spte,
+	 *                  fault->prefetch, false, fault->map_writable, &new_spte);
+	 */
 	if (unlikely(!fault->slot))
 		new_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);
 	else
@@ -1062,6 +1199,34 @@ static int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,
 static int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,
 				   struct kvm_mmu_page *sp, bool shared);
 
+/*
+ * 在kvm_mmu_do_page_fault()部分初始化(其他的没记录在这里)
+ *
+ * 327         struct kvm_page_fault fault = {
+ * 328                 .addr = cr2_or_gpa,
+ * 329                 .error_code = err,
+ * 330                 .exec = err & PFERR_FETCH_MASK,
+ * 331                 .write = err & PFERR_WRITE_MASK,
+ * 332                 .present = err & PFERR_PRESENT_MASK,
+ * 333                 .rsvd = err & PFERR_RSVD_MASK,
+ * 334                 .user = err & PFERR_USER_MASK,
+ * 335                 .prefetch = prefetch,
+ * 336                 .is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),
+ * 337                 .nx_huge_page_workaround_enabled =
+ * 338                         is_nx_huge_page_enabled(vcpu->kvm),
+ * 339
+ * 340                 .max_level = KVM_MAX_HUGEPAGE_LEVEL,
+ * 341                 .req_level = PG_LEVEL_4K,
+ * 342                 .goal_level = PG_LEVEL_4K,
+ * 343                 .is_private = err & PFERR_PRIVATE_ACCESS,
+ * 344
+ * 345                 .pfn = KVM_PFN_ERR_FAULT,
+ * 346         };
+ *
+ *
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|5374| <<kvm_tdp_mmu_page_fault>> r = kvm_tdp_mmu_map(vcpu, fault);
+ */
 /*
  * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing
  * page tables and SPTEs to translate the faulting guest physical address.
@@ -1074,8 +1239,20 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	struct kvm_mmu_page *sp;
 	int ret = RET_PF_RETRY;
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|3216| <<direct_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|711| <<FNAME(fetch)>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1101| <<kvm_tdp_mmu_map>> kvm_mmu_hugepage_adjust(vcpu, fault);
+	 */
 	kvm_mmu_hugepage_adjust(vcpu, fault);
 
+	/*
+	 * 在3个地方trace这个trace_kvm_mmu_spte_requested():
+	 *   - arch/x86/kvm/mmu/mmu.c|3664| <<direct_map>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/paging_tmpl.h|736| <<FNAME(fetch)>> trace_kvm_mmu_spte_requested(fault);
+	 *   - arch/x86/kvm/mmu/tdp_mmu.c|1127| <<kvm_tdp_mmu_map>> trace_kvm_mmu_spte_requested(fault);
+	 */
 	trace_kvm_mmu_spte_requested(fault);
 
 	rcu_read_lock();
@@ -1083,6 +1260,14 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 	tdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {
 		int r;
 
+		/*
+		 * 在以下使用kvm_page_fault->nx_huge_page_workaround_enabled:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 *   - arch/x86/kvm/mmu/mmu.c|3343| <<direct_map>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/mmu_internal.h|337| <<kvm_mmu_do_page_fault>> .nx_huge_page_workaround_enabled = is_nx_huge_page_enabled(vcpu->kvm),
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|743| <<FNAME(fetch)>> if (fault->nx_huge_page_workaround_enabled)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1116| <<kvm_tdp_mmu_map>> if (fault->nx_huge_page_workaround_enabled)
+		 */
 		if (fault->nx_huge_page_workaround_enabled)
 			disallowed_hugepage_adjust(fault, iter.old_spte, iter.level);
 
@@ -1108,6 +1293,36 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 		sp = tdp_mmu_alloc_sp(vcpu);
 		tdp_mmu_init_child_sp(sp, &iter);
 
+		/*
+		 * 在以下设置kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|815| <<account_nx_huge_page>> sp->nx_huge_page_disallowed = true;
+		 *   - arch/x86/kvm/mmu/mmu.c|848| <<unaccount_nx_huge_page>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|320| <<tdp_mmu_unlink_sp>> sp->nx_huge_page_disallowed = false;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1149| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 * 在以下使用kvm_mmu_page->nx_huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|2575| <<__kvm_mmu_prepare_zap_page>> if (sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3282| <<disallowed_hugepage_adjust>> if (...spte_to_child_sp(spte)->nx_huge_page_disallowed) {
+		 *   - arch/x86/kvm/mmu/mmu.c|7791| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(!sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/mmu.c|7829| <<kvm_recover_nx_huge_pages>> WARN_ON_ONCE(sp->nx_huge_page_disallowed);
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|316| <<tdp_mmu_unlink_sp>> if (!sp->nx_huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1168| <<kvm_tdp_mmu_map>> if (sp->nx_huge_page_disallowed)
+		 *
+		 * The shadow page can't be replaced by an equivalent huge page
+		 * because it is being used to map an executable page in the guest
+		 * and the NX huge page mitigation is enabled.
+		 *
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
 
 		if (is_shadow_present_pte(iter.old_spte))
@@ -1124,6 +1339,19 @@ int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 			goto retry;
 		}
 
+		/*
+		 * 在以下设置kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3226| <<kvm_mmu_hugepage_adjust>> fault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;
+		 * 在以下使用kvm_page_fault->huge_page_disallowed:
+		 *   - arch/x86/kvm/mmu/mmu.c|3244| <<kvm_mmu_hugepage_adjust>> if (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/mmu.c|3367| <<direct_map(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/paging_tmpl.h|770| <<FNAME(fetch)>> if (fault->huge_page_disallowed)
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1141| <<kvm_tdp_mmu_map>> sp->nx_huge_page_disallowed = fault->huge_page_disallowed;
+		 *   - arch/x86/kvm/mmu/tdp_mmu.c|1157| <<kvm_tdp_mmu_map>> if (fault->huge_page_disallowed && fault->req_level >= iter.level) {
+		 *
+		 * Whether a >4KB mapping can be created or is forbidden due to NX
+		 * hugepages.
+		 */
 		if (fault->huge_page_disallowed &&
 		    fault->req_level >= iter.level) {
 			spin_lock(&kvm->arch.tdp_mmu_pages_lock);
diff --git a/arch/x86/kvm/pmu.c b/arch/x86/kvm/pmu.c
index 47a46283c..abc249549 100644
--- a/arch/x86/kvm/pmu.c
+++ b/arch/x86/kvm/pmu.c
@@ -23,15 +23,80 @@
 #include "lapic.h"
 #include "pmu.h"
 
+/*
+ * 在以下使用KVM_PMU_EVENT_FILTER_MAX_EVENTS:
+ *   - tools/testing/selftests/kvm/include/x86_64/pmu.h|10| <<global>> #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|30| <<global>> __u64 events[KVM_PMU_EVENT_FILTER_MAX_EVENTS];
+ *   - arch/x86/kvm/pmu.c|1004| <<kvm_vm_ioctl_set_pmu_event_filter>> if (tmp.nevents > KVM_PMU_EVENT_FILTER_MAX_EVENTS)
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|22| <<PMU_EVENT_FILTER_INVALID_NEVENTS>> #define PMU_EVENT_FILTER_INVALID_NEVENTS
+ *               (KVM_PMU_EVENT_FILTER_MAX_EVENTS + 1)
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|649| <<test_masked_events>> int nevents = KVM_PMU_EVENT_FILTER_MAX_EVENTS - MAX_TEST_EVENTS;
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|650| <<test_masked_events>> uint64_t events[KVM_PMU_EVENT_FILTER_MAX_EVENTS];
+ *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|656| <<test_masked_events>> add_dummy_events(events, KVM_PMU_EVENT_FILTER_MAX_EVENTS);
+ */
 /* This is enough to filter the vast majority of currently defined events. */
 #define KVM_PMU_EVENT_FILTER_MAX_EVENTS 300
 
+/*
+ * 在以下使用kvm_pmu_cap:
+ *   - arch/x86/kvm/cpuid.c|1064| <<__do_cpuid_func>> eax.split.version_id = kvm_pmu_cap.version;
+ *   - arch/x86/kvm/cpuid.c|1065| <<__do_cpuid_func>> eax.split.num_counters = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/cpuid.c|1066| <<__do_cpuid_func>> eax.split.bit_width = kvm_pmu_cap.bit_width_gp;
+ *   - arch/x86/kvm/cpuid.c|1067| <<__do_cpuid_func>> eax.split.mask_length = kvm_pmu_cap.events_mask_len;
+ *   - arch/x86/kvm/cpuid.c|1068| <<__do_cpuid_func>> edx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;
+ *   - arch/x86/kvm/cpuid.c|1069| <<__do_cpuid_func>> edx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;
+ *   - arch/x86/kvm/cpuid.c|1071| <<__do_cpuid_func>> if (kvm_pmu_cap.version)
+ *   - arch/x86/kvm/cpuid.c|1077| <<__do_cpuid_func>> entry->ebx = kvm_pmu_cap.events_mask;
+ *   - arch/x86/kvm/cpuid.c|1403| <<__do_cpuid_func>> ebx.split.num_core_pmc = kvm_pmu_cap.num_counters_gp;
+ *   - arch/x86/kvm/pmu.h|198| <<kvm_init_pmu_capability>> perf_get_x86_pmu_capability(&kvm_pmu_cap);
+ *   - arch/x86/kvm/pmu.h|206| <<kvm_init_pmu_capability>> if (!kvm_pmu_cap.num_counters_gp || WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|207| <<kvm_init_pmu_capability>> if (!kvm_pmu_cap.num_counters_gp || WARN_ON_ONCE(kvm_pmu_cap.num_counters_gp < min_nr_gp_ctrs))
+ *   - arch/x86/kvm/pmu.h|209| <<kvm_init_pmu_capability>> else if (is_intel && !kvm_pmu_cap.version)
+ *   - arch/x86/kvm/pmu.h|214| <<kvm_init_pmu_capability>> memset(&kvm_pmu_cap, 0, sizeof(kvm_pmu_cap));
+ *   - arch/x86/kvm/pmu.h|218| <<kvm_init_pmu_capability>> kvm_pmu_cap.version = min(kvm_pmu_cap.version, 2);
+ *   - arch/x86/kvm/pmu.h|219| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_gp = min(kvm_pmu_cap.num_counters_gp, pmu_ops->MAX_NR_GP_COUNTERS);
+ *   - arch/x86/kvm/pmu.h|221| <<kvm_init_pmu_capability>> kvm_pmu_cap.num_counters_fixed = min(kvm_pmu_cap.num_counters_fixed, KVM_MAX_NR_FIXED_COUNTERS);
+ *   - arch/x86/kvm/svm/pmu.c|199| <<amd_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(unsigned int, pmu->nr_arch_gp_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5274| <<svm_set_cpu_caps>> if (kvm_pmu_cap.num_counters_gp < AMD64_NUM_COUNTERS_CORE)
+ *   - arch/x86/kvm/svm/svm.c|5275| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5276| <<svm_set_cpu_caps>> kvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/svm/svm.c|5280| <<svm_set_cpu_caps>> if (kvm_pmu_cap.version != 2 || !kvm_cpu_cap_has(X86_FEATURE_PERFCTR_CORE))
+ *   - arch/x86/kvm/vmx/capabilities.h|393| <<vmx_pebs_supported>> return boot_cpu_has(X86_FEATURE_PEBS) && kvm_pmu_cap.pebs_ept;
+ *   - arch/x86/kvm/vmx/pmu_intel.c|447| <<intel_get_fixed_pmc_eventsel>> WARN_ON_ONCE(!eventsel && index < kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|491| <<intel_pmu_refresh>> pmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters, kvm_pmu_cap.num_counters_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|493| <<intel_pmu_refresh>> eax.split.bit_width = min_t(int, eax.split.bit_width, kvm_pmu_cap.bit_width_gp);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|496| <<intel_pmu_refresh>> eax.split.mask_length = min_t(int, eax.split.mask_length, kvm_pmu_cap.events_mask_len);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|504| <<intel_pmu_refresh>> pmu->nr_arch_fixed_counters = min_t(int, edx.split.num_counters_fixed, kvm_pmu_cap.num_counters_fixed);
+ *   - arch/x86/kvm/vmx/pmu_intel.c|506| <<intel_pmu_refresh>> edx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed, kvm_pmu_cap.bit_width_fixed);
+ *   - arch/x86/kvm/x86.c|7437| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_PERFCTR0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7443| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_EVENTSEL0 >= kvm_pmu_cap.num_counters_gp)
+ *   - arch/x86/kvm/x86.c|7449| <<kvm_probe_msr_to_save>> if (msr_index - MSR_ARCH_PERFMON_FIXED_CTR0 >= kvm_pmu_cap.num_counters_fixed)
+ */
 struct x86_pmu_capability __read_mostly kvm_pmu_cap;
 EXPORT_SYMBOL_GPL(kvm_pmu_cap);
 
+/*
+ * 在以下使用kvm_pmu_eventsel:
+ *   - arch/x86/kvm/pmu.c|78| <<global>> struct kvm_pmu_emulated_event_selectors __read_mostly kvm_pmu_eventsel;
+ *   - arch/x86/kvm/pmu.h|233| <<kvm_init_pmu_capability>> kvm_pmu_eventsel.INSTRUCTIONS_RETIRED = perf_get_hw_event_config(PERF_COUNT_HW_INSTRUCTIONS);
+ *   - arch/x86/kvm/pmu.h|235| <<kvm_init_pmu_capability>> kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED = perf_get_hw_event_config(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);
+ *   - arch/x86/kvm/vmx/nested.c|3687| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|8945| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9276| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9278| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *
+ * struct kvm_pmu_emulated_event_selectors {
+ *     u64 INSTRUCTIONS_RETIRED;
+ *     u64 BRANCH_INSTRUCTIONS_RETIRED;
+ * };
+ */
 struct kvm_pmu_emulated_event_selectors __read_mostly kvm_pmu_eventsel;
 EXPORT_SYMBOL_GPL(kvm_pmu_eventsel);
 
+/*
+ * 在以下使用vmx_pebs_pdir_cpu[]:
+ *   - arch/x86/kvm/pmu.c|216| <<pmc_get_pebs_precise_level>> (pmc->idx == 32 && x86_match_cpu(vmx_pebs_pdir_cpu)))
+ */
 /* Precise Distribution of Instructions Retired (PDIR) */
 static const struct x86_cpu_id vmx_pebs_pdir_cpu[] = {
 	X86_MATCH_VFM(INTEL_ICELAKE_D, NULL),
@@ -77,6 +142,20 @@ static const struct x86_cpu_id vmx_pebs_pdist_cpu[] = {
 
 static struct kvm_pmu_ops kvm_pmu_ops __read_mostly;
 
+/*
+ * 在以下调用KVM_X86_PMU_OP():
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|2| <<global>> #if !defined(KVM_X86_PMU_OP) || !defined(KVM_X86_PMU_OP_OPTIONAL)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|14| <<BUILD_BUG_ON>> KVM_X86_PMU_OP(rdpmc_ecx_to_pmc)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|15| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(msr_idx_to_pmc)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|17| <<KVM_X86_PMU_OP_OPTIONAL>> KVM_X86_PMU_OP(is_valid_msr)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|18| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(get_msr)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|19| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(set_msr)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|20| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(refresh)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|21| <<KVM_X86_PMU_OP>> KVM_X86_PMU_OP(init)
+ *   - arch/x86/include/asm/kvm-x86-pmu-ops.h|26| <<KVM_X86_PMU_OP_OPTIONAL>> #undef KVM_X86_PMU_OP
+ *   - arch/x86/kvm/pmu.c|144| <<KVM_X86_PMU_OP_OPTIONAL>> #define KVM_X86_PMU_OP_OPTIONAL KVM_X86_PMU_OP
+ *   - arch/x86/kvm/pmu.c|153| <<kvm_pmu_ops_update>> #define KVM_X86_PMU_OP(func) \
+ */
 #define KVM_X86_PMU_OP(func)					     \
 	DEFINE_STATIC_CALL_NULL(kvm_x86_pmu_##func,			     \
 				*(((struct kvm_pmu_ops *)0)->func));
@@ -96,6 +175,11 @@ void kvm_pmu_ops_update(const struct kvm_pmu_ops *pmu_ops)
 #undef __KVM_X86_PMU_OP
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|201| <<kvm_perf_overflow>> __kvm_perf_overflow(pmc, true);
+ *   - arch/x86/kvm/pmu.c|549| <<reprogram_counter>> __kvm_perf_overflow(pmc, false);
+ */
 static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 {
 	struct kvm_pmu *pmu = pmc_to_pmu(pmc);
@@ -119,10 +203,23 @@ static inline void __kvm_perf_overflow(struct kvm_pmc *pmc, bool in_pmi)
 		__set_bit(pmc->idx, (unsigned long *)&pmu->global_status);
 	}
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	if (pmc->intr && !skip_pmi)
 		kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
 }
 
+/*
+ * 在以下使用kvm_perf_overflow():
+ *   -  arch/x86/kvm/pmu.c|277| <<pmc_reprogram_counter>> event = perf_event_create_kernel_counter(&attr, -1, current, kvm_perf_overflow, pmc);
+ */
 static void kvm_perf_overflow(struct perf_event *perf_event,
 			      struct perf_sample_data *data,
 			      struct pt_regs *regs)
@@ -244,6 +341,18 @@ static bool pmc_pause_counter(struct kvm_pmc *pmc)
 	 */
 	prev_counter = counter & pmc_bitmask(pmc);
 
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	counter += pmc->emulated_counter;
 	pmc->counter = counter & pmc_bitmask(pmc);
 
@@ -313,6 +422,18 @@ void pmc_write_counter(struct kvm_pmc *pmc, u64 val)
 	 * reset it to '0'.  Note, this very sneakily offsets the accumulated
 	 * emulated count too, by using pmc_read_counter()!
 	 */
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	pmc->emulated_counter = 0;
 	pmc->counter += val - pmc_read_counter(pmc);
 	pmc->counter &= pmc_bitmask(pmc);
@@ -492,6 +613,21 @@ static int reprogram_counter(struct kvm_pmc *pmc)
 				     eventsel & ARCH_PERFMON_EVENTSEL_INT);
 }
 
+/*
+ * 在以下使用KVM_REQ_PMU:
+ *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+ *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+ *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+ *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+ *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+ *
+ * 处理的函数: kvm_pmu_handle_event()
+ *
+ *
+ * 在以下调用kvm_pmu_handle_event():
+ *   - arch/x86/kvm/x86.c|11024| <<vcpu_enter_guest(KVM_REQ_PMU)>> kvm_pmu_handle_event(vcpu);
+ */
 void kvm_pmu_handle_event(struct kvm_vcpu *vcpu)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
@@ -604,6 +740,18 @@ int kvm_pmu_rdpmc(struct kvm_vcpu *vcpu, unsigned idx, u64 *data)
 	return 0;
 }
 
+/*
+ * 在以下使用KVM_REQ_PMI:
+ *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+ *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+ *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+ *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+ *
+ * 处理的函数: kvm_pmu_deliver_pmi()
+ *
+ * 在以下调用kvm_pmu_deliver_pmi():
+ *   - arch/x86/kvm/x86.c|11026| <<vcpu_enter_guest(KVM_REQ_PMI)>> kvm_pmu_deliver_pmi(vcpu);
+ */
 void kvm_pmu_deliver_pmi(struct kvm_vcpu *vcpu)
 {
 	if (lapic_in_kernel(vcpu)) {
@@ -732,6 +880,18 @@ static void kvm_pmu_reset(struct kvm_vcpu *vcpu)
 	kvm_for_each_pmc(pmu, pmc, i, pmu->all_valid_pmc_idx) {
 		pmc_stop_counter(pmc);
 		pmc->counter = 0;
+		/*
+		 * 在以下使用kvm_pmc->emulated_counter:
+		 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+		 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+		 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+		 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+		 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+		 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+		 *
+		 * PMC events triggered by KVM emulation that haven't been fully
+		 * processed, i.e. haven't undergone overflow detection.
+		 */
 		pmc->emulated_counter = 0;
 
 		if (pmc_is_gp(pmc))
@@ -828,8 +988,24 @@ void kvm_pmu_destroy(struct kvm_vcpu *vcpu)
 	kvm_pmu_reset(vcpu);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/pmu.c|944| <<kvm_pmu_trigger_event>> kvm_pmu_incr_counter(pmc);
+ */
 static void kvm_pmu_incr_counter(struct kvm_pmc *pmc)
 {
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	pmc->emulated_counter++;
 	kvm_pmu_request_counter_reprogram(pmc);
 }
@@ -861,6 +1037,13 @@ static inline bool cpl_is_matched(struct kvm_pmc *pmc)
 							 select_user;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3687| <<nested_vmx_run>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|8945| <<kvm_skip_emulated_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9276| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.INSTRUCTIONS_RETIRED);
+ *   - arch/x86/kvm/x86.c|9278| <<x86_emulate_instruction>> kvm_pmu_trigger_event(vcpu, kvm_pmu_eventsel.BRANCH_INSTRUCTIONS_RETIRED);
+ */
 void kvm_pmu_trigger_event(struct kvm_vcpu *vcpu, u64 eventsel)
 {
 	DECLARE_BITMAP(bitmap, X86_PMC_IDX_MAX);
diff --git a/arch/x86/kvm/pmu.h b/arch/x86/kvm/pmu.h
index ad89d0bd6..6cd907ef3 100644
--- a/arch/x86/kvm/pmu.h
+++ b/arch/x86/kvm/pmu.h
@@ -106,6 +106,18 @@ static inline u64 pmc_read_counter(struct kvm_pmc *pmc)
 {
 	u64 counter, enabled, running;
 
+	/*
+	 * 在以下使用kvm_pmc->emulated_counter:
+	 *   - arch/x86/kvm/pmu.c|293| <<pmc_pause_counter>> counter += pmc->emulated_counter;
+	 *   - arch/x86/kvm/pmu.c|296| <<pmc_pause_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|362| <<pmc_write_counter>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|781| <<kvm_pmu_reset>> pmc->emulated_counter = 0;
+	 *   - arch/x86/kvm/pmu.c|879| <<kvm_pmu_incr_counter>> pmc->emulated_counter++;
+	 *   - arch/x86/kvm/pmu.h|109| <<pmc_read_counter>> counter = pmc->counter + pmc->emulated_counter;
+	 *
+	 * PMC events triggered by KVM emulation that haven't been fully
+	 * processed, i.e. haven't undergone overflow detection.
+	 */
 	counter = pmc->counter + pmc->emulated_counter;
 
 	if (pmc->perf_event && !pmc->is_paused)
@@ -180,6 +192,15 @@ static inline bool pmc_speculative_in_use(struct kvm_pmc *pmc)
 extern struct x86_pmu_capability kvm_pmu_cap;
 extern struct kvm_pmu_emulated_event_selectors kvm_pmu_eventsel;
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|9774| <<kvm_x86_vendor_init>> kvm_init_pmu_capability(ops->pmu_ops);
+ *
+ * vmx_init() or svm_init()
+ * -> kvm_x86_vendor_init()
+ *    -> kvm_init_pmu_capability()
+ *       -> perf_get_x86_pmu_capability()
+ */
 static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 {
 	bool is_intel = boot_cpu_data.x86_vendor == X86_VENDOR_INTEL;
@@ -230,6 +251,17 @@ static inline void kvm_init_pmu_capability(const struct kvm_pmu_ops *pmu_ops)
 static inline void kvm_pmu_request_counter_reprogram(struct kvm_pmc *pmc)
 {
 	set_bit(pmc->idx, pmc_to_pmu(pmc)->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
 }
 
@@ -242,6 +274,17 @@ static inline void reprogram_counters(struct kvm_pmu *pmu, u64 diff)
 
 	for_each_set_bit(bit, (unsigned long *)&diff, X86_PMC_IDX_MAX)
 		set_bit(bit, pmu->reprogram_pmi);
+	/*
+	 * 在以下使用KVM_REQ_PMU:
+	 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+	 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+	 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+	 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+	 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_handle_event()
+	 */
 	kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
 }
 
diff --git a/arch/x86/kvm/svm/pmu.c b/arch/x86/kvm/svm/pmu.c
index 22d5a65b4..dd9655a35 100644
--- a/arch/x86/kvm/svm/pmu.c
+++ b/arch/x86/kvm/svm/pmu.c
@@ -173,6 +173,21 @@ static int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	return 1;
 }
 
+/*
+ * 230 struct kvm_pmu_ops amd_pmu_ops __initdata = {
+ * 231         .rdpmc_ecx_to_pmc = amd_rdpmc_ecx_to_pmc,
+ * 232         .msr_idx_to_pmc = amd_msr_idx_to_pmc,
+ * 233         .check_rdpmc_early = amd_check_rdpmc_early,
+ * 234         .is_valid_msr = amd_is_valid_msr,
+ * 235         .get_msr = amd_pmu_get_msr,
+ * 236         .set_msr = amd_pmu_set_msr,
+ * 237         .refresh = amd_pmu_refresh,
+ * 238         .init = amd_pmu_init,
+ * 239         .EVENTSEL_EVENT = AMD64_EVENTSEL_EVENT,
+ * 240         .MAX_NR_GP_COUNTERS = KVM_MAX_NR_AMD_GP_COUNTERS,
+ * 241         .MIN_NR_GP_COUNTERS = AMD64_NUM_COUNTERS,
+ * 242 };
+ */
 static void amd_pmu_refresh(struct kvm_vcpu *vcpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
diff --git a/arch/x86/kvm/svm/svm.c b/arch/x86/kvm/svm/svm.c
index 21dacd312..e3486a481 100644
--- a/arch/x86/kvm/svm/svm.c
+++ b/arch/x86/kvm/svm/svm.c
@@ -2070,10 +2070,34 @@ static int npf_interception(struct kvm_vcpu *vcpu)
 	if (WARN_ON_ONCE(error_code & PFERR_SYNTHETIC_MASK))
 		error_code &= ~PFERR_SYNTHETIC_MASK;
 
+	/*
+	 * 在以下使用PFERR_PRIVATE_ACCESS:
+	 *   - arch/x86/include/asm/kvm_host.h|284| <<PFERR_SYNTHETIC_MASK>> #define PFERR_SYNTHETIC_MASK (PFERR_IMPLICIT_ACCESS | PFERR_PRIVATE_ACCESS)
+	 *   - arch/x86/kvm/mmu/mmu.c|4244| <<kvm_arch_async_page_ready>> if (WARN_ON_ONCE(work->arch.error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu.c|4756| <<kvm_arch_vcpu_pre_fault_memory>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6086| <<kvm_mmu_page_fault>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *   - arch/x86/kvm/mmu/mmu.c|6090| <<kvm_mmu_page_fault>> if (WARN_ON_ONCE(error_code & PFERR_PRIVATE_ACCESS))
+	 *   - arch/x86/kvm/mmu/mmu_internal.h|319| <<kvm_mmu_do_page_fault>> .is_private = err & PFERR_PRIVATE_ACCESS,
+	 *   - arch/x86/kvm/svm/svm.c|2074| <<npf_interception>> error_code |= PFERR_PRIVATE_ACCESS;
+	 *
+	 * PRIVATE_ACCESS is a KVM-defined flag us to indicate that a fault occurred
+	 * when the guest was accessing private memory.
+	 */
 	if (sev_snp_guest(vcpu->kvm) && (error_code & PFERR_GUEST_ENC_MASK))
 		error_code |= PFERR_PRIVATE_ACCESS;
 
 	trace_kvm_page_fault(vcpu, fault_address, error_code);
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *                      insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                      svm->vmcb->control.insn_bytes : NULL,
+	 *                      svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
 				static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
 				svm->vmcb->control.insn_bytes : NULL,
@@ -3856,6 +3880,21 @@ static int svm_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 	return 1;
 }
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ */
 static void svm_enable_irq_window(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
@@ -5172,6 +5211,10 @@ static __init void svm_adjust_mmio_mask(void)
 	kvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);
 }
 
+/*
+ * caled by:
+ *   - arch/x86/kvm/svm/svm.c|5441| <<svm_hardware_setup>> svm_set_cpu_caps();
+ */
 static __init void svm_set_cpu_caps(void)
 {
 	kvm_set_cpu_caps();
@@ -5250,6 +5293,14 @@ static __init void svm_set_cpu_caps(void)
 	kvm_cpu_cap_clear(X86_FEATURE_BUS_LOCK_DETECT);
 }
 
+/*
+ * 5470 static struct kvm_x86_init_ops svm_init_ops __initdata = {
+ * 5471         .hardware_setup = svm_hardware_setup,
+ * 5472
+ * 5473         .runtime_ops = &svm_x86_ops,
+ * 5474         .pmu_ops = &amd_pmu_ops,
+ * 5475 };
+ */
 static __init int svm_hardware_setup(void)
 {
 	int cpu;
diff --git a/arch/x86/kvm/vmx/nested.c b/arch/x86/kvm/vmx/nested.c
index aa78b6f38..dd4434599 100644
--- a/arch/x86/kvm/vmx/nested.c
+++ b/arch/x86/kvm/vmx/nested.c
@@ -2634,6 +2634,10 @@ static void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)
  * Returns 0 on success, 1 on failure. Invalid state exit qualification code
  * is assigned to entry_failure_code on failure.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/nested.c|3589| <<nested_vmx_enter_non_root_mode>> if (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {
+ */
 static int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,
 			  bool from_vmentry,
 			  enum vm_entry_failure_code *entry_failure_code)
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 893366e53..79e4857f8 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -1523,6 +1523,19 @@ void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 
 	vmx_vcpu_pi_load(vcpu, cpu);
 
+	/*
+	 * 在以下使用vcpu_vmx->host_debugctlmsr:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_run>> if (vmx->host_debugctlmsr)
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 *
+	 * 在以下调用get_debugctlmsr():
+	 *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+	 *   - arch/x86/events/intel/ds.c|832| <<intel_pmu_enable_bts>> debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/events/intel/ds.c|856| <<intel_pmu_disable_bts>> debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kernel/step.c|188| <<set_task_blockstep>> debugctl = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 */
 	vmx->host_debugctlmsr = get_debugctlmsr();
 }
 
@@ -5818,6 +5831,16 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 		error_code |= (exit_qualification & EPT_VIOLATION_GVA_TRANSLATED) ?
 			      PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;
 
+	/*
+	 * 在以下使用kvm_vcpu_is_legal_gpa():
+	 *   - arch/x86/kvm/cpuid.h|53| <<kvm_vcpu_is_legal_aligned_gpa>> return IS_ALIGNED(gpa, alignment) && kvm_vcpu_is_legal_gpa(vcpu, gpa);
+	 *   - arch/x86/kvm/cpuid.h|274| <<kvm_vcpu_is_legal_cr3>> return kvm_vcpu_is_legal_gpa(vcpu, cr3);
+	 *   - arch/x86/kvm/svm/nested.c|256| <<nested_svm_check_bitmap_pa>> return kvm_vcpu_is_legal_gpa(vcpu, addr) &&
+	 *   - arch/x86/kvm/svm/nested.c|257| <<nested_svm_check_bitmap_pa>> kvm_vcpu_is_legal_gpa(vcpu, addr + size - 1);
+	 *   - arch/x86/kvm/vmx/nested.c|834| <<nested_vmx_check_msr_switch>> !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))
+	 *   - arch/x86/kvm/vmx/nested.c|2823| <<nested_vmx_check_eptp>> if (CC(!kvm_vcpu_is_legal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))
+	 *   - arch/x86/kvm/vmx/vmx.c|5829| <<handle_ept_violation>> if (unlikely(allow_smaller_maxphyaddr && !kvm_vcpu_is_legal_gpa(vcpu, gpa)))
+	 */
 	/*
 	 * Check that the GPA doesn't exceed physical memory limits, as that is
 	 * a guest page fault.  We have to emulate the instruction here, because
@@ -5829,6 +5852,17 @@ static int handle_ept_violation(struct kvm_vcpu *vcpu)
 	if (unlikely(allow_smaller_maxphyaddr && !kvm_vcpu_is_legal_gpa(vcpu, gpa)))
 		return kvm_emulate_instruction(vcpu, 0);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *                      insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                      svm->vmcb->control.insn_bytes : NULL,
+	 *                      svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
 }
 
@@ -5850,6 +5884,17 @@ static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 		return kvm_skip_emulated_instruction(vcpu);
 	}
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|4614| <<kvm_handle_page_fault>> r = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,
+	 *                      insn_len);
+	 *   - arch/x86/kvm/svm/svm.c|2077| <<npf_interception>> rc = kvm_mmu_page_fault(vcpu, fault_address, error_code,
+	 *                      static_cpu_has(X86_FEATURE_DECODEASSISTS) ?
+	 *                      svm->vmcb->control.insn_bytes : NULL,
+	 *                      svm->vmcb->control.insn_len);
+	 *   - arch/x86/kvm/vmx/vmx.c|5832| <<handle_ept_violation>> return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
+	 *   - arch/x86/kvm/vmx/vmx.c|5853| <<handle_ept_misconfig>> return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
+	 */
 	return kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);
 }
 
@@ -5865,6 +5910,11 @@ static int handle_nmi_window(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/vmx/vmx.c|5927| <<handle_invalid_guest_state>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ *   - arch/x86/kvm/vmx/vmx.c|5951| <<vmx_vcpu_pre_run>> if (vmx_emulation_required_with_pending_exception(vcpu)) {
+ */
 static bool vmx_emulation_required_with_pending_exception(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
@@ -7445,6 +7495,19 @@ fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu, bool force_immediate_exit)
 		current_evmcs->hv_vp_id = kvm_hv_get_vpindex(vcpu);
 	}
 
+	/*
+	 * 在以下使用vcpu_vmx->host_debugctlmsr:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_run>> if (vmx->host_debugctlmsr)
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 *
+	 * 在以下调用update_debugctlmsr():
+	 *   - arch/x86/events/intel/core.c|2980| <<intel_pmu_reset>> update_debugctlmsr(get_debugctlmsr() &
+	 *   - arch/x86/events/intel/ds.c|845| <<intel_pmu_enable_bts>> update_debugctlmsr(debugctlmsr);
+	 *   - arch/x86/events/intel/ds.c|862| <<intel_pmu_disable_bts>> update_debugctlmsr(debugctlmsr);
+	 *   - arch/x86/kernel/step.c|197| <<set_task_blockstep>> update_debugctlmsr(debugctl);
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 */
 	/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */
 	if (vmx->host_debugctlmsr)
 		update_debugctlmsr(vmx->host_debugctlmsr);
@@ -8338,6 +8401,15 @@ static unsigned int vmx_handle_intel_pt_intr(void)
 	if (!vcpu || !kvm_handling_nmi_from_guest(vcpu))
 		return 0;
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	kvm_make_request(KVM_REQ_PMI, vcpu);
 	__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,
 		  (unsigned long *)&vcpu->arch.pmu.global_status);
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index 43f573f6c..7b892e994 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -184,6 +184,18 @@ struct nested_vmx {
 	 */
 	bool enlightened_vmcs_enabled;
 
+	/*
+	 * 在以下设置nested_vmx->nested_run_pending:
+	 *   - arch/x86/kvm/vmx/nested.c|3748| <<nested_vmx_run>> vmx->nested.nested_run_pending = 1;
+	 *   - arch/x86/kvm/vmx/nested.c|3788| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|3793| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|3803| <<nested_vmx_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|6722| <<vmx_leave_nested>> to_vmx(vcpu)->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/nested.c|6859| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = !!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);
+	 *   - arch/x86/kvm/vmx/nested.c|6911| <<vmx_set_nested_state>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|7510| <<vmx_vcpu_run>> vmx->nested.nested_run_pending = 0;
+	 *   - arch/x86/kvm/vmx/vmx.c|8277| <<vmx_leave_smm>> vmx->nested.nested_run_pending = 1;
+	 */
 	/* L2 must run next, and mustn't decide to exit to L1. */
 	bool nested_run_pending;
 
@@ -336,6 +348,12 @@ struct vcpu_vmx {
 	/* apic deadline value in host tsc */
 	u64 hv_deadline_tsc;
 
+	/*
+	 * 在以下使用vcpu_vmx->host_debugctlmsr:
+	 *   - arch/x86/kvm/vmx/vmx.c|1526| <<vmx_vcpu_load>> vmx->host_debugctlmsr = get_debugctlmsr();
+	 *   - arch/x86/kvm/vmx/vmx.c|7486| <<vmx_vcpu_run>> if (vmx->host_debugctlmsr)
+	 *   - arch/x86/kvm/vmx/vmx.c|7487| <<vmx_vcpu_run>> update_debugctlmsr(vmx->host_debugctlmsr);
+	 */
 	unsigned long host_debugctlmsr;
 
 	/*
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index c79a8cc57..c41bd6e08 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -185,6 +185,33 @@ module_param(force_emulation_prefix, int, 0644);
 int __read_mostly pi_inject_timer = -1;
 module_param(pi_inject_timer, bint, 0644);
 
+/*
+ * 在以下使用global的enable_pmu:
+ *   - arch/x86/kvm/cpuid.c|1063| <<__do_cpuid_func>> if (!enable_pmu || !static_cpu_has(X86_FEATURE_ARCH_PERFMON)) {
+ *   - arch/x86/kvm/cpuid.c|1399| <<__do_cpuid_func>> if (!enable_pmu || !kvm_cpu_cap_has(X86_FEATURE_PERFMON_V2)) {
+ *   - arch/x86/kvm/pmu.h|216| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|218| <<kvm_init_pmu_capability>> if (enable_pmu) {
+ *   - arch/x86/kvm/pmu.h|229| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|231| <<kvm_init_pmu_capability>> enable_pmu = false;
+ *   - arch/x86/kvm/pmu.h|234| <<kvm_init_pmu_capability>> if (!enable_pmu) {
+ *   - arch/x86/kvm/svm/svm.c|5272| <<svm_set_cpu_caps>> if (enable_pmu) {
+ *   - arch/x86/kvm/svm/svm.c|5450| <<svm_hardware_setup>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|7968| <<vmx_get_perf_capabilities>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|8037| <<vmx_set_cpu_caps>> if (!enable_pmu)
+ *   - arch/x86/kvm/vmx/vmx.c|8627| <<vmx_hardware_setup>> if (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())
+ *   - arch/x86/kvm/x86.c|4803| <<kvm_vm_ioctl_check_extension>> r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
+ *   - arch/x86/kvm/x86.c|6726| <<kvm_vm_ioctl_enable_cap>> if (!enable_pmu || (cap->args[0] & ~KVM_CAP_PMU_VALID_MASK))
+ *   - arch/x86/kvm/x86.c|7542| <<kvm_init_msr_lists>> if (enable_pmu) {
+ *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+ *
+ * 在以下使用kvm_arch->enable_pmu:
+ *   - arch/x86/kvm/pmu.c|938| <<kvm_pmu_refresh>> if (!vcpu->kvm->arch.enable_pmu)
+ *   - arch/x86/kvm/svm/pmu.c|44| <<get_gp_pmc_amd>> if (!vcpu->kvm->arch.enable_pmu)
+ *   - arch/x86/kvm/x86.c|13109| <<kvm_arch_init_vm>> kvm->arch.enable_pmu = enable_pmu;
+ *   - arch/x86/kvm/x86.c|6731| <<kvm_vm_ioctl_enable_cap>> kvm->arch.enable_pmu = !(cap->args[0] & KVM_PMU_CAP_DISABLE);
+ *
+ * enable_pmu不可以被动态修改
+ */
 /* Enable/disable PMU virtualization */
 bool __read_mostly enable_pmu = true;
 EXPORT_SYMBOL_GPL(enable_pmu);
@@ -974,9 +1001,24 @@ void kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvm_inject_emulated_page_fault);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/lapic.c|1370| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+ *   - arch/x86/kvm/x86.c|5207| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+ */
 void kvm_inject_nmi(struct kvm_vcpu *vcpu)
 {
 	atomic_inc(&vcpu->arch.nmi_queued);
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * 处理的函数: process_nmi()
+	 */
 	kvm_make_request(KVM_REQ_NMI, vcpu);
 }
 
@@ -2282,6 +2324,11 @@ static s64 get_kvmclock_base_ns(void)
 }
 #endif
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|3953| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK_NEW)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ *   - arch/x86/kvm/x86.c|3960| <<kvm_set_msr_common(MSR_KVM_WALL_CLOCK)>> kvm_write_wall_clock(vcpu->kvm, data, 0);
+ */
 static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_ofs)
 {
 	int version;
@@ -2307,6 +2354,13 @@ static void kvm_write_wall_clock(struct kvm *kvm, gpa_t wall_clock, int sec_hi_o
 
 	wall_nsec = kvm_get_wall_clock_epoch(kvm);
 
+	/*
+	 * struct pvclock_wall_clock {
+	 *     u32   version;
+	 *     u32   sec;
+	 *     u32   nsec;
+	 * } __attribute__((__packed__));
+	 */
 	wc.nsec = do_div(wall_nsec, NSEC_PER_SEC);
 	wc.sec = (u32)wall_nsec; /* overflow in 2106 guest time */
 	wc.version = version;
@@ -3315,6 +3369,11 @@ static int kvm_guest_time_update(struct kvm_vcpu *v)
  * Fall back to using their values at slightly different moments by
  * calling ktime_get_real_ns() and get_kvmclock_ns() separately.
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|2308| <<kvm_write_wall_clock>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ *   - arch/x86/kvm/xen.c|63| <<kvm_xen_shared_info_init>> wall_nsec = kvm_get_wall_clock_epoch(kvm);
+ */
 uint64_t kvm_get_wall_clock_epoch(struct kvm *kvm)
 {
 #ifdef CONFIG_X86_64
@@ -4573,6 +4632,10 @@ static bool kvm_is_vm_type_supported(unsigned long type)
 	return type < 32 && (kvm_caps.supported_vm_types & BIT(type));
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4888| <<kvm_vm_ioctl_check_extension_generic>> return kvm_vm_ioctl_check_extension(kvm, arg);
+ */
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r = 0;
@@ -4768,6 +4831,15 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	}
 	case KVM_CAP_PMU_CAPABILITY:
+		/*
+		 * 在以下使用KVM_CAP_PMU_CAPABILITY:
+		 *   - include/uapi/linux/kvm.h|909| <<global>> #define KVM_CAP_PMU_CAPABILITY 212
+		 *   - tools/include/uapi/linux/kvm.h|909| <<global>> #define KVM_CAP_PMU_CAPABILITY 212
+		 *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|328| <<global>> r = kvm_check_cap(KVM_CAP_PMU_CAPABILITY);
+		 *   - tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c|334| <<global>> vm_enable_cap(vm, KVM_CAP_PMU_CAPABILITY, KVM_PMU_CAP_DISABLE);
+		 *   - arch/x86/kvm/x86.c|4802| <<kvm_vm_ioctl_check_extension>> case KVM_CAP_PMU_CAPABILITY:
+		 *   - arch/x86/kvm/x86.c|6724| <<kvm_vm_ioctl_enable_cap>> case KVM_CAP_PMU_CAPABILITY:
+		 */
 		r = enable_pmu ? KVM_CAP_PMU_VALID_MASK : 0;
 		break;
 	case KVM_CAP_DISABLE_QUIRKS2:
@@ -4870,6 +4942,14 @@ long kvm_arch_dev_ioctl(struct file *filp,
 		if (copy_from_user(&cpuid, cpuid_arg, sizeof(cpuid)))
 			goto out;
 
+		/*
+		 * // for KVM_SET_CPUID2
+		 * struct kvm_cpuid2 {
+		 *     __u32 nent;
+		 *     __u32 padding;
+		 *     struct kvm_cpuid_entry2 entries[];
+		 * };
+		 */
 		r = kvm_dev_ioctl_get_cpuid(&cpuid, cpuid_arg->entries,
 					    ioctl);
 		if (r)
@@ -4952,6 +5032,11 @@ static bool need_emulate_wbinvd(struct kvm_vcpu *vcpu)
 	return kvm_arch_has_noncoherent_dma(vcpu->kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|172| <<vcpu_load>> kvm_arch_vcpu_load(vcpu, cpu);
+ *   - virt/kvm/kvm_main.c|6320| <<kvm_sched_in>> kvm_arch_vcpu_load(vcpu, cpu);
+ */
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct kvm_pmu *pmu = vcpu_to_pmu(vcpu);
@@ -4960,6 +5045,17 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 
 	if (vcpu->scheduled_out && pmu->version && pmu->event_count) {
 		pmu->need_cleanup = true;
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *
+		 * 处理的函数: kvm_pmu_handle_event()
+		 */
 		kvm_make_request(KVM_REQ_PMU, vcpu);
 	}
 
@@ -5171,6 +5267,11 @@ static int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu,
 
 static int kvm_vcpu_ioctl_nmi(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/lapic.c|1370| <<__apic_accept_irq>> kvm_inject_nmi(vcpu);
+	 *   - arch/x86/kvm/x86.c|5207| <<kvm_vcpu_ioctl_nmi>> kvm_inject_nmi(vcpu);
+	 */
 	kvm_inject_nmi(vcpu);
 
 	return 0;
@@ -5463,6 +5564,16 @@ static int kvm_vcpu_ioctl_x86_set_vcpu_events(struct kvm_vcpu *vcpu,
 	if (events->flags & KVM_VCPUEVENT_VALID_NMI_PENDING) {
 		vcpu->arch.nmi_pending = 0;
 		atomic_set(&vcpu->arch.nmi_queued, events->nmi.pending);
+		/*
+		 * 在以下使用KVM_REQ_NMI:
+		 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *
+		 * 处理的函数: process_nmi()
+		 */
 		if (events->nmi.pending)
 			kvm_make_request(KVM_REQ_NMI, vcpu);
 	}
@@ -6443,6 +6554,10 @@ static int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)
 	return 0;
 }
 
+/*
+ * 处理KVM_REINJECT_CONTROL:
+ *   - arch/x86/kvm/x86.c|7223| <<kvm_arch_vm_ioctl(KVM_REINJECT_CONTROL)>> r = kvm_vm_ioctl_reinject(kvm, &control);
+ */
 static int kvm_vm_ioctl_reinject(struct kvm *kvm,
 				 struct kvm_reinject_control *control)
 {
@@ -9068,6 +9183,12 @@ int x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,
 }
 EXPORT_SYMBOL_GPL(x86_decode_emulated_instruction);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+ *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+ *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+ */
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len)
 {
@@ -9272,6 +9393,12 @@ int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 
 int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
@@ -9279,6 +9406,12 @@ EXPORT_SYMBOL_GPL(kvm_emulate_instruction);
 int kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,
 					void *insn, int insn_len)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|6193| <<kvm_mmu_page_fault>> return x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,
+	 *   - arch/x86/kvm/x86.c|9296| <<kvm_emulate_instruction>> return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
+	 *   - arch/x86/kvm/x86.c|9303| <<kvm_emulate_instruction_from_buffer>> return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
+	 */
 	return x86_emulate_instruction(vcpu, 0, 0, insn, insn_len);
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_instruction_from_buffer);
@@ -9659,6 +9792,11 @@ static void kvm_x86_check_cpu_compat(void *ret)
 	*(int *)ret = kvm_x86_check_processor_compatibility();
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|5487| <<svm_init>> r = kvm_x86_vendor_init(&svm_init_ops);
+ *   - arch/x86/kvm/vmx/vmx.c|8654| <<vmx_init>> r = kvm_x86_vendor_init(&vt_init_ops);
+ */
 int kvm_x86_vendor_init(struct kvm_x86_init_ops *ops)
 {
 	u64 host_pat;
@@ -9897,15 +10035,58 @@ static void kvm_pv_kick_cpu_op(struct kvm *kvm, int apicid)
 	kvm_irq_delivery_to_apic(kvm, NULL, &lapic_irq, NULL);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/ioapic.c|229| <<ioapic_set_irq>> if (edge && kvm_apicv_activated(ioapic->kvm))
+ *   - arch/x86/kvm/mmu/mmu.c|4434| <<kvm_mmu_faultin_pfn>> if (!kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/avic.c|256| <<avic_init_vmcb>> if (kvm_apicv_activated(svm->vcpu.kvm))
+ *   - arch/x86/kvm/svm/avic.c|290| <<avic_init_backing_page>> if (kvm_apicv_activated(vcpu->kvm)) {
+ *   - arch/x86/kvm/svm/nested.c|1164| <<nested_svm_vmexit>> if (kvm_apicv_activated(vcpu->kvm))
+ *   - arch/x86/kvm/svm/nested.c|1248| <<svm_leave_nested>> if (kvm_apicv_activated(vcpu->kvm))
+ */
 bool kvm_apicv_activated(struct kvm *kvm)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
 }
 EXPORT_SYMBOL_GPL(kvm_apicv_activated);
 
+/*
+ * called by:
+ *   - arch/x86/kvm/svm/svm.c|1642| <<svm_set_vintr>> WARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));
+ *   - arch/x86/kvm/x86.c|10584| <<__kvm_vcpu_update_apicv>> activate = kvm_vcpu_apicv_activated(vcpu) &&
+ *                  (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED);
+ *   - arch/x86/kvm/x86.c|11074| <<vcpu_enter_guest>> WARN_ON_ONCE((kvm_vcpu_apicv_activated(vcpu) !=
+ *                  kvm_vcpu_apicv_active(vcpu)) && (kvm_get_apic_mode(vcpu) != LAPIC_MODE_DISABLED));
+ */
 bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 {
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	/*
+	 * 只有svm:
+	 * avic_vcpu_get_apicv_inhibit_reasons()
+	 */
 	ulong vcpu_reasons =
 			kvm_x86_call(vcpu_get_apicv_inhibit_reasons)(vcpu);
 
@@ -9913,6 +10094,25 @@ bool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_apicv_activated);
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+ *   - arch/x86/kvm/x86.c|10614| <<__kvm_set_or_clear_apicv_inhibit>> set_or_clear_apicv_inhibit(&new, reason, set);
+ */
 static void set_or_clear_apicv_inhibit(unsigned long *inhibits,
 				       enum kvm_apicv_inhibit reason, bool set)
 {
@@ -9925,16 +10125,47 @@ static void set_or_clear_apicv_inhibit(unsigned long *inhibits,
 	else
 		__clear_bit(reason, inhibits);
 
+	/*
+	 * 只在此处trace
+	 */
 	trace_kvm_apicv_inhibit_changed(reason, set, *inhibits);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12757| <<kvm_arch_init_vm>> kvm_apicv_init(kvm);
+ */
 static void kvm_apicv_init(struct kvm *kvm)
 {
 	enum kvm_apicv_inhibit reason = enable_apicv ? APICV_INHIBIT_REASON_ABSENT :
 						       APICV_INHIBIT_REASON_DISABLED;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	init_rwsem(&kvm->arch.apicv_update_lock);
 }
 
@@ -10251,6 +10482,21 @@ static void kvm_inject_exception(struct kvm_vcpu *vcpu)
  * ordering between that side effect, the instruction completing, _and_ the
  * delivery of the asynchronous event.
  */
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ */
 static int kvm_check_and_inject_events(struct kvm_vcpu *vcpu,
 				       bool *req_immediate_exit)
 {
@@ -10522,6 +10768,19 @@ void __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	if (!lapic_in_kernel(vcpu))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_read(&vcpu->kvm->arch.apicv_update_lock);
 	preempt_disable();
 
@@ -10574,16 +10833,62 @@ static void kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu)
 	__kvm_vcpu_update_apicv(vcpu);
 }
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/kvm/hyperv.c|148| <<synic_update_vector>> __kvm_set_or_clear_apicv_inhibit(vcpu->kvm,
+ *                  APICV_INHIBIT_REASON_HYPERV, !!hv->synic_auto_eoi_used);
+ *   - arch/x86/kvm/x86.c|10642| <<kvm_set_or_clear_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
+ *   - arch/x86/kvm/x86.c|12051| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> __kvm_set_or_clear_apicv_inhibit(kvm,
+ *                  APICV_INHIBIT_REASON_BLOCKIRQ, set);
+ */
 void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				      enum kvm_apicv_inhibit reason, bool set)
 {
 	unsigned long old, new;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
 
 	if (!(kvm_x86_ops.required_apicv_inhibits & BIT(reason)))
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_inhibit_reasons:
+	 *   - arch/x86/kvm/x86.c|9919| <<kvm_apicv_activated>> return (READ_ONCE(kvm->arch.apicv_inhibit_reasons) == 0);
+	 *   - arch/x86/kvm/x86.c|9925| <<kvm_vcpu_apicv_activated>> ulong vm_reasons = READ_ONCE(vcpu->kvm->arch.apicv_inhibit_reasons);
+	 *   - arch/x86/kvm/x86.c|9953| <<kvm_apicv_init>> set_or_clear_apicv_inhibit(&kvm->arch.apicv_inhibit_reasons, reason, true);
+	 *   - arch/x86/kvm/x86.c|10612| <<__kvm_set_or_clear_apicv_inhibit>> old = new = kvm->arch.apicv_inhibit_reasons;
+	 *   - arch/x86/kvm/x86.c|10630| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *   - arch/x86/kvm/x86.c|10639| <<__kvm_set_or_clear_apicv_inhibit>> kvm->arch.apicv_inhibit_reasons = new;
+	 *
+	 * 被kvm_arch->apicv_update_lock保护.
+	 */
 	old = new = kvm->arch.apicv_inhibit_reasons;
 
 	set_or_clear_apicv_inhibit(&new, reason, set);
@@ -10615,12 +10920,44 @@ void __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 	}
 }
 
+/*
+ * => trace_event_raw_event_kvm_apicv_inhibit_changed
+ * => set_or_clear_apicv_inhibit
+ * => __kvm_set_or_clear_apicv_inhibit.part.0
+ * => kvm_set_or_clear_apicv_inhibit
+ * => svm_enable_irq_window
+ * => kvm_check_and_inject_events
+ * => vcpu_enter_guest.constprop.0
+ * => vcpu_run
+ * => kvm_arch_vcpu_ioctl_run
+ * => kvm_vcpu_ioctl
+ * => __x64_sys_ioctl
+ * => do_syscall_64
+ * => entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - arch/x86/include/asm/kvm_host.h|2175| <<kvm_set_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, true);
+ *   - arch/x86/include/asm/kvm_host.h|2181| <<kvm_clear_apicv_inhibit>> kvm_set_or_clear_apicv_inhibit(kvm, reason, false);
+ */
 void kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,
 				    enum kvm_apicv_inhibit reason, bool set)
 {
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 	__kvm_set_or_clear_apicv_inhibit(kvm, reason, set);
 	up_write(&kvm->arch.apicv_update_lock);
@@ -10777,14 +11114,44 @@ static int vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 		if (kvm_check_request(KVM_REQ_STEAL_UPDATE, vcpu))
 			record_steal_time(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMU:
+		 *   - arch/x86/kvm/pmu.c|230| <<kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.c|1185| <<kvm_vm_ioctl_set_pmu_event_filter>> kvm_make_all_cpus_request(kvm, KVM_REQ_PMU);
+		 *   - arch/x86/kvm/pmu.h|254| <<kvm_pmu_request_counter_reprogram>> kvm_make_request(KVM_REQ_PMU, pmc->vcpu);
+		 *   - arch/x86/kvm/pmu.h|266| <<reprogram_counters>> kvm_make_request(KVM_REQ_PMU, pmu_to_vcpu(pmu));
+		 *   - arch/x86/kvm/x86.c|4980| <<kvm_arch_vcpu_load>> kvm_make_request(KVM_REQ_PMU, vcpu);
+		 *   - arch/x86/kvm/x86.c|11023| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMU, vcpu))
+		 *
+		 * 处理的函数: kvm_pmu_handle_event()
+		 */
 		if (kvm_check_request(KVM_REQ_PMU, vcpu))
 			kvm_pmu_handle_event(vcpu);
+		/*
+		 * 在以下使用KVM_REQ_PMI:
+		 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+		 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+		 *
+		 * 处理的函数: kvm_pmu_deliver_pmi()
+		 */
 		if (kvm_check_request(KVM_REQ_PMI, vcpu))
 			kvm_pmu_deliver_pmi(vcpu);
 #ifdef CONFIG_KVM_SMM
 		if (kvm_check_request(KVM_REQ_SMI, vcpu))
 			process_smi(vcpu);
 #endif
+		/*
+		 * 在以下使用KVM_REQ_NMI:
+		 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+		 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+		 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+		 *
+		 * 处理的函数: process_nmi()
+		 */
 		if (kvm_check_request(KVM_REQ_NMI, vcpu))
 			process_nmi(vcpu);
 		if (kvm_check_request(KVM_REQ_IOAPIC_EOI_EXIT, vcpu)) {
@@ -11112,6 +11479,16 @@ static bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 	if (kvm_is_exception_pending(vcpu))
 		return true;
 
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * 处理的函数: process_nmi()
+	 */
 	if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
 	    (vcpu->arch.nmi_pending &&
 	     kvm_x86_call(nmi_allowed)(vcpu, false)))
@@ -11124,6 +11501,15 @@ static bool kvm_vcpu_has_events(struct kvm_vcpu *vcpu)
 		return true;
 #endif
 
+	/*
+	 * 在以下使用KVM_REQ_PMI:
+	 *   - arch/x86/kvm/pmu.c|207| <<__kvm_perf_overflow>> kvm_make_request(KVM_REQ_PMI, pmc->vcpu);
+	 *   - arch/x86/kvm/vmx/vmx.c|8378| <<vmx_handle_intel_pt_intr>> kvm_make_request(KVM_REQ_PMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11025| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_PMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11370| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_PMI, vcpu))
+	 *
+	 * 处理的函数: kvm_pmu_deliver_pmi()
+	 */
 	if (kvm_test_request(KVM_REQ_PMI, vcpu))
 		return true;
 
@@ -11274,6 +11660,11 @@ static int vcpu_run(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|11544| <<kvm_emulate_halt_noskip>> return __kvm_emulate_halt(vcpu, KVM_MP_STATE_HALTED, KVM_EXIT_HLT);
+ *   - arch/x86/kvm/x86.c|11581| <<kvm_emulate_ap_reset_hold>> return __kvm_emulate_halt(vcpu, KVM_MP_STATE_AP_RESET_HOLD, KVM_EXIT_AP_RESET_HOLD) && ret;
+ */
 static int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)
 {
 	/*
@@ -11285,6 +11676,17 @@ static int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)
 	 */
 	++vcpu->stat.halt_exits;
 	if (lapic_in_kernel(vcpu)) {
+		/*
+		 * 在以下使用pv_unhalted:
+		 *   - arch/x86/kvm/lapic.c|1356| <<__apic_accept_irq>> vcpu->arch.pv.pv_unhalted = 1;
+		 *   - arch/x86/kvm/svm/sev.c|3833| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/svm/sev.c|3872| <<__sev_snp_update_protected_guest_state>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11352| <<kvm_vcpu_has_events>> if (vcpu->arch.pv.pv_unhalted)
+		 *   - arch/x86/kvm/x86.c|11453| <<vcpu_block>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11532| <<__kvm_emulate_halt>> vcpu->arch.pv.pv_unhalted = false;
+		 *   - arch/x86/kvm/x86.c|11599| <<kvm_arch_dy_runnable>> if (READ_ONCE(vcpu->arch.pv.pv_unhalted))
+		 *   - arch/x86/kvm/x86.c|12002| <<kvm_arch_vcpu_ioctl_get_mpstate>> vcpu->arch.pv.pv_unhalted)
+		 */
 		if (kvm_vcpu_has_events(vcpu))
 			vcpu->arch.pv.pv_unhalted = false;
 		else
@@ -11356,6 +11758,16 @@ bool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)
 	if (READ_ONCE(vcpu->arch.pv.pv_unhalted))
 		return true;
 
+	/*
+	 * 在以下使用KVM_REQ_NMI:
+	 *   - arch/x86/kvm/x86.c|980| <<kvm_inject_nmi>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|5500| <<kvm_vcpu_ioctl_x86_set_vcpu_events>> kvm_make_request(KVM_REQ_NMI, vcpu);
+	 *   - arch/x86/kvm/x86.c|11058| <<vcpu_enter_guest>> if (kvm_check_request(KVM_REQ_NMI, vcpu))
+	 *   - arch/x86/kvm/x86.c|11385| <<kvm_vcpu_has_events>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *   - arch/x86/kvm/x86.c|11645| <<kvm_arch_dy_runnable>> if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
+	 *
+	 * 处理的函数: process_nmi()
+	 */
 	if (kvm_test_request(KVM_REQ_NMI, vcpu) ||
 #ifdef CONFIG_KVM_SMM
 		kvm_test_request(KVM_REQ_SMI, vcpu) ||
@@ -12014,6 +12426,10 @@ int kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|12107| <<kvm_arch_vcpu_ioctl_set_guest_debug>> kvm_arch_vcpu_guestdbg_update_apicv_inhibit(vcpu->kvm);
+ */
 static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 {
 	bool set = false;
@@ -12023,6 +12439,19 @@ static void kvm_arch_vcpu_guestdbg_update_apicv_inhibit(struct kvm *kvm)
 	if (!enable_apicv)
 		return;
 
+	/*
+	 * 在以下使用kvm_arch->apicv_update_lock:
+	 *   - arch/x86/kvm/hyperv.c|137| <<synic_update_vector>> down_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/hyperv.c|152| <<synic_update_vector>> up_write(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|9955| <<kvm_apicv_init>> init_rwsem(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10542| <<__kvm_vcpu_update_apicv>> down_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10567| <<__kvm_vcpu_update_apicv>> up_read(&vcpu->kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10607| <<__kvm_set_or_clear_apicv_inhibit>> lockdep_assert_held_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10654| <<kvm_set_or_clear_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|10656| <<kvm_set_or_clear_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12060| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> down_write(&kvm->arch.apicv_update_lock);
+	 *   - arch/x86/kvm/x86.c|12069| <<kvm_arch_vcpu_guestdbg_update_apicv_inhibit>> up_write(&kvm->arch.apicv_update_lock);
+	 */
 	down_write(&kvm->arch.apicv_update_lock);
 
 	kvm_for_each_vcpu(i, vcpu, kvm) {
@@ -12225,6 +12654,10 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return kvm_x86_call(vcpu_precreate)(kvm);
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|4106| <<kvm_vm_ioctl_create_vcpu>> r = kvm_arch_vcpu_create(vcpu);
+ */
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	struct page *page;
@@ -12241,6 +12674,9 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	else
 		vcpu->arch.mp_state = KVM_MP_STATE_UNINITIALIZED;
 
+	/*
+	 * 只在此处调用
+	 */
 	r = kvm_mmu_create(vcpu);
 	if (r < 0)
 		return r;
@@ -13051,6 +13487,10 @@ static void kvm_mmu_update_cpu_dirty_logging(struct kvm *kvm, bool enable)
 		kvm_make_all_cpus_request(kvm, KVM_REQ_UPDATE_CPU_DIRTY_LOGGING);
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/x86.c|13368| <<kvm_arch_commit_memory_region>> kvm_mmu_slot_apply_flags(kvm, old, new, change);
+ */
 static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *old,
 				     const struct kvm_memory_slot *new,
@@ -13168,6 +13608,10 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 	}
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|1668| <<kvm_commit_memory_region>> kvm_arch_commit_memory_region(kvm, old, new, change);
+ */
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				struct kvm_memory_slot *old,
 				const struct kvm_memory_slot *new,
diff --git a/drivers/block/virtio_blk.c b/drivers/block/virtio_blk.c
index 3efe378f1..b9e321224 100644
--- a/drivers/block/virtio_blk.c
+++ b/drivers/block/virtio_blk.c
@@ -220,6 +220,25 @@ static int virtblk_map_data(struct blk_mq_hw_ctx *hctx, struct request *req,
 		return 0;
 
 	vbr->sg_table.sgl = vbr->sg;
+	/*
+	 * called by:
+	 *   - drivers/block/rnbd/rnbd-clt.c|1131| <<rnbd_queue_rq>> err = sg_alloc_table_chained(&iu->sgt,
+	 *   - drivers/block/virtio_blk.c|223| <<virtblk_map_data>> err = sg_alloc_table_chained(&vbr->sg_table,
+	 *   - drivers/nvme/host/fc.c|2608| <<nvme_fc_map_data>> ret = sg_alloc_table_chained(&freq->sg_table,
+	 *   - drivers/nvme/host/rdma.c|1473| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->data_sgl.sg_table,
+	 *   - drivers/nvme/host/rdma.c|1492| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->metadata_sgl->sg_table,
+	 *   - drivers/nvme/target/loop.c|157| <<nvme_loop_queue_rq>> if (sg_alloc_table_chained(&iod->sg_table,
+	 *   - drivers/scsi/scsi_lib.c|1135| <<scsi_alloc_sgtables>> if (unlikely(sg_alloc_table_chained(&cmd->sdb.table, nr_segs,
+	 *   - drivers/scsi/scsi_lib.c|1180| <<scsi_alloc_sgtables>> if (sg_alloc_table_chained(&prot_sdb->table,
+	 *   - fs/smb/server/transport_rdma.c|1422| <<smb_direct_rdma_xmit>> ret = sg_alloc_table_chained(&msg->sgt,
+	 *   - net/sunrpc/xprtrdma/svc_rdma_rw.c|78| <<svc_rdma_get_rw_ctxt>> if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges, ctxt->rw_sg_table.sgl, first_sgl_nents))
+	 *
+	 * struct sg_table {
+	 *     struct scatterlist *sgl;        // the list
+	 *     unsigned int nents;             // number of mapped entries
+	 *     unsigned int orig_nents;        // original size of list
+	 * };
+	 */
 	err = sg_alloc_table_chained(&vbr->sg_table,
 				     blk_rq_nr_phys_segments(req),
 				     vbr->sg_table.sgl,
diff --git a/drivers/net/virtio_net.c b/drivers/net/virtio_net.c
index 7646ddd9b..142115a0e 100644
--- a/drivers/net/virtio_net.c
+++ b/drivers/net/virtio_net.c
@@ -444,6 +444,14 @@ struct virtnet_info {
 	/* The lock to synchronize the access to refill_enabled */
 	spinlock_t refill_lock;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	/* Work struct for config space updates */
 	struct work_struct config_work;
 
@@ -1059,6 +1067,13 @@ static void virtnet_rq_unmap_free_buf(struct virtqueue *vq, void *buf)
 	virtnet_rq_free_buf(vi, rq, buf);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|1130| <<check_sq_full_and_disable>> free_old_xmit(sq, txq, false);
+ *   - drivers/net/virtio_net.c|2976| <<virtnet_poll_cleantx>> free_old_xmit(sq, txq, !!budget);
+ *   - drivers/net/virtio_net.c|3174| <<virtnet_poll_tx>> free_old_xmit(sq, txq, !!budget);
+ *   - drivers/net/virtio_net.c|3278| <<start_xmit>> free_old_xmit(sq, txq, false);
+ */
 static void free_old_xmit(struct send_queue *sq, struct netdev_queue *txq,
 			  bool in_napi)
 {
@@ -2789,6 +2804,15 @@ static void skb_recv_done(struct virtqueue *rvq)
 	virtqueue_napi_schedule(&rq->napi, rvq);
 }
 
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|2828| <<virtnet_napi_tx_enable>> return virtnet_napi_enable(vq, napi);
+ *   - drivers/net/virtio_net.c|2849| <<refill_work>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|3066| <<virtnet_enable_queue_pair>> virtnet_napi_enable(vi->rq[qp_index].vq, &vi->rq[qp_index].napi);
+ *   - drivers/net/virtio_net.c|3331| <<virtnet_rx_resume>> virtnet_napi_enable(rq->vq, &rq->napi);
+ *   - drivers/net/virtio_net.c|5991| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ *   - drivers/net/virtio_net.c|6008| <<virtnet_xdp_set>> virtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);
+ */
 static void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)
 {
 	napi_enable(napi);
@@ -3658,6 +3682,14 @@ static int virtnet_close(struct net_device *dev)
 	/* Stop getting status/speed updates: we don't care until next
 	 * open
 	 */
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	cancel_work_sync(&vi->config_work);
 
 	for (i = 0; i < vi->max_queue_pairs; i++) {
@@ -5607,6 +5639,14 @@ static void virtnet_freeze_down(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	/* Make sure no work handler is accessing the device */
 	flush_work(&vi->config_work);
 	disable_rx_mode_work(vi);
@@ -6156,6 +6196,14 @@ static void virtnet_config_changed(struct virtio_device *vdev)
 {
 	struct virtnet_info *vi = vdev->priv;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	schedule_work(&vi->config_work);
 }
 
@@ -6716,6 +6764,14 @@ static int virtnet_probe(struct virtio_device *vdev)
 	vi->vdev = vdev;
 	vdev->priv = vi;
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	INIT_WORK(&vi->config_work, virtnet_config_changed_work);
 	INIT_WORK(&vi->rx_mode_work, virtnet_rx_mode_work);
 	spin_lock_init(&vi->refill_lock);
@@ -6935,6 +6991,14 @@ static int virtnet_probe(struct virtio_device *vdev)
 	   otherwise get link status from config. */
 	netif_carrier_off(dev);
 	if (virtio_has_feature(vi->vdev, VIRTIO_NET_F_STATUS)) {
+		/*
+		 * 在以下使用virtnet_info->config_work:
+		 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+		 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+		 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+		 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+		 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+		 */
 		virtnet_config_changed_work(&vi->config_work);
 	} else {
 		vi->status = VIRTIO_NET_S_LINK_UP;
@@ -7003,6 +7067,14 @@ static void virtnet_remove(struct virtio_device *vdev)
 
 	virtnet_cpu_notif_remove(vi);
 
+	/*
+	 * 在以下使用virtnet_info->config_work:
+	 *   - drivers/net/virtio_net.c|3661| <<virtnet_close>> cancel_work_sync(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|5611| <<virtnet_freeze_down>> flush_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6125| <<virtnet_config_changed_work>> container_of(work, struct virtnet_info, config_work);
+	 *   - drivers/net/virtio_net.c|6159| <<virtnet_config_changed>> schedule_work(&vi->config_work);
+	 *   - drivers/net/virtio_net.c|6719| <<virtnet_probe>> INIT_WORK(&vi->config_work, virtnet_config_changed_work);
+	 */
 	/* Make sure no work handler is accessing the device. */
 	flush_work(&vi->config_work);
 	disable_rx_mode_work(vi);
diff --git a/drivers/net/xen-netfront.c b/drivers/net/xen-netfront.c
index 63fe51d0e..3c807001d 100644
--- a/drivers/net/xen-netfront.c
+++ b/drivers/net/xen-netfront.c
@@ -267,6 +267,10 @@ static void xennet_maybe_wake_tx(struct netfront_queue *queue)
 }
 
 
+/*
+ * called by:
+ *   - drivers/net/xen-netfront.c|316| <<xennet_alloc_rx_buffers>> skb = xennet_alloc_one_rx_buffer(queue);
+ */
 static struct sk_buff *xennet_alloc_one_rx_buffer(struct netfront_queue *queue)
 {
 	struct sk_buff *skb;
diff --git a/drivers/scsi/virtio_scsi.c b/drivers/scsi/virtio_scsi.c
index 8471f38b7..6c1f00bf1 100644
--- a/drivers/scsi/virtio_scsi.c
+++ b/drivers/scsi/virtio_scsi.c
@@ -42,6 +42,18 @@ module_param(virtscsi_poll_queues, uint, 0644);
 MODULE_PARM_DESC(virtscsi_poll_queues,
 		 "The number of dedicated virtqueues for polling I/O");
 
+/*
+ * 在以下使用virtio_scsi_cmd->resp:
+ *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+ *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+ *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+ *             req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+ *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+ *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+ *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+ */
 /* Command queue element */
 struct virtio_scsi_cmd {
 	struct scsi_cmnd *sc;
@@ -93,7 +105,31 @@ struct virtio_scsi {
 	struct virtio_scsi_vq req_vqs[];
 };
 
+/*
+ * 在以下使用virtscsi_cmd_cache:
+ *   - drivers/scsi/virtio_scsi.c|108| <<global>> static struct kmem_cache *virtscsi_cmd_cache;
+ *   - drivers/scsi/virtio_scsi.c|1261| <<virtio_scsi_init>> virtscsi_cmd_cache = KMEM_CACHE(virtio_scsi_cmd, 0);
+ *   - drivers/scsi/virtio_scsi.c|1262| <<virtio_scsi_init>> if (!virtscsi_cmd_cache) {
+ *   - drivers/scsi/virtio_scsi.c|1270| <<virtio_scsi_init>> virtscsi_cmd_pool = mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
+ *             virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1284| <<virtio_scsi_init>> kmem_cache_destroy(virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1285| <<virtio_scsi_init>> virtscsi_cmd_cache = NULL;
+ *   - drivers/scsi/virtio_scsi.c|1293| <<virtio_scsi_fini>> kmem_cache_destroy(virtscsi_cmd_cache);
+ */
 static struct kmem_cache *virtscsi_cmd_cache;
+/*
+ * 在以下使用virtscsi_cmd_pool:
+ *   - drivers/scsi/virtio_scsi.c|109| <<global>> static mempool_t *virtscsi_cmd_pool;
+ *   - drivers/scsi/virtio_scsi.c|818| <<virtscsi_tmf>> mempool_free(cmd, virtscsi_cmd_pool);
+ *   - drivers/scsi/virtio_scsi.c|831| <<virtscsi_device_reset>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+ *   - drivers/scsi/virtio_scsi.c|905| <<virtscsi_abort>> cmd = mempool_alloc(virtscsi_cmd_pool, GFP_NOIO);
+ *   - drivers/scsi/virtio_scsi.c|1268| <<virtio_scsi_init>> virtscsi_cmd_pool = mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
+ *             virtscsi_cmd_cache);
+ *   - drivers/scsi/virtio_scsi.c|1271| <<virtio_scsi_init>> if (!virtscsi_cmd_pool) {
+ *   - drivers/scsi/virtio_scsi.c|1282| <<virtio_scsi_init>> mempool_destroy(virtscsi_cmd_pool);
+ *   - drivers/scsi/virtio_scsi.c|1283| <<virtio_scsi_init>> virtscsi_cmd_pool = NULL;
+ *   - drivers/scsi/virtio_scsi.c|1292| <<virtio_scsi_fini>> mempool_destroy(virtscsi_cmd_pool);
+ */
 static mempool_t *virtscsi_cmd_pool;
 
 static inline struct Scsi_Host *virtio_scsi_host(struct virtio_device *vdev)
@@ -112,10 +148,29 @@ static void virtscsi_compute_resid(struct scsi_cmnd *sc, u32 resid)
  *
  * Called with vq_lock held.
  */
+/*
+ * 在以下使用virtscsi_complete_cmd():
+ *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_req_done>> virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|253| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi, &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|699| <<virtscsi_queuecommand>> virtscsi_complete_cmd(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|889| <<virtscsi_mq_poll>> virtscsi_complete_cmd(vscsi, buf);
+ */
 static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
 	struct scsi_cmnd *sc = cmd->sc;
+	/*
+	 * 在以下使用virtio_scsi_cmd->resp:
+	 *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+	 *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+	 *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+	 *             req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+	 *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+	 *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+	 */
 	struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
 
 	dev_dbg(&sc->device->sdev_gendev,
@@ -173,6 +228,17 @@ static void virtscsi_complete_cmd(struct virtio_scsi *vscsi, void *buf)
 	scsi_done(sc);
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+ *             req_vq, virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+ *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+ *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+ *             &vscsi->ctrl_vq, virtscsi_complete_free);
+ *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+ *             &vscsi->event_vq, virtscsi_complete_event);
+ */
 static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 			     struct virtio_scsi_vq *virtscsi_vq,
 			     void (*fn)(struct virtio_scsi *vscsi, void *buf))
@@ -192,6 +258,10 @@ static void virtscsi_vq_done(struct virtio_scsi *vscsi,
 	spin_unlock_irqrestore(&virtscsi_vq->vq_lock, flags);
 }
 
+/*
+ * 在以下使用virtscsi_req_done():
+ *   - drivers/scsi/virtio_scsi.c|919| <<virtscsi_init>> vqs_info[i].callback = virtscsi_req_done;
+ */
 static void virtscsi_req_done(struct virtqueue *vq)
 {
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
@@ -199,19 +269,51 @@ static void virtscsi_req_done(struct virtqueue *vq)
 	int index = vq->index - VIRTIO_SCSI_VQ_BASE;
 	struct virtio_scsi_vq *req_vq = &vscsi->req_vqs[index];
 
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, req_vq, virtscsi_complete_cmd);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|664| <<virtscsi_tmf>> virtscsi_poll_requests(vscsi);
+ */
 static void virtscsi_poll_requests(struct virtio_scsi *vscsi)
 {
 	int i, num_vqs;
 
 	num_vqs = vscsi->num_queues;
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 *
+	 * 这里是&vscsi->req_vqs[i], 不是ctrl或者event!
+	 */
 	for (i = 0; i < num_vqs; i++)
 		virtscsi_vq_done(vscsi, &vscsi->req_vqs[i],
 				 virtscsi_complete_cmd);
 }
 
+/*
+ * 只在以下使用virtscsi_complete_free():
+ *   - drivers/scsi/virtio_scsi.c|289| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
+ */
 static void virtscsi_complete_free(struct virtio_scsi *vscsi, void *buf)
 {
 	struct virtio_scsi_cmd *cmd = buf;
@@ -225,6 +327,17 @@ static void virtscsi_ctrl_done(struct virtqueue *vq)
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
 	struct virtio_scsi *vscsi = shost_priv(sh);
 
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, &vscsi->ctrl_vq, virtscsi_complete_free);
 };
 
@@ -423,9 +536,24 @@ static void virtscsi_event_done(struct virtqueue *vq)
 	struct Scsi_Host *sh = virtio_scsi_host(vq->vdev);
 	struct virtio_scsi *vscsi = shost_priv(sh);
 
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|202| <<virtscsi_req_done>> virtscsi_vq_done(vscsi,
+	 *             req_vq, virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|211| <<virtscsi_poll_requests>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->req_vqs[i], virtscsi_complete_cmd);
+	 *   - drivers/scsi/virtio_scsi.c|228| <<virtscsi_ctrl_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->ctrl_vq, virtscsi_complete_free);
+	 *   - drivers/scsi/virtio_scsi.c|426| <<virtscsi_event_done>> virtscsi_vq_done(vscsi,
+	 *             &vscsi->event_vq, virtscsi_complete_event);
+	 */
 	virtscsi_vq_done(vscsi, &vscsi->event_vq, virtscsi_complete_event);
 };
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|505| <<virtscsi_add_cmd>> err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
+ */
 static int __virtscsi_add_cmd(struct virtqueue *vq,
 			    struct virtio_scsi_cmd *cmd,
 			    size_t req_size, size_t resp_size)
@@ -456,6 +584,34 @@ static int __virtscsi_add_cmd(struct virtqueue *vq,
 		sgs[out_num++] = out->sgl;
 	}
 
+	/*
+	 * 在以下使用virtio_scsi_cmd->resp:
+	 *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+	 *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+	 *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+	 *             req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+	 *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+	 *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+	 *
+	 *
+	 * struct virtio_scsi_cmd_resp {
+	 *     __virtio32 sense_len;           // Sense data length
+	 *     __virtio32 resid;               // Residual bytes in data buffer
+	 *     __virtio16 status_qualifier;    // Status qualifier
+	 *     __u8 status;            // Command completion status
+	 *     __u8 response;          // Response values
+	 *     __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+	 * } __attribute__((packed));
+	 *
+	 * 参考代码:
+	 * memset(sgl, 0, sizeof(*sgl) * nents);
+	 * sg_set_page(sg, virt_to_page(buf), buflen, offset_in_page(buf));
+	 * sg_mark_end(&sgl[nents - 1]);
+	 */
+
 	/* Response header.  */
 	sg_init_one(&resp, &cmd->resp, resp_size);
 	sgs[out_num + in_num++] = &resp;
@@ -492,6 +648,13 @@ static void virtscsi_kick_vq(struct virtio_scsi_vq *vq)
  * @resp_size	: size of the response buffer
  * @kick	: whether to kick the virtqueue immediately
  */
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|602| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+ *             cmd, req_size, sizeof(cmd->resp.cmd), kick);
+ *   - drivers/scsi/virtio_scsi.c|620| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+ *             cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+ */
 static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 			     struct virtio_scsi_cmd *cmd,
 			     size_t req_size, size_t resp_size,
@@ -502,6 +665,9 @@ static int virtscsi_add_cmd(struct virtio_scsi_vq *vq,
 	bool needs_kick = false;
 
 	spin_lock_irqsave(&vq->vq_lock, flags);
+	/*
+	 * 只在此处调用
+	 */
 	err = __virtscsi_add_cmd(vq->vq, cmd, req_size, resp_size);
 	if (!err && kick)
 		needs_kick = virtqueue_kick_prepare(vq->vq);
@@ -598,6 +764,18 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 		req_size = sizeof(cmd->req.cmd);
 	}
 
+	/*
+	 * 在以下使用virtio_scsi_cmd->resp:
+	 *   - drivers/scsi/virtio_scsi.c|126| <<virtscsi_complete_cmd>> struct virtio_scsi_cmd_resp *resp = &cmd->resp.cmd;
+	 *   - drivers/scsi/virtio_scsi.c|556| <<__virtscsi_add_cmd>> sg_init_one(&resp, &cmd->resp, resp_size);
+	 *   - drivers/scsi/virtio_scsi.c|708| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq, cmd,
+	 *             req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|710| <<virtscsi_queuecommand>> cmd->resp.cmd.response = VIRTIO_SCSI_S_BAD_TARGET;
+	 *   - drivers/scsi/virtio_scsi.c|739| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
+	 *             sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 *   - drivers/scsi/virtio_scsi.c|751| <<virtscsi_tmf>> if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
+	 *   - drivers/scsi/virtio_scsi.c|752| <<virtscsi_tmf>> cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
+	 */
 	kick = (sc->flags & SCMD_LAST) != 0;
 	ret = virtscsi_add_cmd(req_vq, cmd, req_size, sizeof(cmd->resp.cmd), kick);
 	if (ret == -EIO) {
@@ -611,17 +789,37 @@ static int virtscsi_queuecommand(struct Scsi_Host *shost,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/scsi/virtio_scsi.c|665| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+ *   - drivers/scsi/virtio_scsi.c|723| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+ */
 static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 {
 	DECLARE_COMPLETION_ONSTACK(comp);
 	int ret = FAILED;
 
 	cmd->comp = &comp;
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|602| <<virtscsi_queuecommand>> ret = virtscsi_add_cmd(req_vq,
+	 *             cmd, req_size, sizeof(cmd->resp.cmd), kick);
+	 *   - drivers/scsi/virtio_scsi.c|620| <<virtscsi_tmf>> if (virtscsi_add_cmd(&vscsi->ctrl_vq,
+	 *             cmd, sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
+	 */
 	if (virtscsi_add_cmd(&vscsi->ctrl_vq, cmd,
 			      sizeof cmd->req.tmf, sizeof cmd->resp.tmf, true) < 0)
 		goto out;
 
 	wait_for_completion(&comp);
+	/*
+	 * struct virtio_scsi_ctrl_tmf_resp {
+	 *     __u8 response;
+	 * } __attribute__((packed));
+	 *
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> struct virtio_scsi_ctrl_tmf_resp tmf;
+	 */
 	if (cmd->resp.tmf.response == VIRTIO_SCSI_S_OK ||
 	    cmd->resp.tmf.response == VIRTIO_SCSI_S_FUNCTION_SUCCEEDED)
 		ret = SUCCESS;
@@ -635,6 +833,9 @@ static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 	 * In the abort case, scsi_done() will do nothing, because the
 	 * command timed out and hence SCMD_STATE_COMPLETE has been set.
 	 */
+	/*
+	 * 只在此处调用	
+	 */
 	virtscsi_poll_requests(vscsi);
 
 out:
@@ -642,6 +843,9 @@ static int virtscsi_tmf(struct virtio_scsi *vscsi, struct virtio_scsi_cmd *cmd)
 	return ret;
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.eh_device_reset_handler = virtscsi_device_reset()
+ */
 static int virtscsi_device_reset(struct scsi_cmnd *sc)
 {
 	struct virtio_scsi *vscsi = shost_priv(sc->device->host);
@@ -653,6 +857,17 @@ static int virtscsi_device_reset(struct scsi_cmnd *sc)
 		return FAILED;
 
 	memset(cmd, 0, sizeof(*cmd));
+	/*
+	 * struct virtio_scsi_ctrl_tmf_req {
+	 *     __virtio32 type;
+	 *     __virtio32 subtype;
+	 *     __u8 lun[8];
+	 *     __virtio64 tag;
+	 * } __attribute__((packed));
+	 *
+	 * struct virtio_scsi_cmd *cmd:
+	 * -> struct virtio_scsi_ctrl_tmf_req tmf;
+	 */
 	cmd->req.tmf = (struct virtio_scsi_ctrl_tmf_req){
 		.type = VIRTIO_SCSI_T_TMF,
 		.subtype = cpu_to_virtio32(vscsi->vdev,
@@ -662,6 +877,11 @@ static int virtscsi_device_reset(struct scsi_cmnd *sc)
 		.lun[2] = (sc->device->lun >> 8) | 0x40,
 		.lun[3] = sc->device->lun & 0xff,
 	};
+	/*
+	 * called by:
+	 *   - drivers/scsi/virtio_scsi.c|665| <<virtscsi_device_reset>> return virtscsi_tmf(vscsi, cmd);
+	 *   - drivers/scsi/virtio_scsi.c|723| <<virtscsi_abort>> return virtscsi_tmf(vscsi, cmd);
+	 */
 	return virtscsi_tmf(vscsi, cmd);
 }
 
@@ -750,6 +970,9 @@ static void virtscsi_map_queues(struct Scsi_Host *shost)
 	}
 }
 
+/*
+ * struct scsi_host_template virtscsi_host_template.mq_poll = virtscsi_mq_poll()
+ */
 static int virtscsi_mq_poll(struct Scsi_Host *shost, unsigned int queue_num)
 {
 	struct virtio_scsi *vscsi = shost_priv(shost);
@@ -1066,6 +1289,11 @@ static int __init virtio_scsi_init(void)
 	}
 
 
+	/*
+	 * mempool_create_slab_pool()有意思的调用:
+	 *   - drivers/scsi/virtio_scsi.c|1269| <<virtio_scsi_init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache);
+	 *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+	 */
 	virtscsi_cmd_pool =
 		mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ,
 					 virtscsi_cmd_cache);
diff --git a/drivers/target/loopback/tcm_loop.c b/drivers/target/loopback/tcm_loop.c
index 761c511ae..e31c21781 100644
--- a/drivers/target/loopback/tcm_loop.c
+++ b/drivers/target/loopback/tcm_loop.c
@@ -1123,6 +1123,23 @@ static int __init tcm_loop_fabric_init(void)
 	if (ret)
 		goto out_destroy_cache;
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3961| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1662| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_ops);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1667| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_npiv_ops);
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4036| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1878| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1882| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+	 *   - drivers/target/iscsi/iscsi_target.c|695| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+	 *   - drivers/target/loopback/tcm_loop.c|1126| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+	 *   - drivers/target/sbp/sbp_target.c|2288| <<sbp_init>> return target_register_template(&sbp_ops);
+	 *   - drivers/target/tcm_fc/tfc_conf.c|448| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+	 *   - drivers/target/tcm_remote/tcm_remote.c|256| <<tcm_remote_fabric_init>> return target_register_template(&remote_ops);
+	 *   - drivers/usb/gadget/function/f_tcm.c|2289| <<tcm_init>> ret = target_register_template(&usbg_ops);
+	 *   - drivers/vhost/scsi.c|2792| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+	 *   - drivers/xen/xen-scsiback.c|1866| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+	 */
 	ret = target_register_template(&loop_ops);
 	if (ret)
 		goto out_release_core_bus;
diff --git a/drivers/target/target_core_configfs.c b/drivers/target/target_core_configfs.c
index c40217f44..c57948538 100644
--- a/drivers/target/target_core_configfs.c
+++ b/drivers/target/target_core_configfs.c
@@ -465,6 +465,23 @@ static void target_set_default_ops(struct target_core_fabric_ops *tfo)
 		tfo->get_cmd_state = target_default_get_cmd_state;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3961| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+ *   - drivers/scsi/elx/efct/efct_lio.c|1662| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_ops);
+ *   - drivers/scsi/elx/efct/efct_lio.c|1667| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_npiv_ops);
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4036| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1878| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1882| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+ *   - drivers/target/iscsi/iscsi_target.c|695| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+ *   - drivers/target/loopback/tcm_loop.c|1126| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+ *   - drivers/target/sbp/sbp_target.c|2288| <<sbp_init>> return target_register_template(&sbp_ops);
+ *   - drivers/target/tcm_fc/tfc_conf.c|448| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+ *   - drivers/target/tcm_remote/tcm_remote.c|256| <<tcm_remote_fabric_init>> return target_register_template(&remote_ops);
+ *   - drivers/usb/gadget/function/f_tcm.c|2289| <<tcm_init>> ret = target_register_template(&usbg_ops);
+ *   - drivers/vhost/scsi.c|2792| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+ *   - drivers/xen/xen-scsiback.c|1866| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+ */
 int target_register_template(const struct target_core_fabric_ops *fo)
 {
 	struct target_core_fabric_ops *tfo;
diff --git a/drivers/target/target_core_file.c b/drivers/target/target_core_file.c
index 2d78ef746..297a8bb2a 100644
--- a/drivers/target/target_core_file.c
+++ b/drivers/target/target_core_file.c
@@ -167,6 +167,21 @@ static int fd_configure_device(struct se_device *dev)
 			" block_device blocks: %llu logical_block_size: %d\n",
 			dev_size, div_u64(dev_size, fd_dev->fd_block_size),
 			fd_dev->fd_block_size);
+		/*
+		 * 在以下使用se_dev_attrib->max_write_same_len:
+		 *   - drivers/target/target_core_configfs.c|595| <<global>> DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+		 *   - drivers/target/target_core_configfs.c|618| <<global>> DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+		 *   - drivers/target/target_core_configfs.c|1318| <<global>> CONFIGFS_ATTR(, max_write_same_len);
+		 *   - drivers/target/target_core_device.c|775| <<target_alloc_device>> dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
+		 *   - drivers/target/target_core_file.c|174| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+		 *   - drivers/target/target_core_file.c|192| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0x1000;
+		 *   - drivers/target/target_core_iblock.c|143| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;
+		 *   - drivers/target/target_core_iblock.c|145| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+		 *   - drivers/target/target_core_sbc.c|294| <<sbc_setup_write_same>> if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
+		 *   - drivers/target/target_core_sbc.c|296| <<sbc_setup_write_same>> pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
+		 *                    sectors, cmd->se_dev->dev_attrib.max_write_same_len);
+		 *   - drivers/target/target_core_spc.c|599| <<spc_emulate_evpd_b0>> put_unaligned_be64(dev->dev_attrib.max_write_same_len, &buf[36]);
+		 */
 		/*
 		 * Enable write same emulation for IBLOCK and use 0xFFFF as
 		 * the smaller WRITE_SAME(10) only has a two-byte block count.
@@ -248,6 +263,10 @@ struct target_core_file_cmd {
 	struct bio_vec	bvecs[];
 };
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|686| <<fd_execute_rw>> return fd_execute_rw_aio(cmd, sgl, sgl_nents, data_direction);
+ */
 static void cmd_rw_aio_complete(struct kiocb *iocb, long ret)
 {
 	struct target_core_file_cmd *cmd;
@@ -309,6 +328,13 @@ fd_execute_rw_aio(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|599| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, pfile, dev->prot_length,
+ *   - drivers/target/target_core_file.c|606| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, file, dev->dev_attrib.block_size,
+ *   - drivers/target/target_core_file.c|631| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, file, dev->dev_attrib.block_size,
+ *   - drivers/target/target_core_file.c|652| <<fd_execute_rw_buffered>> ret = fd_do_rw(cmd, pfile, dev->prot_length,
+ */
 static int fd_do_rw(struct se_cmd *cmd, struct file *fd,
 		    u32 block_size, struct scatterlist *sgl,
 		    u32 sgl_nents, u32 data_length, int is_write)
@@ -934,6 +960,14 @@ static const struct target_backend_ops fileio_ops = {
 
 static int __init fileio_module_init(void)
 {
+	/*
+	 * called by:
+	 *   - drivers/target/target_core_file.c|937| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+	 *   - drivers/target/target_core_iblock.c|1189| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+	 *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+	 *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+	 *   - drivers/target/target_core_user.c|3359| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+	 */
 	return transport_backend_register(&fileio_ops);
 }
 
diff --git a/drivers/target/target_core_hba.c b/drivers/target/target_core_hba.c
index d508b343b..20f9cb8bb 100644
--- a/drivers/target/target_core_hba.c
+++ b/drivers/target/target_core_hba.c
@@ -35,6 +35,14 @@ static DEFINE_SPINLOCK(hba_lock);
 static LIST_HEAD(hba_list);
 
 
+/*
+ * called by:
+ *   - drivers/target/target_core_file.c|937| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+ *   - drivers/target/target_core_iblock.c|1189| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+ *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+ *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+ *   - drivers/target/target_core_user.c|3359| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+ */
 int transport_backend_register(const struct target_backend_ops *ops)
 {
 	struct target_backend *tb, *old;
diff --git a/drivers/target/target_core_iblock.c b/drivers/target/target_core_iblock.c
index c8dc92a7d..b141f6e33 100644
--- a/drivers/target/target_core_iblock.c
+++ b/drivers/target/target_core_iblock.c
@@ -381,6 +381,12 @@ static struct bio *iblock_get_bio(struct se_cmd *cmd, sector_t lba, u32 sg_num,
 	return bio;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_iblock.c|562| <<iblock_execute_write_same>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|810| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ *   - drivers/target/target_core_iblock.c|834| <<iblock_execute_rw>> iblock_submit_bios(&list);
+ */
 static void iblock_submit_bios(struct bio_list *list)
 {
 	struct blk_plug plug;
@@ -1186,6 +1192,14 @@ static const struct target_backend_ops iblock_ops = {
 
 static int __init iblock_module_init(void)
 {
+	/*
+	 * called by:
+	 *   - drivers/target/target_core_file.c|937| <<fileio_module_init>> return transport_backend_register(&fileio_ops);
+	 *   - drivers/target/target_core_iblock.c|1189| <<iblock_module_init>> return transport_backend_register(&iblock_ops);
+	 *   - drivers/target/target_core_pscsi.c|1059| <<pscsi_module_init>> return transport_backend_register(&pscsi_ops);
+	 *   - drivers/target/target_core_rd.c|678| <<rd_module_init>> return transport_backend_register(&rd_mcp_ops);
+	 *   - drivers/target/target_core_user.c|3359| <<tcmu_module_init>> ret = transport_backend_register(&tcmu_ops);
+	 */
 	return transport_backend_register(&iblock_ops);
 }
 
diff --git a/drivers/target/target_core_sbc.c b/drivers/target/target_core_sbc.c
index fe8beb7db..e99f4d929 100644
--- a/drivers/target/target_core_sbc.c
+++ b/drivers/target/target_core_sbc.c
@@ -270,6 +270,12 @@ static inline unsigned long long transport_lba_64(unsigned char *cdb)
 	return get_unaligned_be64(&cdb[2]);
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+ *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+ *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+ */
 static sense_reason_t
 sbc_setup_write_same(struct se_cmd *cmd, unsigned char flags,
 		     struct exec_cmd_ops *ops)
@@ -285,6 +291,21 @@ sbc_setup_write_same(struct se_cmd *cmd, unsigned char flags,
 			" Emulation\n");
 		return TCM_UNSUPPORTED_SCSI_OPCODE;
 	}
+	/*
+	 * 在以下使用se_dev_attrib->max_write_same_len:
+	 *   - drivers/target/target_core_configfs.c|595| <<global>> DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|618| <<global>> DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|1318| <<global>> CONFIGFS_ATTR(, max_write_same_len);
+	 *   - drivers/target/target_core_device.c|775| <<target_alloc_device>> dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
+	 *   - drivers/target/target_core_file.c|174| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_file.c|192| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0x1000;
+	 *   - drivers/target/target_core_iblock.c|143| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;
+	 *   - drivers/target/target_core_iblock.c|145| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_sbc.c|294| <<sbc_setup_write_same>> if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
+	 *   - drivers/target/target_core_sbc.c|296| <<sbc_setup_write_same>> pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
+	 *                    sectors, cmd->se_dev->dev_attrib.max_write_same_len);
+	 *   - drivers/target/target_core_spc.c|599| <<spc_emulate_evpd_b0>> put_unaligned_be64(dev->dev_attrib.max_write_same_len, &buf[36]);
+	 */
 	if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
 		pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
 			sectors, cmd->se_dev->dev_attrib.max_write_same_len);
@@ -889,6 +910,12 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 			size = sbc_get_size(cmd, 1);
 			cmd->t_task_lba = get_unaligned_be64(&cdb[12]);
 
+			/*
+			 * called by:
+			 *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+			 *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+			 *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+			 */
 			ret = sbc_setup_write_same(cmd, cdb[10], ops);
 			if (ret)
 				return ret;
@@ -988,6 +1015,12 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 		size = sbc_get_size(cmd, 1);
 		cmd->t_task_lba = get_unaligned_be64(&cdb[2]);
 
+		/*
+		 * called by:
+		 *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+		 *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 */
 		ret = sbc_setup_write_same(cmd, cdb[1], ops);
 		if (ret)
 			return ret;
@@ -1006,6 +1039,12 @@ sbc_parse_cdb(struct se_cmd *cmd, struct exec_cmd_ops *ops)
 		 * Follow sbcr26 with WRITE_SAME (10) and check for the existence
 		 * of byte 1 bit 3 UNMAP instead of original reserved field
 		 */
+		/*
+		 * called by:
+		 *   - drivers/target/target_core_sbc.c|892| <<sbc_parse_cdb(VARIABLE_LENGTH_CMD)>> ret = sbc_setup_write_same(cmd, cdb[10], ops);
+		 *   - drivers/target/target_core_sbc.c|991| <<sbc_parse_cdb(WRITE_SAME_16)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 *   - drivers/target/target_core_sbc.c|1009| <<sbc_parse_cdb(WRITE_SAME)>> ret = sbc_setup_write_same(cmd, cdb[1], ops);
+		 */
 		ret = sbc_setup_write_same(cmd, cdb[1], ops);
 		if (ret)
 			return ret;
diff --git a/drivers/target/target_core_spc.c b/drivers/target/target_core_spc.c
index ea14a3835..745280030 100644
--- a/drivers/target/target_core_spc.c
+++ b/drivers/target/target_core_spc.c
@@ -1015,6 +1015,11 @@ static int spc_modesense_long_blockdesc(unsigned char *buf, u64 blocks, u32 bloc
 	return 17;
 }
 
+/*
+ * called by:
+ *   - drivers/target/target_core_spc.c|2298| <<spc_parse_cdb>> cmd->execute_cmd = spc_emulate_modesense;
+ *   - drivers/target/target_core_spc.c|2302| <<spc_parse_cdb>> cmd->execute_cmd = spc_emulate_modesense;
+ */
 static sense_reason_t spc_emulate_modesense(struct se_cmd *cmd)
 {
 	struct se_device *dev = cmd->se_dev;
diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c
index 05d29201b..307adca53 100644
--- a/drivers/target/target_core_transport.c
+++ b/drivers/target/target_core_transport.c
@@ -945,6 +945,52 @@ void target_complete_cmd_with_sense(struct se_cmd *cmd, u8 scsi_status,
 }
 EXPORT_SYMBOL(target_complete_cmd_with_sense);
 
+/*
+ * called by:
+ *   - drivers/target/iscsi/iscsi_target.c|4239| <<iscsit_release_commands_from_conn>> target_complete_cmd(&cmd->se_cmd, SAM_STAT_TASK_ABORTED);
+ *   - drivers/target/target_core_alua.c|126| <<target_emulate_report_referrals>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_alua.c|430| <<target_emulate_set_target_port_groups>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|258| <<cmd_rw_aio_complete>> target_complete_cmd(cmd->cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_file.c|260| <<cmd_rw_aio_complete>> target_complete_cmd(cmd->cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|396| <<fd_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|420| <<fd_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_file.c|422| <<fd_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|478| <<fd_execute_write_same>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_file.c|663| <<fd_execute_rw_buffered>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_iblock.c|336| <<iblock_complete_cmd>> target_complete_cmd(cmd, status);
+ *   - drivers/target/target_core_iblock.c|407| <<iblock_end_io_flush>> target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_iblock.c|409| <<iblock_end_io_flush>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_iblock.c|431| <<iblock_execute_sync_cache>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_iblock.c|490| <<iblock_execute_zero_out>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|237| <<target_scsi2_reservation_release>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|300| <<target_scsi2_reservation_reserve>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|3725| <<target_scsi3_emulate_pr_out>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pr.c|4161| <<target_scsi3_emulate_pr_in>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_pscsi.c|1028| <<pscsi_req_done>> target_complete_cmd(cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_rd.c|433| <<rd_execute_rw>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_rd.c|528| <<rd_execute_rw>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|165| <<sbc_emulate_startstop>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|205| <<sbc_execute_write_same_unmap>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|212| <<sbc_emulate_noop>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|1118| <<sbc_execute_unmap>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_sbc.c|1181| <<sbc_execute_unmap>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1162| <<spc_emulate_modeselect>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1205| <<spc_emulate_modeselect>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1238| <<spc_emulate_request_sense>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_spc.c|1313| <<spc_emulate_testunitready>> target_complete_cmd(cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_transport.c|978| <<target_complete_cmd_with_length>> target_complete_cmd(cmd, scsi_status);
+ *   - drivers/target/target_core_user.c|1284| <<tcmu_tmr_notify>> target_complete_cmd(se_cmd, SAM_STAT_TASK_ABORTED);
+ *   - drivers/target/target_core_user.c|1390| <<tcmu_handle_completion>> target_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);
+ *   - drivers/target/target_core_user.c|1534| <<tcmu_check_expired_ring_cmd>> target_complete_cmd(se_cmd, SAM_STAT_CHECK_CONDITION);
+ *   - drivers/target/target_core_user.c|1552| <<tcmu_check_expired_queue_cmd>> target_complete_cmd(se_cmd, SAM_STAT_TASK_SET_FULL);
+ *   - drivers/target/target_core_user.c|1779| <<run_qfull_queue>> target_complete_cmd(tcmu_cmd->se_cmd, SAM_STAT_BUSY);
+ *   - drivers/target/target_core_user.c|1793| <<run_qfull_queue>> target_complete_cmd(tcmu_cmd->se_cmd,
+ *   - drivers/target/target_core_user.c|2389| <<tcmu_reset_ring>> target_complete_cmd(cmd->se_cmd, SAM_STAT_BUSY);
+ *   - drivers/target/target_core_user.c|2392| <<tcmu_reset_ring>> target_complete_cmd(cmd->se_cmd,
+ *   - drivers/target/target_core_xcopy.c|763| <<target_xcopy_do_work>> target_complete_cmd(ec_cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_xcopy.c|892| <<target_do_xcopy>> target_complete_cmd(se_cmd, SAM_STAT_GOOD);
+ *   - drivers/target/target_core_xcopy.c|1002| <<target_rcr_operating_parameters>> target_complete_cmd(se_cmd, SAM_STAT_GOOD);
+ */
 void target_complete_cmd(struct se_cmd *cmd, u8 scsi_status)
 {
 	target_complete_cmd_with_sense(cmd, scsi_status, scsi_status ?
@@ -1976,6 +2022,17 @@ static void target_complete_tmr_failure(struct work_struct *work)
  * Callable from all contexts.
  **/
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1667| <<srpt_handle_tsk_mgmt>> rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,
+ *   - drivers/scsi/elx/efct/efct_lio.c|1449| <<efct_scsi_recv_tmf>> rc = target_submit_tmr(&ocp->cmd, se_sess, NULL, lun, ocp, tmr_func,
+ *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2799| <<ibmvscsis_parse_task>> rc = target_submit_tmr(&cmd->se_cmd, nexus->se_sess, NULL,
+ *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|617| <<tcm_qla2xxx_handle_tmr>> return target_submit_tmr(se_cmd, sess->se_sess, NULL, lun, mcmd,
+ *   - drivers/target/loopback/tcm_loop.c|216| <<tcm_loop_issue_tmr>> rc = target_submit_tmr(se_cmd, se_sess, tl_cmd->tl_sense_buf, lun,
+ *   - drivers/target/tcm_fc/tfc_cmd.c|365| <<ft_send_tm>> rc = target_submit_tmr(&cmd->se_cmd, cmd->sess->se_sess,
+ *   - drivers/vhost/scsi.c|1701| <<vhost_scsi_handle_tmf>> if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
+ *   - drivers/xen/xen-scsiback.c|626| <<scsiback_device_action>> rc = target_submit_tmr(&pending_req->se_cmd, nexus->tvn_se_sess,
+ */
 int target_submit_tmr(struct se_cmd *se_cmd, struct se_session *se_sess,
 		unsigned char *sense, u64 unpacked_lun,
 		void *fabric_tmr_ptr, unsigned char tm_type,
diff --git a/drivers/vdpa/ifcvf/ifcvf_main.c b/drivers/vdpa/ifcvf/ifcvf_main.c
index ccf64d7bb..f016468db 100644
--- a/drivers/vdpa/ifcvf/ifcvf_main.c
+++ b/drivers/vdpa/ifcvf/ifcvf_main.c
@@ -518,6 +518,22 @@ static int ifcvf_vdpa_set_vq_address(struct vdpa_device *vdpa_dev, u16 qid,
 	return ifcvf_set_vq_address(vf, qid, desc_area, driver_area, device_area);
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void ifcvf_vdpa_kick_vq(struct vdpa_device *vdpa_dev, u16 qid)
 {
 	struct ifcvf_hw *vf = vdpa_to_vf(vdpa_dev);
diff --git a/drivers/vdpa/mlx5/net/mlx5_vnet.c b/drivers/vdpa/mlx5/net/mlx5_vnet.c
index 5f581e71e..cca3ecfd2 100644
--- a/drivers/vdpa/mlx5/net/mlx5_vnet.c
+++ b/drivers/vdpa/mlx5/net/mlx5_vnet.c
@@ -1455,6 +1455,10 @@ static irqreturn_t mlx5_vdpa_int_handler(int irq, void *priv)
 	return IRQ_HANDLED;
 }
 
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|1528| <<setup_vq>> alloc_vector(ndev, mvq);
+ */
 static void alloc_vector(struct mlx5_vdpa_net *ndev,
 			 struct mlx5_vdpa_virtqueue *mvq)
 {
@@ -2425,6 +2429,22 @@ static void mlx5_cvq_kick_handler(struct work_struct *work)
 	up_write(&ndev->reslock);
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void mlx5_vdpa_kick_vq(struct vdpa_device *vdev, u16 idx)
 {
 	struct mlx5_vdpa_dev *mvdev = to_mvdev(vdev);
@@ -2446,6 +2466,13 @@ static void mlx5_vdpa_kick_vq(struct vdpa_device *vdev, u16 idx)
 	if (unlikely(!mvq->ready))
 		return;
 
+	/*
+	 * struct mlx5_vdpa_net *ndev:
+	 * -> struct mlx5_vdpa_dev mvdev;
+	 *    -> struct vdpa_device vdev;
+	 *    -> struct mlx5_vdpa_resources res;
+	 *       -> void __iomem *kick_addr;
+	 */
 	iowrite16(idx, ndev->mvdev.res.kick_addr);
 }
 
diff --git a/drivers/vdpa/vdpa.c b/drivers/vdpa/vdpa.c
index 8a372b51c..205cbb500 100644
--- a/drivers/vdpa/vdpa.c
+++ b/drivers/vdpa/vdpa.c
@@ -16,6 +16,13 @@
 #include <linux/mod_devicetable.h>
 #include <linux/virtio_ids.h>
 
+/*
+ * 在以下使用mdev_head:
+ *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+ *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+ *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+ *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+ */
 static LIST_HEAD(mdev_head);
 /* A global mutex that protects vdpa management device and device level operations. */
 static DECLARE_RWSEM(vdpa_dev_lock);
@@ -24,6 +31,11 @@ static DEFINE_IDA(vdpa_index_ida);
 void vdpa_set_status(struct vdpa_device *vdev, u8 status)
 {
 	down_write(&vdev->cf_lock);
+	/*
+	 * struct vdpa_device *vdev:
+	 * -> const struct vdpa_config_ops *config;
+	 *    -> void (*set_status)(struct vdpa_device *vdev, u8 status);
+	 */
 	vdev->config->set_status(vdev, status);
 	up_write(&vdev->cf_lock);
 }
@@ -154,6 +166,11 @@ static void vdpa_release_dev(struct device *d)
  * Return: Returns an error when parent/config/dma_dev is not set or fail to get
  *	   ida.
  */
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|218| <<vdpasim_create>> vdpa = __vdpa_alloc_device(NULL, ops,
+ *   - include/linux/vdpa.h|469| <<vdpa_alloc_device>> container_of((__vdpa_alloc_device( \
+ */
 struct vdpa_device *__vdpa_alloc_device(struct device *parent,
 					const struct vdpa_config_ops *config,
 					unsigned int ngroups, unsigned int nas,
@@ -245,6 +262,17 @@ static int __vdpa_register_device(struct vdpa_device *vdev, u32 nvqs)
  *
  * Return: Returns an error when fail to add device to vDPA bus
  */
+/*
+ * called by:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|737| <<ifcvf_vdpa_dev_add>> ret = _vdpa_register_device(&adapter->vdpa, vf->nr_vring);
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3991| <<mlx5_vdpa_dev_add>> err = _vdpa_register_device(&mvdev->vdev, max_vqs + 1);
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|515| <<octep_vdpa_dev_add>> ret = _vdpa_register_device(&oct_vdpa->vdpa, oct_hw->nr_vring);
+ *   - drivers/vdpa/pds/vdpa_dev.c|749| <<pds_vdpa_dev_add>> err = _vdpa_register_device(&pdsv->vdpa_dev, pdsv->num_vqs);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|448| <<vdpasim_blk_dev_add>> ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_BLK_VQ_NUM);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|510| <<vdpasim_net_dev_add>> ret = _vdpa_register_device(&simdev->vdpa, VDPASIM_NET_VQ_NUM);
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|2057| <<vdpa_dev_add>> ret = _vdpa_register_device(&dev->vdev->vdpa, dev->vq_num);
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|562| <<vp_vdpa_dev_add>> ret = _vdpa_register_device(&vp_vdpa->vdpa, vp_vdpa->queues);
+ */
 int _vdpa_register_device(struct vdpa_device *vdev, u32 nvqs)
 {
 	if (!vdev->mdev)
@@ -334,6 +362,17 @@ EXPORT_SYMBOL_GPL(vdpa_unregister_driver);
  * Return: Returns 0 on success or failure when required callback ops are not
  *         initialized.
  */
+/*
+ * called by:
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|838| <<ifcvf_probe>> ret = vdpa_mgmtdev_register(&ifcvf_mgmt_dev->mdev);
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|4108| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|604| <<octep_vdpa_setup_task>> ret = vdpa_mgmtdev_register(&mgmt_dev->mdev);
+ *   - drivers/vdpa/pds/aux_drv.c|67| <<pds_vdpa_probe>> err = vdpa_mgmtdev_register(&vdpa_aux->vdpa_mdev);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|493| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|561| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|2117| <<vduse_mgmtdev_init>> ret = vdpa_mgmtdev_register(&vduse_mgmt->mgmt_dev);
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|648| <<vp_vdpa_probe>> err = vdpa_mgmtdev_register(mgtdev);
+ */
 int vdpa_mgmtdev_register(struct vdpa_mgmt_dev *mdev)
 {
 	if (!mdev->device || !mdev->ops || !mdev->ops->dev_add || !mdev->ops->dev_del)
@@ -341,6 +380,13 @@ int vdpa_mgmtdev_register(struct vdpa_mgmt_dev *mdev)
 
 	INIT_LIST_HEAD(&mdev->list);
 	down_write(&vdpa_dev_lock);
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+	 *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_add_tail(&mdev->list, &mdev_head);
 	up_write(&vdpa_dev_lock);
 	return 0;
@@ -448,6 +494,13 @@ static struct vdpa_mgmt_dev *vdpa_mgmtdev_get_from_attr(struct nlattr **attrs)
 	if (attrs[VDPA_ATTR_MGMTDEV_BUS_NAME])
 		busname = nla_data(attrs[VDPA_ATTR_MGMTDEV_BUS_NAME]);
 
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+	 *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_for_each_entry(mdev, &mdev_head, list) {
 		if (mgmtdev_handle_match(mdev, busname, devname))
 			return mdev;
@@ -561,6 +614,13 @@ vdpa_nl_cmd_mgmtdev_get_dumpit(struct sk_buff *msg, struct netlink_callback *cb)
 	int err;
 
 	down_read(&vdpa_dev_lock);
+	/*
+	 * 在以下使用mdev_head:
+	 *   - drivers/vdpa/vdpa.c|19| <<global>> static LIST_HEAD(mdev_head);
+	 *   - drivers/vdpa/vdpa.c|344| <<vdpa_mgmtdev_register>> list_add_tail(&mdev->list, &mdev_head);
+	 *   - drivers/vdpa/vdpa.c|451| <<vdpa_mgmtdev_get_from_attr>> list_for_each_entry(mdev, &mdev_head, list) {
+	 *   - drivers/vdpa/vdpa.c|564| <<vdpa_nl_cmd_mgmtdev_get_dumpit>> list_for_each_entry(mdev, &mdev_head, list) {
+	 */
 	list_for_each_entry(mdev, &mdev_head, list) {
 		if (idx < start) {
 			idx++;
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim.c b/drivers/vdpa/vdpa_sim/vdpa_sim.c
index 8ffea8430..cdfa7d29f 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim.c
@@ -79,6 +79,10 @@ static struct vdpasim *vdpa_to_sim(struct vdpa_device *vdpa)
 	return container_of(vdpa, struct vdpasim, vdpa);
 }
 
+/*
+ * 在以下使用vdpasim_vq_notify():
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|124| <<vdpasim_queue_ready>> vq->vring.notify = vdpasim_vq_notify;
+ */
 static void vdpasim_vq_notify(struct vringh *vring)
 {
 	struct vdpasim_virtqueue *vq =
@@ -283,6 +287,12 @@ struct vdpasim *vdpasim_create(struct vdpasim_dev_attr *dev_attr,
 }
 EXPORT_SYMBOL_GPL(vdpasim_create);
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|357| <<vdpasim_kick_vq>> vdpasim_schedule_work(vdpasim);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|363| <<vdpasim_blk_work>> vdpasim_schedule_work(vdpasim);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|262| <<vdpasim_net_work>> vdpasim_schedule_work(vdpasim);
+ */
 void vdpasim_schedule_work(struct vdpasim *vdpasim)
 {
 	kthread_queue_work(vdpasim->worker, &vdpasim->work);
@@ -322,6 +332,22 @@ static u16 vdpasim_get_vq_size(struct vdpa_device *vdpa, u16 idx)
 		return VDPASIM_QUEUE_MAX;
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void vdpasim_kick_vq(struct vdpa_device *vdpa, u16 idx)
 {
 	struct vdpasim *vdpasim = vdpa_to_sim(vdpa);
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c b/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
index b137f3679..522cfde5f 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim_blk.c
@@ -490,6 +490,17 @@ static int __init vdpasim_blk_init(void)
 		return ret;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|838| <<ifcvf_probe>> ret = vdpa_mgmtdev_register(&ifcvf_mgmt_dev->mdev);
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|4108| <<mlx5v_probe>> err = vdpa_mgmtdev_register(&mgtdev->mgtdev);
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|604| <<octep_vdpa_setup_task>> ret = vdpa_mgmtdev_register(&mgmt_dev->mdev);
+	 *   - drivers/vdpa/pds/aux_drv.c|67| <<pds_vdpa_probe>> err = vdpa_mgmtdev_register(&vdpa_aux->vdpa_mdev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|493| <<vdpasim_blk_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|561| <<vdpasim_net_init>> ret = vdpa_mgmtdev_register(&mgmt_dev);
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|2117| <<vduse_mgmtdev_init>> ret = vdpa_mgmtdev_register(&vduse_mgmt->mgmt_dev);
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|648| <<vp_vdpa_probe>> err = vdpa_mgmtdev_register(mgtdev);
+	 */
 	ret = vdpa_mgmtdev_register(&mgmt_dev);
 	if (ret)
 		goto parent_err;
diff --git a/drivers/vdpa/vdpa_sim/vdpa_sim_net.c b/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
index 6caf09a19..b00f10d0e 100644
--- a/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
+++ b/drivers/vdpa/vdpa_sim/vdpa_sim_net.c
@@ -66,6 +66,13 @@ static struct vdpasim_net *sim_to_net(struct vdpasim *vdpasim)
 	return container_of(vdpasim, struct vdpasim_net, vdpasim);
 }
 
+/*
+ * called by:
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|236| <<vdpasim_net_work>> vdpasim_net_complete(txq, 0);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|244| <<vdpasim_net_work>> vdpasim_net_complete(txq, 0);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|258| <<vdpasim_net_work>> vdpasim_net_complete(txq, 0);
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|259| <<vdpasim_net_work>> vdpasim_net_complete(rxq, write);
+ */
 static void vdpasim_net_complete(struct vdpasim_virtqueue *vq, size_t len)
 {
 	/* Make sure data is wrote before advancing index */
@@ -237,6 +244,19 @@ static void vdpasim_net_work(struct vdpasim *vdpasim)
 			continue;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2388| <<mlx5_cvq_kick_handler>> err = vringh_getdesc_iotlb(&cvq->vring,
+		 *             &cvq->riov, &cvq->wiov, &cvq->head,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|123| <<vdpasim_blk_handle_req>> ret = vringh_getdesc_iotlb(&vq->vring,
+		 *             &vq->out_iov, &vq->in_iov,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|144| <<vdpasim_handle_cvq>> err = vringh_getdesc_iotlb(&cvq->vring,
+		 *             &cvq->in_iov,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|220| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&txq->vring,
+		 *             &txq->out_iov, NULL,
+		 *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|240| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&rxq->vring,
+		 *             NULL, &rxq->in_iov,
+		 */
 		err = vringh_getdesc_iotlb(&rxq->vring, NULL, &rxq->in_iov,
 					   &rxq->head, GFP_ATOMIC);
 		if (err <= 0) {
diff --git a/drivers/vdpa/vdpa_user/vduse_dev.c b/drivers/vdpa/vdpa_user/vduse_dev.c
index 7ae99691e..e77fee971 100644
--- a/drivers/vdpa/vdpa_user/vduse_dev.c
+++ b/drivers/vdpa/vdpa_user/vduse_dev.c
@@ -511,6 +511,22 @@ static void vduse_vq_kick_work(struct work_struct *work)
 	vduse_vq_kick(vq);
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void vduse_vdpa_kick_vq(struct vdpa_device *vdpa, u16 idx)
 {
 	struct vduse_dev *dev = vdpa_to_vduse(vdpa);
diff --git a/drivers/vdpa/virtio_pci/vp_vdpa.c b/drivers/vdpa/virtio_pci/vp_vdpa.c
index 163807642..3901e5ad0 100644
--- a/drivers/vdpa/virtio_pci/vp_vdpa.c
+++ b/drivers/vdpa/virtio_pci/vp_vdpa.c
@@ -360,6 +360,22 @@ static int vp_vdpa_set_vq_address(struct vdpa_device *vdpa, u16 qid,
 	return 0;
 }
 
+/*
+ * 在以下调用vdpa_config_ops->kick_vq:
+ *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+ *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+ * 在以下设置vdpa_config_ops->kick_vq:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+ *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+ *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+ */
 static void vp_vdpa_kick_vq(struct vdpa_device *vdpa, u16 qid)
 {
 	struct vp_vdpa *vp_vdpa = vdpa_to_vp(vdpa);
diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index 9ad37c012..8a47f1c5a 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -123,6 +123,18 @@ struct vhost_net_virtqueue {
 	/* Reference counting for outstanding ubufs.
 	 * Protected by vq mutex. Writers must also take device mutex. */
 	struct vhost_net_ubuf_ref *ubufs;
+	/*
+	 * 在以下使用vhost_net_virtqueue->rx_ring:
+	 *   - drivers/vhost/net.c|180| <<vhost_net_buf_produce>> rxq->tail = ptr_ring_consume_batched(nvq->rx_ring, rxq->queue,
+	 *   - drivers/vhost/net.c|189| <<vhost_net_buf_unproduce>> if (nvq->rx_ring && !vhost_net_buf_is_empty(rxq)) {
+	 *   - drivers/vhost/net.c|190| <<vhost_net_buf_unproduce>> ptr_ring_unconsume(nvq->rx_ring, rxq->queue + rxq->head,
+	 *   - drivers/vhost/net.c|1076| <<peek_head_len>> if (rvq->rx_ring)
+	 *   - drivers/vhost/net.c|1304| <<handle_rx>> if (nvq->rx_ring)
+	 *   - drivers/vhost/net.c|1479| <<vhost_net_open>> n->vqs[i].rx_ring = NULL;
+	 *   - drivers/vhost/net.c|1526| <<vhost_net_stop_vq>> nvq->rx_ring = NULL;
+	 *   - drivers/vhost/net.c|1716| <<vhost_net_set_backend>> nvq->rx_ring = get_tap_ptr_ring(sock->file);
+	 *   - drivers/vhost/net.c|1718| <<vhost_net_set_backend>> nvq->rx_ring = NULL;
+	 */
 	struct ptr_ring *rx_ring;
 	struct vhost_net_buf rxq;
 	/* Batched XDP buffs */
@@ -205,6 +217,10 @@ static int vhost_net_buf_peek_len(void *ptr)
 	return __skb_array_len_with_tag(ptr);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1077| <<peek_head_len>> return vhost_net_buf_peek(rvq);
+ */
 static int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)
 {
 	struct vhost_net_buf *rxq = &nvq->rxq;
@@ -373,6 +389,11 @@ static void vhost_zerocopy_signal_used(struct vhost_net *net,
 	}
 	while (j) {
 		add = min(UIO_MAXIOV - nvq->done_idx, j);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|376| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+		 *   - drivers/vhost/net.c|460| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+		 */
 		vhost_add_used_and_signal_n(vq->dev, vq,
 					    &vq->heads[nvq->done_idx], add);
 		nvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;
@@ -431,6 +452,18 @@ static void vhost_net_disable_vq(struct vhost_net *n,
 	struct vhost_poll *poll = n->poll + (nvq - n->vqs);
 	if (!vhost_vq_get_backend(vq))
 		return;
+	/*
+	 * 在以下调用vhost_poll_stop():
+	 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+	 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+	 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+	 *
+	 * 注释:
+	 * Stop polling a file. After this function returns, it becomes safe to drop the
+	 * file reference. You must also flush afterwards.
+	 */
 	vhost_poll_stop(poll);
 }
 
@@ -446,6 +479,17 @@ static int vhost_net_enable_vq(struct vhost_net *n,
 	if (!sock)
 		return 0;
 
+	/*
+	 * 在以下调用vhost_poll_start():
+	 *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+	 *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.h|56| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+	 *
+	 * 注释:
+	 * Start polling a file. We add ourselves to file's wait queue. The caller must
+	 * keep a reference to a file until after vhost_poll_stop is called.
+	 */
 	return vhost_poll_start(poll, sock->file);
 }
 
@@ -457,6 +501,11 @@ static void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)
 	if (!nvq->done_idx)
 		return;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|376| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+	 *   - drivers/vhost/net.c|460| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+	 */
 	vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
 	nvq->done_idx = 0;
 }
@@ -580,6 +629,25 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 	struct vhost_virtqueue *rvq = &rnvq->vq;
 	struct vhost_virtqueue *tvq = &tnvq->vq;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+	 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	int r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
 				  out_num, in_num, NULL, NULL);
 
@@ -592,6 +660,25 @@ static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
 
 		vhost_net_busy_poll(net, rvq, tvq, busyloop_intr, false);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),
 				      out_num, in_num, NULL, NULL);
 	}
@@ -790,6 +877,17 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 				goto done;
 			} else if (unlikely(err != -ENOSPC)) {
 				vhost_tx_batch(net, nvq, sock, &msg);
+				/*
+				 * called by:
+				 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *
+				 * 把vhost_virtqueue->last_avail_idx减少n
+				 */
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
@@ -811,6 +909,17 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		err = sock->ops->sendmsg(sock, &msg, len);
 		if (unlikely(err < 0)) {
 			if (err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS) {
+				/*
+				 * called by:
+				 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *
+				 * 把vhost_virtqueue->last_avail_idx减少n
+				 */
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
@@ -919,6 +1028,17 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
 					vq->heads[ubuf->desc].len = VHOST_DMA_DONE_LEN;
 			}
 			if (retry) {
+				/*
+				 * called by:
+				 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+				 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+				 *
+				 * 把vhost_virtqueue->last_avail_idx减少n
+				 */
 				vhost_discard_vq_desc(vq, 1);
 				vhost_net_enable_vq(net, vq);
 				break;
@@ -963,12 +1083,23 @@ static void handle_tx(struct vhost_net *net)
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1098| <<vhost_net_rx_peek_head_len>> int len = peek_head_len(rnvq, sk);
+ *   - drivers/vhost/net.c|1106| <<vhost_net_rx_peek_head_len>> len = peek_head_len(rnvq, sk);
+ */
 static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 {
 	struct sk_buff *head;
 	int len = 0;
 	unsigned long flags;
 
+	/*
+	 * struct vhost_net_virtqueue *rvq:
+	 * -> struct ptr_ring *rx_ring;
+	 *
+	 * 只在此处调用vhost_net_buf_peek()
+	 */
 	if (rvq->rx_ring)
 		return vhost_net_buf_peek(rvq);
 
@@ -984,6 +1115,10 @@ static int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)
 	return len;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1274| <<handle_rx>> sock_len = vhost_net_rx_peek_head_len(net, sock->sk,
+ */
 static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,
 				      bool *busyloop_intr)
 {
@@ -993,6 +1128,16 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,
 	struct vhost_virtqueue *tvq = &tnvq->vq;
 	int len = peek_head_len(rnvq, sk);
 
+	/*
+	 * 在以下使用vhost_virtqueue->busyloop_timeout:
+	 *   - drivers/vhost/net.c|577| <<vhost_net_busy_poll>> busyloop_timeout = poll_rx ?
+	 *             rvq->busyloop_timeout: tvq->busyloop_timeout;
+	 *   - drivers/vhost/net.c|638| <<vhost_net_tx_get_vq_desc>> if (r == tvq->num && tvq->busyloop_timeout) {
+	 *   - drivers/vhost/net.c|1100| <<vhost_net_rx_peek_head_len>> if (!len && rvq->busyloop_timeout) {
+	 *   - drivers/vhost/vhost.c|550| <<vhost_vq_reset>> vq->busyloop_timeout = 0;
+	 *   - drivers/vhost/vhost.c|2369| <<vhost_vring_ioctl(VHOST_SET_VRING_BUSYLOOP_TIMEOUT)>> vq->busyloop_timeout = s.num;
+	 *   - drivers/vhost/vhost.c|2373| <<vhost_vring_ioctl(VHOST_GET_VRING_BUSYLOOP_TIMEOUT)>> s.num = vq->busyloop_timeout;
+	 */
 	if (!len && rvq->busyloop_timeout) {
 		/* Flush batched heads first */
 		vhost_net_signal_used(rnvq);
@@ -1015,6 +1160,12 @@ static int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,
  * @quota       - headcount quota, 1 for big buffer
  *	returns number of buffer heads allocated, negative on error
  */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1274| <<handle_rx>> headcount = get_rx_bufs(vq,
+ *             vq->heads + nvq->done_idx, vhost_len, &in, vq_log, &log,
+ *             likely(mergeable) ? UIO_MAXIOV : 1);
+ */
 static int get_rx_bufs(struct vhost_virtqueue *vq,
 		       struct vring_used_elem *heads,
 		       int datalen,
@@ -1038,6 +1189,25 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 			r = -ENOBUFS;
 			goto err;
 		}
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		r = vhost_get_vq_desc(vq, vq->iov + seg,
 				      ARRAY_SIZE(vq->iov) - seg, &out,
 				      &in, log, log_num);
@@ -1078,10 +1248,26 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,
 	}
 	return headcount;
 err:
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+	 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+	 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+	 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+	 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+	 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+	 *
+	 * 把vhost_virtqueue->last_avail_idx减少n
+	 */
 	vhost_discard_vq_desc(vq, headcount);
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1454| <<handle_rx_kick>> handle_rx(net);
+ *   - drivers/vhost/net.c|1468| <<handle_rx_net>> handle_rx(net);
+ */
 /* Expects to be always run from workqueue - which acts as
  * read-size critical section for our kind of RCU. */
 static void handle_rx(struct vhost_net *net)
@@ -1131,12 +1317,18 @@ static void handle_rx(struct vhost_net *net)
 	mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
 
 	do {
+		/*
+		 * 只在此处调用
+		 */
 		sock_len = vhost_net_rx_peek_head_len(net, sock->sk,
 						      &busyloop_intr);
 		if (!sock_len)
 			break;
 		sock_len += sock_hlen;
 		vhost_len = sock_len + vhost_hlen;
+		/*
+		 * 只在此处调用
+		 */
 		headcount = get_rx_bufs(vq, vq->heads + nvq->done_idx,
 					vhost_len, &in, vq_log, &log,
 					likely(mergeable) ? UIO_MAXIOV : 1);
@@ -1185,6 +1377,17 @@ static void handle_rx(struct vhost_net *net)
 		if (unlikely(err != sock_len)) {
 			pr_debug("Discarded rx packet: "
 				 " len %d, expected %zd\n", err, sock_len);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *
+			 * 把vhost_virtqueue->last_avail_idx减少n
+			 */
 			vhost_discard_vq_desc(vq, headcount);
 			continue;
 		}
@@ -1209,6 +1412,17 @@ static void handle_rx(struct vhost_net *net)
 		    copy_to_iter(&num_buffers, sizeof num_buffers,
 				 &fixup) != sizeof num_buffers) {
 			vq_err(vq, "Failed num_buffers write");
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+			 *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+			 *
+			 * 把vhost_virtqueue->last_avail_idx减少n
+			 */
 			vhost_discard_vq_desc(vq, headcount);
 			goto out;
 		}
@@ -1314,6 +1528,22 @@ static int vhost_net_open(struct inode *inode, struct file *f)
 		n->vqs[i].rx_ring = NULL;
 		vhost_net_buf_init(&n->vqs[i].rxq);
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,
 		       UIO_MAXIOV + VHOST_NET_BATCH,
 		       VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,
@@ -1514,8 +1744,32 @@ static long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)
 		}
 
 		vhost_net_disable_vq(n, vq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+		 *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+		 *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+		 *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+		 *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+		 *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+		 *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+		 */
 		vhost_vq_set_backend(vq, sock);
 		vhost_net_buf_unproduce(nvq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(vq);
 		if (r)
 			goto err_used;
diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c
index 718fa4e0b..09b766ce9 100644
--- a/drivers/vhost/scsi.c
+++ b/drivers/vhost/scsi.c
@@ -39,6 +39,111 @@
 
 #include "vhost.h"
 
+/*
+ * 在以下调用for_each_sgtable_sg():
+ *   - drivers/dma-buf/dma-buf.c|788| <<mangle_sg_table>> for_each_sgtable_sg(sg_table, sg, i)
+ *   - drivers/dma-buf/heaps/system_heap.c|74| <<dup_sg_table>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|292| <<system_heap_dma_buf_release>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|406| <<system_heap_allocate>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|703| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i)
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|713| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|734| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|765| <<amdgpu_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|409| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/armada/armada_gem.c|442| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|465| <<armada_gem_prime_unmap_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/msm/msm_iommu.c|124| <<msm_iommu_pagetable_map>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/core/firmware.c|272| <<nvkm_firmware_ctor>> for_each_sgtable_sg(&fw->mem.sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2195| <<nvkm_gsp_sg_free>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2215| <<nvkm_gsp_sg>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2308| <<nvkm_gsp_radix3_sg>> for_each_sgtable_sg(&rx3->lvl2, sg, i) {
+ *   - drivers/gpu/drm/rockchip/rockchip_drm_gem.c|104| <<rockchip_gem_get_pages>> for_each_sgtable_sg(rk_obj->sgt, s, i)
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|222| <<drm_gem_shmem_test_get_pages_sgt>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|257| <<drm_gem_shmem_test_get_sg_table>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_object.c|169| <<virtio_gpu_object_shmem_init>> for_each_sgtable_sg(pages, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|296| <<vmalloc_to_sgt>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|404| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i)
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|414| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|435| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|456| <<xe_ttm_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/host1x/job.c|238| <<pin_job>> for_each_sgtable_sg(map->sgt, sg, j)
+ *   - drivers/infiniband/core/umem.c|58| <<__ib_umem_release>> for_each_sgtable_sg(&umem->sgt_append.sgt, sg, i)
+ *   - drivers/media/common/videobuf2/videobuf2-vmalloc.c|234| <<vb2_vmalloc_dmabuf_ops_attach>> for_each_sgtable_sg(sgt, sg, i) {
+ *
+ * 注释: Loop over each sg element in the given sg_table object.
+ */
+/*
+ * 2796.238623 |   114)  vhost-7-7071  |  .N... |   0.086 us    |      vhost_exceeds_weight [vhost]();
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |      vhost_scsi_get_desc [vhost_scsi]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |        vhost_get_vq_desc [vhost]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |   0.106 us    |          vhost_copy_from_user.constprop.0 [vhost]();
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |          get_indirect [vhost]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |   0.085 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |   0.252 us    |            }
+ * 2796.238624 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.085 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.256 us    |            }
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.085 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.278 us    |            }
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |               |            translate_desc [vhost]() {
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.083 us    |              vhost_iotlb_itree_first [vhost_iotlb]();
+ * 2796.238625 |   114)  vhost-7-7071  |  .N... |   0.251 us    |            }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   1.660 us    |          }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   2.044 us    |        }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   2.359 us    |      }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.082 us    |      vhost_scsi_chk_size [vhost_scsi]();
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |               |      vhost_scsi_get_req [vhost_scsi]() {
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |               |        __check_object_size() {
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |               |          __check_object_size.part.0() {
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.082 us    |            check_stack_object();
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.249 us    |          }
+ * 2796.238626 |   114)  vhost-7-7071  |  .N... |   0.408 us    |        }
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |   0.719 us    |      }
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |   0.130 us    |      vhost_scsi_get_cmd.isra.0 [vhost_scsi]();
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |      vhost_scsi_mapal [vhost_scsi]() {
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |   0.085 us    |        vhost_scsi_calc_sgls.constprop.0 [vhost_scsi]();
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |        vhost_scsi_iov_to_sgl.constprop.0 [vhost_scsi]() {
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |          get_user_pages_fast() {
+ * 2796.238627 |   114)  vhost-7-7071  |  .N... |               |            internal_get_user_pages_fast() {
+ * 2796.238628 |   114)  vhost-7-7071  |  .N... |               |              lockless_pages_from_mm() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |               |                gup_pgd_range() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.081 us    |                  pud_huge();
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |               |                  gup_pmd_range.constprop.0() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |               |                    gup_huge_pmd() {
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.141 us    |                      try_grab_compound_head();
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.314 us    |                    }
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.489 us    |                  }
+ * 2796.238628 |   114)  vhost-7-7071  |  dN... |   0.829 us    |                }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.008 us    |              }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.179 us    |            }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.337 us    |          }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.537 us    |        }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   1.887 us    |      }
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |               |      target_init_cmd [target_core_mod]() {
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   0.083 us    |        __init_swait_queue_head();
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   0.089 us    |        target_get_sess_cmd [target_core_mod]();
+ * 2796.238629 |   114)  vhost-7-7071  |  .N... |   0.434 us    |      }
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |      target_submit_prep [target_core_mod]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |   0.095 us    |        target_cmd_init_cdb [target_core_mod]();
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |   0.118 us    |        transport_lookup_cmd_lun [target_core_mod]();
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |        target_cmd_parse_cdb [target_core_mod]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |          iblock_parse_cdb [target_core_iblock]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |               |            sbc_parse_cdb [target_core_mod]() {
+ * 2796.238630 |   114)  vhost-7-7071  |  .N... |   0.083 us    |              sbc_check_dpofua [target_core_mod]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.084 us    |              sbc_check_prot [target_core_mod]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.087 us    |              iblock_get_blocks [target_core_iblock]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.086 us    |              target_cmd_size_check [target_core_mod]();
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.770 us    |            }
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   0.929 us    |          }
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   1.092 us    |        }
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |   1.828 us    |      } // target_submit_prep [target_core_mod]
+ * 2796.238631 |   114)  vhost-7-7071  |  .N... |               |      target_queue_submission [target_core_mod]() {
+ * 2796.238632 |   114)  vhost-7-7071  |  .N... |   0.091 us    |        queue_work_on();
+ * 2796.238632 |   114)  vhost-7-7071  |  .N... |   0.261 us    |      }
+ */
+
 #define VHOST_SCSI_VERSION  "v0.1"
 #define VHOST_SCSI_NAMELEN 256
 #define VHOST_SCSI_MAX_CDB_SIZE 32
@@ -64,12 +169,22 @@ struct vhost_scsi_cmd {
 	int tvc_vq_desc;
 	/* virtio-scsi initiator task attribute */
 	int tvc_task_attr;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_in_iovs:
+	 *   - drivers/vhost/scsi.c|855| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter,
+	 *             ITER_DEST, cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+	 *   - drivers/vhost/scsi.c|1562| <<vhost_scsi_handle_vq>> cmd->tvc_in_iovs = vc.in;
+	 */
 	/* virtio-scsi response incoming iovecs */
 	int tvc_in_iovs;
 	/* virtio-scsi initiator data direction */
 	enum dma_data_direction tvc_data_direction;
 	/* Expected data transfer length from virtio-scsi header */
 	u32 tvc_exp_data_len;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_tag:
+	 *   - drivers/vhost/scsi.c|1105| <<vhost_scsi_get_cmd>> cmd->tvc_tag = scsi_tag;
+	 */
 	/* The Tag from include/linux/virtio_scsi.h:struct virtio_scsi_cmd_req */
 	u64 tvc_tag;
 	/* The number of scatterlists associated with this cmd */
@@ -81,8 +196,50 @@ struct vhost_scsi_cmd {
 	const void *saved_iter_addr;
 	struct iov_iter saved_iter;
 	/* Pointer to the SGL formatted memory from virtio-scsi */
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_release_cmd_res>> __free_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|576| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|950| <<vhost_scsi_copy_sgl_to_iov>> struct scatterlist *sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1086| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|1400| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1403| <<vhost_scsi_mapal>> pr_debug( ... cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1405| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1408| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1412| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1448| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|2490| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|2541| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|2544| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	struct scatterlist *tvc_sgl;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_prot_sgl:
+	 *   - drivers/vhost/scsi.c|567| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
+	 *   - drivers/vhost/scsi.c|1072| <<vhost_scsi_get_cmd>> prot_sg = cmd->tvc_prot_sgl;
+	 *   - drivers/vhost/scsi.c|1087| <<vhost_scsi_get_cmd>> cmd->tvc_prot_sgl = prot_sg;
+	 *   - drivers/vhost/scsi.c|1352| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_prot_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1355| <<vhost_scsi_mapal>> pr_debug(... cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count);
+	 *   - drivers/vhost/scsi.c|1358| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, prot_iter,
+	 *             cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count, true);
+	 *   - drivers/vhost/scsi.c|1421| <<vhost_scsi_target_queue_cmd>> sg_prot_ptr = cmd->tvc_prot_sgl;
+	 *   - drivers/vhost/scsi.c|2461| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_prot_sgl);
+	 *   - drivers/vhost/scsi.c|2545| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_prot_sgl = kcalloc(VHOST_SCSI_PREALLOC_PROT_SGLS,
+	 *             sizeof(struct scatterlist), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2548| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_prot_sgl) {
+	 */
 	struct scatterlist *tvc_prot_sgl;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_upages:
+	 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+	 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+	 *                                          sizeof(struct page *), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+	 */
 	struct page **tvc_upages;
 	/* Pointer to response header iovec */
 	struct iovec *tvc_resp_iov;
@@ -112,8 +269,24 @@ struct vhost_scsi_nexus {
 struct vhost_scsi_tpg {
 	/* Vhost port target portal group tag for TCM */
 	u16 tport_tpgt;
+	/*
+	 * 在以下使用vhost_scsi_tpg->tv_tpg_port_count:
+	 *   - drivers/vhost/scsi.c|2927| <<vhost_scsi_port_link>> tpg->tv_tpg_port_count++;
+	 *   - drivers/vhost/scsi.c|2944| <<vhost_scsi_port_unlink>> tpg->tv_tpg_port_count--;
+	 *   - drivers/vhost/scsi.c|3046| <<vhost_scsi_drop_nexus>> if (tpg->tv_tpg_port_count != 0) {
+	 *   - drivers/vhost/scsi.c|3050| <<vhost_scsi_drop_nexus>> pr_err("Unable ... tpg->tv_tpg_port_count);
+	 */
 	/* Used to track number of TPG Port/Lun Links wrt to explict I_T Nexus shutdown */
 	int tv_tpg_port_count;
+	/*
+	 * 在以下使用vhost_scsi_tpg->tv_tpg_vhost_count:
+	 *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_set_endpoint>> if (tpg->tv_tpg_vhost_count != 0) {
+	 *   - drivers/vhost/scsi.c|2355| <<vhost_scsi_set_endpoint>> tpg->tv_tpg_vhost_count++;
+	 *   - drivers/vhost/scsi.c|2424| <<vhost_scsi_set_endpoint>> tpg->tv_tpg_vhost_count--;
+	 *   - drivers/vhost/scsi.c|2519| <<vhost_scsi_clear_endpoint>> tpg->tv_tpg_vhost_count--;
+	 *   - drivers/vhost/scsi.c|3054| <<vhost_scsi_drop_nexus>> if (tpg->tv_tpg_vhost_count != 0) {
+	 *   - drivers/vhost/scsi.c|3058| <<vhost_scsi_drop_nexus>> pr_err(... tpg->tv_tpg_vhost_count);
+	 */
 	/* Used for vhost_scsi device reference to tpg_nexus, protected by tv_tpg_mutex */
 	int tv_tpg_vhost_count;
 	/* Used for enabling T10-PI with legacy devices */
@@ -128,6 +301,13 @@ struct vhost_scsi_tpg {
 	struct vhost_scsi_tport *tport;
 	/* Returned by vhost_scsi_make_tpg() */
 	struct se_portal_group se_tpg;
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi:
+	 *   - drivers/vhost/scsi.c|2349| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = vs;
+	 *   - drivers/vhost/scsi.c|2416| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2513| <<vhost_scsi_clear_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2828| <<vhost_scsi_do_plug>> struct vhost_scsi *vs = tpg->vhost_scsi;
+	 */
 	/* Pointer back to vhost_scsi, protected by tv_tpg_mutex */
 	struct vhost_scsi *vhost_scsi;
 };
@@ -184,7 +364,28 @@ struct vhost_scsi_virtqueue {
 	 * Writers must also take dev mutex and flush under it.
 	 */
 	int inflight_idx;
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	struct vhost_scsi_cmd *scsi_cmds;
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	struct sbitmap scsi_tags;
 	int max_cmds;
 
@@ -201,20 +402,68 @@ struct vhost_scsi {
 	struct vhost_scsi_virtqueue *vqs;
 	struct vhost_scsi_inflight **old_inflight;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	struct vhost_work vs_event_work; /* evt injection work item */
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|541| <<vhost_scsi_complete_events>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	struct llist_head vs_event_list; /* evt injection queue */
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 */
 	bool vs_events_missed; /* any missed events, protected by vq->mutex */
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	int vs_events_nr; /* num of pending events, protected by vq->mutex */
 };
 
 struct vhost_scsi_tmf {
+	/*
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	struct vhost_work vwork;
+	/*
+	 * 在以下使用vhost_scsi_tmf->flush_work:
+	 *   - drivers/vhost/scsi.c|457| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+	 *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	 */
 	struct work_struct flush_work;
 	struct vhost_scsi *vhost;
 	struct vhost_scsi_virtqueue *svq;
 
 	struct se_cmd se_cmd;
+	/*
+	 * 在以下使用vhost_scsi_tmf->scsi_resp:
+	 *   - drivers/vhost/scsi.c|543| <<vhost_scsi_queue_tm_rsp>> tmf->scsi_resp = se_cmd->se_tmr_req->response;
+	 *   - drivers/vhost/scsi.c|1680| <<vhost_scsi_tmf_resp_work>> if (tmf->scsi_resp == TMR_FUNCTION_COMPLETE)
+	 */
 	u8 scsi_resp;
 	struct vhost_scsi_inflight *inflight;
 	struct iovec resp_iov;
@@ -230,6 +479,15 @@ struct vhost_scsi_ctx {
 	unsigned int out, in;
 	size_t req_size, rsp_size;
 	size_t out_size, in_size;
+	/*
+	 * 在以下使用vhost_scsi_ctx->lunp:
+	 *   - drivers/vhost/scsi.c|1273| <<vhost_scsi_get_req>> } else if (unlikely(*vc->lunp != 1)) {
+	 *   - drivers/vhost/scsi.c|1275| <<vhost_scsi_get_req>> vq_err(vq, "Illegal virtio-scsi lun: %u\n", *vc->lunp);
+	 *   - drivers/vhost/scsi.c|1349| <<vhost_scsi_handle_vq>> vc.lunp = &v_req_pi.lun[0];
+	 *   - drivers/vhost/scsi.c|1354| <<vhost_scsi_handle_vq>> vc.lunp = &v_req.lun[0];
+	 *   - drivers/vhost/scsi.c|1778| <<vhost_scsi_ctl_handle_vq>> vc.lunp = &v_req.tmf.lun[0];
+	 *   - drivers/vhost/scsi.c|1786| <<vhost_scsi_ctl_handle_vq>> vc.lunp = &v_req.an.lun[0];
+	 */
 	u8 *target, *lunp;
 	void *req;
 	struct iov_iter out_iter;
@@ -240,6 +498,12 @@ struct vhost_scsi_ctx {
  * configfs management operations.
  */
 static DEFINE_MUTEX(vhost_scsi_mutex);
+/*
+ * 在以下使用vhost_scsi_list:
+ *   - drivers/vhost/scsi.c|308| <<global>> static LIST_HEAD(vhost_scsi_list);
+ *   - drivers/vhost/scsi.c|2322| <<vhost_scsi_set_endpoint>> list_for_each_entry(tpg, &vhost_scsi_list, tv_tpg_list) {
+ *   - drivers/vhost/scsi.c|3220| <<vhost_scsi_make_tpg>> list_add_tail(&tpg->tv_tpg_list, &vhost_scsi_list);
+ */
 static LIST_HEAD(vhost_scsi_list);
 
 static void vhost_scsi_done_inflight(struct kref *kref)
@@ -250,6 +514,11 @@ static void vhost_scsi_done_inflight(struct kref *kref)
 	complete(&inflight->comp);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2143| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+ *   - drivers/vhost/scsi.c|2618| <<vhost_scsi_open>> vhost_scsi_init_inflight(vs, NULL);
+ */
 static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 				    struct vhost_scsi_inflight *old_inflight[])
 {
@@ -277,6 +546,11 @@ static void vhost_scsi_init_inflight(struct vhost_scsi *vs,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|851| <<vhost_scsi_get_cmd>> cmd->inflight = vhost_scsi_get_inflight(vq);
+ *   - drivers/vhost/scsi.c|1571| <<vhost_scsi_handle_tmf>> tmf->inflight = vhost_scsi_get_inflight(vq);
+ */
 static struct vhost_scsi_inflight *
 vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 {
@@ -290,16 +564,28 @@ vhost_scsi_get_inflight(struct vhost_virtqueue *vq)
 	return inflight;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|397| <<vhost_scsi_release_cmd_res>> vhost_scsi_put_inflight(inflight);
+ *   - drivers/vhost/scsi.c|405| <<vhost_scsi_release_tmf_res>> vhost_scsi_put_inflight(inflight);
+ */
 static void vhost_scsi_put_inflight(struct vhost_scsi_inflight *inflight)
 {
 	kref_put(&inflight->kref, vhost_scsi_done_inflight);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode = vhost_scsi_check_true()
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_demo_mode_cache = vhost_scsi_check_true()
+ */
 static int vhost_scsi_check_true(struct se_portal_group *se_tpg)
 {
 	return 1;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_wwn = vhost_scsi_get_fabric_wwn()
+ */
 static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -309,6 +595,9 @@ static char *vhost_scsi_get_fabric_wwn(struct se_portal_group *se_tpg)
 	return &tport->tport_name[0];
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_get_tag = vhost_scsi_get_tpgt()
+ */
 static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -316,6 +605,9 @@ static u16 vhost_scsi_get_tpgt(struct se_portal_group *se_tpg)
 	return tpg->tport_tpgt;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tpg_check_prot_fabric_only = vhost_scsi_check_prot_fabric_only()
+ */
 static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -324,6 +616,12 @@ static int vhost_scsi_check_prot_fabric_only(struct se_portal_group *se_tpg)
 	return tpg->tv_fabric_prot_type;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|468| <<vhost_scsi_drop_cmds>> vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
+ *   - drivers/vhost/scsi.c|882| <<vhost_scsi_complete_cmd_work>> vhost_scsi_release_cmd_res(se_cmd);
+ *   - drivers/vhost/scsi.c|1592| <<vhost_scsi_handle_vq>> vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
+ */
 static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_cmd *tv_cmd = container_of(se_cmd,
@@ -347,10 +645,25 @@ static void vhost_scsi_release_cmd_res(struct se_cmd *se_cmd)
 			put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
 	}
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1524| <<vhost_scsi_tmf_resp_work>> vhost_scsi_release_tmf_res(tmf);
+ *   - drivers/vhost/scsi.c|1538| <<vhost_scsi_tmf_flush_work>> vhost_scsi_release_tmf_res(tmf);
+ *   - drivers/vhost/scsi.c|1577| <<vhost_scsi_handle_tmf>> vhost_scsi_release_tmf_res(tmf);
+ */
 static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 {
 	struct vhost_scsi_inflight *inflight = tmf->inflight;
@@ -359,6 +672,10 @@ static void vhost_scsi_release_tmf_res(struct vhost_scsi_tmf *tmf)
 	vhost_scsi_put_inflight(inflight);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|503| <<vhost_scsi_release_cmd>> vhost_scsi_drop_cmds(svq);
+ */
 static void vhost_scsi_drop_cmds(struct vhost_scsi_virtqueue *svq)
 {
 	struct vhost_scsi_cmd *cmd, *t;
@@ -369,12 +686,20 @@ static void vhost_scsi_drop_cmds(struct vhost_scsi_virtqueue *svq)
 		vhost_scsi_release_cmd_res(&cmd->tvc_se_cmd);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.release_cmd = vhost_scsi_release_cmd()
+ */
 static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 {
 	if (se_cmd->se_cmd_flags & SCF_SCSI_TMR_CDB) {
 		struct vhost_scsi_tmf *tmf = container_of(se_cmd,
 					struct vhost_scsi_tmf, se_cmd);
 
+		/*
+		 * 在以下使用vhost_scsi_tmf->flush_work:
+		 *   - drivers/vhost/scsi.c|457| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+		 *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+		 */
 		schedule_work(&tmf->flush_work);
 	} else {
 		struct vhost_scsi_cmd *cmd = container_of(se_cmd,
@@ -383,11 +708,23 @@ static void vhost_scsi_release_cmd(struct se_cmd *se_cmd)
 					struct vhost_scsi_virtqueue, vq);
 
 		llist_add(&cmd->tvc_completion_list, &svq->completion_list);
+		/*
+		 * 在以下使用vhost_vq_work_queue():
+		 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+		 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+		 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+		 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+		 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+		 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+		 */
 		if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
 			vhost_scsi_drop_cmds(svq);
 	}
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.write_pending = vhost_scsi_write_pending()
+ */
 static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 {
 	/* Go ahead and process the write immediately */
@@ -395,18 +732,27 @@ static int vhost_scsi_write_pending(struct se_cmd *se_cmd)
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_data_in = vhost_scsi_queue_data_in()
+ */
 static int vhost_scsi_queue_data_in(struct se_cmd *se_cmd)
 {
 	transport_generic_free_cmd(se_cmd, 0);
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_status = vhost_scsi_queue_status()
+ */
 static int vhost_scsi_queue_status(struct se_cmd *se_cmd)
 {
 	transport_generic_free_cmd(se_cmd, 0);
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.queue_tm_rsp = vhost_scsi_queue_tm_rsp()
+ */
 static void vhost_scsi_queue_tm_rsp(struct se_cmd *se_cmd)
 {
 	struct vhost_scsi_tmf *tmf = container_of(se_cmd, struct vhost_scsi_tmf,
@@ -416,17 +762,36 @@ static void vhost_scsi_queue_tm_rsp(struct se_cmd *se_cmd)
 	transport_generic_free_cmd(&tmf->se_cmd, 0);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.aborted_task = vhost_scsi_aborted_task()
+ */
 static void vhost_scsi_aborted_task(struct se_cmd *se_cmd)
 {
 	return;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|545| <<vhost_scsi_complete_events>> vhost_scsi_free_evt(vs, evt);
+ */
 static void vhost_scsi_free_evt(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 {
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	vs->vs_events_nr--;
 	kfree(evt);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1719| <<vhost_scsi_send_evt>> evt = vhost_scsi_allocate_evt(vs, event, reason);
+ */
 static struct vhost_scsi_evt *
 vhost_scsi_allocate_evt(struct vhost_scsi *vs,
 		       u32 event, u32 reason)
@@ -434,6 +799,28 @@ vhost_scsi_allocate_evt(struct vhost_scsi *vs,
 	struct vhost_virtqueue *vq = &vs->vqs[VHOST_SCSI_VQ_EVT].vq;
 	struct vhost_scsi_evt *evt;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 *
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 */
 	if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
 		vs->vs_events_missed = true;
 		return NULL;
@@ -448,16 +835,38 @@ vhost_scsi_allocate_evt(struct vhost_scsi *vs,
 
 	evt->event.event = cpu_to_vhost32(vq, event);
 	evt->event.reason = cpu_to_vhost32(vq, reason);
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	vs->vs_events_nr++;
 
 	return evt;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.check_stop_free = vhost_scsi_check_stop_free()
+ */
 static int vhost_scsi_check_stop_free(struct se_cmd *se_cmd)
 {
 	return target_put_sess_cmd(se_cmd);
 }
 
+/*
+ * vhost_scsi_evt_work() or vhost_scsi_send_evt()
+ * -> vhost_scsi_complete_events()
+ *    -> vhost_scsi_do_evt_work()
+ *       -> vhost_get_vq_desc()
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|521| <<vhost_scsi_complete_events>> vhost_scsi_do_evt_work(vs, evt);
+ *
+ * vq->mutex被拿着
+ */
 static void
 vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 {
@@ -467,6 +876,21 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 	unsigned out, in;
 	int head, ret;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 */
 	if (!vhost_vq_get_backend(vq)) {
 		vs->vs_events_missed = true;
 		return;
@@ -474,6 +898,25 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 
 again:
 	vhost_disable_notify(&vs->dev, vq);
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+	 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	head = vhost_get_vq_desc(vq, vq->iov,
 			ARRAY_SIZE(vq->iov), &out, &in,
 			NULL, NULL);
@@ -488,6 +931,15 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 		return;
 	}
 
+	/*
+	 * struct virtio_scsi_event {
+	 *     __virtio32 event;
+	 *     __u8 lun[8];
+	 *     __virtio32 reason;
+	 * } __attribute__((packed));
+	 *
+	 * 对于event, 就一个desc, 不会是两个!
+	 */
 	if ((vq->iov[out].iov_len != sizeof(struct virtio_scsi_event))) {
 		vq_err(vq, "Expecting virtio_scsi_event, got %zu bytes\n",
 				vq->iov[out].iov_len);
@@ -500,14 +952,25 @@ vhost_scsi_do_evt_work(struct vhost_scsi *vs, struct vhost_scsi_evt *evt)
 		vs->vs_events_missed = false;
 	}
 
+	/*
+	 * struct virtio_scsi_event __user *eventp;
+	 */
 	eventp = vq->iov[out].iov_base;
 	ret = __copy_to_user(eventp, event, sizeof(*event));
+	/*
+	 * 这里被vq保护着, 可以log!
+	 */
 	if (!ret)
 		vhost_add_used_and_signal(&vs->dev, vq, head, 0);
 	else
 		vq_err(vq, "Faulted on vhost_scsi_send_event\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|554| <<vhost_scsi_evt_work>> vhost_scsi_complete_events(vs, false);
+ *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_send_evt>> vhost_scsi_complete_events(vs, true);
+ */
 static void vhost_scsi_complete_events(struct vhost_scsi *vs, bool drop)
 {
 	struct vhost_virtqueue *vq = &vs->vqs[VHOST_SCSI_VQ_EVT].vq;
@@ -515,25 +978,64 @@ static void vhost_scsi_complete_events(struct vhost_scsi *vs, bool drop)
 	struct llist_node *llnode;
 
 	mutex_lock(&vq->mutex);
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|541| <<vhost_scsi_complete_events>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	llnode = llist_del_all(&vs->vs_event_list);
 	llist_for_each_entry_safe(evt, t, llnode, list) {
 		if (!drop)
 			vhost_scsi_do_evt_work(vs, evt);
+		/*
+		 * 只在此处调用
+		 */
 		vhost_scsi_free_evt(vs, evt);
 	}
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi->vs_event_work:
+ *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+ *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ */
 static void vhost_scsi_evt_work(struct vhost_work *work)
 {
 	struct vhost_scsi *vs = container_of(work, struct vhost_scsi,
 					     vs_event_work);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|554| <<vhost_scsi_evt_work>> vhost_scsi_complete_events(vs, false);
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_send_evt>> vhost_scsi_complete_events(vs, true);
+	 */
 	vhost_scsi_complete_events(vs, false);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|821| <<vhost_scsi_complete_cmd_work>> if (cmd->saved_iter_addr && vhost_scsi_copy_sgl_to_iov(cmd)) {
+ */
 static int vhost_scsi_copy_sgl_to_iov(struct vhost_scsi_cmd *cmd)
 {
 	struct iov_iter *iter = &cmd->saved_iter;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_release_cmd_res>> __free_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|576| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|950| <<vhost_scsi_copy_sgl_to_iov>> struct scatterlist *sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1086| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|1400| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1403| <<vhost_scsi_mapal>> pr_debug( ... cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1405| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1408| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1412| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1448| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|2490| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|2541| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|2544| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	struct scatterlist *sg = cmd->tvc_sgl;
 	struct page *page;
 	size_t len;
@@ -558,6 +1060,10 @@ static int vhost_scsi_copy_sgl_to_iov(struct vhost_scsi_cmd *cmd)
  * This is scheduled in the vhost work queue so we are called with the owner
  * process mm and can access the vring.
  */
+/*
+ * 在以下使用vhost_scsi_complete_cmd_work():
+ *   - vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+ */
 static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(work,
@@ -578,9 +1084,16 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 			cmd, se_cmd->residual_count, se_cmd->scsi_status);
 		memset(&v_rsp, 0, sizeof(v_rsp));
 
+		/*
+		 * 只在此处调用vhost_scsi_copy_sgl_to_iov()
+		 */
 		if (cmd->saved_iter_addr && vhost_scsi_copy_sgl_to_iov(cmd)) {
 			v_rsp.response = VIRTIO_SCSI_S_BAD_TARGET;
 		} else {
+			/*
+			 * 就算是这里也看不出是好是坏!!!
+			 * 谁会看status???
+			 */
 			v_rsp.resid = cpu_to_vhost32(cmd->tvc_vq,
 						     se_cmd->residual_count);
 			/* TODO is status_qualifier field needed? */
@@ -604,10 +1117,22 @@ static void vhost_scsi_complete_cmd_work(struct vhost_work *work)
 		vhost_scsi_release_cmd_res(se_cmd);
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	if (signal)
 		vhost_signal(&svq->vs->dev, &svq->vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1496| <<vhost_scsi_handle_vq>> cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
+ */
 static struct vhost_scsi_cmd *
 vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		   unsigned char *cdb, u64 scsi_tag, u16 lun, u8 task_attr,
@@ -628,15 +1153,46 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
 		return ERR_PTR(-EIO);
 	}
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	tag = sbitmap_get(&svq->scsi_tags);
 	if (tag < 0) {
 		pr_err("Unable to obtain tag for vhost_scsi_cmd\n");
 		return ERR_PTR(-ENOMEM);
 	}
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	cmd = &svq->scsi_cmds[tag];
 	sg = cmd->tvc_sgl;
 	prot_sg = cmd->tvc_prot_sgl;
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_upages:
+	 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+	 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+	 *                                          sizeof(struct page *), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+	 */
 	pages = cmd->tvc_upages;
 	tvc_resp_iov = cmd->tvc_resp_iov;
 	memset(cmd, 0, sizeof(*cmd));
@@ -663,18 +1219,59 @@ vhost_scsi_get_cmd(struct vhost_virtqueue *vq, struct vhost_scsi_tpg *tpg,
  *
  * Returns the number of scatterlist entries used or -errno on error.
  */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1231| <<vhost_scsi_map_iov_to_sgl>> ret = vhost_scsi_map_to_sgl(cmd, iter, sg, is_prot);
+ *
+ * 猜测, 似乎是把iter所对应的page地址全部存储到cmd->tvc_upages.
+ */
 static int
 vhost_scsi_map_to_sgl(struct vhost_scsi_cmd *cmd,
 		      struct iov_iter *iter,
 		      struct scatterlist *sgl,
 		      bool is_prot)
 {
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_upages:
+	 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+	 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+	 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+	 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+	 *                                          sizeof(struct page *), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+	 */
 	struct page **pages = cmd->tvc_upages;
 	struct scatterlist *sg = sgl;
 	ssize_t bytes, mapped_bytes;
 	size_t offset, mapped_offset;
 	unsigned int npages = 0;
 
+	/*
+	 * 在以下调用iov_iter_get_pages2():
+	 *   - drivers/block/ublk_drv.c|879| <<ublk_copy_user_pages>> len = iov_iter_get_pages2(uiter,
+	 *             iter.pages, iov_iter_count(uiter), UBLK_MAX_PIN_PAGES, &off);
+	 *   - drivers/vhost/scsi.c|1135| <<vhost_scsi_map_to_sgl>> bytes = iov_iter_get_pages2(iter,
+	 *             pages, LONG_MAX, VHOST_SCSI_PREALLOC_UPAGES, &offset);
+	 *   - fs/ceph/file.c|100| <<__iter_get_bvecs>> bytes = iov_iter_get_pages2(iter, pages,
+	 *             maxsize - size, ITER_GET_BVECS_PAGES, &start);
+	 *   - fs/fuse/dev.c|786| <<fuse_copy_fill>> err = iov_iter_get_pages2(cs->iter, &page,
+	 *             PAGE_SIZE, 1, &off);
+	 *   - fs/splice.c|1465| <<iter_to_pipe>> left = iov_iter_get_pages2(from, pages, ~0UL, 16, &start);
+	 *   - net/ceph/messenger.c|992| <<ceph_msg_data_iter_next>> len = iov_iter_get_pages2(&cursor->iov_iter,
+	 *             &page, PAGE_SIZE, 1, page_offset);
+	 *   - net/core/datagram.c|642| <<zerocopy_fill_skb_from_iter>> copied = iov_iter_get_pages2(from, pages,
+	 *             length, MAX_SKB_FRAGS - frag, &start);
+	 *   - net/core/skmsg.c|328| <<sk_msg_zerocopy_from_iter>> copied = iov_iter_get_pages2(from, pages,
+	 *             bytes, maxpages, &offset);
+	 *   - net/rds/message.c|393| <<rds_message_zcopy_from_user>> copied = iov_iter_get_pages2(from, &pages,
+	 *             PAGE_SIZE, 1, &start);
+	 *   - net/tls/tls_sw.c|1382| <<tls_setup_from_iter>> copied = iov_iter_get_pages2(from, pages, length,
+	 *             maxpages, &offset);
+	 *
+	 * 目前猜测, 把iter的页面的地址全部存到pages,
+	 * 也就是cmd->tvc_upages
+	 */
 	bytes = iov_iter_get_pages2(iter, pages, LONG_MAX,
 				VHOST_SCSI_PREALLOC_UPAGES, &offset);
 	/* No pages were pinned */
@@ -730,6 +1327,11 @@ vhost_scsi_map_to_sgl(struct vhost_scsi_cmd *cmd,
 	return -EINVAL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1300| <<vhost_scsi_mapal>> sgl_count = vhost_scsi_calc_sgls(prot_iter, prot_bytes, VHOST_SCSI_PREALLOC_PROT_SGLS);
+ *   - drivers/vhost/scsi.c|1318| <<vhost_scsi_mapal>> sgl_count = vhost_scsi_calc_sgls(data_iter, data_bytes, VHOST_SCSI_PREALLOC_SGLS);
+ */
 static int
 vhost_scsi_calc_sgls(struct iov_iter *iter, size_t bytes, int max_sgls)
 {
@@ -750,6 +1352,10 @@ vhost_scsi_calc_sgls(struct iov_iter *iter, size_t bytes, int max_sgls)
 	return sgl_count;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1294| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl, cmd->tvc_sgl_count);
+ */
 static int
 vhost_scsi_copy_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 			   struct scatterlist *sg, int sg_count)
@@ -796,6 +1402,13 @@ vhost_scsi_copy_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 	return -ENOMEM;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1310| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd,
+ *             prot_iter, cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count, true);
+ *   - drivers/vhost/scsi.c|1328| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd,
+ *             data_iter, cmd->tvc_sgl, cmd->tvc_sgl_count, false);
+ */
 static int
 vhost_scsi_map_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 			  struct scatterlist *sg, int sg_count, bool is_prot)
@@ -805,6 +1418,9 @@ vhost_scsi_map_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 	int ret;
 
 	while (iov_iter_count(iter)) {
+		/*
+		 * 只在此处调用vhost_scsi_map_to_sgl()
+		 */
 		ret = vhost_scsi_map_to_sgl(cmd, iter, sg, is_prot);
 		if (ret < 0) {
 			revert_bytes = 0;
@@ -828,6 +1444,11 @@ vhost_scsi_map_iov_to_sgl(struct vhost_scsi_cmd *cmd, struct iov_iter *iter,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1771| <<vhost_scsi_handle_vq>> if (unlikely(vhost_scsi_mapal(cmd,
+ *                     prot_bytes, &prot_iter, exp_data_len, &data_iter))) {
+ */
 static int
 vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 		 size_t prot_bytes, struct iov_iter *prot_iter,
@@ -841,6 +1462,21 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 		if (sgl_count < 0)
 			return sgl_count;
 
+		/*
+		 * 在以下使用vhost_scsi_cmd->tvc_prot_sgl:
+		 *   - drivers/vhost/scsi.c|567| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_prot_sgl[i]));
+		 *   - drivers/vhost/scsi.c|1072| <<vhost_scsi_get_cmd>> prot_sg = cmd->tvc_prot_sgl;
+		 *   - drivers/vhost/scsi.c|1087| <<vhost_scsi_get_cmd>> cmd->tvc_prot_sgl = prot_sg;
+		 *   - drivers/vhost/scsi.c|1352| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_prot_sgl, sgl_count);
+		 *   - drivers/vhost/scsi.c|1355| <<vhost_scsi_mapal>> pr_debug(... cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count);
+		 *   - drivers/vhost/scsi.c|1358| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, prot_iter,
+		 *             cmd->tvc_prot_sgl, cmd->tvc_prot_sgl_count, true);
+		 *   - drivers/vhost/scsi.c|1421| <<vhost_scsi_target_queue_cmd>> sg_prot_ptr = cmd->tvc_prot_sgl;
+		 *   - drivers/vhost/scsi.c|2461| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_prot_sgl);
+		 *   - drivers/vhost/scsi.c|2545| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_prot_sgl = kcalloc(VHOST_SCSI_PREALLOC_PROT_SGLS,
+		 *             sizeof(struct scatterlist), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2548| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_prot_sgl) {
+		 */
 		sg_init_table(cmd->tvc_prot_sgl, sgl_count);
 		cmd->tvc_prot_sgl_count = sgl_count;
 		pr_debug("%s prot_sg %p prot_sgl_count %u\n", __func__,
@@ -859,6 +1495,23 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 	if (sgl_count < 0)
 		return sgl_count;
 
+	/*
+	 * 在以下使用vhost_scsi_cmd->tvc_sgl:
+	 *   - drivers/vhost/scsi.c|574| <<vhost_scsi_release_cmd_res>> __free_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|576| <<vhost_scsi_release_cmd_res>> put_page(sg_page(&tv_cmd->tvc_sgl[i]));
+	 *   - drivers/vhost/scsi.c|950| <<vhost_scsi_copy_sgl_to_iov>> struct scatterlist *sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1086| <<vhost_scsi_get_cmd>> sg = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|1101| <<vhost_scsi_get_cmd>> cmd->tvc_sgl = sg;
+	 *   - drivers/vhost/scsi.c|1400| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, sgl_count);
+	 *   - drivers/vhost/scsi.c|1403| <<vhost_scsi_mapal>> pr_debug( ... cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1405| <<vhost_scsi_mapal>> ret = vhost_scsi_map_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1408| <<vhost_scsi_mapal>> sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+	 *   - drivers/vhost/scsi.c|1412| <<vhost_scsi_mapal>> ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
+	 *   - drivers/vhost/scsi.c|1448| <<vhost_scsi_target_queue_cmd>> sg_ptr = cmd->tvc_sgl;
+	 *   - drivers/vhost/scsi.c|2490| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_sgl);
+	 *   - drivers/vhost/scsi.c|2541| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_sgl = kcalloc(VHOST_SCSI_PREALLOC_SGLS,
+	 *   - drivers/vhost/scsi.c|2544| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_sgl) {
+	 */
 	sg_init_table(cmd->tvc_sgl, sgl_count);
 	cmd->tvc_sgl_count = sgl_count;
 	pr_debug("%s data_sg %p data_sgl_count %u\n", __func__,
@@ -868,6 +1521,9 @@ vhost_scsi_mapal(struct vhost_scsi_cmd *cmd,
 					cmd->tvc_sgl_count, false);
 	if (ret == -EINVAL) {
 		sg_init_table(cmd->tvc_sgl, cmd->tvc_sgl_count);
+		/*
+		 * 只在此处调用
+		 */
 		ret = vhost_scsi_copy_iov_to_sgl(cmd, data_iter, cmd->tvc_sgl,
 						 cmd->tvc_sgl_count);
 	}
@@ -929,6 +1585,29 @@ static void vhost_scsi_target_queue_cmd(struct vhost_scsi_cmd *cmd)
 	target_submit(se_cmd);
 }
 
+/*
+ * struct virtio_scsi_ctrl_tmf_resp {
+ *     __u8 response;
+ * } __attribute__((packed));
+ * 
+ * struct virtio_scsi_ctrl_an_resp {
+ *     __virtio32 event_actual;
+ *     __u8 response;
+ * } __attribute__((packed));
+ *
+ * struct virtio_scsi_cmd_resp {
+ *     __virtio32 sense_len;           // Sense data length
+ *     __virtio32 resid;               // Residual bytes in data buffer
+ *     __virtio16 status_qualifier;    // Status qualifier
+ *     __u8 status;            // Command completion status
+ *     __u8 response;          // Response values
+ *     __u8 sense[VIRTIO_SCSI_SENSE_SIZE];
+ * } __attribute__((packed));
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|1510| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+ */
 static void
 vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 			   struct vhost_virtqueue *vq,
@@ -940,6 +1619,9 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 
 	memset(&rsp, 0, sizeof(rsp));
 	rsp.response = VIRTIO_SCSI_S_BAD_TARGET;
+	/*
+	 * 怎么可以做这种假设?????!!!!
+	 */
 	resp = vq->iov[out].iov_base;
 	ret = __copy_to_user(resp, &rsp, sizeof(rsp));
 	if (!ret)
@@ -948,12 +1630,36 @@ vhost_scsi_send_bad_target(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_cmd_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1093| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+ */
 static int
 vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_ctx *vc)
 {
 	int ret = -ENXIO;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+	 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+	 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+	 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+	 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+	 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+	 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+	 */
 	vc->head = vhost_get_vq_desc(vq, vq->iov,
 				     ARRAY_SIZE(vq->iov), &vc->out, &vc->in,
 				     NULL, NULL);
@@ -998,6 +1704,11 @@ vhost_scsi_get_desc(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 	return ret;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1432| <<vhost_scsi_handle_vq>> ret = vhost_scsi_chk_size(vq, &vc);
+ *   - drivers/vhost/scsi.c|1937| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_chk_size(vq, &vc);
+ */
 static int
 vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 {
@@ -1016,6 +1727,11 @@ vhost_scsi_chk_size(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1367| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ *   - drivers/vhost/scsi.c|1809| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+ */
 static int
 vhost_scsi_get_req(struct vhost_virtqueue *vq, struct vhost_scsi_ctx *vc,
 		   struct vhost_scsi_tpg **tpgp)
@@ -1054,6 +1770,10 @@ static u16 vhost_buf_to_lun(u8 *lun_buf)
 	return ((lun_buf[2] << 8) | lun_buf[3]) & 0x3FFF;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1826| <<vhost_scsi_handle_kick>> vhost_scsi_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1086,6 +1806,11 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	do {
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1093| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
@@ -1212,6 +1937,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 				scsi_command_size(cdb), VHOST_SCSI_MAX_CDB_SIZE);
 				goto err;
 		}
+		/*
+		 * 只在此处调用
+		 */
 		cmd = vhost_scsi_get_cmd(vq, tpg, cdb, tag, lun, task_attr,
 					 exp_data_len + prot_bytes,
 					 data_direction);
@@ -1232,6 +1960,9 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			 " %d\n", cmd, exp_data_len, prot_bytes, data_direction);
 
 		if (data_direction != DMA_NONE) {
+			/*
+			 * 只在此处调用vhost_scsi_mapal()
+			 */
 			if (unlikely(vhost_scsi_mapal(cmd, prot_bytes,
 						      &prot_iter, exp_data_len,
 						      &data_iter))) {
@@ -1259,16 +1990,35 @@ vhost_scsi_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			break;
 		else if (ret == -EIO)
 			vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		/*
+		 * 调用vhost_scsi_send_bad_target()的地方:
+		 *   - drivers/vhost/scsi.c|1510| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 */
 	} while (likely(!vhost_exceeds_weight(vq, ++c, 0)));
 out:
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_tmf_resp_work>> vhost_scsi_send_tmf_resp(tmf->vhost,
+ *                   &tmf->svq->vq, tmf->in_iovs, tmf->vq_desc, &tmf->resp_iov, resp_code);
+ *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_handle_tmf>> vhost_scsi_send_tmf_resp(vs, vq,
+ *                   vc->in, vc->head, &vq->iov[vc->out], VIRTIO_SCSI_S_FUNCTION_REJECTED);
+ */
 static void
 vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 			 int in_iovs, int vq_desc, struct iovec *resp_iov,
 			 int tmf_resp_code)
 {
+	/*
+	 * 大小一定是1个byte, 怎么都不会垮两个desc了!
+	 *
+	 * struct virtio_scsi_ctrl_tmf_resp {
+	 *     __u8 response;
+	 * } __attribute__((packed));
+	 */
 	struct virtio_scsi_ctrl_tmf_resp rsp;
 	struct iov_iter iov_iter;
 	int ret;
@@ -1279,6 +2029,9 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 
 	iov_iter_init(&iov_iter, ITER_DEST, resp_iov, in_iovs, sizeof(rsp));
 
+	/*
+	 * copy, 可以是多个desc
+	 */
 	ret = copy_to_iter(&rsp, sizeof(rsp), &iov_iter);
 	if (likely(ret == sizeof(rsp)))
 		vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
@@ -1286,6 +2039,16 @@ vhost_scsi_send_tmf_resp(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		pr_err("Faulted on virtio_scsi_ctrl_tmf_resp\n");
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf->vwork:
+ *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+ *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *
+ * 在以下使用vhost_scsi_tmf_resp_work():
+ *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *
+ * 一个work对应一个tmf cmd
+ */
 static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
@@ -1297,25 +2060,71 @@ static void vhost_scsi_tmf_resp_work(struct vhost_work *work)
 	else
 		resp_code = VIRTIO_SCSI_S_FUNCTION_REJECTED;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_tmf_resp_work>> vhost_scsi_send_tmf_resp(tmf->vhost,
+	 *                   &tmf->svq->vq, tmf->in_iovs, tmf->vq_desc, &tmf->resp_iov, resp_code);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_handle_tmf>> vhost_scsi_send_tmf_resp(vs, vq,
+	 *                   vc->in, vc->head, &vq->iov[vc->out], VIRTIO_SCSI_S_FUNCTION_REJECTED);
+	 */
 	vhost_scsi_send_tmf_resp(tmf->vhost, &tmf->svq->vq, tmf->in_iovs,
 				 tmf->vq_desc, &tmf->resp_iov, resp_code);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|1524| <<vhost_scsi_tmf_resp_work>> vhost_scsi_release_tmf_res(tmf);
+	 *   - drivers/vhost/scsi.c|1538| <<vhost_scsi_tmf_flush_work>> vhost_scsi_release_tmf_res(tmf);
+	 *   - drivers/vhost/scsi.c|1577| <<vhost_scsi_handle_tmf>> vhost_scsi_release_tmf_res(tmf);
+	 */
 	vhost_scsi_release_tmf_res(tmf);
 }
 
+/*
+ * 在以下使用vhost_scsi_tmf_flush_work():
+ *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+ */
 static void vhost_scsi_tmf_flush_work(struct work_struct *work)
 {
 	struct vhost_scsi_tmf *tmf = container_of(work, struct vhost_scsi_tmf,
 						 flush_work);
 	struct vhost_virtqueue *vq = &tmf->svq->vq;
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1432| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/net.c|1623| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/scsi.c|1639| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+	 *   - drivers/vhost/scsi.c|2044| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+	 *   - drivers/vhost/test.c|165| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/vhost.c|1050| <<vhost_dev_stop>> vhost_dev_flush(dev);
+	 *   - drivers/vhost/vhost.c|2147| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+	 *   - drivers/vhost/vhost.h|61| <<vhost_vring_ioctl>> void vhost_dev_flush(struct vhost_dev *dev);
+	 *   - drivers/vhost/vsock.c|751| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+	 */
 	/*
 	 * Make sure we have sent responses for other commands before we
 	 * send our response.
 	 */
 	vhost_dev_flush(vq->dev);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	if (!vhost_vq_work_queue(vq, &tmf->vwork))
 		vhost_scsi_release_tmf_res(tmf);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1700| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_handle_tmf(vs, tpg, vq, &v_req.tmf, &vc);
+ */
 static void
 vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 		      struct vhost_virtqueue *vq,
@@ -1335,19 +2144,57 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 		goto send_reject;
 	}
 
+	/*
+	 * struct virtio_scsi_ctrl_tmf_req {
+	 *     __virtio32 type;
+	 *     __virtio32 subtype;
+	 *     __u8 lun[8];
+	 *     __virtio64 tag;
+	 * } __attribute__((packed));
+	 */
 	tmf = kzalloc(sizeof(*tmf), GFP_KERNEL);
 	if (!tmf)
 		goto send_reject;
 
+	/*
+	 * 在以下使用vhost_scsi_tmf->flush_work:
+	 *   - drivers/vhost/scsi.c|457| <<vhost_scsi_release_cmd>> schedule_work(&tmf->flush_work);
+	 *   - drivers/vhost/scsi.c|1660| <<vhost_scsi_handle_tmf>> INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	 */
 	INIT_WORK(&tmf->flush_work, vhost_scsi_tmf_flush_work);
+	/*
+	 * 在以下使用vhost_scsi_tmf->vwork:
+	 *   - drivers/vhost/scsi.c|1537| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|1565| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+	 */
 	vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
 	tmf->vhost = vs;
 	tmf->svq = svq;
+	/*
+	 * 大小一定是1个byte, 怎么都不会垮两个desc了!
+	 *
+	 * struct virtio_scsi_ctrl_tmf_resp { 
+	 *     __u8 response;
+	 * } __attribute__((packed));
+	 *
+	 * 这里假设一定是一个????
+	 */
 	tmf->resp_iov = vq->iov[vc->out];
 	tmf->vq_desc = vc->head;
 	tmf->in_iovs = vc->in;
 	tmf->inflight = vhost_scsi_get_inflight(vq);
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1667| <<srpt_handle_tsk_mgmt>> rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL,
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1449| <<efct_scsi_recv_tmf>> rc = target_submit_tmr(&ocp->cmd, se_sess, NULL, lun, ocp, tmr_func,
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|2799| <<ibmvscsis_parse_task>> rc = target_submit_tmr(&cmd->se_cmd, nexus->se_sess, NULL,
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|617| <<tcm_qla2xxx_handle_tmr>> return target_submit_tmr(se_cmd, sess->se_sess, NULL, lun, mcmd,
+	 *   - drivers/target/loopback/tcm_loop.c|216| <<tcm_loop_issue_tmr>> rc = target_submit_tmr(se_cmd, se_sess, tl_cmd->tl_sense_buf, lun,
+	 *   - drivers/target/tcm_fc/tfc_cmd.c|365| <<ft_send_tm>> rc = target_submit_tmr(&cmd->se_cmd, cmd->sess->se_sess,
+	 *   - drivers/vhost/scsi.c|1701| <<vhost_scsi_handle_tmf>> if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
+	 *   - drivers/xen/xen-scsiback.c|626| <<scsiback_device_action>> rc = target_submit_tmr(&pending_req->se_cmd, nexus->tvn_se_sess,
+	 */
 	if (target_submit_tmr(&tmf->se_cmd, tpg->tpg_nexus->tvn_se_sess, NULL,
 			      vhost_buf_to_lun(vtmf->lun), NULL,
 			      TMR_LUN_RESET, GFP_KERNEL, 0,
@@ -1359,10 +2206,21 @@ vhost_scsi_handle_tmf(struct vhost_scsi *vs, struct vhost_scsi_tpg *tpg,
 	return;
 
 send_reject:
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|1522| <<vhost_scsi_tmf_resp_work>> vhost_scsi_send_tmf_resp(tmf->vhost,
+	 *                   &tmf->svq->vq, tmf->in_iovs, tmf->vq_desc, &tmf->resp_iov, resp_code);
+	 *   - drivers/vhost/scsi.c|1584| <<vhost_scsi_handle_tmf>> vhost_scsi_send_tmf_resp(vs, vq,
+	 *                   vc->in, vc->head, &vq->iov[vc->out], VIRTIO_SCSI_S_FUNCTION_REJECTED);
+	 */
 	vhost_scsi_send_tmf_resp(vs, vq, vc->in, vc->head, &vq->iov[vc->out],
 				 VIRTIO_SCSI_S_FUNCTION_REJECTED);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1702| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_an_resp(vs, vq, &vc);
+ */
 static void
 vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 			struct vhost_virtqueue *vq,
@@ -1378,6 +2236,16 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 
 	iov_iter_init(&iov_iter, ITER_DEST, &vq->iov[vc->out], vc->in, sizeof(rsp));
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|969| <<handle_tx_zerocopy>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+	 *   - drivers/vhost/scsi.c|529| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+	 *   - drivers/vhost/scsi.c|969| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+	 *   - drivers/vhost/scsi.c|1331| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+	 *   - drivers/vhost/scsi.c|1430| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+	 *   - drivers/vhost/test.c|87| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+	 */
+
 	ret = copy_to_iter(&rsp, sizeof(rsp), &iov_iter);
 	if (likely(ret == sizeof(rsp)))
 		vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
@@ -1385,6 +2253,10 @@ vhost_scsi_send_an_resp(struct vhost_scsi *vs,
 		pr_err("Faulted on virtio_scsi_ctrl_an_resp\n");
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1726| <<vhost_scsi_ctl_handle_kick>> vhost_scsi_ctl_handle_vq(vs, vq);
+ */
 static void
 vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 {
@@ -1411,10 +2283,28 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 	vhost_disable_notify(&vs->dev, vq);
 
 	do {
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1093| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *   - drivers/vhost/scsi.c|1418| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_desc(vs, vq, &vc);
+		 *
+		 * 可以假设log只有一个吗???
+		 */
 		ret = vhost_scsi_get_desc(vs, vq, &vc);
 		if (ret)
 			goto err;
 
+		/*
+		 * struct vhost_scsi_ctx {
+		 *     int head;
+		 *     unsigned int out, in;
+		 *     size_t req_size, rsp_size;
+		 *     size_t out_size, in_size;
+		 *     u8 *target, *lunp;
+		 *     void *req;
+		 *     struct iov_iter out_iter;
+		 * };
+		 */
 		/*
 		 * Get the request type first in order to setup
 		 * other parameters dependent on the type.
@@ -1470,10 +2360,23 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 		vc.req += typ_size;
 		vc.req_size -= typ_size;
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/scsi.c|1367| <<vhost_scsi_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+		 *   - drivers/vhost/scsi.c|1809| <<vhost_scsi_ctl_handle_vq>> ret = vhost_scsi_get_req(vq, &vc, &tpg);
+		 *
+		 * 有ret err立刻跳下去!!!
+		 */
 		ret = vhost_scsi_get_req(vq, &vc, &tpg);
 		if (ret)
 			goto err;
 
+		/*
+		 * TMF
+		 * - virtio_scsi_ctrl_tmf_resp
+		 * AN
+		 * - virtio_scsi_ctrl_an_resp
+		 */
 		if (v_req.type == VIRTIO_SCSI_T_TMF)
 			vhost_scsi_handle_tmf(vs, tpg, vq, &v_req.tmf, &vc);
 		else
@@ -1489,11 +2392,20 @@ vhost_scsi_ctl_handle_vq(struct vhost_scsi *vs, struct vhost_virtqueue *vq)
 			break;
 		else if (ret == -EIO)
 			vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		/*
+		 * 调用vhost_scsi_send_bad_target()的地方:
+		 *   - drivers/vhost/scsi.c|1510| <<vhost_scsi_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 *   - drivers/vhost/scsi.c|1827| <<vhost_scsi_ctl_handle_vq>> vhost_scsi_send_bad_target(vs, vq, vc.head, vc.out);
+		 */
 	} while (likely(!vhost_exceeds_weight(vq, ++c, 0)));
 out:
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * 在以下使用vhost_scsi_ctl_handle_kick():
+ *   - drivers/vhost/scsi.c|2271| <<vhost_scsi_open>> vs->vqs[VHOST_SCSI_VQ_CTL].vq.handle_kick = vhost_scsi_ctl_handle_kick;
+ */
 static void vhost_scsi_ctl_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1504,6 +2416,11 @@ static void vhost_scsi_ctl_handle_kick(struct vhost_work *work)
 	vhost_scsi_ctl_handle_vq(vs, vq);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1594| <<vhost_scsi_evt_handle_kick>> vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
+ *   - drivers/vhost/scsi.c|2222| <<vhost_scsi_do_plug>> vhost_scsi_send_evt(vs, vq, tpg, lun, VIRTIO_SCSI_T_TRANSPORT_RESET, reason);
+ */
 static void
 vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		    struct vhost_scsi_tpg *tpg, struct se_lun *lun,
@@ -1511,6 +2428,9 @@ vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 {
 	struct vhost_scsi_evt *evt;
 
+	/*
+	 * 只在此处调用
+	 */
 	evt = vhost_scsi_allocate_evt(vs, event, reason);
 	if (!evt)
 		return;
@@ -1528,11 +2448,37 @@ vhost_scsi_send_evt(struct vhost_scsi *vs, struct vhost_virtqueue *vq,
 		evt->event.lun[3] = lun->unpacked_lun & 0xFF;
 	}
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_list:
+	 *   - drivers/vhost/scsi.c|541| <<vhost_scsi_complete_events>> llnode = llist_del_all(&vs->vs_event_list);
+	 *   - drivers/vhost/scsi.c|1578| <<vhost_scsi_send_evt>> llist_add(&evt->list, &vs->vs_event_list);
+	 */
 	llist_add(&evt->list, &vs->vs_event_list);
+	/*
+	 * 在以下使用vhost_vq_work_queue():
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *
+	 * 在以下调用vhost_scsi_complete_events():
+	 *   - drivers/vhost/scsi.c|554| <<vhost_scsi_evt_work>> vhost_scsi_complete_events(vs, false);
+	 *   - drivers/vhost/scsi.c|1580| <<vhost_scsi_send_evt>> vhost_scsi_complete_events(vs, true);
+	 *
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
 		vhost_scsi_complete_events(vs, true);
 }
 
+/*
+ * 在以下使用vhost_scsi_evt_handle_kick():
+ *   - drivers/vhost/scsi.c|2136| <<vhost_scsi_open>> vs->vqs[VHOST_SCSI_VQ_EVT].vq.handle_kick = vhost_scsi_evt_handle_kick;
+ */
 static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 {
 	struct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,
@@ -1543,6 +2489,25 @@ static void vhost_scsi_evt_handle_kick(struct vhost_work *work)
 	if (!vhost_vq_get_backend(vq))
 		goto out;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_missed:
+	 *   - drivers/vhost/scsi.c|483| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|490| <<vhost_scsi_allocate_evt>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|530| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|559| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|565| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|579| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = true;
+	 *   - drivers/vhost/scsi.c|583| <<vhost_scsi_do_evt_work>> if (vs->vs_events_missed) {
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_do_evt_work>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|1693| <<vhost_scsi_evt_handle_kick>> if (vs->vs_events_missed)
+	 *   - drivers/vhost/scsi.c|2131| <<vhost_scsi_open>> vs->vs_events_missed = false;
+	 *   - drivers/vhost/scsi.c|2221| <<vhost_scsi_ioctl(VHOST_SCSI_SET_EVENTS_MISSED)>> vs->vs_events_missed = events_missed;
+	 *   - drivers/vhost/scsi.c|2226| <<vhost_scsi_ioctl(VHOST_SCSI_GET_EVENTS_MISSED)>> events_missed = vs->vs_events_missed;
+	 *
+	 * 在以下使用vhost_scsi_send_evt():
+	 *   - drivers/vhost/scsi.c|1594| <<vhost_scsi_evt_handle_kick>> vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
+	 *   - drivers/vhost/scsi.c|2222| <<vhost_scsi_do_plug>> vhost_scsi_send_evt(vs, vq, tpg, lun,
+	 */
 	if (vs->vs_events_missed)
 		vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
 				    0);
@@ -1559,11 +2524,22 @@ static void vhost_scsi_handle_kick(struct vhost_work *work)
 	vhost_scsi_handle_vq(vs, vq);
 }
 
+/*
+ * caled by:
+ *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+ *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+ */
 /* Callers must hold dev mutex */
 static void vhost_scsi_flush(struct vhost_scsi *vs)
 {
 	int i;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2143| <<vhost_scsi_flush>> vhost_scsi_init_inflight(vs, vs->old_inflight);
+	 *   - drivers/vhost/scsi.c|2618| <<vhost_scsi_open>> vhost_scsi_init_inflight(vs, NULL);
+	 */
 	/* Init new inflight and remember the old inflight */
 	vhost_scsi_init_inflight(vs, vs->old_inflight);
 
@@ -1575,6 +2551,18 @@ static void vhost_scsi_flush(struct vhost_scsi *vs)
 	for (i = 0; i < vs->dev.nvqs; i++)
 		kref_put(&vs->old_inflight[i]->kref, vhost_scsi_done_inflight);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1432| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/net.c|1623| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/scsi.c|1639| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+	 *   - drivers/vhost/scsi.c|2044| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+	 *   - drivers/vhost/test.c|165| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+	 *   - drivers/vhost/vhost.c|1050| <<vhost_dev_stop>> vhost_dev_flush(dev);
+	 *   - drivers/vhost/vhost.c|2147| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+	 *   - drivers/vhost/vhost.h|61| <<vhost_vring_ioctl>> void vhost_dev_flush(struct vhost_dev *dev);
+	 *   - drivers/vhost/vsock.c|751| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+	 */
 	/* Flush both the vhost poll and vhost work */
 	vhost_dev_flush(&vs->dev);
 
@@ -1583,6 +2571,12 @@ static void vhost_scsi_flush(struct vhost_scsi *vs)
 		wait_for_completion(&vs->old_inflight[i]->comp);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2328| <<vhost_scsi_setup_vq_cmds>> vhost_scsi_destroy_vq_cmds(vq);
+ *   - drivers/vhost/scsi.c|2474| <<vhost_scsi_set_endpoint>> vhost_scsi_destroy_vq_cmds(&vs->vqs[i].vq);
+ *   - drivers/vhost/scsi.c|2562| <<vhost_scsi_clear_endpoint>> vhost_scsi_destroy_vq_cmds(vq);
+ */
 static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1590,6 +2584,18 @@ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 	struct vhost_scsi_cmd *tv_cmd;
 	unsigned int i;
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	if (!svq->scsi_cmds)
 		return;
 
@@ -1602,11 +2608,24 @@ static void vhost_scsi_destroy_vq_cmds(struct vhost_virtqueue *vq)
 		kfree(tv_cmd->tvc_resp_iov);
 	}
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	sbitmap_free(&svq->scsi_tags);
 	kfree(svq->scsi_cmds);
 	svq->scsi_cmds = NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2419| <<vhost_scsi_set_endpoint>> ret = vhost_scsi_setup_vq_cmds(vq, vq->num);
+ */
 static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 {
 	struct vhost_scsi_virtqueue *svq = container_of(vq,
@@ -1614,9 +2633,30 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 	struct vhost_scsi_cmd *tv_cmd;
 	unsigned int i;
 
+	/*
+	 * 在以下使用vhost_scsi_virtqueue->scsi_cmds:
+	 *   - drivers/vhost/scsi.c|964| <<vhost_scsi_get_cmd>> cmd = &svq->scsi_cmds[tag];
+	 *   - drivers/vhost/scsi.c|2249| <<vhost_scsi_destroy_vq_cmds>> if (!svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2253| <<vhost_scsi_destroy_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 *   - drivers/vhost/scsi.c|2262| <<vhost_scsi_destroy_vq_cmds>> kfree(svq->scsi_cmds);
+	 *   - drivers/vhost/scsi.c|2263| <<vhost_scsi_destroy_vq_cmds>> svq->scsi_cmds = NULL;
+	 *   - drivers/vhost/scsi.c|2277| <<vhost_scsi_setup_vq_cmds>> if (svq->scsi_cmds)
+	 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_setup_vq_cmds>> svq->scsi_cmds = kcalloc(max_cmds, sizeof(*tv_cmd), GFP_KERNEL);
+	 *   - drivers/vhost/scsi.c|2286| <<vhost_scsi_setup_vq_cmds>> if (!svq->scsi_cmds) {
+	 *   - drivers/vhost/scsi.c|2292| <<vhost_scsi_setup_vq_cmds>> tv_cmd = &svq->scsi_cmds[i];
+	 */
 	if (svq->scsi_cmds)
 		return 0;
 
+	/*
+	 * 在以下使用vhost_scso_virtqueue->scsi_tags:
+	 *   - drivers/vhost/scsi.c|585| <<vhost_scsi_release_cmd_res>> sbitmap_clear_bit(&svq->scsi_tags, se_cmd->map_tag);
+	 *   - drivers/vhost/scsi.c|1067| <<vhost_scsi_get_cmd>> tag = sbitmap_get(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2496| <<vhost_scsi_destroy_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_setup_vq_cmds>> if (sbitmap_init_node(&svq->scsi_tags, max_cmds,
+	 *             -1, GFP_KERNEL, NUMA_NO_NODE, false, true))
+	 *   - drivers/vhost/scsi.c|2534| <<vhost_scsi_setup_vq_cmds>> sbitmap_free(&svq->scsi_tags);
+	 */
 	if (sbitmap_init_node(&svq->scsi_tags, max_cmds, -1, GFP_KERNEL,
 			      NUMA_NO_NODE, false, true))
 		return -ENOMEM;
@@ -1639,6 +2679,16 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
 			goto out;
 		}
 
+		/*
+		 * 在以下使用vhost_scsi_cmd->tvc_upages:
+		 *   - drivers/vhost/scsi.c|1063| <<vhost_scsi_get_cmd>> pages = cmd->tvc_upages;
+		 *   - drivers/vhost/scsi.c|1068| <<vhost_scsi_get_cmd>> cmd->tvc_upages = pages;
+		 *   - drivers/vhost/scsi.c|1095| <<vhost_scsi_map_to_sgl>> struct page **pages = cmd->tvc_upages;
+		 *   - drivers/vhost/scsi.c|2371| <<vhost_scsi_destroy_vq_cmds>> kfree(tv_cmd->tvc_upages);
+		 *   - drivers/vhost/scsi.c|2428| <<vhost_scsi_setup_vq_cmds>> tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
+		 *                                          sizeof(struct page *), GFP_KERNEL);
+		 *   - drivers/vhost/scsi.c|2431| <<vhost_scsi_setup_vq_cmds>> if (!tv_cmd->tvc_upages) {
+		 */
 		tv_cmd->tvc_upages = kcalloc(VHOST_SCSI_PREALLOC_UPAGES,
 					     sizeof(struct page *),
 					     GFP_KERNEL);
@@ -1676,6 +2726,10 @@ static int vhost_scsi_setup_vq_cmds(struct vhost_virtqueue *vq, int max_cmds)
  *  The lock nesting rule is:
  *    vs->dev.mutex -> vhost_scsi_mutex -> tpg->tv_tpg_mutex -> vq->mutex
  */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2772| <<vhost_scsi_ioctl(VHOST_SCSI_SET_ENDPOINT)>> return vhost_scsi_set_endpoint(vs, &backend);
+ */
 static int
 vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 			struct vhost_scsi_target *t)
@@ -1766,9 +2820,37 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 		}
 
 		for (i = 0; i < vs->dev.nvqs; i++) {
+			/*
+			 * struct vhost_scsi *vs:
+			 * -> struct vhost_scsi_virtqueue *vqs;
+			 */
 			vq = &vs->vqs[i].vq;
 			mutex_lock(&vq->mutex);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+			 *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+			 *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+			 *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+			 *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+			 *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+			 *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+			 *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+			 */
 			vhost_vq_set_backend(vq, vs_tpg);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+			 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+			 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+			 */
 			vhost_vq_init_access(vq);
 			mutex_unlock(&vq->mutex);
 		}
@@ -1781,6 +2863,12 @@ vhost_scsi_set_endpoint(struct vhost_scsi *vs,
 	 * Act as synchronize_rcu to make sure access to
 	 * old vs->vs_tpg is finished.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 */
 	vhost_scsi_flush(vs);
 	kfree(vs->vs_tpg);
 	vs->vs_tpg = vs_tpg;
@@ -1863,9 +2951,31 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 	for (i = 0; i < vs->dev.nvqs; i++) {
 		vq = &vs->vqs[i].vq;
 		mutex_lock(&vq->mutex);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+		 *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+		 *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+		 *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+		 *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+		 *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+		 *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+		 *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+		 */
 		vhost_vq_set_backend(vq, NULL);
 		mutex_unlock(&vq->mutex);
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 */
 	/* Make sure cmds are not running before tearing them down. */
 	vhost_scsi_flush(vs);
 
@@ -1901,9 +3011,23 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 	 * Act as synchronize_rcu to make sure access to
 	 * old vs->vs_tpg is finished.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|2367| <<vhost_scsi_set_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2453| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 *   - drivers/vhost/scsi.c|2487| <<vhost_scsi_clear_endpoint>> vhost_scsi_flush(vs);
+	 */
 	vhost_scsi_flush(vs);
 	kfree(vs->vs_tpg);
 	vs->vs_tpg = NULL;
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	WARN_ON(vs->vs_events_nr);
 	mutex_unlock(&vs->dev.mutex);
 	return 0;
@@ -1913,6 +3037,10 @@ vhost_scsi_clear_endpoint(struct vhost_scsi *vs,
 	return ret;
 }
 
+/*
+ * 处理VHOST_SET_FEATURES:
+ *   - drivers/vhost/scsi.c|2806| <<vhost_scsi_ioctl(VHOST_SET_FEATURES)>> return vhost_scsi_set_features(vs, features);
+ */
 static int vhost_scsi_set_features(struct vhost_scsi *vs, u64 features)
 {
 	struct vhost_virtqueue *vq;
@@ -1929,6 +3057,15 @@ static int vhost_scsi_set_features(struct vhost_scsi *vs, u64 features)
 	}
 
 	for (i = 0; i < vs->dev.nvqs; i++) {
+		/*
+		 * struct vhost_scsi *vs:
+		 * -> struct vhost_scsi_virtqueue *vqs;
+		 *    -> struct vhost_scsi_cmd *scsi_cmds;
+		 *
+		 * 在以下分配:
+		 *   - drivers/vhost/scsi.c|2673| <<vhost_scsi_open>> vs->vqs = kmalloc_array(nvqs,
+		 *             sizeof(*vs->vqs), GFP_KERNEL | __GFP_ZERO);
+		 */
 		vq = &vs->vqs[i].vq;
 		mutex_lock(&vq->mutex);
 		vq->acked_features = features;
@@ -1973,8 +3110,21 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 	if (!vqs)
 		goto err_local_vqs;
 
+	/*
+	 * 在以下使用vhost_scsi->vs_event_work:
+	 *   - drivers/vhost/scsi.c|1623| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/scsi.c|2067| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+	 */
 	vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
 
+	/*
+	 * 在以下使用vhost_scsi->vs_events_nr:
+	 *   - drivers/vhost/scsi.c|471| <<vhost_scsi_free_evt>> vs->vs_events_nr--;
+	 *   - drivers/vhost/scsi.c|482| <<vhost_scsi_allocate_evt>> if (vs->vs_events_nr > VHOST_SCSI_MAX_EVENT) {
+	 *   - drivers/vhost/scsi.c|496| <<vhost_scsi_allocate_evt>> vs->vs_events_nr++;
+	 *   - drivers/vhost/scsi.c|2054| <<vhost_scsi_clear_endpoint>> WARN_ON(vs->vs_events_nr);
+	 *   - drivers/vhost/scsi.c|2130| <<vhost_scsi_open>> vs->vs_events_nr = 0;
+	 */
 	vs->vs_events_nr = 0;
 	vs->vs_events_missed = false;
 
@@ -1992,6 +3142,22 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)
 				vhost_scsi_complete_cmd_work);
 		svq->vq.handle_kick = vhost_scsi_handle_kick;
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vs->dev, vqs, nvqs, UIO_MAXIOV,
 		       VHOST_SCSI_WEIGHT, 0, true, NULL);
 
@@ -2145,11 +3311,51 @@ static char *vhost_scsi_dump_proto_id(struct vhost_scsi_tport *tport)
 	return "Unknown";
 }
 
+/*
+ * add:
+ *
+ * vhost_scsi_do_plug
+ * vhost_scsi_port_link
+ * target_fabric_port_link
+ * configfs_symlink
+ * vfs_symlink
+ * do_symlinkat
+ * __x64_sys_symlink
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * del:
+ *
+ * vhost_scsi_do_plug
+ * vhost_scsi_port_unlink
+ * target_fabric_port_unlink
+ * configfs_unlink
+ * vfs_unlink
+ * do_unlinkat
+ * __x64_sys_unlink
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * QEMU是下面的代码:
+ *   - hw/scsi/virtio-scsi.c|1148| <<virtio_scsi_hotplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+ *   - hw/scsi/virtio-scsi.c|1185| <<virtio_scsi_hotunplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+ *
+ * called by:
+ *   - drivers/vhost/scsi.c|2230| <<vhost_scsi_hotplug>> vhost_scsi_do_plug(tpg, lun, true);
+ *   - drivers/vhost/scsi.c|2235| <<vhost_scsi_hotunplug>> vhost_scsi_do_plug(tpg, lun, false);
+ */
 static void
 vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 		  struct se_lun *lun, bool plug)
 {
 
+	/*
+	 * 在以下使用vhost_scsi_tpg->vhost_scsi:
+	 *   - drivers/vhost/scsi.c|2349| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = vs;
+	 *   - drivers/vhost/scsi.c|2416| <<vhost_scsi_set_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2513| <<vhost_scsi_clear_endpoint>> tpg->vhost_scsi = NULL;
+	 *   - drivers/vhost/scsi.c|2828| <<vhost_scsi_do_plug>> struct vhost_scsi *vs = tpg->vhost_scsi;
+	 */
 	struct vhost_scsi *vs = tpg->vhost_scsi;
 	struct vhost_virtqueue *vq;
 	u32 reason;
@@ -2171,6 +3377,15 @@ vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 	if (!vhost_vq_get_backend(vq))
 		goto unlock;
 
+	/*
+	 * QEMU是下面的代码:
+	 *   - hw/scsi/virtio-scsi.c|1148| <<virtio_scsi_hotplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+	 *   - hw/scsi/virtio-scsi.c|1185| <<virtio_scsi_hotunplug>> if (virtio_vdev_has_feature(vdev, VIRTIO_SCSI_F_HOTPLUG)) {
+	 *
+	 * 在以下使用vhost_scsi_send_evt():
+	 *   - drivers/vhost/scsi.c|1594| <<vhost_scsi_evt_handle_kick>> vhost_scsi_send_evt(vs, vq, NULL, NULL, VIRTIO_SCSI_T_NO_EVENT,
+	 *   - drivers/vhost/scsi.c|2222| <<vhost_scsi_do_plug>> vhost_scsi_send_evt(vs, vq, tpg, lun,
+	 */
 	if (vhost_has_feature(vq, VIRTIO_SCSI_F_HOTPLUG))
 		vhost_scsi_send_evt(vs, vq, tpg, lun,
 				   VIRTIO_SCSI_T_TRANSPORT_RESET, reason);
@@ -2178,6 +3393,10 @@ vhost_scsi_do_plug(struct vhost_scsi_tpg *tpg,
 	mutex_unlock(&vq->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2246| <<vhost_scsi_port_link>> vhost_scsi_hotplug(tpg, lun);
+ */
 static void vhost_scsi_hotplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 {
 	vhost_scsi_do_plug(tpg, lun, true);
@@ -2188,6 +3407,9 @@ static void vhost_scsi_hotunplug(struct vhost_scsi_tpg *tpg, struct se_lun *lun)
 	vhost_scsi_do_plug(tpg, lun, false);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_post_link = vhost_scsi_port_link()
+ */
 static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 			       struct se_lun *lun)
 {
@@ -2202,6 +3424,9 @@ static int vhost_scsi_port_link(struct se_portal_group *se_tpg,
 	return 0;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_pre_unlink = vhost_scsi_port_unlink()
+ */
 static void vhost_scsi_port_unlink(struct se_portal_group *se_tpg,
 				  struct se_lun *lun)
 {
@@ -2248,11 +3473,18 @@ static ssize_t vhost_scsi_tpg_attrib_fabric_prot_type_show(
 
 CONFIGFS_ATTR(vhost_scsi_tpg_attrib_, fabric_prot_type);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_attrib_attrs = vhost_scsi_tpg_attrib_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrib_attrs[] = {
 	&vhost_scsi_tpg_attrib_attr_fabric_prot_type,
 	NULL,
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|3169| <<vhost_scsi_tpg_nexus_store>> ret = vhost_scsi_make_nexus(tpg, port_ptr);
+ */
 static int vhost_scsi_make_nexus(struct vhost_scsi_tpg *tpg,
 				const char *name)
 {
@@ -2440,11 +3672,17 @@ static ssize_t vhost_scsi_tpg_nexus_store(struct config_item *item,
 
 CONFIGFS_ATTR(vhost_scsi_tpg_, nexus);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_tpg_base_attrs = vhost_scsi_tpg_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_tpg_attrs[] = {
 	&vhost_scsi_tpg_attr_nexus,
 	NULL,
 };
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_tpg = vhost_scsi_make_tpg()
+ */
 static struct se_portal_group *
 vhost_scsi_make_tpg(struct se_wwn *wwn, const char *name)
 {
@@ -2482,6 +3720,9 @@ vhost_scsi_make_tpg(struct se_wwn *wwn, const char *name)
 	return &tpg->se_tpg;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_tpg = vhost_scsi_drop_tpg()
+ */
 static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 {
 	struct vhost_scsi_tpg *tpg = container_of(se_tpg,
@@ -2501,6 +3742,9 @@ static void vhost_scsi_drop_tpg(struct se_portal_group *se_tpg)
 	kfree(tpg);
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_make_wwn = vhost_scsi_make_tport()
+ */
 static struct se_wwn *
 vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 		     struct config_group *group,
@@ -2562,6 +3806,9 @@ vhost_scsi_make_tport(struct target_fabric_configfs *tf,
 	return &tport->tport_wwn;
 }
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.fabric_drop_wwn = vhost_scsi_drop_tport()
+ */
 static void vhost_scsi_drop_tport(struct se_wwn *wwn)
 {
 	struct vhost_scsi_tport *tport = container_of(wwn,
@@ -2584,6 +3831,9 @@ vhost_scsi_wwn_version_show(struct config_item *item, char *page)
 
 CONFIGFS_ATTR_RO(vhost_scsi_wwn_, version);
 
+/*
+ * struct target_core_fabric_ops vhost_scsi_ops.tfc_wwn_attrs = vhost_scsi_wwn_attrs()
+ */
 static struct configfs_attribute *vhost_scsi_wwn_attrs[] = {
 	&vhost_scsi_wwn_attr_version,
 	NULL,
@@ -2636,6 +3886,23 @@ static int __init vhost_scsi_init(void)
 	if (ret < 0)
 		goto out;
 
+	/*
+	 * called by:
+	 *   - drivers/infiniband/ulp/srpt/ib_srpt.c|3961| <<srpt_init_module>> ret = target_register_template(&srpt_template);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1662| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_ops);
+	 *   - drivers/scsi/elx/efct/efct_lio.c|1667| <<efct_scsi_tgt_driver_init>> rc = target_register_template(&efct_lio_npiv_ops);
+	 *   - drivers/scsi/ibmvscsi_tgt/ibmvscsi_tgt.c|4036| <<ibmvscsis_init>> rc = target_register_template(&ibmvscsis_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1878| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_ops);
+	 *   - drivers/scsi/qla2xxx/tcm_qla2xxx.c|1882| <<tcm_qla2xxx_register_configfs>> ret = target_register_template(&tcm_qla2xxx_npiv_ops);
+	 *   - drivers/target/iscsi/iscsi_target.c|695| <<iscsi_target_init_module>> ret = target_register_template(&iscsi_ops);
+	 *   - drivers/target/loopback/tcm_loop.c|1126| <<tcm_loop_fabric_init>> ret = target_register_template(&loop_ops);
+	 *   - drivers/target/sbp/sbp_target.c|2288| <<sbp_init>> return target_register_template(&sbp_ops);
+	 *   - drivers/target/tcm_fc/tfc_conf.c|448| <<ft_init>> ret = target_register_template(&ft_fabric_ops);
+	 *   - drivers/target/tcm_remote/tcm_remote.c|256| <<tcm_remote_fabric_init>> return target_register_template(&remote_ops);
+	 *   - drivers/usb/gadget/function/f_tcm.c|2289| <<tcm_init>> ret = target_register_template(&usbg_ops);
+	 *   - drivers/vhost/scsi.c|2792| <<vhost_scsi_init>> ret = target_register_template(&vhost_scsi_ops);
+	 *   - drivers/xen/xen-scsiback.c|1866| <<scsiback_init>> ret = target_register_template(&scsiback_ops);
+	 */
 	ret = target_register_template(&vhost_scsi_ops);
 	if (ret < 0)
 		goto out_vhost_scsi_deregister;
diff --git a/drivers/vhost/test.c b/drivers/vhost/test.c
index 42c955a5b..d5b29fb89 100644
--- a/drivers/vhost/test.c
+++ b/drivers/vhost/test.c
@@ -119,6 +119,22 @@ static int vhost_test_open(struct inode *inode, struct file *f)
 	dev = &n->dev;
 	vqs[VHOST_TEST_VQ] = &n->vqs[VHOST_TEST_VQ];
 	n->vqs[VHOST_TEST_VQ].handle_kick = handle_vq_kick;
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, VHOST_TEST_VQ_MAX, UIO_MAXIOV,
 		       VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
 
@@ -194,6 +210,14 @@ static long vhost_test_run(struct vhost_test *n, int test)
 		oldpriv = vhost_vq_get_backend(vq);
 		vhost_vq_set_backend(vq, priv);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(&n->vqs[index]);
 
 		mutex_unlock(&vq->mutex);
@@ -282,12 +306,43 @@ static long vhost_test_set_backend(struct vhost_test *n, unsigned index, int fd)
 		goto err_vq;
 	}
 	if (!enable) {
+		/*
+		 * 在以下调用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 *
+		 * 注释:
+		 * Stop polling a file. After this function returns, it becomes safe to drop the
+		 * file reference. You must also flush afterwards.
+		 */
 		vhost_poll_stop(&vq->poll);
 		backend = vhost_vq_get_backend(vq);
 		vhost_vq_set_backend(vq, NULL);
 	} else {
 		vhost_vq_set_backend(vq, backend);
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+		 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+		 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+		 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+		 */
 		r = vhost_vq_init_access(vq);
+		/*
+		 * 在以下调用vhost_poll_start():
+		 *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+		 *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+		 *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+		 *   - drivers/vhost/vhost.h|56| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+		 *
+		 * 注释:
+		 * Start polling a file. We add ourselves to file's wait queue. The caller must
+		 * keep a reference to a file until after vhost_poll_stop is called.
+		 */
 		if (r == 0)
 			r = vhost_poll_start(&vq->poll, vq->kick);
 	}
diff --git a/drivers/vhost/vdpa.c b/drivers/vhost/vdpa.c
index 5a49b5a6d..ff1ab9197 100644
--- a/drivers/vhost/vdpa.c
+++ b/drivers/vhost/vdpa.c
@@ -170,9 +170,29 @@ static void handle_vq_kick(struct vhost_work *work)
 	struct vhost_vdpa *v = container_of(vq->dev, struct vhost_vdpa, vdev);
 	const struct vdpa_config_ops *ops = v->vdpa->config;
 
+	/*
+	 * 在以下调用vdpa_config_ops->kick_vq:
+	 *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+	 *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+	 * 在以下设置vdpa_config_ops->kick_vq:
+	 *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+	 *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+	 *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+	 */
 	ops->kick_vq(v->vdpa, vq - v->vqs);
 }
 
+/*
+ * 在以下使用vhost_vdpa_virtqueue_cb():
+ *   - drivers/vhost/vdpa.c|753| <<vhost_vdpa_vring_ioctl(VHOST_SET_VRING_CALL)>> cb.callback = vhost_vdpa_virtqueue_cb;
+ */
 static irqreturn_t vhost_vdpa_virtqueue_cb(void *private)
 {
 	struct vhost_virtqueue *vq = private;
@@ -989,6 +1009,12 @@ static int perm_to_iommu_flags(u32 perm)
 	return flags | IOMMU_CACHE;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vdpa.c|1075| <<vhost_vdpa_va_map>> ret = vhost_vdpa_map(v, iotlb, map_iova, map_size, uaddr,
+ *   - drivers/vhost/vdpa.c|1160| <<vhost_vdpa_pa_map>> ret = vhost_vdpa_map(v, iotlb, iova, csize,
+ *   - drivers/vhost/vdpa.c|1190| <<vhost_vdpa_pa_map>> ret = vhost_vdpa_map(v, iotlb, iova, PFN_PHYS(last_pfn - map_pfn + 1),
+ */
 static int vhost_vdpa_map(struct vhost_vdpa *v, struct vhost_iotlb *iotlb,
 			  u64 iova, u64 size, u64 pa, u32 perm, void *opaque)
 {
@@ -1430,6 +1456,22 @@ static int vhost_vdpa_open(struct inode *inode, struct file *filep)
 		vqs[i]->handle_kick = handle_vq_kick;
 		vqs[i]->call_ctx.ctx = NULL;
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,
 		       vhost_vdpa_process_iotlb_msg);
 
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 9ac25d08f..74efae45c 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -143,6 +143,10 @@ struct vhost_flush_struct {
 	struct completion wait_event;
 };
 
+/*
+ * 在以下使用vhost_flush_work():
+ *   - drivers/vhost/vhost.c|307| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ */
 static void vhost_flush_work(struct vhost_work *work)
 {
 	struct vhost_flush_struct *s;
@@ -151,6 +155,10 @@ static void vhost_flush_work(struct vhost_work *work)
 	complete(&s->wait_event);
 }
 
+/*
+ * 在以下使用vhost_poll_func():
+ *   - drivers/vhost/vhost.c|212| <<vhost_poll_init>> init_poll_funcptr(&poll->table, vhost_poll_func);
+ */
 static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 			    poll_table *pt)
 {
@@ -161,6 +169,11 @@ static void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,
 	add_wait_queue(wqh, &poll->wait);
 }
 
+/*
+ * 在以下使用vhost_poll_wakeup():
+ *   - drivers/vhost/vhost.c|211| <<vhost_poll_init>> init_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);
+ *   - drivers/vhost/vhost.c|233| <<vhost_poll_start>> vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
+ */
 static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 			     void *key)
 {
@@ -170,6 +183,23 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	if (!(key_to_poll(key) & poll->mask))
 		return 0;
 
+	/*
+	 * 在以下使用vhost_poll_queue():
+	 *   - drivers/vhost/net.c|411| <<vhost_zerocopy_complete>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|526| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|529| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|821| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|937| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|1260| <<handle_rx>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/net.c|1358| <<handle_rx>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/vhost.c|176| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+	 *   - drivers/vhost/vhost.c|531| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+	 *   - drivers/vhost/vhost.c|1552| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+	 *   - drivers/vhost/vhost.h|58| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+	 *   - drivers/vhost/vsock.c|284| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+	 *   - drivers/vhost/vsock.c|347| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+	 */
+
 	if (!poll->dev->use_worker)
 		work->fn(work);
 	else
@@ -178,6 +208,15 @@ static int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|1547| <<vhost_scsi_handle_tmf>> vhost_work_init(&tmf->vwork, vhost_scsi_tmf_resp_work);
+ *   - drivers/vhost/scsi.c|2231| <<vhost_scsi_open>> vhost_work_init(&vs->vs_event_work, vhost_scsi_evt_work);
+ *   - drivers/vhost/scsi.c|2254| <<vhost_scsi_open>> vhost_work_init(&svq->completion_work, vhost_scsi_complete_cmd_work);
+ *   - drivers/vhost/vhost.c|200| <<vhost_poll_init>> vhost_work_init(&poll->work, fn);
+ *   - drivers/vhost/vhost.c|280| <<__vhost_worker_flush>> vhost_work_init(&flush.work, vhost_flush_work);
+ *   - drivers/vhost/vsock.c|725| <<vhost_vsock_dev_open>> vhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);
+ */
 void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 {
 	clear_bit(VHOST_WORK_QUEUED, &work->flags);
@@ -185,6 +224,15 @@ void vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)
 }
 EXPORT_SYMBOL_GPL(vhost_work_init);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|570| <<vhost_dev_init>> vhost_poll_init(&vq->poll,
+ *                   vq->handle_kick, EPOLLIN, dev, vq);
+ *   - drivers/vhost/net.c|1379| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_TX,
+ *                   handle_tx_net, EPOLLOUT, dev, vqs[VHOST_NET_VQ_TX]);
+ *   - drivers/vhost/net.c|1381| <<vhost_net_open>> vhost_poll_init(n->poll + VHOST_NET_VQ_RX,
+ *                   handle_rx_net, EPOLLIN, dev, vqs[VHOST_NET_VQ_RX]);
+ */
 /* Init poll structure */
 void vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,
 		     __poll_t mask, struct vhost_dev *dev,
@@ -203,6 +251,16 @@ EXPORT_SYMBOL_GPL(vhost_poll_init);
 
 /* Start polling a file. We add ourselves to file's wait queue. The caller must
  * keep a reference to a file until after vhost_poll_stop is called. */
+/*
+ * 在以下调用vhost_poll_start():
+ *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+ *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+ *
+ * 注释:
+ * Start polling a file. We add ourselves to file's wait queue. The caller must
+ * keep a reference to a file until after vhost_poll_stop is called.
+ */
 int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 {
 	__poll_t mask;
@@ -214,6 +272,18 @@ int vhost_poll_start(struct vhost_poll *poll, struct file *file)
 	if (mask)
 		vhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));
 	if (mask & EPOLLERR) {
+		/*
+		 * 在以下调用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 *
+		 * 注释:
+		 * Stop polling a file. After this function returns, it becomes safe to drop the
+		 * file reference. You must also flush afterwards.
+		 */
 		vhost_poll_stop(poll);
 		return -EINVAL;
 	}
@@ -224,6 +294,18 @@ EXPORT_SYMBOL_GPL(vhost_poll_start);
 
 /* Stop polling a file. After this function returns, it becomes safe to drop the
  * file reference. You must also flush afterwards. */
+/*
+ * 在以下调用vhost_poll_stop():
+ *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+ *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+ *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+ *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+ *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+ *
+ * 注释:
+ * Stop polling a file. After this function returns, it becomes safe to drop the
+ * file reference. You must also flush afterwards.
+ */
 void vhost_poll_stop(struct vhost_poll *poll)
 {
 	if (poll->wqh) {
@@ -233,6 +315,11 @@ void vhost_poll_stop(struct vhost_poll *poll)
 }
 EXPORT_SYMBOL_GPL(vhost_poll_stop);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|285| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+ *   - drivers/vhost/vhost.c|309| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+ */
 static void vhost_worker_queue(struct vhost_worker *worker,
 			       struct vhost_work *work)
 {
@@ -246,6 +333,15 @@ static void vhost_worker_queue(struct vhost_worker *worker,
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+ *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+ *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+ *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+ *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+ *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+ */
 bool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)
 {
 	struct vhost_worker *worker;
@@ -255,6 +351,11 @@ bool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)
 	worker = rcu_dereference(vq->worker);
 	if (worker) {
 		queued = true;
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|285| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+		 *   - drivers/vhost/vhost.c|309| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+		 */
 		vhost_worker_queue(worker, work);
 	}
 	rcu_read_unlock();
@@ -269,6 +370,12 @@ EXPORT_SYMBOL_GPL(vhost_vq_work_queue);
  *
  * The worker's flush_mutex must be held.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+ *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+ *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+ */
 static void __vhost_worker_flush(struct vhost_worker *worker)
 {
 	struct vhost_flush_struct flush;
@@ -279,6 +386,11 @@ static void __vhost_worker_flush(struct vhost_worker *worker)
 	init_completion(&flush.wait_event);
 	vhost_work_init(&flush.work, vhost_flush_work);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|285| <<vhost_vq_work_queue>> vhost_worker_queue(worker, work);
+	 *   - drivers/vhost/vhost.c|309| <<__vhost_worker_flush>> vhost_worker_queue(worker, &flush.work);
+	 */
 	vhost_worker_queue(worker, &flush.work);
 	/*
 	 * Drop mutex in case our worker is killed and it needs to take the
@@ -289,23 +401,52 @@ static void __vhost_worker_flush(struct vhost_worker *worker)
 	mutex_lock(&worker->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|344| <<vhost_dev_flush>> vhost_worker_flush(worker);
+ */
 static void vhost_worker_flush(struct vhost_worker *worker)
 {
 	mutex_lock(&worker->mutex);
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	mutex_unlock(&worker->mutex);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1432| <<vhost_net_flush>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/net.c|1623| <<vhost_net_set_backend>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/scsi.c|1639| <<vhost_scsi_tmf_flush_work>> vhost_dev_flush(vq->dev);
+ *   - drivers/vhost/scsi.c|2044| <<vhost_scsi_flush>> vhost_dev_flush(&vs->dev);
+ *   - drivers/vhost/test.c|165| <<vhost_test_flush>> vhost_dev_flush(&n->dev);
+ *   - drivers/vhost/vhost.c|1050| <<vhost_dev_stop>> vhost_dev_flush(dev);
+ *   - drivers/vhost/vhost.c|2147| <<vhost_vring_ioctl>> vhost_dev_flush(vq->poll.dev);
+ *   - drivers/vhost/vhost.h|61| <<vhost_vring_ioctl>> void vhost_dev_flush(struct vhost_dev *dev);
+ *   - drivers/vhost/vsock.c|751| <<vhost_vsock_flush>> vhost_dev_flush(&vsock->dev);
+ */
 void vhost_dev_flush(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
 	unsigned long i;
 
+	/*
+	 * 只在此处调用vhost_worker_flush()
+	 */
 	xa_for_each(&dev->worker_xa, i, worker)
 		vhost_worker_flush(worker);
 }
 EXPORT_SYMBOL_GPL(vhost_dev_flush);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|561| <<vhost_net_busy_poll>> if (vhost_vq_has_work(vq)) {
+ */
 /* A lockless hint for busy polling code to exit the loop */
 bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 {
@@ -322,8 +463,33 @@ bool vhost_vq_has_work(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_has_work);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|411| <<vhost_zerocopy_complete>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|526| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|529| <<vhost_net_busy_poll_try_queue>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|821| <<handle_tx_copy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|937| <<handle_tx_zerocopy>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1260| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/net.c|1358| <<handle_rx>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|176| <<vhost_poll_wakeup>> vhost_poll_queue(poll);
+ *   - drivers/vhost/vhost.c|531| <<vhost_exceeds_weight>> vhost_poll_queue(&vq->poll);
+ *   - drivers/vhost/vhost.c|1552| <<vhost_iotlb_notify_vq>> vhost_poll_queue(&node->vq->poll);
+ *   - drivers/vhost/vhost.h|58| <<vhost_iotlb_notify_vq>> void vhost_poll_queue(struct vhost_poll *poll);
+ *   - drivers/vhost/vsock.c|284| <<vhost_transport_do_send_pkt>> vhost_poll_queue(&tx_vq->poll);
+ *   - drivers/vhost/vsock.c|347| <<vhost_transport_cancel_pkt>> vhost_poll_queue(&tx_vq->poll);
+ */
 void vhost_poll_queue(struct vhost_poll *poll)
 {
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(poll->vq, &poll->work);
 }
 EXPORT_SYMBOL_GPL(vhost_poll_queue);
@@ -350,12 +516,21 @@ static void vhost_vring_call_reset(struct vhost_vring_call *call_ctx)
 	memset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer));
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|2335| <<vhost_scsi_set_endpoint>> if (!vhost_vq_is_setup(vq))
+ */
 bool vhost_vq_is_setup(struct vhost_virtqueue *vq)
 {
 	return vq->avail && vq->desc && vq->used && vhost_vq_access_ok(vq);
 }
 EXPORT_SYMBOL_GPL(vhost_vq_is_setup);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|772| <<vhost_vq_reset>> vhost_vq_reset(dev, vq);
+ *   - drivers/vhost/vhost.c|1285| <<vhost_dev_cleanup>> vhost_vq_reset(dev, dev->vqs[i]);
+ */
 static void vhost_vq_reset(struct vhost_dev *dev,
 			   struct vhost_virtqueue *vq)
 {
@@ -369,11 +544,43 @@ static void vhost_vq_reset(struct vhost_dev *dev,
 	vq->signalled_used = 0;
 	vq->signalled_used_valid = false;
 	vq->used_flags = 0;
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	vq->log_used = false;
+	/*
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	vq->log_addr = -1ull;
 	vq->private_data = NULL;
 	vq->acked_features = 0;
 	vq->acked_backend_features = 0;
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	vq->log_base = NULL;
 	vq->error_ctx = NULL;
 	vq->kick = NULL;
@@ -413,6 +620,11 @@ static bool vhost_run_work_list(void *data)
 	return !!node;
 }
 
+/*
+ * 在以下使用vhost_worker_killed():
+ *   - drivers/vhost/vhost.c|903| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+ *             vhost_worker_killed, worker, name);
+ */
 static void vhost_worker_killed(void *data)
 {
 	struct vhost_worker *worker = data;
@@ -530,6 +742,22 @@ static size_t vhost_get_desc_size(struct vhost_virtqueue *vq,
 	return sizeof(*vq->desc) * num;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+ *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+ *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+ *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+ *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+ *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+ *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+ *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+ *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+ *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+ *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+ *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+ *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+ */
 void vhost_dev_init(struct vhost_dev *dev,
 		    struct vhost_virtqueue **vqs, int nvqs,
 		    int iov_limit, int weight, int byte_weight,
@@ -649,6 +877,11 @@ static void vhost_workers_free(struct vhost_dev *dev)
 	xa_destroy(&dev->worker_xa);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|821| <<vhost_new_worker>> worker = vhost_worker_create(dev);
+ *   - drivers/vhost/vhost.c|979| <<vhost_dev_set_owner>> worker = vhost_worker_create(dev);
+ */
 static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 {
 	struct vhost_worker *worker;
@@ -664,6 +897,14 @@ static struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)
 	worker->dev = dev;
 	snprintf(name, sizeof(name), "vhost-%d", current->pid);
 
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7420| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread =
+	 *            vhost_task_create(kvm_nx_huge_page_recovery_worker,
+	 *                              kvm_nx_huge_page_recovery_worker_kill, kvm, "kvm-nx-lpage-recovery");
+	 * drivers/vhost/vhost.c|701| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+	 *            vhost_worker_killed, worker, name);
+	 */
 	vtsk = vhost_task_create(vhost_run_work_list, vhost_worker_killed,
 				 worker, name);
 	if (!vtsk)
@@ -753,6 +994,12 @@ static void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,
 
 	/* Make sure new vq queue/flush/poll calls see the new worker */
 	synchronize_rcu();
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	/* Make sure whatever was queued gets run */
 	__vhost_worker_flush(old_worker);
 	old_worker->attachment_cnt--;
@@ -813,6 +1060,12 @@ static int vhost_free_worker(struct vhost_dev *dev,
 	 * to zero. Make sure flushes are flushed from the queue before
 	 * freeing.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|322| <<vhost_worker_flush>> __vhost_worker_flush(worker);
+	 *   - drivers/vhost/vhost.c|845| <<__vhost_vq_attach_worker>> __vhost_worker_flush(old_worker);
+	 *   - drivers/vhost/vhost.c|904| <<vhost_free_worker>> __vhost_worker_flush(worker);
+	 */
 	__vhost_worker_flush(worker);
 	mutex_unlock(&worker->mutex);
 
@@ -963,6 +1216,12 @@ long vhost_dev_set_owner(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_set_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1182| <<vhost_dev_reset_owner_prepare>> return iotlb_alloc();
+ *   - drivers/vhost/vhost.c|2117| <<vhost_set_memory>> newumem = iotlb_alloc();
+ *   - drivers/vhost/vhost.c|2411| <<vhost_init_device_iotlb>> niotlb = iotlb_alloc();
+ */
 static struct vhost_iotlb *iotlb_alloc(void)
 {
 	return vhost_iotlb_alloc(max_iotlb_entries,
@@ -991,11 +1250,33 @@ void vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_reset_owner);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1611| <<vhost_net_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/net.c|1828| <<vhost_net_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/scsi.c|2642| <<vhost_scsi_release>> vhost_dev_stop(&vs->dev);
+ *   - drivers/vhost/test.c|175| <<vhost_test_release>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/test.c|258| <<vhost_test_reset_owner>> vhost_dev_stop(&n->dev);
+ *   - drivers/vhost/vdpa.c|1486| <<vhost_vdpa_release>> vhost_dev_stop(&v->vdev);
+ *   - drivers/vhost/vsock.c|846| <<vhost_vsock_dev_release>> vhost_dev_stop(&vsock->dev);
+ */
 void vhost_dev_stop(struct vhost_dev *dev)
 {
 	int i;
 
 	for (i = 0; i < dev->nvqs; ++i) {
+		/*
+		 * 在以下调用vhost_poll_stop():
+		 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+		 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+		 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+		 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+		 *
+		 * 注释:
+		 * Stop polling a file. After this function returns, it becomes safe to drop the
+		 * file reference. You must also flush afterwards.
+		 */
 		if (dev->vqs[i]->kick && dev->vqs[i]->handle_kick)
 			vhost_poll_stop(&dev->vqs[i]->poll);
 	}
@@ -1053,6 +1334,11 @@ void vhost_dev_cleanup(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_dev_cleanup);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1366| <<vq_memory_access_ok>> else if (log_all && !log_access_ok(log_base,
+ *   - drivers/vhost/vhost.c|2118| <<vq_log_used_access_ok>> return !log_used || log_access_ok(log_base, log_addr,
+ */
 static bool log_access_ok(void __user *log_base, u64 addr, unsigned long sz)
 {
 	u64 a = addr / VHOST_PAGE_SIZE / 8;
@@ -1066,6 +1352,11 @@ static bool log_access_ok(void __user *log_base, u64 addr, unsigned long sz)
 			 (sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1360| <<vq_memory_access_ok>> if (vhost_overflow(map->addr, map->size))
+ *   - drivers/vhost/vhost.c|1787| <<umem_access_ok>> if (vhost_overflow(uaddr, size))
+ */
 /* Make sure 64 bit math will not overflow. */
 static bool vhost_overflow(u64 uaddr, u64 size)
 {
@@ -1078,6 +1369,13 @@ static bool vhost_overflow(u64 uaddr, u64 size)
 	return uaddr > ULONG_MAX - size + 1;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1414| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+ *   - drivers/vhost/vhost.c|2150| <<vq_log_access_ok>> return vq_memory_access_ok(log_base, vq->umem,
+ *             vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
+ *             vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+ */
 /* Caller should have vq mutex and device mutex. */
 static bool vq_memory_access_ok(void __user *log_base, struct vhost_iotlb *umem,
 				int log_all)
@@ -1118,6 +1416,11 @@ static inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,
 
 /* Can we switch to this memory table? */
 /* Caller should have device mutex but not vq mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2104| <<vhost_log_access_ok>> return memory_access_ok(dev, dev->umem, 1);
+ *   - drivers/vhost/vhost.c|2231| <<vhost_set_memory>> if (!memory_access_ok(d, newumem, 0))
+ */
 static bool memory_access_ok(struct vhost_dev *d, struct vhost_iotlb *umem,
 			     int log_all)
 {
@@ -1129,6 +1432,19 @@ static bool memory_access_ok(struct vhost_dev *d, struct vhost_iotlb *umem,
 
 		mutex_lock(&d->vqs[i]->mutex);
 		log = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);
+		/*
+		 * 在以下设置vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+		 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+		 * 在以下使用vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+		 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+		 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+		 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+		 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 */
 		/* If ring is inactive, will check when it's enabled. */
 		if (d->vqs[i]->private_data)
 			ok = vq_memory_access_ok(d->vqs[i]->log_base,
@@ -1166,6 +1482,29 @@ static int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,
 		if (uaddr)
 			return __copy_to_user(uaddr, from, size);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,
 				     ARRAY_SIZE(vq->iotlb_iov),
 				     VHOST_ACCESS_WO);
@@ -1201,6 +1540,29 @@ static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
 		if (uaddr)
 			return __copy_from_user(to, uaddr, size);
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,
 				     ARRAY_SIZE(vq->iotlb_iov),
 				     VHOST_ACCESS_RO);
@@ -1226,6 +1588,29 @@ static void __user *__vhost_get_user_slow(struct vhost_virtqueue *vq,
 {
 	int ret;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+	 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,
 			     ARRAY_SIZE(vq->iotlb_iov),
 			     VHOST_ACCESS_RO);
@@ -1697,6 +2082,12 @@ static void vhost_vq_meta_update(struct vhost_virtqueue *vq,
 		vq->meta_iotlb[type] = map;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2090| <<vq_meta_prefetch>> return iotlb_access_ok(vq, VHOST_MAP_RO, (u64)(uintptr_t)vq->desc,
+ *   - drivers/vhost/vhost.c|2092| <<vq_meta_prefetch>> iotlb_access_ok(vq, VHOST_MAP_RO, (u64)(uintptr_t)vq->avail,
+ *   - drivers/vhost/vhost.c|2095| <<vq_meta_prefetch>> iotlb_access_ok(vq, VHOST_MAP_WO, (u64)(uintptr_t)vq->used,
+ */
 static bool iotlb_access_ok(struct vhost_virtqueue *vq,
 			    int access, u64 addr, u64 len, int type)
 {
@@ -1748,6 +2139,13 @@ int vq_meta_prefetch(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vq_meta_prefetch);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1860| <<vhost_net_set_features>> !vhost_log_access_ok(&n->dev))
+ *   - drivers/vhost/scsi.c|2517| <<vhost_scsi_set_features>> !vhost_log_access_ok(&vs->dev)) {
+ *   - drivers/vhost/test.c|271| <<vhost_test_set_features>> !vhost_log_access_ok(&n->dev)) {
+ *   - drivers/vhost/vsock.c|903| <<vhost_vsock_set_features>> !vhost_log_access_ok(&vsock->dev)) {
+ */
 /* Can we log writes? */
 /* Caller should have device mutex but not vq mutex */
 bool vhost_log_access_ok(struct vhost_dev *dev)
@@ -1756,6 +2154,11 @@ bool vhost_log_access_ok(struct vhost_dev *dev)
 }
 EXPORT_SYMBOL_GPL(vhost_log_access_ok);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2180| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+ *   - drivers/vhost/vhost.c|2353| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+ */
 static bool vq_log_used_access_ok(struct vhost_virtqueue *vq,
 				  void __user *log_base,
 				  bool log_used,
@@ -1772,9 +2175,34 @@ static bool vq_log_used_access_ok(struct vhost_virtqueue *vq,
 
 /* Verify access for write logging. */
 /* Caller should have vq mutex and device mutex */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2137| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+ *   - drivers/vhost/vhost.c|2585| <<vhost_dev_ioctl>> if (vq->private_data && !vq_log_access_ok(vq, base))
+ *
+ * 判断log_base合法性的唯一入口!
+ */
 static bool vq_log_access_ok(struct vhost_virtqueue *vq,
 			     void __user *log_base)
 {
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	return vq_memory_access_ok(log_base, vq->umem,
 				   vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
 		vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
@@ -1784,6 +2212,19 @@ static bool vq_log_access_ok(struct vhost_virtqueue *vq,
 /* Caller should have vq mutex and device mutex */
 bool vhost_vq_access_ok(struct vhost_virtqueue *vq)
 {
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	if (!vq_log_access_ok(vq, vq->log_base))
 		return false;
 
@@ -1794,6 +2235,14 @@ EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
 	struct vhost_memory mem, *newmem;
+	/*
+	 * struct vhost_memory_region {
+	 *     __u64 guest_phys_addr;
+	 *     __u64 memory_size; // bytes
+	 *     __u64 userspace_addr;
+	 *     __u64 flags_padding; // No flags are currently specified.
+	 * };
+	 */
 	struct vhost_memory_region *region;
 	struct vhost_iotlb *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
@@ -1879,6 +2328,10 @@ static long vhost_vring_set_num(struct vhost_dev *d,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2401| <<vhost_vring_set_num_addr(VHOST_SET_VRING_ADDR)>> r = vhost_vring_set_addr(d, vq, argp);
+ */
 static long vhost_vring_set_addr(struct vhost_dev *d,
 				 struct vhost_virtqueue *vq,
 				 void __user *argp)
@@ -1915,6 +2368,19 @@ static long vhost_vring_set_addr(struct vhost_dev *d,
 			(void __user *)(unsigned long)a.used_user_addr))
 			return -EINVAL;
 
+		/*
+		 * 在以下设置vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+		 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+		 * 在以下使用vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+		 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+		 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+		 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+		 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 */
 		/* Also validate log access for used ring if enabled. */
 		if (!vq_log_used_access_ok(vq, vq->log_base,
 				a.flags & (0x1 << VHOST_VRING_F_LOG),
@@ -1922,9 +2388,28 @@ static long vhost_vring_set_addr(struct vhost_dev *d,
 			return -EINVAL;
 	}
 
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
 	vq->desc = (void __user *)(unsigned long)a.desc_user_addr;
 	vq->avail = (void __user *)(unsigned long)a.avail_user_addr;
+	/*
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	vq->log_addr = a.log_guest_addr;
 	vq->used = (void __user *)(unsigned long)a.used_user_addr;
 
@@ -2075,6 +2560,18 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 		r = -ENOIOCTLCMD;
 	}
 
+	/*
+	 * 在以下调用vhost_poll_stop():
+	 *   - drivers/vhost/net.c|439| <<vhost_net_disable_vq>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/test.c|309| <<vhost_test_set_backend>> vhost_poll_stop(&vq->poll);
+	 *   - drivers/vhost/vhost.c|235| <<vhost_poll_start>> vhost_poll_stop(poll);
+	 *   - drivers/vhost/vhost.c|1088| <<vhost_dev_stop>> vhost_poll_stop(&dev->vqs[i]->poll);
+	 *   - drivers/vhost/vhost.c|2244| <<vhost_vring_ioctl>> vhost_poll_stop(&vq->poll);
+	 *
+	 * 注释:
+	 * Stop polling a file. After this function returns, it becomes safe to drop the
+	 * file reference. You must also flush afterwards.
+	 */
 	if (pollstop && vq->handle_kick)
 		vhost_poll_stop(&vq->poll);
 
@@ -2083,6 +2580,17 @@ long vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *arg
 	if (filep)
 		fput(filep);
 
+	/*
+	 * 在以下调用vhost_poll_start():
+	 *   - drivers/vhost/net.c|454| <<vhost_net_enable_vq>> return vhost_poll_start(poll, sock->file);
+	 *   - drivers/vhost/test.c|324| <<vhost_test_set_backend>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.c|2252| <<vhost_vring_ioctl>> r = vhost_poll_start(&vq->poll, vq->kick);
+	 *   - drivers/vhost/vhost.h|56| <<vhost_vring_ioctl>> int vhost_poll_start(struct vhost_poll *poll, struct file *file);
+	 *
+	 * 注释:
+	 * Start polling a file. We add ourselves to file's wait queue. The caller must
+	 * keep a reference to a file until after vhost_poll_stop is called.
+	 */
 	if (pollstart && vq->handle_kick)
 		r = vhost_poll_start(&vq->poll, vq->kick);
 
@@ -2121,6 +2629,14 @@ int vhost_init_device_iotlb(struct vhost_dev *d)
 }
 EXPORT_SYMBOL_GPL(vhost_init_device_iotlb);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1849| <<vhost_net_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/scsi.c|2648| <<vhost_scsi_ioctl>> r = vhost_dev_ioctl(&vs->dev, ioctl, argp);
+ *   - drivers/vhost/test.c|361| <<vhost_test_ioctl>> r = vhost_dev_ioctl(&n->dev, ioctl, argp);
+ *   - drivers/vhost/vdpa.c|888| <<vhost_vdpa_unlocked_ioctl>> r = vhost_dev_ioctl(&v->vdev, cmd, argp);
+ *   - drivers/vhost/vsock.c|947| <<vhost_vsock_dev_ioctl>> r = vhost_dev_ioctl(&vsock->dev, ioctl, argp); 
+ */
 /* Caller must have device mutex */
 long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
@@ -2158,6 +2674,19 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 			void __user *base = (void __user *)(unsigned long)p;
 			vq = d->vqs[i];
 			mutex_lock(&vq->mutex);
+			/*
+			 * 在以下设置vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+			 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+			 * 在以下使用vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+			 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+			 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+			 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+			 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+			 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+			 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+			 */
 			/* If ring is inactive, will check when it's enabled. */
 			if (vq->private_data && !vq_log_access_ok(vq, base))
 				r = -EFAULT;
@@ -2197,6 +2726,10 @@ EXPORT_SYMBOL_GPL(vhost_dev_ioctl);
  * (instruction directly accesses the data, with an exception table entry
  * returning -EFAULT). See Documentation/arch/x86/exception-tables.rst.
  */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2370| <<log_write>> r = set_bit_to_user(bit, (void __user *)(unsigned long )log);
+ */
 static int set_bit_to_user(int nr, void __user *addr)
 {
 	unsigned long log = (unsigned long)addr;
@@ -2216,6 +2749,12 @@ static int set_bit_to_user(int nr, void __user *addr)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+ *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+ *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+ */
 static int log_write(void __user *log_base,
 		     u64 write_address, u64 write_length)
 {
@@ -2242,6 +2781,11 @@ static int log_write(void __user *log_base,
 	return r;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2457| <<log_used>> ret = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+ *   - drivers/vhost/vhost.c|2476| <<vhost_log_write>> r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+ */
 static int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)
 {
 	struct vhost_iotlb *umem = vq->umem;
@@ -2262,6 +2806,24 @@ static int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)
 			start = max(u->addr, hva);
 			end = min(u->addr - 1 + u->size, hva - 1 + len);
 			l = end - start + 1;
+			/*
+			 * 在以下设置vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+			 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+			 * 在以下使用vhost_virtqueue->log_base:
+			 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+			 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+			 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+			 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+			 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+			 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+			 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+			 *
+			 * called by:
+			 *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+			 *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+			 *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+			 */
 			r = log_write(vq->log_base,
 				      u->start + start - u->addr,
 				      l);
@@ -2281,20 +2843,80 @@ static int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+ *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+ *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+ *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+ */
 static int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)
 {
 	struct iovec *iov = vq->log_iov;
 	int i, ret;
 
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 *
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	if (!vq->iotlb)
 		return log_write(vq->log_base, vq->log_addr + used_offset, len);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+	 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, (uintptr_t)vq->used + used_offset,
 			     len, iov, 64, VHOST_ACCESS_WO);
 	if (ret < 0)
 		return ret;
 
 	for (i = 0; i < ret; i++) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2457| <<log_used>> ret = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+		 *   - drivers/vhost/vhost.c|2476| <<vhost_log_write>> r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+		 */
 		ret = log_write_hva(vq,	(uintptr_t)iov[i].iov_base,
 				    iov[i].iov_len);
 		if (ret)
@@ -2304,6 +2926,10 @@ static int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1352| <<handle_rx>> vhost_log_write(vq, vq_log, log, vhost_len, vq->iov, in);
+ */
 int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 		    unsigned int log_num, u64 len, struct iovec *iov, int count)
 {
@@ -2314,6 +2940,11 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 
 	if (vq->iotlb) {
 		for (i = 0; i < count; i++) {
+			/*
+			 * called by:
+			 *   - drivers/vhost/vhost.c|2457| <<log_used>> ret = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+			 *   - drivers/vhost/vhost.c|2476| <<vhost_log_write>> r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
+			 */
 			r = log_write_hva(vq, (uintptr_t)iov[i].iov_base,
 					  iov[i].iov_len);
 			if (r < 0)
@@ -2324,6 +2955,24 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 
 	for (i = 0; i < log_num; ++i) {
 		u64 l = min(log[i].len, len);
+		/*
+		 * 在以下设置vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+		 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+		 * 在以下使用vhost_virtqueue->log_base:
+		 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+		 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+		 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+		 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+		 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 *
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2401| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+		 *   - drivers/vhost/vhost.c|2426| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+		 *   - drivers/vhost/vhost.c|2486| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+		 */
 		r = log_write(vq->log_base, log[i].addr, l);
 		if (r < 0)
 			return r;
@@ -2340,16 +2989,40 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 }
 EXPORT_SYMBOL_GPL(vhost_log_write);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+ *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+ */
 static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 {
 	void __user *used;
 	if (vhost_put_used_flags(vq))
 		return -EFAULT;
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		/* Make sure the flag is seen before log. */
 		smp_wmb();
 		/* Log used flag write. */
 		used = &vq->used->flags;
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		log_used(vq, (used - (void __user *)vq->used),
 			 sizeof vq->used->flags);
 		if (vq->log_ctx)
@@ -2358,16 +3031,38 @@ static int vhost_update_used_flags(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|3387| <<vhost_enable_notify>> r = vhost_update_avail_event(vq);
+ */
 static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 {
 	if (vhost_put_avail_event(vq))
 		return -EFAULT;
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		void __user *used;
 		/* Make sure the event is seen before log. */
 		smp_wmb();
 		/* Log avail event write */
 		used = vhost_avail_event(vq);
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		log_used(vq, (used - (void __user *)vq->used),
 			 sizeof *vhost_avail_event(vq));
 		if (vq->log_ctx)
@@ -2376,6 +3071,14 @@ static int vhost_update_avail_event(struct vhost_virtqueue *vq)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+ *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+ *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+ *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+ */
 int vhost_vq_init_access(struct vhost_virtqueue *vq)
 {
 	__virtio16 last_used_idx;
@@ -2387,6 +3090,12 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 
 	vhost_init_is_le(vq);
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+	 *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+	 *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+	 */
 	r = vhost_update_used_flags(vq);
 	if (r)
 		goto err;
@@ -2411,6 +3120,29 @@ int vhost_vq_init_access(struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+ *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+ *             VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+ *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+ *             VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+ *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+ *             VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+ *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+ *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+ *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+ *             VHOST_ACCESS_RO);
+ *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+ *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+ *             iov + iov_count, iov_size - iov_count, access);
+ *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+ *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+ *             iov + iov_count, iov_size - iov_count, access);
+ */
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 			  struct iovec iov[], int iov_size, int access)
 {
@@ -2459,6 +3191,11 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 /* Each buffer in the virtqueues is actually a chain of descriptors.  This
  * function returns the next descriptor in the chain,
  * or -1U if we're at the end. */
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2796| <<get_indirect>> } while ((i = next_desc(vq, &desc)) != -1);
+ *   - drivers/vhost/vhost.c|2961| <<vhost_get_vq_desc>> } while ((i = next_desc(vq, &desc)) != -1);
+ */
 static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 {
 	unsigned int next;
@@ -2472,6 +3209,10 @@ static unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)
 	return next;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2841| <<vhost_get_vq_desc>> ret = get_indirect(vq, iov, iov_size,
+ */
 static int get_indirect(struct vhost_virtqueue *vq,
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
@@ -2493,6 +3234,29 @@ static int get_indirect(struct vhost_virtqueue *vq,
 		return -EINVAL;
 	}
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+	 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+	 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+	 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+	 *             VHOST_ACCESS_RO);
+	 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+	 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+	 *             iov + iov_count, iov_size - iov_count, access);
+	 */
 	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,
 			     UIO_MAXIOV, VHOST_ACCESS_RO);
 	if (unlikely(ret < 0)) {
@@ -2534,6 +3298,29 @@ static int get_indirect(struct vhost_virtqueue *vq,
 		else
 			access = VHOST_ACCESS_RO;
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2573,6 +3360,25 @@ static int get_indirect(struct vhost_virtqueue *vq,
  * This function returns the descriptor number found, or vq->num (which is
  * never a valid descriptor number) if none was found.  A negative code is
  * returned on error. */
+/*
+ * called by:
+ *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+ *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+ *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+ *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+ *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+ *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+ *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+ *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+ */
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
@@ -2637,6 +3443,9 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			return -EFAULT;
 		}
 		if (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {
+			/*
+			 * 只在此处调用
+			 */
 			ret = get_indirect(vq, iov, iov_size,
 					   out_num, in_num,
 					   log, log_num, &desc);
@@ -2653,6 +3462,29 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 			access = VHOST_ACCESS_WO;
 		else
 			access = VHOST_ACCESS_RO;
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|1228| <<vhost_copy_to_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)to, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|1263| <<vhost_copy_from_user>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)from, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|1288| <<__vhost_get_user_slow>> ret = translate_desc(vq,
+		 *             (u64)(uintptr_t)addr, size, vq->iotlb_iov, ARRAY_SIZE(vq->iotlb_iov),
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2359| <<log_used>> ret = translate_desc(vq,
+		 *             (uintptr_t)vq->used + used_offset, len, iov, 64, VHOST_ACCESS_WO);
+		 *   - drivers/vhost/vhost.c|2563| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, indirect->addr), len, vq->indirect, UIO_MAXIOV,
+		 *             VHOST_ACCESS_RO);
+		 *   - drivers/vhost/vhost.c|2604| <<get_indirect>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 *   - drivers/vhost/vhost.c|2742| <<vhost_get_vq_desc>> ret = translate_desc(vq,
+		 *             vhost64_to_cpu(vq, desc.addr), vhost32_to_cpu(vq, desc.len),
+		 *             iov + iov_count, iov_size - iov_count, access);
+		 */
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
 				     iov_size - iov_count, access);
@@ -2693,6 +3525,17 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 }
 EXPORT_SYMBOL_GPL(vhost_get_vq_desc);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|831| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|852| <<handle_tx_copy>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|960| <<handle_tx_zerocopy>> vhost_discard_vq_desc(vq, 1);
+ *   - drivers/vhost/net.c|1138| <<get_rx_bufs>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1245| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *   - drivers/vhost/net.c|1269| <<handle_rx>> vhost_discard_vq_desc(vq, headcount);
+ *
+ * 把vhost_virtqueue->last_avail_idx减少n
+ */
 /* Reverse the effect of vhost_get_vq_desc. Useful for error handling. */
 void vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)
 {
@@ -2702,6 +3545,14 @@ EXPORT_SYMBOL_GPL(vhost_discard_vq_desc);
 
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|851| <<vhost_scsi_complete_cmd_work>> vhost_add_used(cmd->tvc_vq, cmd->tvc_vq_desc, 0);
+ *   - drivers/vhost/vhost.c|3122| <<vhost_add_used_and_signal>> vhost_add_used(vq, head, len);
+ *   - drivers/vhost/vhost.h|214| <<vhost_add_used_and_signal>> int vhost_add_used(struct vhost_virtqueue *, unsigned int head, int len);
+ *   - drivers/vhost/vsock.c|235| <<vhost_transport_do_send_pkt>> vhost_add_used(vq, head, sizeof(*hdr) + payload_len);
+ *   - drivers/vhost/vsock.c|581| <<vhost_vsock_handle_tx_kick>> vhost_add_used(vq, head, 0);
+ */
 int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 {
 	struct vring_used_elem heads = {
@@ -2709,10 +3560,20 @@ int vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)
 		cpu_to_vhost32(vq, len)
 	};
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2997| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+	 *   - drivers/vhost/vhost.c|3140| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+	 */
 	return vhost_add_used_n(vq, &heads, 1);
 }
 EXPORT_SYMBOL_GPL(vhost_add_used);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|3043| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+ *   - drivers/vhost/vhost.c|3049| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+ */
 static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 			    struct vring_used_elem *heads,
 			    unsigned count)
@@ -2727,9 +3588,27 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 		vq_err(vq, "Failed to write used");
 		return -EFAULT;
 	}
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		/* Make sure data is seen before log. */
 		smp_wmb();
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		/* Log used ring entry write. */
 		log_used(vq, ((void __user *)used - (void __user *)vq->used),
 			 count * sizeof *used);
@@ -2745,6 +3624,11 @@ static int __vhost_add_used_n(struct vhost_virtqueue *vq,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|2997| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+ *   - drivers/vhost/vhost.c|3140| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+ */
 /* After we've used one of their buffers, we tell them about it.  We'll then
  * want to notify the guest, using eventfd. */
 int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
@@ -2755,12 +3639,22 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 	start = vq->last_used_idx & (vq->num - 1);
 	n = vq->num - start;
 	if (n < count) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|3043| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+		 *   - drivers/vhost/vhost.c|3049| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+		 */
 		r = __vhost_add_used_n(vq, heads, n);
 		if (r < 0)
 			return r;
 		heads += n;
 		count -= n;
 	}
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|3043| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, n);
+	 *   - drivers/vhost/vhost.c|3049| <<vhost_add_used_n>> r = __vhost_add_used_n(vq, heads, count);
+	 */
 	r = __vhost_add_used_n(vq, heads, count);
 
 	/* Make sure buffer is written before we update index. */
@@ -2769,9 +3663,27 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 		vq_err(vq, "Failed to increment used idx");
 		return -EFAULT;
 	}
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	if (unlikely(vq->log_used)) {
 		/* Make sure used idx is seen before log. */
 		smp_wmb();
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2512| <<vhost_update_used_flags>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|2530| <<vhost_update_avail_event>> log_used(vq, (used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3040| <<__vhost_add_used_n>> log_used(vq, ((void __user *)used - (void __user *)vq->used),
+		 *   - drivers/vhost/vhost.c|3097| <<vhost_add_used_n>> log_used(vq, offsetof(struct vring_used, idx),
+		 */
 		/* Log used index update. */
 		log_used(vq, offsetof(struct vring_used, idx),
 			 sizeof vq->used->idx);
@@ -2782,6 +3694,10 @@ int vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/vhost.c|3147| <<vhost_signal>> if (vq->call_ctx.ctx && vhost_notify(dev, vq))
+ */
 static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
 	__u16 old, new;
@@ -2819,35 +3735,88 @@ static bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 	return vring_need_event(vhost16_to_cpu(vq, event), new, old);
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+ *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+ *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+ */
 /* This actually signals the guest, using eventfd. */
 void vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
+	/*
+	 * 只在此处调用vhost_notify()
+	 */
 	/* Signal the Guest tell them we used something up. */
 	if (vq->call_ctx.ctx && vhost_notify(dev, vq))
 		eventfd_signal(vq->call_ctx.ctx);
 }
 EXPORT_SYMBOL_GPL(vhost_signal);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|969| <<handle_tx_zerocopy>> vhost_add_used_and_signal(&net->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|529| <<vhost_scsi_do_evt_work>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|969| <<vhost_scsi_send_bad_target>> vhost_add_used_and_signal(&vs->dev, vq, head, 0);
+ *   - drivers/vhost/scsi.c|1331| <<vhost_scsi_send_tmf_resp>> vhost_add_used_and_signal(&vs->dev, vq, vq_desc, 0);
+ *   - drivers/vhost/scsi.c|1430| <<vhost_scsi_send_an_resp>> vhost_add_used_and_signal(&vs->dev, vq, vc->head, 0);
+ *   - drivers/vhost/test.c|87| <<handle_vq>> vhost_add_used_and_signal(&n->dev, vq, head, 0);
+ */
 /* And here's the combo meal deal.  Supersize me! */
 void vhost_add_used_and_signal(struct vhost_dev *dev,
 			       struct vhost_virtqueue *vq,
 			       unsigned int head, int len)
 {
 	vhost_add_used(vq, head, len);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	vhost_signal(dev, vq);
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|376| <<vhost_zerocopy_signal_used>> vhost_add_used_and_signal_n(vq->dev, vq,
+ *   - drivers/vhost/net.c|460| <<vhost_net_signal_used>> vhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);
+ */
 /* multi-buffer version of vhost_add_used_and_signal */
 void vhost_add_used_and_signal_n(struct vhost_dev *dev,
 				 struct vhost_virtqueue *vq,
 				 struct vring_used_elem *heads, unsigned count)
 {
+	/*
+	 * called by:
+	 *   - drivers/vhost/vhost.c|2997| <<vhost_add_used>> return vhost_add_used_n(vq, &heads, 1);
+	 *   - drivers/vhost/vhost.c|3140| <<vhost_add_used_and_signal_n>> vhost_add_used_n(vq, heads, count);
+	 */
 	vhost_add_used_n(vq, heads, count);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	vhost_signal(dev, vq);
 }
 EXPORT_SYMBOL_GPL(vhost_add_used_and_signal_n);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|515| <<vhost_net_busy_poll_try_queue>> if (!vhost_vq_avail_empty(&net->dev, vq)) {
+ *   - drivers/vhost/net.c|557| <<vhost_net_busy_poll>> !vhost_vq_avail_empty(&net->dev, rvq)) ||
+ *   - drivers/vhost/net.c|558| <<vhost_net_busy_poll>> !vhost_vq_avail_empty(&net->dev, tvq))
+ *   - drivers/vhost/net.c|695| <<tx_can_batch>> !vhost_vq_avail_empty(vq->dev, vq);
+ */
 /* return true if we're sure that avaiable ring is empty */
 bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
@@ -2863,6 +3832,20 @@ bool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_vq_avail_empty);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|517| <<vhost_net_busy_poll_try_queue>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|569| <<vhost_net_busy_poll>> vhost_enable_notify(&net->dev, rvq);
+ *   - drivers/vhost/net.c|812| <<handle_tx_copy>> } else if (unlikely(vhost_enable_notify(&net->dev,
+ *   - drivers/vhost/net.c|928| <<handle_tx_zerocopy>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/net.c|1251| <<handle_rx>> } else if (unlikely(vhost_enable_notify(&net->dev, vq))) {
+ *   - drivers/vhost/scsi.c|683| <<vhost_scsi_do_evt_work>> if (vhost_enable_notify(&vs->dev, vq))
+ *   - drivers/vhost/scsi.c|1265| <<vhost_scsi_get_desc>> if (unlikely(vhost_enable_notify(&vs->dev, vq))) {
+ *   - drivers/vhost/test.c|70| <<handle_vq>> if (unlikely(vhost_enable_notify(&n->dev, vq))) {
+ *   - drivers/vhost/vsock.c|123| <<vhost_transport_do_send_pkt>> vhost_enable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|158| <<vhost_transport_do_send_pkt>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ *   - drivers/vhost/vsock.c|561| <<vhost_vsock_handle_tx_kick>> if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
+ */
 /* OK, now we need to know about added descriptors. */
 bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
@@ -2872,6 +3855,12 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		return false;
 	vq->used_flags &= ~VRING_USED_F_NO_NOTIFY;
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+		 */
 		r = vhost_update_used_flags(vq);
 		if (r) {
 			vq_err(vq, "Failed to enable notification at %p: %d\n",
@@ -2879,6 +3868,9 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 			return false;
 		}
 	} else {
+		/*
+		 * 只在此处调用
+		 */
 		r = vhost_update_avail_event(vq);
 		if (r) {
 			vq_err(vq, "Failed to update avail event index at %p: %d\n",
@@ -2899,6 +3891,26 @@ bool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 }
 EXPORT_SYMBOL_GPL(vhost_enable_notify);
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|518| <<vhost_net_busy_poll_try_queue>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|541| <<vhost_net_busy_poll>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|814| <<handle_tx_copy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|929| <<handle_tx_zerocopy>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1025| <<handle_tx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1224| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/net.c|1254| <<handle_rx>> vhost_disable_notify(&net->dev, vq);
+ *   - drivers/vhost/scsi.c|655| <<vhost_scsi_do_evt_work>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1266| <<vhost_scsi_get_desc>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1390| <<vhost_scsi_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/scsi.c|1845| <<vhost_scsi_ctl_handle_vq>> vhost_disable_notify(&vs->dev, vq);
+ *   - drivers/vhost/test.c|58| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/test.c|71| <<handle_vq>> vhost_disable_notify(&n->dev, vq);
+ *   - drivers/vhost/vsock.c|107| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|159| <<vhost_transport_do_send_pkt>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|524| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ *   - drivers/vhost/vsock.c|562| <<vhost_vsock_handle_tx_kick>> vhost_disable_notify(&vsock->dev, vq);
+ */
 /* We don't need to be notified again. */
 void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 {
@@ -2908,6 +3920,12 @@ void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)
 		return;
 	vq->used_flags |= VRING_USED_F_NO_NOTIFY;
 	if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+		/*
+		 * called by:
+		 *   - drivers/vhost/vhost.c|2615| <<vhost_vq_init_access>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3338| <<vhost_enable_notify>> r = vhost_update_used_flags(vq);
+		 *   - drivers/vhost/vhost.c|3394| <<vhost_disable_notify>> r = vhost_update_used_flags(vq);
+		 */
 		r = vhost_update_used_flags(vq);
 		if (r)
 			vq_err(vq, "Failed to disable notification at %p: %d\n",
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index bb75a292d..f8c9b2579 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -122,7 +122,26 @@ struct vhost_virtqueue {
 	bool signalled_used_valid;
 
 	/* Log writes to used structure. */
+	/*
+	 * 在以下设置vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|539| <<vhost_vq_reset>> vq->log_used = false;
+	 *   - drivers/vhost/vhost.c|2228| <<vhost_vring_set_addr>> vq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));
+	 * 在以下使用vhost_virtqueue->log_used:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2765| <<vhost_update_used_flags>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|2793| <<vhost_update_avail_event>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3331| <<__vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 *   - drivers/vhost/vhost.c|3395| <<vhost_add_used_n>> if (unlikely(vq->log_used)) {
+	 */
 	bool log_used;
+	/*
+	 * 在以下设置vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|540| <<vhost_vq_reset>> vq->log_addr = -1ull;
+	 *   - drivers/vhost/vhost.c|2231| <<vhost_vring_set_addr>> vq->log_addr = a.log_guest_addr;
+	 * 在以下使用vhost_virtqueue->log_addr:
+	 *   - drivers/vhost/vhost.c|2075| <<vq_log_access_ok>> vq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 */
 	u64 log_addr;
 
 	struct iovec iov[UIO_MAXIOV];
@@ -136,6 +155,19 @@ struct vhost_virtqueue {
 	u64 acked_features;
 	u64 acked_backend_features;
 	/* Log write descriptors */
+	/*
+	 * 在以下设置vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|544| <<vhost_vq_reset>> vq->log_base = NULL;
+	 *   - drivers/vhost/vhost.c|2499| <<vhost_dev_ioctl(VHOST_SET_LOG_BASE)>> vq->log_base = base;
+	 * 在以下使用vhost_virtqueue->log_base:
+	 *   - drivers/vhost/vhost.c|1360| <<memory_access_ok>> ok = vq_memory_access_ok(d->vqs[i]->log_base, umem, log);
+	 *   - drivers/vhost/vhost.c|2082| <<vhost_vq_access_ok>> if (!vq_log_access_ok(vq, vq->log_base))
+	 *   - drivers/vhost/vhost.c|2222| <<vhost_vring_set_addr>> if (!vq_log_used_access_ok(vq, vq->log_base,
+	 *             a.flags & (0x1 << VHOST_VRING_F_LOG), a.log_guest_addr))
+	 *   - drivers/vhost/vhost.c|2620| <<log_write_hva>> r = log_write(vq->log_base, u->start + start - u->addr, l);
+	 *   - drivers/vhost/vhost.c|2658| <<log_used>> return log_write(vq->log_base, vq->log_addr + used_offset, len);
+	 *   - drivers/vhost/vhost.c|2738| <<vhost_log_write>> r = log_write(vq->log_base, log[i].addr, l);
+	 */
 	void __user *log_base;
 	struct vhost_log *log;
 	struct iovec log_iov[64];
@@ -147,6 +179,16 @@ struct vhost_virtqueue {
 	/* Ring endianness requested by userspace for cross-endian support. */
 	bool user_be;
 #endif
+	/*
+	 * 在以下使用vhost_virtqueue->busyloop_timeout:
+	 *   - drivers/vhost/net.c|577| <<vhost_net_busy_poll>> busyloop_timeout = poll_rx ?
+	 *             rvq->busyloop_timeout: tvq->busyloop_timeout;
+	 *   - drivers/vhost/net.c|638| <<vhost_net_tx_get_vq_desc>> if (r == tvq->num && tvq->busyloop_timeout) {
+	 *   - drivers/vhost/net.c|1100| <<vhost_net_rx_peek_head_len>> if (!len && rvq->busyloop_timeout) {
+	 *   - drivers/vhost/vhost.c|550| <<vhost_vq_reset>> vq->busyloop_timeout = 0;
+	 *   - drivers/vhost/vhost.c|2369| <<vhost_vring_ioctl(VHOST_SET_VRING_BUSYLOOP_TIMEOUT)>> vq->busyloop_timeout = s.num;
+	 *   - drivers/vhost/vhost.c|2373| <<vhost_vring_ioctl(VHOST_GET_VRING_BUSYLOOP_TIMEOUT)>> s.num = vq->busyloop_timeout;
+	 */
 	u32 busyloop_timeout;
 };
 
@@ -262,6 +304,22 @@ enum {
 			 (1ULL << VIRTIO_F_VERSION_1)
 };
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1573| <<vhost_net_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/net.c|1747| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, sock);
+ *   - drivers/vhost/net.c|1796| <<vhost_net_set_backend>> vhost_vq_set_backend(vq, oldsock);
+ *   - drivers/vhost/scsi.c|2493| <<vhost_scsi_set_endpoint>> vhost_vq_set_backend(vq, vs_tpg);
+ *   - drivers/vhost/scsi.c|2602| <<vhost_scsi_clear_endpoint>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|153| <<vhost_test_stop_vq>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|211| <<vhost_test_run>> vhost_vq_set_backend(vq, priv);
+ *   - drivers/vhost/test.c|323| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/test.c|325| <<vhost_test_set_backend>> vhost_vq_set_backend(vq, backend);
+ *   - drivers/vhost/vsock.c|651| <<vhost_vsock_start>> vhost_vq_set_backend(vq, vsock);
+ *   - drivers/vhost/vsock.c|686| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|693| <<vhost_vsock_start>> vhost_vq_set_backend(vq, NULL);
+ *   - drivers/vhost/vsock.c|718| <<vhost_vsock_stop>> vhost_vq_set_backend(vq, NULL);
+ */
 /**
  * vhost_vq_set_backend - Set backend.
  *
@@ -276,6 +334,34 @@ static inline void vhost_vq_set_backend(struct vhost_virtqueue *vq,
 	vq->private_data = private_data;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|453| <<vhost_net_disable_vq>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/net.c|478| <<vhost_net_enable_vq>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|591| <<vhost_net_busy_poll>> sock = vhost_vq_get_backend(rvq);
+ *   - drivers/vhost/net.c|656| <<vhost_net_tx_get_vq_desc>> if (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))
+ *   - drivers/vhost/net.c|658| <<vhost_net_tx_get_vq_desc>> vhost_vq_get_backend(tvq),
+ *   - drivers/vhost/net.c|755| <<vhost_net_build_xdp>> struct socket *sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1067| <<handle_tx>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1302| <<handle_rx>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1571| <<vhost_net_stop_vq>> sock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/net.c|1737| <<vhost_net_set_backend>> oldsock = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|725| <<vhost_scsi_do_evt_work>> if (!vhost_vq_get_backend(vq)) {
+ *   - drivers/vhost/scsi.c|1447| <<vhost_scsi_get_req>> vs_tpg = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|1494| <<vhost_scsi_handle_vq>> vs_tpg = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/scsi.c|1970| <<vhost_scsi_ctl_handle_vq>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/scsi.c|2181| <<vhost_scsi_evt_handle_kick>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/scsi.c|2527| <<vhost_scsi_set_endpoint>> if (!vhost_vq_get_backend(&vs->vqs[i].vq))
+ *   - drivers/vhost/scsi.c|3009| <<vhost_scsi_do_plug>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/test.c|52| <<handle_vq>> private = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|152| <<vhost_test_stop_vq>> private = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|210| <<vhost_test_run>> oldpriv = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/test.c|322| <<vhost_test_set_backend>> backend = vhost_vq_get_backend(vq);
+ *   - drivers/vhost/vhost.c|979| <<__vhost_vq_attach_worker>> if (!vhost_vq_get_backend(vq) && !vq->kick) {
+ *   - drivers/vhost/vsock.c|100| <<vhost_transport_do_send_pkt>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/vsock.c|527| <<vhost_vsock_handle_tx_kick>> if (!vhost_vq_get_backend(vq))
+ *   - drivers/vhost/vsock.c|650| <<vhost_vsock_start>> if (!vhost_vq_get_backend(vq)) {
+ */
 /**
  * vhost_vq_get_backend - Get backend.
  *
@@ -289,6 +375,27 @@ static inline void *vhost_vq_get_backend(struct vhost_virtqueue *vq)
 	return vq->private_data;
 }
 
+/*
+ * called by:
+ *   - drivers/vhost/net.c|1186| <<handle_rx>> vq_log = unlikely(vhost_has_feature(vq, VHOST_F_LOG_ALL)) ? 
+ *   - drivers/vhost/net.c|1188| <<handle_rx>> mergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);
+ *   - drivers/vhost/scsi.c|1275| <<vhost_scsi_handle_vq>> bool t10_pi = vhost_has_feature(vq, VIRTIO_SCSI_F_T10_PI);
+ *   - drivers/vhost/scsi.c|2447| <<vhost_scsi_do_plug>> if (vhost_has_feature(vq, VIRTIO_SCSI_F_HOTPLUG))
+ *   - drivers/vhost/vdpa.c|701| <<vhost_vdpa_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {
+ *   - drivers/vhost/vdpa.c|740| <<vhost_vdpa_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {
+ *   - drivers/vhost/vhost.c|111| <<vhost_init_is_le>> vq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1) || !vq->user_be;
+ *   - drivers/vhost/vhost.c|131| <<vhost_init_is_le>> vq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1)
+ *   - drivers/vhost/vhost.c|513| <<vhost_get_avail_size>> vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|522| <<vhost_get_used_size>> vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
+ *   - drivers/vhost/vhost.c|1131| <<memory_access_ok>> log = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);
+ *   - drivers/vhost/vhost.c|1779| <<vq_log_access_ok>> vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/vhost.c|1992| <<vhost_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {
+ *   - drivers/vhost/vhost.c|2007| <<vhost_vring_ioctl>> if (vhost_has_feature(vq, VIRTIO_F_RING_PACKED))
+ *   - drivers/vhost/vhost.c|2814| <<vhost_notify>> if (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&
+ *   - drivers/vhost/vhost.c|2818| <<vhost_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vhost.c|2902| <<vhost_enable_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ *   - drivers/vhost/vhost.c|2938| <<vhost_disable_notify>> if (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {
+ */
 static inline bool vhost_has_feature(struct vhost_virtqueue *vq, int bit)
 {
 	return vq->acked_features & (1ULL << bit);
diff --git a/drivers/vhost/vringh.c b/drivers/vhost/vringh.c
index 73e153f9b..11b9f8e39 100644
--- a/drivers/vhost/vringh.c
+++ b/drivers/vhost/vringh.c
@@ -1477,6 +1477,14 @@ EXPORT_SYMBOL(vringh_set_iotlb);
  * When you don't have to use riov and wiov anymore, you should clean up them
  * calling vringh_kiov_cleanup() to release the memory, even on error!
  */
+/*
+ * called by:
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|2388| <<mlx5_cvq_kick_handler>> err = vringh_getdesc_iotlb(&cvq->vring, &cvq->riov, &cvq->wiov, &cvq->head,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_blk.c|123| <<vdpasim_blk_handle_req>> ret = vringh_getdesc_iotlb(&vq->vring, &vq->out_iov, &vq->in_iov,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|144| <<vdpasim_handle_cvq>> err = vringh_getdesc_iotlb(&cvq->vring, &cvq->in_iov,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|220| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&txq->vring, &txq->out_iov, NULL,
+ *   - drivers/vdpa/vdpa_sim/vdpa_sim_net.c|240| <<vdpasim_net_work>> err = vringh_getdesc_iotlb(&rxq->vring, NULL, &rxq->in_iov,
+ */
 int vringh_getdesc_iotlb(struct vringh *vrh,
 			 struct vringh_kiov *riov,
 			 struct vringh_kiov *wiov,
diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
index 802153e23..c16c091d9 100644
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@ -124,6 +124,25 @@ vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 			break;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
 					 &out, &in, NULL, NULL);
 		if (head < 0) {
@@ -247,6 +266,14 @@ vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 			virtio_transport_consume_skb_sent(skb, true);
 		}
 	} while(likely(!vhost_exceeds_weight(vq, ++pkts, total_len)));
+	/*
+	 * 在以下调用vhost_signal():
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	if (added)
 		vhost_signal(&vsock->dev, vq);
 
@@ -289,6 +316,15 @@ vhost_transport_send_pkt(struct sk_buff *skb)
 		atomic_inc(&vsock->queued_replies);
 
 	virtio_vsock_skb_queue_tail(&vsock->send_pkt_queue, skb);
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
 
 	rcu_read_unlock();
@@ -506,6 +542,25 @@ static void vhost_vsock_handle_tx_kick(struct vhost_work *work)
 			goto no_more_replies;
 		}
 
+		/*
+		 * called by:
+		 *   - drivers/vhost/net.c|583| <<vhost_net_tx_get_vq_desc>> int r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|595| <<vhost_net_tx_get_vq_desc>> r = vhost_get_vq_desc(
+		 *                tvq, tvq->iov, ARRAY_SIZE(tvq->iov), out_num, in_num, NULL, NULL);
+		 *   - drivers/vhost/net.c|1041| <<get_rx_bufs>> r = vhost_get_vq_desc(
+		 *                vq, vq->iov + seg, ARRAY_SIZE(vq->iov) - seg, &out, &in, log, log_num);
+		 *   - drivers/vhost/scsi.c|477| <<vhost_scsi_do_evt_work>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/scsi.c|957| <<vhost_scsi_get_desc>> vc->head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &vc->out, &vc->in, NULL, NULL);
+		 *   - drivers/vhost/test.c|61| <<handle_vq>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|127| <<vhost_transport_do_send_pkt>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 *   - drivers/vhost/vsock.c|509| <<vhost_vsock_handle_tx_kick>> head = vhost_get_vq_desc(
+		 *                vq, vq->iov, ARRAY_SIZE(vq->iov), &out, &in, NULL, NULL);
+		 */
 		head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
 					 &out, &in, NULL, NULL);
 		if (head < 0)
@@ -545,6 +600,14 @@ static void vhost_vsock_handle_tx_kick(struct vhost_work *work)
 	} while(likely(!vhost_exceeds_weight(vq, ++pkts, total_len)));
 
 no_more_replies:
+	/*
+	 * 在以下使用vhost_signal():
+	 *   - drivers/vhost/scsi.c|859| <<vhost_scsi_complete_cmd_work>> vhost_signal(&svq->vs->dev, &svq->vq);
+	 *   - drivers/vhost/vhost.c|3167| <<vhost_add_used_and_signal>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vhost.c|3182| <<vhost_add_used_and_signal_n>> vhost_signal(dev, vq);
+	 *   - drivers/vhost/vsock.c|270| <<vhost_transport_do_send_pkt>> vhost_signal(&vsock->dev, vq);
+	 *   - drivers/vhost/vsock.c|587| <<vhost_vsock_handle_tx_kick>> vhost_signal(&vsock->dev, vq);
+	 */
 	if (added)
 		vhost_signal(&vsock->dev, vq);
 
@@ -586,6 +649,14 @@ static int vhost_vsock_start(struct vhost_vsock *vsock)
 
 		if (!vhost_vq_get_backend(vq)) {
 			vhost_vq_set_backend(vq, vsock);
+			/*
+			 * called by:
+			 *   - drivers/vhost/net.c|1658| <<vhost_net_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/scsi.c|2285| <<vhost_scsi_set_endpoint>> vhost_vq_init_access(vq);
+			 *   - drivers/vhost/test.c|213| <<vhost_test_run>> r = vhost_vq_init_access(&n->vqs[index]);
+			 *   - drivers/vhost/test.c|306| <<vhost_test_set_backend>> r = vhost_vq_init_access(vq);
+			 *   - drivers/vhost/vsock.c|643| <<vhost_vsock_start>> ret = vhost_vq_init_access(vq);
+			 */
 			ret = vhost_vq_init_access(vq);
 			if (ret)
 				goto err_vq;
@@ -597,6 +668,15 @@ static int vhost_vsock_start(struct vhost_vsock *vsock)
 	/* Some packets may have been queued before the device was started,
 	 * let's kick the send worker to send them.
 	 */
+	/*
+	 * called by:
+	 *   - drivers/vhost/scsi.c|475| <<vhost_scsi_release_cmd>> if (!vhost_vq_work_queue(&svq->vq, &svq->completion_work))
+	 *   - drivers/vhost/scsi.c|1688| <<vhost_scsi_tmf_flush_work>> if (!vhost_vq_work_queue(vq, &tmf->vwork))
+	 *   - drivers/vhost/scsi.c|2022| <<vhost_scsi_send_evt>> if (!vhost_vq_work_queue(vq, &vs->vs_event_work))
+	 *   - drivers/vhost/vhost.c|357| <<vhost_poll_queue>> vhost_vq_work_queue(poll->vq, &poll->work);
+	 *   - drivers/vhost/vsock.c|319| <<vhost_transport_send_pkt>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 *   - drivers/vhost/vsock.c|662| <<vhost_vsock_start>> vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
+	 */
 	vhost_vq_work_queue(&vsock->vqs[VSOCK_VQ_RX], &vsock->send_pkt_work);
 
 	mutex_unlock(&vsock->dev.mutex);
@@ -678,6 +758,22 @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)
 	vsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;
 	vsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;
 
+	/*
+	 * called by:
+	 *   - drivers/vhost/net.c|1374| <<vhost_net_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_NET_VQ_MAX, UIO_MAXIOV + VHOST_NET_BATCH,
+	 *             HOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true, NULL);
+	 *   - drivers/vhost/scsi.c|2258| <<vhost_scsi_open>> vhost_dev_init(&vs->dev, vqs,
+	 *             nvqs, UIO_MAXIOV, VHOST_SCSI_WEIGHT, 0, true, NULL);
+	 *   - drivers/vhost/test.c|122| <<vhost_test_open>> vhost_dev_init(dev, vqs,
+	 *             VHOST_TEST_VQ_MAX, UIO_MAXIOV,
+	 *             VHOST_TEST_PKT_WEIGHT, VHOST_TEST_WEIGHT, true, NULL);
+	 *   - drivers/vhost/vdpa.c|1433| <<vhost_vdpa_open>> vhost_dev_init(dev, vqs,
+	 *             nvqs, 0, 0, 0, false, vhost_vdpa_process_iotlb_msg);
+	 *   - drivers/vhost/vsock.c|719| <<vhost_vsock_dev_open>> vhost_dev_init(&vsock->dev, vqs,
+	 *             ARRAY_SIZE(vsock->vqs), UIO_MAXIOV,
+	 *             VHOST_VSOCK_PKT_WEIGHT, VHOST_VSOCK_WEIGHT, true, NULL);
+	 */
 	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs),
 		       UIO_MAXIOV, VHOST_VSOCK_PKT_WEIGHT,
 		       VHOST_VSOCK_WEIGHT, true, NULL);
diff --git a/drivers/virtio/virtio.c b/drivers/virtio/virtio.c
index b9095751e..0f89d713c 100644
--- a/drivers/virtio/virtio.c
+++ b/drivers/virtio/virtio.c
@@ -123,10 +123,22 @@ void virtio_check_driver_offered_feature(const struct virtio_device *vdev,
 }
 EXPORT_SYMBOL_GPL(virtio_check_driver_offered_feature);
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio.c|149| <<virtio_config_changed>> __virtio_config_changed(dev);
+ *   - drivers/virtio/virtio.c|198| <<virtio_config_driver_enable>> __virtio_config_changed(dev);
+ *   - drivers/virtio/virtio.c|215| <<virtio_config_core_enable>> __virtio_config_changed(dev);
+ */
 static void __virtio_config_changed(struct virtio_device *dev)
 {
 	struct virtio_driver *drv = drv_to_virtio(dev->dev.driver);
 
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	if (!dev->config_core_enabled || dev->config_driver_disabled)
 		dev->config_change_pending = true;
 	else if (drv && drv->config_changed) {
@@ -135,6 +147,14 @@ static void __virtio_config_changed(struct virtio_device *dev)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|451| <<vu_req_interrupt>> virtio_config_changed(&vu_dev->vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1217| <<virtio_ccw_int_handler>> virtio_config_changed(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|313| <<vm_interrupt>> virtio_config_changed(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|77| <<vp_config_changed>> virtio_config_changed(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|131| <<virtio_vdpa_config_cb>> virtio_config_changed(&vd_dev->vdev);
+ */
 void virtio_config_changed(struct virtio_device *dev)
 {
 	unsigned long flags;
@@ -152,9 +172,20 @@ EXPORT_SYMBOL_GPL(virtio_config_changed);
  * This is only allowed to be called by a driver and disabling can't
  * be nested.
  */
+/*
+ * called by:
+ *   - drivers/net/virtio_net.c|3657| <<virtnet_close>> virtio_config_driver_disable(vi->vdev);
+ *   - drivers/net/virtio_net.c|6874| <<virtnet_probe>> virtio_config_driver_disable(vi->vdev);
+ */
 void virtio_config_driver_disable(struct virtio_device *dev)
 {
 	spin_lock_irq(&dev->config_lock);
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	dev->config_driver_disabled = true;
 	spin_unlock_irq(&dev->config_lock);
 }
@@ -170,6 +201,12 @@ EXPORT_SYMBOL_GPL(virtio_config_driver_disable);
 void virtio_config_driver_enable(struct virtio_device *dev)
 {
 	spin_lock_irq(&dev->config_lock);
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	dev->config_driver_disabled = false;
 	if (dev->config_change_pending)
 		__virtio_config_changed(dev);
@@ -454,6 +491,16 @@ static int virtio_device_of_init(struct virtio_device *dev)
  *
  * Returns: 0 on suceess, -error on failure
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virtio_uml.c|1252| <<virtio_uml_probe>> rc = register_virtio_device(&vu_dev->vdev);
+ *   - drivers/platform/mellanox/mlxbf-tmfifo.c|1233| <<mlxbf_tmfifo_create_vdev>> ret = register_virtio_device(&tm_vdev->vdev);
+ *   - drivers/remoteproc/remoteproc_virtio.c|446| <<rproc_add_virtio_dev>> ret = register_virtio_device(vdev);
+ *   - drivers/s390/virtio/virtio_ccw.c|1401| <<virtio_ccw_online>> ret = register_virtio_device(&vcdev->vdev);
+ *   - drivers/virtio/virtio_mmio.c|688| <<virtio_mmio_probe>> rc = register_virtio_device(&vm_dev->vdev);
+ *   - drivers/virtio/virtio_pci_common.c|723| <<virtio_pci_probe>> rc = register_virtio_device(&vp_dev->vdev);
+ *   - drivers/virtio/virtio_vdpa.c|512| <<virtio_vdpa_probe>> ret = register_virtio_device(&vd_dev->vdev);
+ */
 int register_virtio_device(struct virtio_device *dev)
 {
 	int err;
diff --git a/drivers/virtio/virtio_ring.c b/drivers/virtio/virtio_ring.c
index fdd2d2b07..c3d30eb6f 100644
--- a/drivers/virtio/virtio_ring.c
+++ b/drivers/virtio/virtio_ring.c
@@ -2297,6 +2297,37 @@ static inline int virtqueue_add(struct virtqueue *_vq,
  *
  * Returns zero or a negative error (ie. ENOSPC, ENOMEM, EIO).
  */
+/*
+ * called by:
+ *   - arch/um/drivers/virt-pci.c|135| <<um_pci_send_cmd>> ret = virtqueue_add_sgs(dev->cmd_vq, sgs_list,
+ *   - drivers/block/virtio_blk.c|158| <<virtblk_add_req>> return virtqueue_add_sgs(vq, sgs, num_out, num_in, vbr, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_akcipher_algs.c|256| <<__virtio_crypto_akcipher_do_req>> ret = virtqueue_add_sgs(data_vq->vq, sgs, num_out, num_in, vc_req, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_core.c|59| <<virtio_crypto_ctrl_vq_request>> err = virtqueue_add_sgs(vcrypto->ctrl_vq, sgs, out_sgs, in_sgs, vc_ctrl_req, GFP_ATOMIC);
+ *   - drivers/crypto/virtio/virtio_crypto_skcipher_algs.c|449| <<__virtio_crypto_skcipher_do_req>> err = virtqueue_add_sgs(data_vq->vq, sgs, num_out,
+ *   - drivers/firmware/arm_scmi/transports/virtio.c|524| <<virtio_send_message>> rc = virtqueue_add_sgs(vioch->vqueue, sgs, 1, 1, msg, GFP_ATOMIC);
+ *   - drivers/gpio/gpio-virtio.c|93| <<_virtio_gpio_req>> ret = virtqueue_add_sgs(vgpio->request_vq, sgs, 1, 1, line, GFP_KERNEL);
+ *   - drivers/gpio/gpio-virtio.c|222| <<virtio_gpio_irq_prepare>> ret = virtqueue_add_sgs(vgpio->event_vq, sgs, 1, 1, irq_line, GFP_ATOMIC);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|357| <<virtio_gpu_queue_ctrl_sgs>> ret = virtqueue_add_sgs(vq, sgs, outcnt, incnt, vbuf, GFP_ATOMIC);
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|466| <<virtio_gpu_queue_cursor>> ret = virtqueue_add_sgs(vq, sgs, outcnt, 0, vbuf, GFP_ATOMIC);
+ *   - drivers/i2c/busses/i2c-virtio.c|100| <<virtio_i2c_prepare_reqs>> if (virtqueue_add_sgs(vq, sgs, outcnt, incnt, &reqs[i], GFP_KERNEL)) {
+ *   - drivers/iommu/virtio-iommu.c|247| <<__viommu_add_req>> ret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);
+ *   - drivers/iommu/virtio-iommu.c|251| <<__viommu_add_req>> ret = virtqueue_add_sgs(vq, sg, 1, 1, req, GFP_ATOMIC);
+ *   - drivers/net/virtio_net.c|3461| <<virtnet_send_command_reply>> ret = virtqueue_add_sgs(vi->cvq, sgs, out_num, in_num, vi, GFP_ATOMIC);
+ *   - drivers/nvdimm/nd_virtio.c|78| <<virtio_pmem_flush>> while ((err = virtqueue_add_sgs(vpmem->req_vq, sgs, 1, 1, req_data,
+ *   - drivers/scsi/virtio_scsi.c|471| <<__virtscsi_add_cmd>> return virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
+ *   - drivers/virtio/virtio_mem.c|1405| <<virtio_mem_send_request>> rc = virtqueue_add_sgs(vm->vq, sgs, 1, 1, vm, GFP_KERNEL);
+ *   - drivers/virtio/virtio_pci_modern.c|93| <<virtqueue_exec_admin_cmd>> ret = virtqueue_add_sgs(vq, sgs, out_num, in_num, cmd, GFP_KERNEL);
+ *   - fs/fuse/virtio_fs.c|1441| <<virtio_fs_enqueue_req>> ret = virtqueue_add_sgs(vq, sgs, out_sgs, in_sgs, req, GFP_ATOMIC);
+ *   - net/9p/trans_virtio.c|281| <<p9_virtio_request>> err = virtqueue_add_sgs(chan->vq, sgs, out_sgs, in_sgs, req,
+ *   - net/9p/trans_virtio.c|509| <<p9_virtio_zc_request>> err = virtqueue_add_sgs(chan->vq, sgs, out_sgs, in_sgs, req,
+ *   - net/vmw_vsock/virtio_transport.c|143| <<virtio_transport_send_skb>> ret = virtqueue_add_sgs(vq, sgs, out_sg, in_sg, skb, gfp);
+ *   - net/vmw_vsock/virtio_transport.c|326| <<virtio_vsock_rx_fill>> ret = virtqueue_add_sgs(vq, &p, 0, 1, skb, GFP_KERNEL);
+ *   - sound/virtio/virtio_card.c|41| <<virtsnd_event_send>> if (virtqueue_add_sgs(vqueue, psgs, 0, 1, event, gfp) || !notify)
+ *   - sound/virtio/virtio_ctl_msg.c|151| <<virtsnd_ctl_msg_send>> rc = virtqueue_add_sgs(queue->vqueue, psgs, nouts, nins, msg,
+ *   - sound/virtio/virtio_pcm_msg.c|234| <<virtsnd_pcm_msg_send>> rc = virtqueue_add_sgs(vqueue, psgs, 2, 1, msg,
+ *   - sound/virtio/virtio_pcm_msg.c|237| <<virtsnd_pcm_msg_send>> rc = virtqueue_add_sgs(vqueue, psgs, 1, 2, msg,
+ *   - tools/virtio/vringh_test.c|517| <<main>> err = virtqueue_add_sgs(vq, sgs, 1, 1, &err, GFP_KERNEL);
+ */
 int virtqueue_add_sgs(struct virtqueue *_vq,
 		      struct scatterlist *sgs[],
 		      unsigned int out_sgs,
diff --git a/drivers/virtio/virtio_vdpa.c b/drivers/virtio/virtio_vdpa.c
index 1f60c9d5c..85fa1133d 100644
--- a/drivers/virtio/virtio_vdpa.c
+++ b/drivers/virtio/virtio_vdpa.c
@@ -108,6 +108,22 @@ static bool virtio_vdpa_notify(struct virtqueue *vq)
 	struct vdpa_device *vdpa = vd_get_vdpa(vq->vdev);
 	const struct vdpa_config_ops *ops = vdpa->config;
 
+	/*
+	 * 在以下调用vdpa_config_ops->kick_vq:
+	 *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+	 *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+	 * 在以下设置vdpa_config_ops->kick_vq:
+	 *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+	 *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+	 *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+	 */
 	ops->kick_vq(vdpa, vq->index);
 
 	return true;
@@ -461,6 +477,10 @@ virtio_vdpa_get_vq_affinity(struct virtio_device *vdev, int index)
 	return NULL;
 }
 
+/*
+ * called by:
+ *   - drivers/virtio/virtio_vdpa.c|502| <<virtio_vdpa_probe>> vd_dev->vdev.config = &virtio_vdpa_config_ops;
+ */
 static const struct virtio_config_ops virtio_vdpa_config_ops = {
 	.get		= virtio_vdpa_get,
 	.set		= virtio_vdpa_set,
diff --git a/include/linux/mempool.h b/include/linux/mempool.h
index 7b1514413..39f8546fa 100644
--- a/include/linux/mempool.h
+++ b/include/linux/mempool.h
@@ -56,6 +56,25 @@ extern mempool_t *mempool_create_node_noprof(int min_nr, mempool_alloc_t *alloc_
 #define mempool_create_node(...)					\
 	alloc_hooks(mempool_create_node_noprof(__VA_ARGS__))
 
+/*
+ * 在以下调用mempool_create():
+ *   - drivers/block/aoe/aoeblk.c|357| <<aoeblk_gdalloc>> mp = mempool_create(MIN_BUFS, mempool_alloc_slab, mempool_free_slab,
+ *   - drivers/md/dm-vdo/flush.c|150| <<vdo_make_flusher>> vdo->flusher->flush_pool = mempool_create(1, allocate_flush, free_flush,
+ *   - drivers/nvme/host/auth.c|1022| <<nvme_init_auth>> nvme_chap_buf_pool = mempool_create(16, mempool_alloc_slab,
+ *   - drivers/nvme/target/io-cmd-file.c|59| <<nvmet_file_ns_enable>> ns->bvec_pool = mempool_create(NVMET_MIN_MPOOL_OBJ, mempool_alloc_slab,
+ *   - drivers/scsi/qla2xxx/qla_target.c|7294| <<qlt_init>> qla_tgt_mgmt_cmd_mempool = mempool_create(25, mempool_alloc_slab,
+ *   - drivers/scsi/qla4xxx/ql4_os.c|4279| <<qla4xxx_mem_alloc>> ha->srb_mempool = mempool_create(SRB_MIN_REQ, mempool_alloc_slab,
+ *   - fs/smb/client/smbdirect.c|1441| <<allocate_caches_and_workqueue>> mempool_create(info->send_credit_target, mempool_alloc_slab,
+ *   - fs/smb/client/smbdirect.c|1457| <<allocate_caches_and_workqueue>> mempool_create(info->receive_credit_max, mempool_alloc_slab,
+ *   - fs/smb/server/transport_rdma.c|1832| <<smb_direct_create_pools>> t->sendmsg_mempool = mempool_create(t->send_credit_target,
+ *   - fs/smb/server/transport_rdma.c|1847| <<smb_direct_create_pools>> mempool_create(t->recv_credit_max, mempool_alloc_slab,
+ *   - include/linux/mempool.h|148| <<mempool_create_slab_pool>> mempool_create((_min_nr), mempool_alloc_slab, mempool_free_slab, (void *)(_kc))
+ *   - include/linux/mempool.h|161| <<mempool_create_kmalloc_pool>> mempool_create((_min_nr), mempool_kmalloc, mempool_kfree, \
+ *   - include/linux/mempool.h|174| <<mempool_create_kvmalloc_pool>> return mempool_create(min_nr, mempool_kvmalloc, mempool_kvfree, (void *) size);
+ *   - include/linux/mempool.h|188| <<mempool_create_page_pool>> mempool_create((_min_nr), mempool_alloc_pages, \
+ *   - lib/btree.c|191| <<btree_init>> head->mempool = mempool_create(0, btree_alloc, btree_free, NULL);
+ *   - net/ceph/msgpool.c|46| <<ceph_msgpool_init>> pool->pool = mempool_create(size, msgpool_alloc, msgpool_free, pool);
+ */
 #define mempool_create(_min_nr, _alloc_fn, _free_fn, _pool_data)	\
 	mempool_create_node(_min_nr, _alloc_fn, _free_fn, _pool_data,	\
 			    GFP_KERNEL, NUMA_NO_NODE)
@@ -78,8 +97,72 @@ extern void mempool_free(void *element, mempool_t *pool);
 void *mempool_alloc_slab(gfp_t gfp_mask, void *pool_data);
 void mempool_free_slab(void *element, void *pool_data);
 
+/*
+ * 在以下使用mempool_init_slab_pool():
+ *   - block/bio-integrity.c|572| <<bioset_integrity_create>> if (mempool_init_slab_pool(&bs->bio_integrity_pool,
+ *   - block/bio.c|1734| <<biovec_init_pool>> return mempool_init_slab_pool(pool, pool_entries, bp->slab); 
+ *   - block/bio.c|1800| <<bioset_init>> if (mempool_init_slab_pool(&bs->bio_pool, pool_size, bs->bio_slab))
+ *   - drivers/block/drbd/drbd_main.c|2128| <<drbd_create_mempools>> ret = mempool_init_slab_pool(&drbd_request_mempool, number,
+ *   - drivers/block/drbd/drbd_main.c|2133| <<drbd_create_mempools>> ret = mempool_init_slab_pool(&drbd_ee_mempool, number, drbd_ee_cache);
+ *   - drivers/md/bcache/super.c|1917| <<bch_cache_set_alloc>> if (mempool_init_slab_pool(&c->search, 32, bch_search_cache))
+ *   - drivers/md/dm-bio-prison-v1.c|57| <<dm_bio_prison_create>> ret = mempool_init_slab_pool(&prison->cell_pool, MIN_CELLS, _cell_cache);
+ *   - drivers/md/dm-bio-prison-v2.c|48| <<dm_bio_prison_create_v2>> ret = mempool_init_slab_pool(&prison->cell_pool, MIN_CELLS, _cell_cache);
+ *   - drivers/md/dm-cache-target.c|2535| <<cache_create>> r = mempool_init_slab_pool(&cache->migration_pool, MIGRATION_POOL_SIZE,
+ *   - drivers/md/dm-clone-target.c|1896| <<clone_ctr>> r = mempool_init_slab_pool(&clone->hydration_pool, MIN_HYDRATIONS,
+ *   - drivers/md/dm-integrity.c|4793| <<dm_integrity_ctr>> r = mempool_init_slab_pool(&ic->journal_io_mempool, JOURNAL_IO_MEMPOOL, journal_io_cache);
+ *   - drivers/md/dm-io.c|59| <<dm_io_client_create>> ret = mempool_init_slab_pool(&client->pool, min_ios, _dm_io_cache);
+ *   - drivers/md/dm-kcopyd.c|932| <<dm_kcopyd_client_create>> r = mempool_init_slab_pool(&kc->job_pool, MIN_JOBS, _job_cache);
+ *   - drivers/md/dm-log-userspace-base.c|253| <<userspace_ctr>> r = mempool_init_slab_pool(&lc->flush_entry_pool, FLUSH_ENTRY_POOL_SIZE,
+ *   - drivers/md/dm-snap.c|1343| <<snapshot_ctr>> r = mempool_init_slab_pool(&s->pending_pool, MIN_IOS, pending_cache);
+ *   - drivers/md/dm-thin.c|3025| <<pool_create>> r = mempool_init_slab_pool(&pool->mapping_pool, MAPPING_POOL_SIZE,
+ *   - drivers/md/dm-verity-fec.c|779| <<verity_fec_ctr>> ret = mempool_init_slab_pool(&f->prealloc_pool, num_online_cpus() *
+ *   - drivers/md/dm-verity-fec.c|787| <<verity_fec_ctr>> ret = mempool_init_slab_pool(&f->extra_pool, 0, f->cache);
+ *   - drivers/md/raid5-cache.c|3099| <<r5l_init_log>> ret = mempool_init_slab_pool(&log->io_pool, R5L_POOL_SIZE, log->io_kc);
+ *   - fs/netfs/main.c|115| <<netfs_init>> if (mempool_init_slab_pool(&netfs_request_pool, 100, netfs_request_slab) < 0)
+ *   - fs/netfs/main.c|125| <<netfs_init>> if (mempool_init_slab_pool(&netfs_subrequest_pool, 100, netfs_subrequest_slab) < 0)
+ *   - fs/smb/client/cifsfs.c|1791| <<cifs_init_netfs>> if (mempool_init_slab_pool(&cifs_io_request_pool, 100, cifs_io_request_cachep) < 0)
+ *   - fs/smb/client/cifsfs.c|1801| <<cifs_init_netfs>> if (mempool_init_slab_pool(&cifs_io_subrequest_pool, 100, cifs_io_subrequest_cachep) < 0)
+ *   - mm/kasan/kasan_test_c.c|1173| <<mempool_prepare_slab>> ret = mempool_init_slab_pool(pool, pool_size, cache);
+ */
 #define mempool_init_slab_pool(_pool, _min_nr, _kc)			\
 	mempool_init(_pool, (_min_nr), mempool_alloc_slab, mempool_free_slab, (void *)(_kc))
+/*
+ * 在以下调用mempool_create_slab_pool():
+ *   - arch/sh/kernel/dwarf.c|1179| <<dwarf_unwinder_init>> dwarf_frame_pool = mempool_create_slab_pool(DWARF_FRAME_MIN_REQ, dwarf_frame_cachep);
+ *   - arch/sh/kernel/dwarf.c|1184| <<dwarf_unwinder_init>> dwarf_reg_pool = mempool_create_slab_pool(DWARF_REG_MIN_REQ, dwarf_reg_cachep);
+ *   - block/blk-crypto-fallback.c|592| <<blk_crypto_fallback_init>> mempool_create_slab_pool(num_prealloc_fallback_crypt_ctxs, bio_fallback_crypt_ctx_cache);
+ *   - block/blk-crypto.c|71| <<bio_crypt_ctx_init>> bio_crypt_ctx_pool = mempool_create_slab_pool(num_prealloc_crypt_ctxs, bio_crypt_ctx_cache);
+ *   - drivers/dma/dmaengine.c|1413| <<dmaengine_init_unmap_pool>> p->pool = mempool_create_slab_pool(1, p->cache);
+ *   - drivers/s390/scsi/zfcp_aux.c|236| <<zfcp_allocate_low_mem_buffers>> mempool_create_slab_pool(4, zfcp_fsf_qtcb_cache);
+ *   - drivers/s390/scsi/zfcp_aux.c|247| <<zfcp_allocate_low_mem_buffers>> mempool_create_slab_pool(1, zfcp_fc_req_cache);
+ *   - drivers/scsi/fnic/fnic_main.c|798| <<fnic_probe>> fnic->io_req_pool = mempool_create_slab_pool(2, fnic_io_req_cache);
+ *   - drivers/scsi/fnic/fnic_main.c|802| <<fnic_probe>> pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_DFLT]);
+ *   - drivers/scsi/fnic/fnic_main.c|807| <<fnic_probe>> pool = mempool_create_slab_pool(2, fnic_sgl_cache[FNIC_SGL_CACHE_MAX]);
+ *   - drivers/scsi/libfc/fc_exch.c|2504| <<bool>> mp->ep_pool = mempool_create_slab_pool(2, fc_em_cachep);
+ *   - drivers/scsi/libfc/fc_fcp.c|2308| <<fc_fcp_init>> si->scsi_pkt_pool = mempool_create_slab_pool(2, scsi_pkt_cachep);
+ *   - drivers/scsi/qedf/qedf_main.c|3367| <<__qedf_probe>> qedf->io_mempool = mempool_create_slab_pool(QEDF_IO_WORK_MIN, qedf_io_work_cache);
+ *   - drivers/scsi/qla2xxx/qla_init.c|9768| <<qla2xxx_create_qpair>> qpair->srb_mempool = mempool_create_slab_pool(SRB_MIN_REQ, srb_cachep);
+ *   - drivers/scsi/qla2xxx/qla_os.c|4182| <<qla2x00_mem_alloc>> ha->srb_mempool = mempool_create_slab_pool(SRB_MIN_REQ, srb_cachep);
+ *   - drivers/scsi/qla2xxx/qla_os.c|4195| <<qla2x00_mem_alloc>> ha->ctx_mempool = mempool_create_slab_pool(SRB_MIN_REQ, ctx_cachep);
+ *   - drivers/scsi/snic/snic_main.c|548| <<snic_probe>> pool = mempool_create_slab_pool(2, snic_glob->req_cache[SNIC_REQ_CACHE_DFLT_SGL]);
+ *   - drivers/scsi/snic/snic_main.c|559| <<snic_probe>> pool = mempool_create_slab_pool(2, snic_glob->req_cache[SNIC_REQ_CACHE_MAX_SGL]);
+ *   - drivers/scsi/snic/snic_main.c|570| <<snic_probe>> pool = mempool_create_slab_pool(2, snic_glob->req_cache[SNIC_REQ_TM_CACHE]);
+ *   - drivers/scsi/virtio_scsi.c|1269| <<virtio_scsi_init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache);
+ *   - drivers/tty/hvc/hvc_iucv.c|1355| <<hvc_iucv_init>> hvc_iucv_mempool = mempool_create_slab_pool(MEMPOOL_MIN_NR, hvc_iucv_buffer_cache);
+ *   - fs/ext4/readpage.c|400| <<ext4_init_post_read_processing>> mempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS, bio_post_read_ctx_cache);
+ *   - fs/f2fs/data.c|4149| <<f2fs_init_post_read_processing>> mempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS, bio_post_read_ctx_cache);
+ *   - fs/f2fs/iostat.c|279| <<f2fs_init_iostat_processing>> mempool_create_slab_pool(NUM_PREALLOC_IOSTAT_CTXS, bio_iostat_ctx_cache);
+ *   - fs/jfs/jfs_metapage.c|210| <<metapage_init>> metapage_mempool = mempool_create_slab_pool(METAPOOL_MIN_PAGES, metapage_cache);
+ *   - fs/nfs/write.c|2151| <<nfs_init_writepagecache>> nfs_wdata_mempool = mempool_create_slab_pool(MIN_POOL_WRITE, nfs_wdata_cachep);
+ *   - fs/nfs/write.c|2163| <<nfs_init_writepagecache>> nfs_commit_mempool = mempool_create_slab_pool(MIN_POOL_COMMIT, nfs_cdata_cachep);
+ *   - fs/smb/client/cifsfs.c|1705| <<cifs_init_request_bufs>> cifs_req_poolp = mempool_create_slab_pool(cifs_min_rcv, cifs_req_cachep);
+ *   - fs/smb/client/cifsfs.c|1736| <<cifs_init_request_bufs>> cifs_sm_req_poolp = mempool_create_slab_pool(cifs_min_small, cifs_sm_req_cachep);
+ *   - fs/smb/client/cifsfs.c|1767| <<init_mids>> cifs_mid_poolp = mempool_create_slab_pool(3, cifs_mid_cachep);
+ *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+ *   - net/ceph/osd_client.c|5240| <<ceph_osdc_init>> osdc->req_mempool = mempool_create_slab_pool(10, ceph_osd_request_cache);
+ *   - net/sunrpc/sched.c|1351| <<rpc_init_mempool>> rpc_task_mempool = mempool_create_slab_pool(RPC_TASK_POOLSIZE, rpc_task_slabp);
+ *   - net/sunrpc/sched.c|1355| <<rpc_init_mempool>> rpc_buffer_mempool = mempool_create_slab_pool(RPC_BUFFER_POOLSIZE, rpc_buffer_slabp);
+ */
 #define mempool_create_slab_pool(_min_nr, _kc)			\
 	mempool_create((_min_nr), mempool_alloc_slab, mempool_free_slab, (void *)(_kc))
 
diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h
index e2dd57ca3..71ed724fc 100644
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@ -428,6 +428,43 @@ static inline int mmu_notifier_test_young(struct mm_struct *mm,
 	return 0;
 }
 
+/*
+ * called by:
+ *   - fs/proc/task_mmu.c|1593| <<clear_refs_write>> mmu_notifier_invalidate_range_start(&range);
+ *   - fs/proc/task_mmu.c|2774| <<do_pagemap_scan>> mmu_notifier_invalidate_range_start(&range);
+ *   - kernel/events/uprobes.c|189| <<__replace_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|1815| <<do_huge_zero_wp_pmd>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|2517| <<move_pages_huge_pmd>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|2675| <<__split_huge_pud>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/huge_memory.c|2983| <<__split_huge_pmd>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5232| <<copy_hugetlb_page_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5464| <<move_hugetlb_page_tables>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5721| <<unmap_hugepage_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|5949| <<hugetlb_wp>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|6798| <<hugetlb_change_protection>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/hugetlb.c|7531| <<hugetlb_unshare_pmds>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/khugepaged.c|1176| <<collapse_huge_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/khugepaged.c|1598| <<collapse_pte_mapped_thp>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/khugepaged.c|1758| <<retract_page_tables>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/ksm.c|1266| <<write_protect_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/ksm.c|1372| <<replace_page>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/madvise.c|821| <<madvise_free_single_vma>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/mapping_dirty_helpers.c|179| <<wp_clean_pre_vma>> mmu_notifier_invalidate_range_start(&wpwalk->range);
+ *   - mm/memory.c|1401| <<copy_page_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|1909| <<unmap_vmas>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|1944| <<zap_page_range_single>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|3398| <<wp_page_copy>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/memory.c|3919| <<remove_device_exclusive_entry>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/migrate_device.c|308| <<migrate_vma_collect>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/migrate_device.c|723| <<__migrate_device_pages>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/mprotect.c|460| <<change_pud_range>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/mremap.c|620| <<move_page_tables>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|1037| <<page_vma_mkclean_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|1690| <<try_to_unmap_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|2062| <<try_to_migrate_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/rmap.c|2403| <<page_make_device_exclusive_one>> mmu_notifier_invalidate_range_start(&range);
+ *   - mm/userfaultfd.c|1147| <<move_pages_pte>> mmu_notifier_invalidate_range_start(&range);
+ */
 static inline void
 mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
 {
diff --git a/include/linux/scatterlist.h b/include/linux/scatterlist.h
index d836e7440..437f37fa2 100644
--- a/include/linux/scatterlist.h
+++ b/include/linux/scatterlist.h
@@ -198,6 +198,39 @@ static inline void sg_set_buf(struct scatterlist *sg, const void *buf,
 /*
  * Loop over each sg element in the given sg_table object.
  */
+/*
+ * 在以下调用for_each_sgtable_sg():
+ *   - drivers/dma-buf/dma-buf.c|788| <<mangle_sg_table>> for_each_sgtable_sg(sg_table, sg, i)
+ *   - drivers/dma-buf/heaps/system_heap.c|74| <<dup_sg_table>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|292| <<system_heap_dma_buf_release>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/dma-buf/heaps/system_heap.c|406| <<system_heap_allocate>> for_each_sgtable_sg(table, sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|703| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) 
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|713| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|734| <<amdgpu_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/amd/amdgpu/amdgpu_vram_mgr.c|765| <<amdgpu_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|409| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/armada/armada_gem.c|442| <<armada_gem_prime_map_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/armada/armada_gem.c|465| <<armada_gem_prime_unmap_dma_buf>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/drm/msm/msm_iommu.c|124| <<msm_iommu_pagetable_map>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/core/firmware.c|272| <<nvkm_firmware_ctor>> for_each_sgtable_sg(&fw->mem.sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2195| <<nvkm_gsp_sg_free>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2215| <<nvkm_gsp_sg>> for_each_sgtable_sg(sgt, sgl, i) {
+ *   - drivers/gpu/drm/nouveau/nvkm/subdev/gsp/r535.c|2308| <<nvkm_gsp_radix3_sg>> for_each_sgtable_sg(&rx3->lvl2, sg, i) {
+ *   - drivers/gpu/drm/rockchip/rockchip_drm_gem.c|104| <<rockchip_gem_get_pages>> for_each_sgtable_sg(rk_obj->sgt, s, i)
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|222| <<drm_gem_shmem_test_get_pages_sgt>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/tests/drm_gem_shmem_test.c|257| <<drm_gem_shmem_test_get_sg_table>> for_each_sgtable_sg(sgt, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_object.c|169| <<virtio_gpu_object_shmem_init>> for_each_sgtable_sg(pages, sg, si) {
+ *   - drivers/gpu/drm/virtio/virtgpu_vq.c|296| <<vmalloc_to_sgt>> for_each_sgtable_sg(sgt, sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|404| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i)
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|414| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|435| <<xe_ttm_vram_mgr_alloc_sgt>> for_each_sgtable_sg((*sgt), sg, i) {
+ *   - drivers/gpu/drm/xe/xe_ttm_vram_mgr.c|456| <<xe_ttm_vram_mgr_free_sgt>> for_each_sgtable_sg(sgt, sg, i)
+ *   - drivers/gpu/host1x/job.c|238| <<pin_job>> for_each_sgtable_sg(map->sgt, sg, j)
+ *   - drivers/infiniband/core/umem.c|58| <<__ib_umem_release>> for_each_sgtable_sg(&umem->sgt_append.sgt, sg, i)
+ *   - drivers/media/common/videobuf2/videobuf2-vmalloc.c|234| <<vb2_vmalloc_dmabuf_ops_attach>> for_each_sgtable_sg(sgt, sg, i) {
+ *
+ * 注释: Loop over each sg element in the given sg_table object.
+ */
 #define for_each_sgtable_sg(sgt, sg, i)		\
 	for_each_sg((sgt)->sgl, sg, (sgt)->orig_nents, i)
 
@@ -510,6 +543,31 @@ size_t sg_zero_buffer(struct scatterlist *sgl, unsigned int nents,
  */
 #define SG_MAX_SINGLE_ALLOC		(PAGE_SIZE / sizeof(struct scatterlist))
 
+/*
+ * 在以下使用SG_CHUNK_SIZE:
+ *   - drivers/scsi/esas2r/esas2r_main.c|248| <<global>> .sg_tablesize = SG_CHUNK_SIZE,
+ *   - drivers/scsi/esas2r/esas2r_main.c|269| <<global>> int sg_tablesize = SG_CHUNK_SIZE;
+ *   - lib/sg_pool.c|27| <<global>> #if (SG_CHUNK_SIZE < 32)
+ *   - lib/sg_pool.c|28| <<global>> #error SG_CHUNK_SIZE is too small (must be 32 or greater)
+ *   - lib/sg_pool.c|41| <<global>> #if (SG_CHUNK_SIZE > 32)
+ *   - lib/sg_pool.c|43| <<global>> #if (SG_CHUNK_SIZE > 64)
+ *   - lib/sg_pool.c|45| <<global>> #if (SG_CHUNK_SIZE > 128)
+ *   - lib/sg_pool.c|47| <<global>> #if (SG_CHUNK_SIZE > 256)
+ *   - lib/sg_pool.c|48| <<global>> #error SG_CHUNK_SIZE is too large (256 MAX)
+ *   - lib/sg_pool.c|48| <<global>> #error SG_CHUNK_SIZE is too large (256 MAX)
+ *   - lib/sg_pool.c|53| <<global>> SP(SG_CHUNK_SIZE)
+ *   - drivers/scsi/mpt3sas/mpt3sas_base.h|109| <<MPT_MAX_PHYS_SEGMENTS>> #define MPT_MAX_PHYS_SEGMENTS SG_CHUNK_SIZE
+ *   - fs/smb/server/transport_rdma.c|1322| <<smb_direct_free_rdma_rw_msg>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1408| <<smb_direct_rdma_xmit>> msg = kzalloc(struct_size(msg, sg_list, SG_CHUNK_SIZE),
+ *   - fs/smb/server/transport_rdma.c|1424| <<smb_direct_rdma_xmit>> msg->sg_list, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1434| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1448| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - include/linux/scatterlist.h|527| <<SG_MAX_SEGMENTS>> #define SG_MAX_SEGMENTS SG_CHUNK_SIZE
+ *   - include/scsi/scsi_host.h|24| <<SG_ALL>> #define SG_ALL SG_CHUNK_SIZE
+ *   - lib/sg_pool.c|66| <<sg_pool_index>> BUG_ON(nents > SG_CHUNK_SIZE);
+ *   - lib/sg_pool.c|141| <<sg_free_table_chained>> __sg_free_table(table, SG_CHUNK_SIZE, nents_first_chunk, sg_pool_free,
+ *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents, SG_CHUNK_SIZE,
+ */
 /*
  * The maximum number of SG segments that we will put inside a
  * scatterlist (unless chaining is used). Should ideally fit inside a
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 64934e083..afadae270 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -548,6 +548,49 @@ struct sched_entity {
 
 	struct list_head		group_node;
 	unsigned char			on_rq;
+	/*
+	 * 在以下设置sched_entity->sched_delayed:
+	 *   - kernel/sched/fair.c|5374| <<set_delayed>> se->sched_delayed = 1;
+	 *   - kernel/sched/fair.c|5386| <<clear_delayed>> se->sched_delayed = 0;
+	 * 在以下使用sched_entity->sched_delayed:
+	 *   - include/linux/sched.h|2164| <<task_is_runnable>> return p->on_rq && !p->se.sched_delayed;
+	 *   - kernel/sched/core.c|270| <<sched_core_enqueue>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|283| <<sched_core_dequeue>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|1757| <<uclamp_rq_inc>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|1784| <<uclamp_rq_dec>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|3784| <<ttwu_runnable>> if (p->se.sched_delayed)
+	 *   - kernel/sched/core.c|4190| <<try_to_wake_up>> SCHED_WARN_ON(p->se.sched_delayed);
+	 *   - kernel/sched/core.c|4484| <<__sched_fork>> SCHED_WARN_ON(p->se.sched_delayed);
+	 *   - kernel/sched/core.c|7213| <<rt_mutex_setprio>> if (prev_class != next_class && p->se.sched_delayed)
+	 *   - kernel/sched/ext.c|4969| <<scx_ops_disable_workfn>> if (old_class != new_class && p->se.sched_delayed)
+	 *   - kernel/sched/ext.c|5687| <<scx_ops_enable>> if (old_class != new_class && p->se.sched_delayed)
+	 *   - kernel/sched/fair.c|5412| <<dequeue_entity>> SCHED_WARN_ON(!se->sched_delayed);
+	 *   - kernel/sched/fair.c|5422| <<dequeue_entity>> SCHED_WARN_ON(delay && se->sched_delayed);
+	 *   - kernel/sched/fair.c|5546| <<pick_next_entity>> SCHED_WARN_ON(cfs_rq->next->sched_delayed);
+	 *   - kernel/sched/fair.c|5551| <<pick_next_entity>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|5887| <<throttle_cfs_rq>> if (se->sched_delayed)
+	 *   - kernel/sched/fair.c|5986| <<unthrottle_cfs_rq>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|6853| <<requeue_delayed_entity>> SCHED_WARN_ON(!se->sched_delayed);
+	 *   - kernel/sched/fair.c|6896| <<enqueue_task_fair>> if (!(p->se.sched_delayed &&
+	 *            (task_on_rq_migrating(p) || (flags & ENQUEUE_RESTORE))))
+	 *   - kernel/sched/fair.c|6913| <<enqueue_task_fair>> h_nr_delayed = !!se->sched_delayed;
+	 *   - kernel/sched/fair.c|6917| <<enqueue_task_fair>> if (se->sched_delayed)
+	 *   - kernel/sched/fair.c|7033| <<dequeue_entities>> h_nr_delayed = !!se->sched_delayed;
+	 *   - kernel/sched/fair.c|7134| <<dequeue_task_fair>> if (!(p->se.sched_delayed &&
+	 *            (task_on_rq_migrating(p) || (flags & DEQUEUE_SAVE))))
+	 *   - kernel/sched/fair.c|8629| <<task_dead_fair>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|8634| <<task_dead_fair>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|8719| <<check_preempt_wakeup_fair>> if (sched_feat(NEXT_BUDDY) &&
+	 *            !(wake_flags & WF_FORK) && !pse->sched_delayed) {
+	 *   - kernel/sched/fair.c|13127| <<switched_to_fair>> SCHED_WARN_ON(p->se.sched_delayed);
+	 *   - kernel/sched/fair.c|13162| <<__set_next_task_fair>> SCHED_WARN_ON(se->sched_delayed);
+	 *   - kernel/sched/fair.c|13307| <<unregister_fair_sched_group>> if (se->sched_delayed) {
+	 *   - kernel/sched/fair.c|13309| <<unregister_fair_sched_group>> if (se->sched_delayed) {
+	 *   - kernel/sched/sched.h|916| <<se_runnable>> if (se->sched_delayed)
+	 *   - kernel/sched/sched.h|933| <<se_runnable>> if (se->sched_delayed)
+	 *   - kernel/sched/stats.h|141| <<psi_enqueue>> if (p->se.sched_delayed) {
+	 *   - kernel/sched/syscalls.c|712| <<__sched_setscheduler>> if (prev_class != next_class && p->se.sched_delayed)
+	 */
 	unsigned char			sched_delayed;
 	unsigned char			rel_deadline;
 	unsigned char			custom_slice;
@@ -1317,6 +1360,28 @@ struct task_struct {
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 	int				numa_scan_seq;
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	unsigned int			numa_scan_period;
 	unsigned int			numa_scan_period_max;
 	int				numa_preferred_nid;
@@ -1325,6 +1390,13 @@ struct task_struct {
 	u64				node_stamp;
 	u64				last_task_numa_placement;
 	u64				last_sum_exec_runtime;
+	/*
+	 * 在以下使用task_struct->numa_work:
+	 *   - kernel/sched/fair.c|3305| <<task_numa_work>> SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
+	 *   - kernel/sched/fair.c|3560| <<init_numa_balancing>> p->numa_work.next = &p->numa_work;
+	 *   - kernel/sched/fair.c|3568| <<init_numa_balancing>> init_task_work(&p->numa_work, task_numa_work);
+	 *   - kernel/sched/fair.c|3595| <<task_tick_numa>> struct callback_head *work = &curr->numa_work;
+	 */
 	struct callback_head		numa_work;
 
 	/*
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 5a64582b0..933bf20a0 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -19,6 +19,27 @@ enum sched_tunable_scaling {
 	SCHED_TUNABLESCALING_END,
 };
 
+/*
+ * 在以下使用NUMA_BALANCING_DISABLED:
+ *   - kernel/sched/core.c|4566| <<set_numabalancing_state>> sysctl_numa_balancing_mode = NUMA_BALANCING_DISABLED;
+ *
+ * 在以下使用NUMA_BALANCING_NORMAL:
+ *   - kernel/sched/core.c|4564| <<set_numabalancing_state>> sysctl_numa_balancing_mode = NUMA_BALANCING_NORMAL;
+ *   - mm/huge_memory.c|2351| <<change_huge_pmd>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_NORMAL) &&
+ *   - mm/mprotect.c|161| <<change_pte_range>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_NORMAL) &&
+ *
+ * 在以下使用NUMA_BALANCING_MEMORY_TIERING:
+ *   - kernel/sched/core.c|4598| <<sysctl_numa_balancing>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
+ *   - kernel/sched/core.c|4599| <<sysctl_numa_balancing>> (state & NUMA_BALANCING_MEMORY_TIERING))
+ *   - kernel/sched/fair.c|1951| <<should_numa_migrate_memory>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
+ *   - kernel/sched/fair.c|3170| <<task_numa_fault>> (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING ||
+ *   - mm/memory-tiers.c|67| <<folio_use_access_time>> return (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) &&
+ *   - mm/migrate.c|709| <<folio_migrate_flags>> if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING) {
+ *   - mm/migrate.c|2652| <<migrate_misplaced_folio_prepare>> if (!(sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING))
+ *   - mm/migrate.c|2705| <<migrate_misplaced_folio>> if ((sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)
+ *   - mm/vmscan.c|4771| <<should_abort_scan>> mark = sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING ?
+ *   - mm/vmscan.c|6699| <<pgdat_balanced>> if (sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING)
+ */
 #define NUMA_BALANCING_DISABLED		0x0
 #define NUMA_BALANCING_NORMAL		0x1
 #define NUMA_BALANCING_MEMORY_TIERING	0x2
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 58009fa66..0d1e4e600 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -938,6 +938,16 @@ struct sk_buff {
 				nohdr:1,
 				fclone:2,
 				peeked:1,
+				/*
+				 * 在以下设置sk_buff->head_frag:
+				 *   - net/core/skbuff.c|494| <<build_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|516| <<build_skb_around>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|562| <<napi_build_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|780| <<__netdev_alloc_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|868| <<napi_alloc_skb>> skb->head_frag = 1;
+				 *   - net/core/skbuff.c|6696| <<pskb_carve_inside_header>> skb->head_frag = 0;
+				 *   - net/core/skbuff.c|6829| <<pskb_carve_inside_nonlinear>> skb->head_frag = 0;
+				 */
 				head_frag:1,
 				pfmemalloc:1,
 				pp_recycle:1; /* page_pool recycle indicator */
diff --git a/include/linux/vdpa.h b/include/linux/vdpa.h
index 2e7a30fe6..0531202e8 100644
--- a/include/linux/vdpa.h
+++ b/include/linux/vdpa.h
@@ -374,6 +374,22 @@ struct vdpa_config_ops {
 			      u16 idx, u64 desc_area, u64 driver_area,
 			      u64 device_area);
 	void (*set_vq_num)(struct vdpa_device *vdev, u16 idx, u32 num);
+	/*
+	 * 在以下调用vdpa_config_ops->kick_vq:
+	 *   - drivers/vhost/vdpa.c|173| <<handle_vq_kick>> ops->kick_vq(v->vdpa, vq - v->vqs);
+	 *   - drivers/virtio/virtio_vdpa.c|111| <<virtio_vdpa_notify>> ops->kick_vq(vdpa, vq->index);
+	 * 在以下设置vdpa_config_ops->kick_vq:
+	 *   - drivers/vdpa/alibaba/eni_vdpa.c|434| <<global>> .kick_vq = eni_vdpa_kick_vq,
+	 *   - drivers/vdpa/ifcvf/ifcvf_main.c|650| <<global>> .kick_vq = ifcvf_vdpa_kick_vq,
+	 *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3656| <<global>> .kick_vq = mlx5_vdpa_kick_vq,
+	 *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|344| <<global>> .kick_vq = octep_vdpa_kick_vq,
+	 *   - drivers/vdpa/pds/vdpa_dev.c|581| <<global>> .kick_vq = pds_vdpa_kick_vq,
+	 *   - drivers/vdpa/solidrun/snet_main.c|529| <<global>> .kick_vq = snet_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|774| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_sim/vdpa_sim.c|815| <<global>> .kick_vq = vdpasim_kick_vq,
+	 *   - drivers/vdpa/vdpa_user/vduse_dev.c|787| <<global>> .kick_vq = vduse_vdpa_kick_vq,
+	 *   - drivers/vdpa/virtio_pci/vp_vdpa.c|474| <<global>> .kick_vq = vp_vdpa_kick_vq,
+	 */
 	void (*kick_vq)(struct vdpa_device *vdev, u16 idx);
 	void (*kick_vq_with_data)(struct vdpa_device *vdev, u32 data);
 	void (*set_vq_cb)(struct vdpa_device *vdev, u16 idx,
@@ -464,6 +480,17 @@ struct vdpa_device *__vdpa_alloc_device(struct device *parent,
  *
  * Return allocated data structure or ERR_PTR upon error
  */
+/*
+ * called by:
+ *   - drivers/vdpa/alibaba/eni_vdpa.c|480| <<eni_vdpa_probe>> eni_vdpa = vdpa_alloc_device(struct eni_vdpa, vdpa,
+ *   - drivers/vdpa/ifcvf/ifcvf_main.c|707| <<ifcvf_vdpa_dev_add>> adapter = vdpa_alloc_device(struct ifcvf_adapter, vdpa,
+ *   - drivers/vdpa/mlx5/net/mlx5_vnet.c|3879| <<mlx5_vdpa_dev_add>> ndev = vdpa_alloc_device(struct mlx5_vdpa_net, mvdev.vdev, mdev->device, &mgtdev->vdpa_ops,
+ *   - drivers/vdpa/octeon_ep/octep_vdpa_main.c|475| <<octep_vdpa_dev_add>> oct_vdpa = vdpa_alloc_device(struct octep_vdpa, vdpa, &pdev->dev, &octep_vdpa_ops, 1, 1,
+ *   - drivers/vdpa/pds/vdpa_dev.c|634| <<pds_vdpa_dev_add>> pdsv = vdpa_alloc_device(struct pds_vdpa_device, vdpa_dev,
+ *   - drivers/vdpa/solidrun/snet_main.c|1012| <<snet_vdpa_probe_vf>> snet = vdpa_alloc_device(struct snet, vdpa, &pdev->dev, &snet_config_ops, 1, 1, NULL,
+ *   - drivers/vdpa/vdpa_user/vduse_dev.c|2010| <<vduse_dev_init_vdpa>> vdev = vdpa_alloc_device(struct vduse_vdpa, vdpa, dev->dev,
+ *   - drivers/vdpa/virtio_pci/vp_vdpa.c|504| <<vp_vdpa_dev_add>> vp_vdpa = vdpa_alloc_device(struct vp_vdpa, vdpa,
+ */
 #define vdpa_alloc_device(dev_struct, member, parent, config, ngroups, nas, \
 			  name, use_va) \
 			  container_of((__vdpa_alloc_device( \
diff --git a/include/linux/virtio.h b/include/linux/virtio.h
index dd88682e2..c5e2f38c8 100644
--- a/include/linux/virtio.h
+++ b/include/linux/virtio.h
@@ -150,6 +150,12 @@ struct virtio_device {
 	int index;
 	bool failed;
 	bool config_core_enabled;
+	/*
+	 * 在以下使用virtio_device->config_driver_disabled:
+	 *   - drivers/virtio/virtio.c|130| <<__virtio_config_changed>> if (!dev->config_core_enabled || dev->config_driver_disabled)
+	 *   - drivers/virtio/virtio.c|158| <<virtio_config_driver_disable>> dev->config_driver_disabled = true;
+	 *   - drivers/virtio/virtio.c|173| <<virtio_config_driver_enable>> dev->config_driver_disabled = false;
+	 */
 	bool config_driver_disabled;
 	bool config_change_pending;
 	spinlock_t config_lock;
diff --git a/include/target/target_core_base.h b/include/target/target_core_base.h
index 97099a5e3..d75090871 100644
--- a/include/target/target_core_base.h
+++ b/include/target/target_core_base.h
@@ -725,6 +725,21 @@ struct se_dev_attrib {
 	u32		max_unmap_block_desc_count;
 	u32		unmap_granularity;
 	u32		unmap_granularity_alignment;
+	/*
+	 * 在以下使用se_dev_attrib->max_write_same_len:
+	 *   - drivers/target/target_core_configfs.c|595| <<global>> DEF_CONFIGFS_ATTRIB_SHOW(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|618| <<global>> DEF_CONFIGFS_ATTRIB_STORE_U32(max_write_same_len);
+	 *   - drivers/target/target_core_configfs.c|1318| <<global>> CONFIGFS_ATTR(, max_write_same_len);
+	 *   - drivers/target/target_core_device.c|775| <<target_alloc_device>> dev->dev_attrib.max_write_same_len = DA_MAX_WRITE_SAME_LEN;
+	 *   - drivers/target/target_core_file.c|174| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_file.c|192| <<fd_configure_device>> dev->dev_attrib.max_write_same_len = 0x1000;
+	 *   - drivers/target/target_core_iblock.c|143| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = max_write_zeroes_sectors;
+	 *   - drivers/target/target_core_iblock.c|145| <<iblock_configure_device>> dev->dev_attrib.max_write_same_len = 0xFFFF;
+	 *   - drivers/target/target_core_sbc.c|294| <<sbc_setup_write_same>> if (sectors > cmd->se_dev->dev_attrib.max_write_same_len) {
+	 *   - drivers/target/target_core_sbc.c|296| <<sbc_setup_write_same>> pr_warn("WRITE_SAME sectors: %u exceeds max_write_same_len: %u\n",
+	 *                    sectors, cmd->se_dev->dev_attrib.max_write_same_len);
+	 *   - drivers/target/target_core_spc.c|599| <<spc_emulate_evpd_b0>> put_unaligned_be64(dev->dev_attrib.max_write_same_len, &buf[36]);
+	 */
 	u32		max_write_same_len;
 	u8		submit_type;
 	struct se_device *da_dev;
diff --git a/include/uapi/linux/vhost_types.h b/include/uapi/linux/vhost_types.h
index d7656908f..16964ba19 100644
--- a/include/uapi/linux/vhost_types.h
+++ b/include/uapi/linux/vhost_types.h
@@ -164,6 +164,17 @@ struct vhost_vdpa_iova_range {
 };
 
 /* Feature bits */
+/*
+ * 在以下使用VHOST_F_LOG_ALL:
+ *   - drivers/vhost/vhost.h|302| <<global>> (1ULL << VHOST_F_LOG_ALL) |
+ *   - drivers/vhost/net.c|1315| <<handle_rx>> vq_log = unlikely(vhost_has_feature(vq, VHOST_F_LOG_ALL)) ?
+ *   - drivers/vhost/net.c|1859| <<vhost_net_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/scsi.c|2516| <<vhost_scsi_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/test.c|270| <<vhost_test_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/vhost.c|1398| <<memory_access_ok>> log = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);
+ *   - drivers/vhost/vhost.c|2151| <<vq_log_access_ok>> vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&
+ *   - drivers/vhost/vsock.c|902| <<vhost_vsock_set_features>> if ((features & (1 << VHOST_F_LOG_ALL)) &&
+ */
 /* Log all write descriptors. Can be changed while device is active. */
 #define VHOST_F_LOG_ALL 26
 /* vhost-net should add virtio_net_hdr for RX, and strip for TX packets. */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 3e5a6bf58..2af57a066 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2074,6 +2074,22 @@ void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 		sched_core_enqueue(rq, p);
 }
 
+/*
+ * 在以下使用dequeue_task():
+ *   - kernel/sched/core.c|2126| <<deactivate_task>> dequeue_task(rq, p, flags);
+ *   - kernel/sched/core.c|2131| <<block_task>> if (dequeue_task(rq, p, DEQUEUE_SLEEP | flags))
+ *   - kernel/sched/core.c|2734| <<__do_set_cpus_allowed>> dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/core.c|7214| <<rt_mutex_setprio>> dequeue_task(rq, p, DEQUEUE_SLEEP | DEQUEUE_DELAYED | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/core.c|7219| <<rt_mutex_setprio>> dequeue_task(rq, p, queue_flag);
+ *   - kernel/sched/core.c|7917| <<sched_setnuma>> dequeue_task(rq, p, DEQUEUE_SAVE);
+ *   - kernel/sched/core.c|9069| <<sched_move_task>> dequeue_task(rq, tsk, queue_flags);
+ *   - kernel/sched/core.c|10687| <<sched_deq_and_put_task>> dequeue_task(rq, p, queue_flags | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/ext.c|4970| <<scx_ops_disable_workfn>> dequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/ext.c|5688| <<scx_ops_enable>> dequeue_task(task_rq(p), p, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/syscalls.c|96| <<set_user_nice>> dequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/syscalls.c|713| <<__sched_setscheduler>> dequeue_task(rq, p, DEQUEUE_SLEEP | DEQUEUE_DELAYED | DEQUEUE_NOCLOCK);
+ *   - kernel/sched/syscalls.c|718| <<__sched_setscheduler>> dequeue_task(rq, p, queue_flags);
+ */
 /*
  * Must only return false when DEQUEUE_SLEEP.
  */
@@ -2095,6 +2111,15 @@ inline bool dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 	 * and mark the task ->sched_delayed.
 	 */
 	uclamp_rq_dec(rq, p);
+	/*
+	 * 在以下使用dequeue_task方法:
+	 *   - kernel/sched/deadline.c|3120| <<global>> .dequeue_task = dequeue_task_dl,
+	 *   - kernel/sched/ext.c|4369| <<global>> .dequeue_task = dequeue_task_scx,
+	 *   - kernel/sched/fair.c|13565| <<global>> .dequeue_task = dequeue_task_fair,
+	 *   - kernel/sched/idle.c|521| <<global>> .dequeue_task = dequeue_task_idle,
+	 *   - kernel/sched/rt.c|2620| <<global>> .dequeue_task = dequeue_task_rt,
+	 *   - kernel/sched/stop_task.c|100| <<global>> .dequeue_task = dequeue_task_stop,
+	 */
 	return p->sched_class->dequeue_task(rq, p, flags);
 }
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 26958431d..32ae1b368 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2628,6 +2628,28 @@ static void numa_migrate_preferred(struct task_struct *p)
 	if (unlikely(p->numa_preferred_nid == NUMA_NO_NODE || !p->numa_faults))
 		return;
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	/* Periodically retry migrating the task to the preferred node */
 	interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
 	p->numa_migrate_retry = jiffies + interval;
@@ -2701,6 +2723,28 @@ static void update_task_scan_period(struct task_struct *p,
 	 * node is overloaded. In either case, scan slower
 	 */
 	if (local + shared == 0 || p->numa_faults_locality[2]) {
+		/*
+		 * 在以下设置task_struct->numa_scan_period:
+		 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+		 *            p->numa_scan_period << 1);
+		 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+		 *            task_scan_min(p), task_scan_max(p));
+		 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+		 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+		 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+		 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+		 * 在以下使用task_struct->numa_scan_period:
+		 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+		 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+		 *            p->numa_scan_period << 1);
+		 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+		 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+		 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+		 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+		 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+		 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+		 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+		 */
 		p->numa_scan_period = min(p->numa_scan_period_max,
 			p->numa_scan_period << 1);
 
@@ -3284,6 +3328,36 @@ static bool vma_is_accessed(struct mm_struct *mm, struct vm_area_struct *vma)
  * The expensive part of numa migration is done from task_work context.
  * Triggered from task_tick_numa().
  */
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * 在以下使用task_struct->numa_work:
+ *   - kernel/sched/fair.c|3305| <<task_numa_work>> SCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work));
+ *   - kernel/sched/fair.c|3560| <<init_numa_balancing>> p->numa_work.next = &p->numa_work;
+ *   - kernel/sched/fair.c|3568| <<init_numa_balancing>> init_task_work(&p->numa_work, task_numa_work);
+ *   - kernel/sched/fair.c|3595| <<task_tick_numa>> struct callback_head *work = &curr->numa_work;
+ *
+ * 在以下使用task_numa_work():
+ *   - kernel/sched/fair.c|3564| <<init_numa_balancing>> init_task_work(&p->numa_work, task_numa_work);
+ */
 static void task_numa_work(struct callback_head *work)
 {
 	unsigned long migrate, next_scan, now = jiffies;
@@ -3324,6 +3398,28 @@ static void task_numa_work(struct callback_head *work)
 	if (time_before(now, migrate))
 		return;
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	if (p->numa_scan_period == 0) {
 		p->numa_scan_period_max = task_scan_max(p);
 		p->numa_scan_period = task_scan_start(p);
@@ -3368,6 +3464,10 @@ static void task_numa_work(struct callback_head *work)
 	}
 
 	for (; vma; vma = vma_next(&vmi)) {
+		/*
+		 * 只在这里调用vma_policy_mof().
+		 * 似乎返回false会更好?
+		 */
 		if (!vma_migratable(vma) || !vma_policy_mof(vma) ||
 			is_vm_hugetlb_page(vma) || (vma->vm_flags & VM_MIXEDMAP)) {
 			trace_sched_skip_vma_numa(mm, vma, NUMAB_SKIP_UNSUITABLE);
@@ -3544,6 +3644,28 @@ void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 			mm->numa_scan_seq = 0;
 		}
 	}
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	p->node_stamp			= 0;
 	p->numa_scan_seq		= mm ? mm->numa_scan_seq : 0;
 	p->numa_scan_period		= sysctl_numa_balancing_scan_delay;
@@ -3582,6 +3704,30 @@ void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 /*
  * Drive the periodic memory faults..
  */
+/*
+ * 5.15的例子:
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - kernel/sched/fair.c|13067| <<task_tick_fair>> task_tick_numa(rq, curr);
+ */
 static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 {
 	struct callback_head *work = &curr->numa_work;
@@ -3593,6 +3739,28 @@ static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 	if (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work)
 		return;
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	/*
 	 * Using runtime rather than walltime has the dual advantage that
 	 * we (mostly) drive the selection from busy threads and that the
@@ -3643,6 +3811,28 @@ static void update_scan_period(struct task_struct *p, int new_cpu)
 			return;
 	}
 
+	/*
+	 * 在以下设置task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2704| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2752| <<update_task_scan_period>> p->numa_scan_period = clamp(p->numa_scan_period + diff,
+	 *            task_scan_min(p), task_scan_max(p));
+	 *   - kernel/sched/fair.c|3333| <<task_numa_work>> p->numa_scan_period = task_scan_start(p);
+	 *   - kernel/sched/fair.c|3557| <<init_numa_balancing>> p->numa_scan_period = sysctl_numa_balancing_scan_delay;
+	 *   - kernel/sched/fair.c|3615| <<task_tick_numa>> curr->numa_scan_period = task_scan_start(curr);
+	 *   - kernel/sched/fair.c|3654| <<update_scan_period>> p->numa_scan_period = task_scan_start(p);
+	 * 在以下使用task_struct->numa_scan_period:
+	 *   - kernel/sched/fair.c|2632| <<numa_migrate_preferred>> interval = min(interval, msecs_to_jiffies(p->numa_scan_period) / 16);
+	 *   - kernel/sched/fair.c|2705| <<update_task_scan_period>> p->numa_scan_period = min(p->numa_scan_period_max,
+	 *            p->numa_scan_period << 1);
+	 *   - kernel/sched/fair.c|2708| <<update_task_scan_period>> p->mm->numa_next_scan = jiffies + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|2719| <<update_task_scan_period>> period_slot = DIV_ROUND_UP(p->numa_scan_period, NUMA_PERIOD_SLOTS);
+	 *   - kernel/sched/fair.c|3331| <<task_numa_work>> if (p->numa_scan_period == 0) {
+	 *   - kernel/sched/fair.c|3336| <<task_numa_work>> next_scan = now + msecs_to_jiffies(p->numa_scan_period);
+	 *   - kernel/sched/fair.c|3584| <<init_numa_balancing>> delay = min_t(unsigned int, task_scan_max(current),
+	 *            current->numa_scan_period * mm_users * NSEC_PER_MSEC);
+	 *   - kernel/sched/fair.c|3611| <<task_tick_numa>> period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
+	 */
 	p->numa_scan_period = task_scan_start(p);
 }
 
@@ -5369,6 +5559,10 @@ static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se)
 
 static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq);
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5427| <<dequeue_entity>> set_delayed(se);
+ */
 static void set_delayed(struct sched_entity *se)
 {
 	se->sched_delayed = 1;
@@ -5381,6 +5575,11 @@ static void set_delayed(struct sched_entity *se)
 	}
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5398| <<finish_delayed_dequeue_entity>> clear_delayed(se);
+ *   - kernel/sched/fair.c|6871| <<requeue_delayed_entity>> clear_delayed(se);
+ */
 static void clear_delayed(struct sched_entity *se)
 {
 	se->sched_delayed = 0;
@@ -5400,6 +5599,12 @@ static inline void finish_delayed_dequeue_entity(struct sched_entity *se)
 		se->vlag = 0;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+ *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+ *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+ */
 static bool
 dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 {
@@ -5421,9 +5626,22 @@ dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
 
 		SCHED_WARN_ON(delay && se->sched_delayed);
 
+		/*
+		 * 注释:
+		 * Delay dequeueing tasks until they get selected or woken.
+		 *
+		 * By delaying the dequeue for non-eligible tasks, they remain in the
+		 * competition and can burn off their negative lag. When they get selected
+		 * they'll have positive lag by definition.
+		 *
+		 * DELAY_ZERO clips the lag on dequeue (or wakeup) to 0.
+		 */
 		if (sched_feat(DELAY_DEQUEUE) && delay &&
 		    !entity_eligible(cfs_rq, se)) {
 			update_load_avg(cfs_rq, se, 0);
+			/*
+			 * 只在此处调用
+			 */
 			set_delayed(se);
 			return false;
 		}
@@ -5831,6 +6049,11 @@ static int tg_throttle_down(struct task_group *tg, void *data)
 	return 0;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|6414| <<check_enqueue_throttle>> throttle_cfs_rq(cfs_rq);
+ *   - kernel/sched/fair.c|6450| <<check_cfs_rq_runtime>> return throttle_cfs_rq(cfs_rq);
+ */
 static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 {
 	struct rq *rq = rq_of(cfs_rq);
@@ -5886,6 +6109,12 @@ static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 		flags = DEQUEUE_SLEEP | DEQUEUE_SPECIAL;
 		if (se->sched_delayed)
 			flags |= DEQUEUE_DELAYED;
+		/*
+		 * called by:
+		 *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+		 */
 		dequeue_entity(qcfs_rq, se, flags);
 
 		if (cfs_rq_is_idle(group_cfs_rq(se)))
@@ -5986,6 +6215,12 @@ void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)
 		if (se->sched_delayed) {
 			int flags = DEQUEUE_SLEEP | DEQUEUE_DELAYED;
 
+			/*
+			 * called by:
+			 *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+			 *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+			 *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+			 */
 			dequeue_entity(qcfs_rq, se, flags);
 		} else if (se->on_rq)
 			break;
@@ -6409,6 +6644,11 @@ static void sync_throttle(struct task_group *tg, int cpu)
 	cfs_rq->throttled_clock_pelt = rq_clock_pelt(cpu_rq(cpu));
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5595| <<put_prev_entity>> check_cfs_rq_runtime(cfs_rq);
+ *   - kernel/sched/fair.c|8821| <<pick_task_fair>> if (unlikely(check_cfs_rq_runtime(cfs_rq)))
+ */
 /* conditionally throttle active cfs_rq's from put_prev_entity() */
 static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 {
@@ -7012,6 +7252,13 @@ static void set_next_buddy(struct sched_entity *se);
  *  0 - dequeue throttled
  *  1 - dequeue complete
  */
+/*
+ * called by:
+ *   - kernel/sched/fair.c|5580| <<pick_next_entity>> dequeue_entities(rq, se, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/fair.c|7194| <<dequeue_task_fair>> if (dequeue_entities(rq, &p->se, flags) < 0)
+ *   - kernel/sched/fair.c|8692| <<task_dead_fair>> dequeue_entities(rq, se, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ *   - kernel/sched/fair.c|13367| <<unregister_fair_sched_group>> dequeue_entities(rq, se, DEQUEUE_SLEEP | DEQUEUE_DELAYED);
+ */
 static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 {
 	bool was_sched_idle = sched_idle_rq(rq);
@@ -7039,6 +7286,12 @@ static int dequeue_entities(struct rq *rq, struct sched_entity *se, int flags)
 	for_each_sched_entity(se) {
 		cfs_rq = cfs_rq_of(se);
 
+		/*
+		 * called by:
+		 *   - kernel/sched/fair.c|5911| <<throttle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|6011| <<unthrottle_cfs_rq>> dequeue_entity(qcfs_rq, se, flags);
+		 *   - kernel/sched/fair.c|7064| <<dequeue_entities>> if (!dequeue_entity(cfs_rq, se, flags)) {
+		 */
 		if (!dequeue_entity(cfs_rq, se, flags)) {
 			if (p && &p->se == se)
 				return -1;
diff --git a/kernel/vhost_task.c b/kernel/vhost_task.c
index 8800f5acc..02c226e5b 100644
--- a/kernel/vhost_task.c
+++ b/kernel/vhost_task.c
@@ -115,6 +115,14 @@ EXPORT_SYMBOL_GPL(vhost_task_stop);
  * failure. The returned task is inactive, and the caller must fire it up
  * through vhost_task_start().
  */
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7420| <<kvm_mmu_post_init_vm>> kvm->arch.nx_huge_page_recovery_thread =
+ *            vhost_task_create(kvm_nx_huge_page_recovery_worker,
+ *                              kvm_nx_huge_page_recovery_worker_kill, kvm, "kvm-nx-lpage-recovery");
+ * drivers/vhost/vhost.c|701| <<vhost_worker_create>> vtsk = vhost_task_create(vhost_run_work_list,
+ *            vhost_worker_killed, worker, name);
+ */
 struct vhost_task *vhost_task_create(bool (*fn)(void *),
 				     void (*handle_sigkill)(void *), void *arg,
 				     const char *name)
diff --git a/lib/iov_iter.c b/lib/iov_iter.c
index 9ec806f98..caca66327 100644
--- a/lib/iov_iter.c
+++ b/lib/iov_iter.c
@@ -159,6 +159,41 @@ size_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t size)
 }
 EXPORT_SYMBOL(fault_in_iov_iter_writeable);
 
+/*
+ * called by:
+ *   - drivers/acpi/pfr_update.c|458| <<pfru_write>> iov_iter_init(&iter, ITER_SOURCE, &iov, 1, len);
+ *   - drivers/fsi/fsi-sbefifo.c|845| <<sbefifo_user_read>> iov_iter_init(&resp_iter, ITER_DEST, &resp_iov, 1, len); 
+ *   - drivers/net/ppp/ppp_generic.c|486| <<ppp_read>> iov_iter_init(&to, ITER_DEST, &iov, 1, count); 
+ *   - drivers/vhost/net.c|704| <<init_iov_iter>> iov_iter_init(iter, ITER_SOURCE, vq->iov, out, len);
+ *   - drivers/vhost/net.c|1357| <<handle_rx>> iov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, 1, 1);
+ *   - drivers/vhost/net.c|1364| <<handle_rx>> iov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, in, vhost_len);
+ *   - drivers/vhost/scsi.c|854| <<vhost_scsi_complete_cmd_work>> iov_iter_init(&iov_iter, ITER_DEST,
+ *             cmd->tvc_resp_iov, cmd->tvc_in_iovs, sizeof(v_rsp));
+ *   - drivers/vhost/scsi.c|1307| <<vhost_scsi_get_desc>> iov_iter_init(&vc->out_iter, ITER_SOURCE,
+ *             vq->iov, vc->out, vc->out_size);
+ *   - drivers/vhost/scsi.c|1481| <<vhost_scsi_handle_vq>> iov_iter_init(&in_iter, ITER_DEST,
+ *             &vq->iov[vc.out], vc.in, vc.rsp_size + exp_data_len);
+ *   - drivers/vhost/scsi.c|1634| <<vhost_scsi_send_tmf_resp>> iov_iter_init(&iov_iter, ITER_DEST,
+ *             resp_iov, in_iovs, sizeof(rsp));
+ *   - drivers/vhost/scsi.c|1841| <<vhost_scsi_send_an_resp>> iov_iter_init(&iov_iter, ITER_DEST,
+ *             &vq->iov[vc->out], vc->in, sizeof(rsp));
+ *   - drivers/vhost/vhost.c|1423| <<vhost_copy_to_user>> iov_iter_init(&t, ITER_DEST,
+ *             vq->iotlb_iov, ret, size);
+ *   - drivers/vhost/vhost.c|1485| <<vhost_copy_from_user>> iov_iter_init(&f, ITER_SOURCE,
+ *             vq->iotlb_iov, ret, size);
+ *   - drivers/vhost/vhost.c|3007| <<get_indirect>> iov_iter_init(&from, ITER_SOURCE,
+ *             vq->indirect, ret, len);
+ *   - drivers/vhost/vringh.c|1204| <<copy_from_iotlb>> iov_iter_init(&iter, ITER_SOURCE, ivec.iov.iovec, ret, translated);
+ *   - drivers/vhost/vringh.c|1250| <<copy_to_iotlb>> iov_iter_init(&iter, ITER_DEST, ivec.iov.iovec, ret, translated);
+ *   - drivers/vhost/vsock.c|178| <<vhost_transport_do_send_pkt>> iov_iter_init(&iov_iter, ITER_DEST, &vq->iov[out], in, iov_len);
+ *   - drivers/vhost/vsock.c|388| <<vhost_vsock_alloc_skb>> iov_iter_init(&iov_iter, ITER_SOURCE, vq->iov, out, len);
+ *   - fs/fuse/ioctl.c|328| <<fuse_do_ioctl>> iov_iter_init(&ii, ITER_SOURCE, in_iov, in_iovs, in_size);
+ *   - fs/fuse/ioctl.c|395| <<fuse_do_ioctl>> iov_iter_init(&ii, ITER_DEST, out_iov, out_iovs, transferred);
+ *   - fs/seq_file.c|159| <<seq_read>> iov_iter_init(&iter, ITER_DEST, &iov, 1, size);
+ *   - io_uring/net.c|639| <<io_send>> iov_iter_init(&kmsg->msg.msg_iter, ITER_SOURCE, arg.iovs, ret, arg.out_len);
+ *   - io_uring/net.c|1108| <<io_recv_buf_select>> iov_iter_init(&kmsg->msg.msg_iter, ITER_DEST, arg.iovs, ret, arg.out_len);
+ *   - lib/iov_iter.c|1487| <<__import_iovec>> iov_iter_init(i, type, iov, nr_segs, total_len);
+ */
 void iov_iter_init(struct iov_iter *i, unsigned int direction,
 			const struct iovec *iov, unsigned long nr_segs,
 			size_t count)
@@ -1209,6 +1244,28 @@ static ssize_t __iov_iter_get_pages_alloc(struct iov_iter *i,
 	return -EFAULT;
 }
 
+/*
+ * 在以下调用iov_iter_get_pages2():
+ *   - drivers/block/ublk_drv.c|879| <<ublk_copy_user_pages>> len = iov_iter_get_pages2(uiter,
+ *             iter.pages, iov_iter_count(uiter), UBLK_MAX_PIN_PAGES, &off);
+ *   - drivers/vhost/scsi.c|1135| <<vhost_scsi_map_to_sgl>> bytes = iov_iter_get_pages2(iter,
+ *             pages, LONG_MAX, VHOST_SCSI_PREALLOC_UPAGES, &offset);
+ *   - fs/ceph/file.c|100| <<__iter_get_bvecs>> bytes = iov_iter_get_pages2(iter, pages,
+ *             maxsize - size, ITER_GET_BVECS_PAGES, &start);
+ *   - fs/fuse/dev.c|786| <<fuse_copy_fill>> err = iov_iter_get_pages2(cs->iter, &page,
+ *             PAGE_SIZE, 1, &off);
+ *   - fs/splice.c|1465| <<iter_to_pipe>> left = iov_iter_get_pages2(from, pages, ~0UL, 16, &start);
+ *   - net/ceph/messenger.c|992| <<ceph_msg_data_iter_next>> len = iov_iter_get_pages2(&cursor->iov_iter,
+ *             &page, PAGE_SIZE, 1, page_offset);
+ *   - net/core/datagram.c|642| <<zerocopy_fill_skb_from_iter>> copied = iov_iter_get_pages2(from, pages,
+ *             length, MAX_SKB_FRAGS - frag, &start);
+ *   - net/core/skmsg.c|328| <<sk_msg_zerocopy_from_iter>> copied = iov_iter_get_pages2(from, pages,
+ *             bytes, maxpages, &offset);
+ *   - net/rds/message.c|393| <<rds_message_zcopy_from_user>> copied = iov_iter_get_pages2(from, &pages,
+ *             PAGE_SIZE, 1, &start);
+ *   - net/tls/tls_sw.c|1382| <<tls_setup_from_iter>> copied = iov_iter_get_pages2(from, pages, length,
+ *             maxpages, &offset);
+ */
 ssize_t iov_iter_get_pages2(struct iov_iter *i, struct page **pages,
 		size_t maxsize, unsigned maxpages, size_t *start)
 {
diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 5bb6b8aff..ba66a73d6 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -193,6 +193,12 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
  *    that previously used with __sg_alloc_table().
  *
  **/
+/*
+ * called by:
+ *   - lib/scatterlist.c|246| <<sg_free_append_table>> __sg_free_table(&table->sgt, SG_MAX_SINGLE_ALLOC, 0, sg_kfree, table->total_nents);
+ *   - lib/scatterlist.c|259| <<sg_free_table>> __sg_free_table(table, SG_MAX_SINGLE_ALLOC, 0, sg_kfree, table->orig_nents);
+ *   - lib/sg_pool.c|141| <<sg_free_table_chained>> __sg_free_table(table, SG_CHUNK_SIZE, nents_first_chunk, sg_pool_free, table->orig_nents);
+ */
 void __sg_free_table(struct sg_table *table, unsigned int max_ents,
 		     unsigned int nents_first_chunk, sg_free_fn *free_fn,
 		     unsigned int num_ents)
@@ -283,6 +289,13 @@ EXPORT_SYMBOL(sg_free_table);
  *   __sg_free_table() to cleanup any leftover allocations.
  *
  **/
+/*
+ * called by:
+ *   - lib/scatterlist.c|385| <<sg_alloc_table>> ret = __sg_alloc_table(table, nents,
+ *         SG_MAX_SINGLE_ALLOC, NULL, 0, gfp_mask, sg_kmalloc);
+ *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents,
+ *         SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+ */
 int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		     unsigned int max_ents, struct scatterlist *first_chunk,
 		     unsigned int nents_first_chunk, gfp_t gfp_mask,
@@ -376,6 +389,13 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 {
 	int ret;
 
+	/*
+	 * called by:
+	 *   - lib/scatterlist.c|385| <<sg_alloc_table>> ret = __sg_alloc_table(table, nents,
+	 *         SG_MAX_SINGLE_ALLOC, NULL, 0, gfp_mask, sg_kmalloc);
+	 *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents,
+	 *         SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+	 */
 	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
 			       NULL, 0, gfp_mask, sg_kmalloc);
 	if (unlikely(ret))
diff --git a/lib/sg_pool.c b/lib/sg_pool.c
index 9bfe60ca3..55e4f8eb0 100644
--- a/lib/sg_pool.c
+++ b/lib/sg_pool.c
@@ -4,7 +4,16 @@
 #include <linux/mempool.h>
 #include <linux/slab.h>
 
+/*
+ * 在以下使用SG_MEMPOOL_NR:
+ *   - lib/sg_pool.c|187| <<sg_pool_init>> for (i = 0; i < SG_MEMPOOL_NR; i++) {
+ *   - lib/sg_pool.c|226| <<sg_pool_init>> for (i = 0; i < SG_MEMPOOL_NR; i++) {
+ */
 #define SG_MEMPOOL_NR		ARRAY_SIZE(sg_pools)
+/*
+ * 在以下使用SG_MEMPOOL_SIZE:
+ *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+ */
 #define SG_MEMPOOL_SIZE		2
 
 struct sg_pool {
@@ -18,6 +27,14 @@ struct sg_pool {
 #if (SG_CHUNK_SIZE < 32)
 #error SG_CHUNK_SIZE is too small (must be 32 or greater)
 #endif
+/*
+ * 在以下使用sg_pools[]:
+ *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+ *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+ *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+ *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+ *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+ */
 static struct sg_pool sg_pools[] = {
 	SP(8),
 	SP(16),
@@ -37,6 +54,11 @@ static struct sg_pool sg_pools[] = {
 };
 #undef SP
 
+/*
+ * called by:
+ *   - lib/sg_pool.c|83| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+ *   - lib/sg_pool.c|99| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+ */
 static inline unsigned int sg_pool_index(unsigned short nents)
 {
 	unsigned int index;
@@ -51,18 +73,44 @@ static inline unsigned int sg_pool_index(unsigned short nents)
 	return index;
 }
 
+/*
+ * called by:
+ *   - lib/sg_pool.c|126| <<sg_free_table_chained>> __sg_free_table(table, SG_CHUNK_SIZE,
+ *      nents_first_chunk, sg_pool_free, table->orig_nents);
+ */
 static void sg_pool_free(struct scatterlist *sgl, unsigned int nents)
 {
 	struct sg_pool *sgp;
 
+	/*
+	 * 在以下使用sg_pools[]:
+	 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+	 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 */
 	sgp = sg_pools + sg_pool_index(nents);
 	mempool_free(sgl, sgp->pool);
 }
 
+/*
+ * called by:
+ *   - lib/sg_pool.c|181| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table,
+ *     nents, SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+ */
 static struct scatterlist *sg_pool_alloc(unsigned int nents, gfp_t gfp_mask)
 {
 	struct sg_pool *sgp;
 
+	/*
+	 * 在以下使用sg_pools[]:
+	 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+	 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+	 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+	 */
 	sgp = sg_pools + sg_pool_index(nents);
 	return mempool_alloc(sgp->pool, gfp_mask);
 }
@@ -81,6 +129,26 @@ static struct scatterlist *sg_pool_alloc(unsigned int nents, gfp_t gfp_mask)
  *    to sg_alloc_table_chained().
  *
  **/
+/*
+ * called by:
+ *   - drivers/block/rnbd/rnbd-clt.c|373| <<rnbd_softirq_done_fn>> sg_free_table_chained(&iu->sgt, RNBD_INLINE_SG_CNT);
+ *   - drivers/block/rnbd/rnbd-clt.c|1153| <<rnbd_queue_rq>> sg_free_table_chained(&iu->sgt, RNBD_INLINE_SG_CNT);
+ *   - drivers/block/virtio_blk.c|210| <<virtblk_unmap_data>> sg_free_table_chained(&vbr->sg_table, VIRTIO_BLK_INLINE_SG_CNT);
+ *   - drivers/nvme/host/fc.c|2619| <<nvme_fc_map_data>> sg_free_table_chained(&freq->sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/host/fc.c|2642| <<nvme_fc_unmap_data>> sg_free_table_chained(&freq->sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1222| <<nvme_rdma_dma_unmap_req>> sg_free_table_chained(&req->metadata_sgl->sg_table, NVME_INLINE_METADATA_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1228| <<nvme_rdma_dma_unmap_req>> sg_free_table_chained(&req->data_sgl.sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1516| <<nvme_rdma_dma_map_req>> sg_free_table_chained(&req->metadata_sgl->sg_table, NVME_INLINE_METADATA_SG_CNT);
+ *   - drivers/nvme/host/rdma.c|1522| <<nvme_rdma_dma_map_req>> sg_free_table_chained(&req->data_sgl.sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/nvme/target/loop.c|78| <<nvme_loop_complete_rq>> sg_free_table_chained(&iod->sg_table, NVME_INLINE_SG_CNT);
+ *   - drivers/scsi/scsi_lib.c|583| <<scsi_free_sgtables>> sg_free_table_chained(&cmd->sdb.table, SCSI_INLINE_SG_CNT);
+ *   - drivers/scsi/scsi_lib.c|586| <<scsi_free_sgtables>> sg_free_table_chained(&cmd->prot_sdb->table, SCSI_INLINE_PROT_SG_CNT);
+ *   - fs/smb/server/transport_rdma.c|1322| <<smb_direct_free_rdma_rw_msg>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1434| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - fs/smb/server/transport_rdma.c|1448| <<smb_direct_rdma_xmit>> sg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);
+ *   - lib/sg_pool.c|204| <<sg_alloc_table_chained>> sg_free_table_chained(table, nents_first_chunk);
+ *   - net/sunrpc/xprtrdma/svc_rdma_rw.c|94| <<__svc_rdma_put_rw_ctxt>> sg_free_table_chained(&ctxt->rw_sg_table, ctxt->rw_first_sgl_nents);
+ */
 void sg_free_table_chained(struct sg_table *table,
 		unsigned nents_first_chunk)
 {
@@ -109,6 +177,25 @@ EXPORT_SYMBOL_GPL(sg_free_table_chained);
  *    non-chain SGL.
  *
  **/
+/*
+ * called by:
+ *   - drivers/block/rnbd/rnbd-clt.c|1131| <<rnbd_queue_rq>> err = sg_alloc_table_chained(&iu->sgt,
+ *   - drivers/block/virtio_blk.c|223| <<virtblk_map_data>> err = sg_alloc_table_chained(&vbr->sg_table,
+ *   - drivers/nvme/host/fc.c|2608| <<nvme_fc_map_data>> ret = sg_alloc_table_chained(&freq->sg_table,
+ *   - drivers/nvme/host/rdma.c|1473| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->data_sgl.sg_table,
+ *   - drivers/nvme/host/rdma.c|1492| <<nvme_rdma_dma_map_req>> ret = sg_alloc_table_chained(&req->metadata_sgl->sg_table,
+ *   - drivers/nvme/target/loop.c|157| <<nvme_loop_queue_rq>> if (sg_alloc_table_chained(&iod->sg_table,
+ *   - drivers/scsi/scsi_lib.c|1135| <<scsi_alloc_sgtables>> if (unlikely(sg_alloc_table_chained(&cmd->sdb.table, nr_segs,
+ *   - drivers/scsi/scsi_lib.c|1180| <<scsi_alloc_sgtables>> if (sg_alloc_table_chained(&prot_sdb->table,
+ *   - fs/smb/server/transport_rdma.c|1422| <<smb_direct_rdma_xmit>> ret = sg_alloc_table_chained(&msg->sgt,
+ *   - net/sunrpc/xprtrdma/svc_rdma_rw.c|78| <<svc_rdma_get_rw_ctxt>> if (sg_alloc_table_chained(&ctxt->rw_sg_table, sges, ctxt->rw_sg_table.sgl, first_sgl_nents))
+ *
+ * struct sg_table {
+ *     struct scatterlist *sgl;        // the list
+ *     unsigned int nents;             // number of mapped entries
+ *     unsigned int orig_nents;        // original size of list
+ * };
+ */
 int sg_alloc_table_chained(struct sg_table *table, int nents,
 		struct scatterlist *first_chunk, unsigned nents_first_chunk)
 {
@@ -130,6 +217,13 @@ int sg_alloc_table_chained(struct sg_table *table, int nents,
 		nents_first_chunk = 0;
 	}
 
+	/*
+	 * called by:
+	 *   - lib/scatterlist.c|385| <<sg_alloc_table>> ret = __sg_alloc_table(table, nents,
+	 *         SG_MAX_SINGLE_ALLOC, NULL, 0, gfp_mask, sg_kmalloc);
+	 *   - lib/sg_pool.c|200| <<sg_alloc_table_chained>> ret = __sg_alloc_table(table, nents,
+	 *         SG_CHUNK_SIZE, first_chunk, nents_first_chunk, GFP_ATOMIC, sg_pool_alloc);
+	 */
 	ret = __sg_alloc_table(table, nents, SG_CHUNK_SIZE,
 			       first_chunk, nents_first_chunk,
 			       GFP_ATOMIC, sg_pool_alloc);
@@ -139,11 +233,30 @@ int sg_alloc_table_chained(struct sg_table *table, int nents,
 }
 EXPORT_SYMBOL_GPL(sg_alloc_table_chained);
 
+/*
+ * 在以下使用sg_pool_init():
+ *   - lib/sg_pool.c|193| <<global>> subsys_initcall(sg_pool_init);
+ */
 static __init int sg_pool_init(void)
 {
 	int i;
 
 	for (i = 0; i < SG_MEMPOOL_NR; i++) {
+		/*
+		 * struct sg_pool {
+		 *     size_t          size;
+		 *     char            *name;
+		 *     struct kmem_cache       *slab;
+		 *     mempool_t       *pool;
+		 * };
+		 *
+		 * 在以下使用sg_pools[]:
+		 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+		 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 */
 		struct sg_pool *sgp = sg_pools + i;
 		int size = sgp->size * sizeof(struct scatterlist);
 
@@ -155,6 +268,11 @@ static __init int sg_pool_init(void)
 			goto cleanup_sdb;
 		}
 
+		/*
+		 * mempool_create_slab_pool()有意思的调用:
+		 *   - drivers/scsi/virtio_scsi.c|1269| <<virtio_scsi_init>> mempool_create_slab_pool(VIRTIO_SCSI_MEMPOOL_SZ, virtscsi_cmd_cache);
+		 *   - lib/sg_pool.c|214| <<sg_pool_init>> sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE, sgp->slab);
+		 */
 		sgp->pool = mempool_create_slab_pool(SG_MEMPOOL_SIZE,
 						     sgp->slab);
 		if (!sgp->pool) {
@@ -168,6 +286,14 @@ static __init int sg_pool_init(void)
 
 cleanup_sdb:
 	for (i = 0; i < SG_MEMPOOL_NR; i++) {
+		/*
+		 * 在以下使用sg_pools[]:
+		 *   - lib/sg_pool.c|7| <<SG_MEMPOOL_NR>> #define SG_MEMPOOL_NR ARRAY_SIZE(sg_pools)
+		 *   - lib/sg_pool.c|58| <<sg_pool_free>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|66| <<sg_pool_alloc>> sgp = sg_pools + sg_pool_index(nents);
+		 *   - lib/sg_pool.c|160| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 *   - lib/sg_pool.c|184| <<sg_pool_init>> struct sg_pool *sgp = sg_pools + i;
+		 */
 		struct sg_pool *sgp = sg_pools + i;
 
 		mempool_destroy(sgp->pool);
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index db64116a4..74163c13a 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -2274,6 +2274,10 @@ bool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,
  *      or if prot_numa but THP migration is not supported
  *  - HPAGE_PMD_NR if protections changed and TLB flush necessary
  */
+/*
+ * called by:
+ *   - mm/mprotect.c|402| <<change_pmd_range>> ret = change_huge_pmd(tlb, vma, pmd,
+ */
 int change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		    pmd_t *pmd, unsigned long addr, pgprot_t newprot,
 		    unsigned long cp_flags)
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 162407fbf..6d75c7d34 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -667,6 +667,29 @@ static int queue_folios_hugetlb(pte_t *pte, unsigned long hmask,
  * an architecture makes a different choice, it will need further
  * changes to the core.
  */
+/*
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl        
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * called by:
+ *   - kernel/sched/fair.c|3472| <<task_numa_work>> nr_pte_updates = change_prot_numa(vma, start, end);
+ */
 unsigned long change_prot_numa(struct vm_area_struct *vma,
 			unsigned long addr, unsigned long end)
 {
@@ -1835,6 +1858,10 @@ struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
 	return pol;
 }
 
+/*
+ * called by:
+ *   - kernel/sched/fair.c|3371| <<task_numa_work>> if (!vma_migratable(vma) || !vma_policy_mof(vma) ||
+ */
 bool vma_policy_mof(struct vm_area_struct *vma)
 {
 	struct mempolicy *pol;
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index fc18fe274..7de75c6c0 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -347,6 +347,10 @@ static void mn_hlist_release(struct mmu_notifier_subscriptions *subscriptions,
 	synchronize_srcu(&srcu);
 }
 
+/*
+ * 在以下使用__mmu_notifier_release():
+ *   - include/linux/mmu_notifier.h|402| <<mmu_notifier_release>> __mmu_notifier_release(mm);
+ */
 void __mmu_notifier_release(struct mm_struct *mm)
 {
 	struct mmu_notifier_subscriptions *subscriptions =
@@ -518,6 +522,40 @@ static int mn_hlist_invalidate_range_start(
 	return ret;
 }
 
+/*
+ * 5.15例子.
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ *
+ * struct mmu_notifier_range {
+ *     struct mm_struct *mm;
+ *     unsigned long start;
+ *     unsigned long end;
+ *     unsigned flags;
+ *     enum mmu_notifier_event event;
+ *     void *owner;
+ * };
+ *
+ * called by:
+ *   - include/linux/mmu_notifier.h|439| <<mmu_notifier_invalidate_range_start>> __mmu_notifier_invalidate_range_start(range);
+ *   - include/linux/mmu_notifier.h|459| <<mmu_notifier_invalidate_range_start_nonblock>> ret = __mmu_notifier_invalidate_range_start(range);
+ */
 int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)
 {
 	struct mmu_notifier_subscriptions *subscriptions =
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 516b1d847..4bda4b5ab 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -358,6 +358,26 @@ pgtable_populate_needed(struct vm_area_struct *vma, unsigned long cp_flags)
 		err;							\
 	})
 
+/*
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static inline long change_pmd_range(struct mmu_gather *tlb,
 		struct vm_area_struct *vma, pud_t *pud, unsigned long addr,
 		unsigned long end, pgprot_t newprot, unsigned long cp_flags)
@@ -510,6 +530,26 @@ static inline long change_p4d_range(struct mmu_gather *tlb,
 	return pages;
 }
 
+/*
+ * mmu_spte_clear_track_bits
+ * pte_list_destroy
+ * kvm_unmap_gfn_range
+ * kvm_mmu_notifier_invalidate_range_start
+ * __mmu_notifier_invalidate_range_start
+ * change_pmd_range.isra.0
+ * change_p4d_range
+ * change_protection_range
+ * change_prot_numa
+ * task_numa_work
+ * task_work_run
+ * xfer_to_guest_mode_handle_work
+ * vcpu_run
+ * kvm_arch_vcpu_ioctl_run
+ * kvm_vcpu_ioctl
+ * __x64_sys_ioctl
+ * do_syscall_64
+ * entry_SYSCALL_64_after_hwframe
+ */
 static long change_protection_range(struct mmu_gather *tlb,
 		struct vm_area_struct *vma, unsigned long addr,
 		unsigned long end, pgprot_t newprot, unsigned long cp_flags)
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 6841e61a6..cd6523271 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -4688,6 +4688,16 @@ EXPORT_SYMBOL_GPL(skb_segment_list);
  *	a pointer to the first in a list of new skbs for the segments.
  *	In case of error it returns ERR_PTR(err).
  */
+/*
+ * called by:
+ *   - lib/test_bpf.c|15096| <<test_skb_segment_single>> segs = skb_segment(skb, test->features);
+ *   - net/core/net_test.c|229| <<gso_test_func>> segs = skb_segment(skb, features);
+ *   - net/ipv4/tcp_offload.c|176| <<tcp_gso_segment>> segs = skb_segment(skb, features);
+ *   - net/ipv4/udp_offload.c|327| <<__udp_gso_segment>> segs = skb_segment(gso_skb, features);
+ *   - net/ipv4/udp_offload.c|468| <<udp4_ufo_fragment>> segs = skb_segment(skb, features);
+ *   - net/ipv6/udp_offload.c|109| <<udp6_ufo_fragment>> segs = skb_segment(skb, features);
+ *   - net/sctp/offload.c|72| <<sctp_gso_segment>> segs = skb_segment(skb, (features | NETIF_F_HW_CSUM) & ~NETIF_F_SG);
+ */
 struct sk_buff *skb_segment(struct sk_buff *head_skb,
 			    netdev_features_t features)
 {
diff --git a/virt/kvm/dirty_ring.c b/virt/kvm/dirty_ring.c
index 7bc74969a..d80634a5e 100644
--- a/virt/kvm/dirty_ring.c
+++ b/virt/kvm/dirty_ring.c
@@ -69,6 +69,18 @@ static void kvm_reset_dirty_gfn(struct kvm *kvm, u32 slot, u64 offset, u64 mask)
 	if (!memslot || (offset + __fls(mask)) >= memslot->npages)
 		return;
 
+	/*
+	 * 在以下使用KVM_MMU_LOCK():
+	 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+	 */
 	KVM_MMU_LOCK(kvm);
 	kvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, mask);
 	KVM_MMU_UNLOCK(kvm);
diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c
index 249ba5b72..a7c6592e0 100644
--- a/virt/kvm/eventfd.c
+++ b/virt/kvm/eventfd.c
@@ -30,8 +30,24 @@
 
 #ifdef CONFIG_HAVE_KVM_IRQCHIP
 
+/*
+ * 在以下使用irqfd_cleanup_wq:
+ *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+ *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+ *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+ *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+ *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+ *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+ *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+ */
 static struct workqueue_struct *irqfd_cleanup_wq;
 
+/*
+ * 使用kvm_arch_irqfd_allowed()的地方:
+ *   - arch/x86/kvm/irq.c|167| <<kvm_arch_irqfd_allowed>> bool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
+ *   - virt/kvm/eventfd.c|36| <<kvm_arch_irqfd_allowed>> kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
+ *   - virt/kvm/eventfd.c|315| <<kvm_irqfd_assign>> if (!kvm_arch_irqfd_allowed(kvm, args))
+ */
 bool __attribute__((weak))
 kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)
 {
@@ -169,6 +185,12 @@ irqfd_is_active(struct kvm_kernel_irqfd *irqfd)
  *
  * assumes kvm->irqfds.lock is held
  */
+/*
+ * called by:
+ *   - virt/kvm/eventfd.c|240| <<irqfd_wakeup>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|548| <<kvm_irqfd_deassign>> irqfd_deactivate(irqfd);
+ *   - virt/kvm/eventfd.c|589| <<kvm_irqfd_release>> irqfd_deactivate(irqfd);
+ */
 static void
 irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 {
@@ -176,6 +198,16 @@ irqfd_deactivate(struct kvm_kernel_irqfd *irqfd)
 
 	list_del_init(&irqfd->list);
 
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
 }
 
@@ -557,6 +589,16 @@ kvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)
 	 * so that we guarantee there will not be any more interrupts on this
 	 * gsi once this deassign function returns.
 	 */
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	flush_workqueue(irqfd_cleanup_wq);
 
 	return 0;
@@ -594,6 +636,16 @@ kvm_irqfd_release(struct kvm *kvm)
 	 * Block until we know all outstanding shutdown jobs have completed
 	 * since we do not take a kvm* reference.
 	 */
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	flush_workqueue(irqfd_cleanup_wq);
 
 }
@@ -660,8 +712,22 @@ bool kvm_notify_irqfd_resampler(struct kvm *kvm,
  * aggregated from all vm* instances. We need our own isolated
  * queue to ease flushing work items when a VM exits.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|6439| <<kvm_init>> r = kvm_irqfd_init();
+ */
 int kvm_irqfd_init(void)
 {
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
 	if (!irqfd_cleanup_wq)
 		return -ENOMEM;
@@ -671,6 +737,16 @@ int kvm_irqfd_init(void)
 
 void kvm_irqfd_exit(void)
 {
+	/*
+	 * 在以下使用irqfd_cleanup_wq:
+	 *   - virt/kvm/eventfd.c|33| <<global>> static struct workqueue_struct *irqfd_cleanup_wq;
+	 *   - virt/kvm/eventfd.c|179| <<irqfd_deactivate>> queue_work(irqfd_cleanup_wq, &irqfd->shutdown);
+	 *   - virt/kvm/eventfd.c|560| <<kvm_irqfd_deassign>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|597| <<kvm_irqfd_release>> flush_workqueue(irqfd_cleanup_wq);
+	 *   - virt/kvm/eventfd.c|665| <<kvm_irqfd_init>> irqfd_cleanup_wq = alloc_workqueue("kvm-irqfd-cleanup", 0, 0);
+	 *   - virt/kvm/eventfd.c|666| <<kvm_irqfd_init>> if (!irqfd_cleanup_wq)
+	 *   - virt/kvm/eventfd.c|674| <<kvm_irqfd_exit>> destroy_workqueue(irqfd_cleanup_wq);
+	 */
 	destroy_workqueue(irqfd_cleanup_wq);
 }
 #endif
diff --git a/virt/kvm/guest_memfd.c b/virt/kvm/guest_memfd.c
index 47a9f68f7..1ecb7f93a 100644
--- a/virt/kvm/guest_memfd.c
+++ b/virt/kvm/guest_memfd.c
@@ -123,6 +123,18 @@ static void kvm_gmem_invalidate_begin(struct kvm_gmem *gmem, pgoff_t start,
 		if (!found_memslot) {
 			found_memslot = true;
 
+			/*
+			 * 在以下使用KVM_MMU_LOCK():
+			 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+			 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+			 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+			 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+			 */
 			KVM_MMU_LOCK(kvm);
 			kvm_mmu_invalidate_begin(kvm);
 		}
@@ -143,6 +155,18 @@ static void kvm_gmem_invalidate_end(struct kvm_gmem *gmem, pgoff_t start,
 	struct kvm *kvm = gmem->kvm;
 
 	if (xa_find(&gmem->bindings, &start, end - 1, XA_PRESENT)) {
+		/*
+		 * 在以下使用KVM_MMU_LOCK():
+		 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+		 */
 		KVM_MMU_LOCK(kvm);
 		kvm_mmu_invalidate_end(kvm);
 		KVM_MMU_UNLOCK(kvm);
@@ -452,6 +476,10 @@ static int __kvm_gmem_create(struct kvm *kvm, loff_t size, u64 flags)
 	return err;
 }
 
+/*
+ * 处理KVM_CREATE_GUEST_MEMFD:
+ *   - virt/kvm/kvm_main.c|5325| <<kvm_vm_ioctl(KVM_CREATE_GUEST_MEMFD)>> r = kvm_gmem_create(kvm, &guest_memfd);
+ */
 int kvm_gmem_create(struct kvm *kvm, struct kvm_create_guest_memfd *args)
 {
 	loff_t size = args->size;
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index de2c11dae..d77b8223d 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -605,6 +605,18 @@ static __always_inline kvm_mn_ret_t __kvm_handle_hva_range(struct kvm *kvm,
 
 			if (!r.found_memslot) {
 				r.found_memslot = true;
+				/*
+				 * 在以下使用KVM_MMU_LOCK():
+				 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+				 */
 				KVM_MMU_LOCK(kvm);
 				if (!IS_KVM_NULL_FN(range->on_lock))
 					range->on_lock(kvm);
@@ -671,6 +683,12 @@ void kvm_mmu_invalidate_begin(struct kvm *kvm)
 	}
 }
 
+/*
+ * called by:
+ *   - arch/x86/kvm/mmu/mmu.c|7511| <<kvm_zap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, gfn_start, gfn_end);
+ *   - virt/kvm/kvm_main.c|719| <<kvm_mmu_unmap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+ *   - virt/kvm/kvm_main.c|2508| <<kvm_pre_set_memory_attributes>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+ */
 void kvm_mmu_invalidate_range_add(struct kvm *kvm, gfn_t start, gfn_t end)
 {
 	lockdep_assert_held_write(&kvm->mmu_lock);
@@ -697,9 +715,26 @@ void kvm_mmu_invalidate_range_add(struct kvm *kvm, gfn_t start, gfn_t end)
 	}
 }
 
+/*
+ * 在以下使用kvm_mmu_unmap_gfn_range():
+ *   - virt/kvm/guest_memfd.c|130| <<kvm_gmem_invalidate_begin>> flush |= kvm_mmu_unmap_gfn_range(kvm, &gfn_range);
+ *   - virt/kvm/kvm_main.c|713| <<kvm_mmu_notifier_invalidate_range_start>> .handler = kvm_mmu_unmap_gfn_range,
+ */
 bool kvm_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)
 {
+	/*
+	 * called by:
+	 *   - arch/x86/kvm/mmu/mmu.c|7511| <<kvm_zap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, gfn_start, gfn_end);
+	 *   - virt/kvm/kvm_main.c|719| <<kvm_mmu_unmap_gfn_range>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+	 *   - virt/kvm/kvm_main.c|2508| <<kvm_pre_set_memory_attributes>> kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+	 */
 	kvm_mmu_invalidate_range_add(kvm, range->start, range->end);
+	/*
+	 * x86的调用:
+	 *   - arch/x86/kvm/mmu/mmu.c|7773| <<kvm_mmu_zap_memslot>> flush = kvm_unmap_gfn_range(kvm, &range);
+	 *   - arch/x86/kvm/mmu/mmu.c|8276| <<kvm_arch_pre_set_memory_attributes>> return kvm_unmap_gfn_range(kvm, range);
+	 *   - virt/kvm/kvm_main.c|703| <<kvm_mmu_unmap_gfn_range>> return kvm_unmap_gfn_range(kvm, range);
+	 */
 	return kvm_unmap_gfn_range(kvm, range);
 }
 
@@ -2194,6 +2229,18 @@ static int kvm_get_dirty_log_protect(struct kvm *kvm, struct kvm_dirty_log *log)
 		dirty_bitmap_buffer = kvm_second_dirty_bitmap(memslot);
 		memset(dirty_bitmap_buffer, 0, n);
 
+		/*
+		 * 在以下使用KVM_MMU_LOCK():
+		 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+		 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+		 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+		 */
 		KVM_MMU_LOCK(kvm);
 		for (i = 0; i < n / sizeof(long); i++) {
 			unsigned long mask;
@@ -2305,6 +2352,18 @@ static int kvm_clear_dirty_log_protect(struct kvm *kvm,
 	if (copy_from_user(dirty_bitmap_buffer, log->dirty_bitmap, n))
 		return -EFAULT;
 
+	/*
+	 * 在以下使用KVM_MMU_LOCK():
+	 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+	 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+	 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+	 */
 	KVM_MMU_LOCK(kvm);
 	for (offset = log->first_page, i = offset / BITS_PER_LONG,
 		 n = DIV_ROUND_UP(log->num_pages, BITS_PER_LONG); n--;
@@ -2422,6 +2481,18 @@ static __always_inline void kvm_handle_gfn_range(struct kvm *kvm,
 
 			if (!found_memslot) {
 				found_memslot = true;
+				/*
+				 * 在以下使用KVM_MMU_LOCK():
+				 *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+				 *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+				 *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+				 */
 				KVM_MMU_LOCK(kvm);
 				if (!IS_KVM_NULL_FN(range->on_lock))
 					range->on_lock(kvm);
@@ -2765,6 +2836,10 @@ static kvm_pfn_t kvm_resolve_pfn(struct kvm_follow_pfn *kfp, struct page *page,
  * The fast path to get the writable pfn which will be stored in @pfn,
  * true indicates success, otherwise false is returned.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2913| <<hva_to_pfn>> if (hva_to_pfn_fast(kfp, &pfn))
+ */
 static bool hva_to_pfn_fast(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
 {
 	struct page *page;
@@ -2797,6 +2872,10 @@ static bool hva_to_pfn_fast(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
  * The slow path to get the pfn of the specified host virtual address,
  * 1 indicates success, -errno is returned if error is detected.
  */
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2916| <<hva_to_pfn>> npages = hva_to_pfn_slow(kfp, &pfn);
+ */
 static int hva_to_pfn_slow(struct kvm_follow_pfn *kfp, kvm_pfn_t *pfn)
 {
 	/*
@@ -2899,6 +2978,11 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 	return r;
 }
 
+/*
+ * called by:
+ *   - virt/kvm/kvm_main.c|2963| <<kvm_follow_pfn>> return hva_to_pfn(kfp);
+ *   - virt/kvm/pfncache.c|209| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(&kfp);
+ */
 kvm_pfn_t hva_to_pfn(struct kvm_follow_pfn *kfp)
 {
 	struct vm_area_struct *vma;
@@ -2960,6 +3044,11 @@ static kvm_pfn_t kvm_follow_pfn(struct kvm_follow_pfn *kfp)
 		kfp->map_writable = NULL;
 	}
 
+	/*
+	 * called by:
+	 *   - virt/kvm/kvm_main.c|2963| <<kvm_follow_pfn>> return hva_to_pfn(kfp);
+	 *   - virt/kvm/pfncache.c|209| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(&kfp);
+	 */
 	return hva_to_pfn(kfp);
 }
 
diff --git a/virt/kvm/kvm_mm.h b/virt/kvm/kvm_mm.h
index acef3f5c5..d3122117d 100644
--- a/virt/kvm/kvm_mm.h
+++ b/virt/kvm/kvm_mm.h
@@ -10,6 +10,18 @@
  * multiple architectures.
  */
 
+/*
+ * 在以下使用KVM_MMU_LOCK():
+ *   - virt/kvm/kvm_mm.h|15| <<global>> #define KVM_MMU_LOCK(kvm) write_lock(&(kvm)->mmu_lock)
+ *   - virt/kvm/kvm_mm.h|19| <<global>> #define KVM_MMU_LOCK(kvm) spin_lock(&(kvm)->mmu_lock)
+ *   - virt/kvm/dirty_ring.c|72| <<kvm_reset_dirty_gfn>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/guest_memfd.c|126| <<kvm_gmem_invalidate_begin>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/guest_memfd.c|146| <<kvm_gmem_invalidate_end>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|608| <<__kvm_handle_hva_range>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|2202| <<kvm_get_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|2313| <<kvm_clear_dirty_log_protect>> KVM_MMU_LOCK(kvm);
+ *   - virt/kvm/kvm_main.c|2430| <<kvm_handle_gfn_range>> KVM_MMU_LOCK(kvm);
+ */
 #ifdef KVM_HAVE_MMU_RWLOCK
 #define KVM_MMU_LOCK_INIT(kvm)		rwlock_init(&(kvm)->mmu_lock)
 #define KVM_MMU_LOCK(kvm)		write_lock(&(kvm)->mmu_lock)
diff --git a/virt/kvm/pfncache.c b/virt/kvm/pfncache.c
index 728d2c1b4..3be3a3729 100644
--- a/virt/kvm/pfncache.c
+++ b/virt/kvm/pfncache.c
@@ -206,6 +206,11 @@ static kvm_pfn_t hva_to_pfn_retry(struct gfn_to_pfn_cache *gpc)
 			cond_resched();
 		}
 
+		/*
+		 * called by:
+		 *   - virt/kvm/kvm_main.c|2963| <<kvm_follow_pfn>> return hva_to_pfn(kfp);
+		 *   - virt/kvm/pfncache.c|209| <<hva_to_pfn_retry>> new_pfn = hva_to_pfn(&kfp);
+		 */
 		new_pfn = hva_to_pfn(&kfp);
 		if (is_error_noslot_pfn(new_pfn))
 			goto out_error;
diff --git a/virt/lib/irqbypass.c b/virt/lib/irqbypass.c
index 28fda42e4..5e7bfcef8 100644
--- a/virt/lib/irqbypass.c
+++ b/virt/lib/irqbypass.c
@@ -22,6 +22,29 @@
 MODULE_LICENSE("GPL v2");
 MODULE_DESCRIPTION("IRQ bypass manager utility module");
 
+/*
+ * 注释:
+ * When a physical I/O device is assigned to a virtual machine through
+ * facilities like VFIO and KVM, the interrupt for the device generally
+ * bounces through the host system before being injected into the VM.
+ * However, hardware technologies exist that often allow the host to be
+ * bypassed for some of these scenarios.  Intel Posted Interrupts allow
+ * the specified physical edge interrupts to be directly injected into a
+ * guest when delivered to a physical processor while the vCPU is
+ * running.  ARM IRQ Forwarding allows forwarded physical interrupts to
+ * be directly deactivated by the guest.
+ *
+ * The IRQ bypass manager here is meant to provide the shim to connect
+ * interrupt producers, generally the host physical device driver, with
+ * interrupt consumers, generally the hypervisor, in order to configure
+ * these bypass mechanism.  To do this, we base the connection on a
+ * shared, opaque token.  For KVM-VFIO this is expected to be an
+ * eventfd_ctx since this is the connection we already use to connect an
+ * eventfd to an irqfd on the in-kernel path.  When a producer and
+ * consumer with matching tokens is found, callbacks via both registered
+ * participants allow the bypass facilities to be automatically enabled.
+ */
+
 static LIST_HEAD(producers);
 static LIST_HEAD(consumers);
 static DEFINE_MUTEX(lock);
-- 
2.39.5 (Apple Git-154)

