From f5dd19c7af99c47c3eeacc63e1e42f96cf753727 Mon Sep 17 00:00:00 2001
From: Dongli Zhang <dongli.zhang@oracle.com>
Date: Wed, 25 Dec 2019 10:15:43 -0800
Subject: [PATCH 1/1] linux uek5 rds

v4.14.35-1902.8.1

Signed-off-by: Dongli Zhang <dongli.zhang0129@gmail.com>
Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
---
 drivers/infiniband/core/cma.c           |  14 +
 drivers/infiniband/core/core_priv.h     |   5 +
 drivers/infiniband/core/cq.c            |   4 +
 drivers/infiniband/core/verbs.c         |  17 ++
 drivers/infiniband/hw/mlx4/cq.c         |  13 +
 drivers/infiniband/hw/mlx4/qp.c         |  64 ++++
 drivers/net/ethernet/mellanox/mlx4/cq.c |  21 ++
 drivers/net/ethernet/mellanox/mlx4/eq.c |  10 +
 include/rdma/ib_verbs.h                 |  18 ++
 net/netfilter/xt_hl.c                   | 171 ++++++++---
 net/netfilter/xt_tcpmss.c               | 383 +++++++++++++++++++-----
 net/rds/ib.h                            |   6 +
 net/rds/ib_cm.c                         |  20 ++
 net/rds/ib_ring.c                       |   7 +
 net/rds/ib_send.c                       |  12 +
 net/rds/message.c                       |   7 +
 net/rds/rds.h                           |  56 ++++
 net/rds/send.c                          | 115 +++++++
 net/rds/threads.c                       |  16 +
 19 files changed, 840 insertions(+), 119 deletions(-)

diff --git a/drivers/infiniband/core/cma.c b/drivers/infiniband/core/cma.c
index 008ae7249435..24521e78bbda 100644
--- a/drivers/infiniband/core/cma.c
+++ b/drivers/infiniband/core/cma.c
@@ -896,6 +896,20 @@ static int cma_init_conn_qp(struct rdma_id_private *id_priv, struct ib_qp *qp)
 	return ib_modify_qp(qp, &qp_attr, qp_attr_mask);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/ulp/iser/iser_verbs.c|478| <<iser_create_ib_conn_res>> ret = rdma_create_qp(ib_conn->cma_id, device->pd, &init_attr);
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|146| <<isert_create_qp>> ret = rdma_create_qp(cma_id, device->pd, &attr);
+ *   - drivers/nvme/host/rdma.c|265| <<nvme_rdma_create_qp>> ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
+ *   - drivers/nvme/target/rdma.c|979| <<nvmet_rdma_create_queue_ib>> ret = rdma_create_qp(queue->cm_id, ndev->pd, &qp_attr);
+ *   - drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c|770| <<kiblnd_create_conn>> rc = rdma_create_qp(cmid, conn->ibc_hdev->ibh_pd, init_qp_attr);
+ *   - net/9p/trans_rdma.c|726| <<rdma_create_trans>> err = rdma_create_qp(rdma->cm_id, rdma->pd, &qp_attr);
+ *   - net/rds/ib_cm.c|910| <<rds_ib_setup_qp>> ret = rdma_create_qp(ic->i_cm_id, ic->i_pd, &qp_attr);
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|816| <<svc_rdma_accept>> ret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|693| <<rpcrdma_ep_recreate_xprt>> err = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|742| <<rpcrdma_ep_reconnect>> err = rdma_create_qp(id, ia->ri_pd, &ep->rep_attr);
+ *   - net/sunrpc/xprtrdma/verbs.c|776| <<rpcrdma_ep_connect>> rc = rdma_create_qp(ia->ri_id, ia->ri_pd, &ep->rep_attr);
+ */
 int rdma_create_qp(struct rdma_cm_id *id, struct ib_pd *pd,
 		   struct ib_qp_init_attr *qp_init_attr)
 {
diff --git a/drivers/infiniband/core/core_priv.h b/drivers/infiniband/core/core_priv.h
index 022faae16e8f..c88f16ff66f0 100644
--- a/drivers/infiniband/core/core_priv.h
+++ b/drivers/infiniband/core/core_priv.h
@@ -324,6 +324,11 @@ struct ib_device *ib_device_get_by_index(u32 ifindex);
 void nldev_init(void);
 void nldev_exit(void);
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/uverbs_cmd.c|2200| <<_ib_create_qp>> qp = _ib_create_qp(device, pd, &attr, uhw,
+ *   - drivers/infiniband/core/verbs.c|881| <<ib_create_qp>> qp = _ib_create_qp(device, pd, qp_init_attr, NULL, NULL);
+ */
 static inline struct ib_qp *_ib_create_qp(struct ib_device *dev,
 					  struct ib_pd *pd,
 					  struct ib_qp_init_attr *attr,
diff --git a/drivers/infiniband/core/cq.c b/drivers/infiniband/core/cq.c
index af5ad6a56ae4..8af4c4e81b84 100644
--- a/drivers/infiniband/core/cq.c
+++ b/drivers/infiniband/core/cq.c
@@ -134,6 +134,10 @@ static void ib_cq_completion_workqueue(struct ib_cq *cq, void *private)
  * specified context. The ULP must use wr->wr_cqe instead of wr->wr_id
  * to use this CQ abstraction.
  */
+/*
+ * called by:
+ *  - include/rdma/ib_verbs.h|3478| <<ib_alloc_cq>> __ib_alloc_cq((device), (priv), (nr_cqe), (comp_vect), (poll_ctx), KBUILD_MODNAME)
+ */
 struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
 			    int nr_cqe, int comp_vector,
 			    enum ib_poll_context poll_ctx, const char *caller)
diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c
index b75a783e3057..136e76ca01a6 100644
--- a/drivers/infiniband/core/verbs.c
+++ b/drivers/infiniband/core/verbs.c
@@ -856,6 +856,23 @@ static struct ib_qp *ib_create_xrc_qp(struct ib_qp *qp,
 	return qp;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/core/cma.c|911| <<rdma_create_qp>> qp = ib_create_qp(pd, qp_init_attr);
+ *   - drivers/infiniband/core/mad.c|3137| <<create_mad_qp>> qp_info->qp = ib_create_qp(qp_info->port_priv->pd, &qp_init_attr);
+ *   - drivers/infiniband/core/uverbs_cmd.c|2198| <<create_mad_qp>> qp = ib_create_qp(pd, &attr);
+ *   - drivers/infiniband/hw/mlx4/mad.c|1837| <<create_pv_sqp>> tun_qp->qp = ib_create_qp(ctx->pd, &qp_init_attr.init_attr);
+ *   - drivers/infiniband/hw/mlx4/qp.c|1577| <<mlx4_ib_create_qp>> sqp->roce_v2_gsi = ib_create_qp(pd, init_attr);
+ *   - drivers/infiniband/hw/mlx5/gsi.c|187| <<mlx5_ib_gsi_create_qp>> gsi->rx_qp = ib_create_qp(pd, &hw_init_attr);
+ *   - drivers/infiniband/hw/mlx5/gsi.c|269| <<create_gsi_ud_qp>> return ib_create_qp(pd, &init_attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_cm.c|273| <<ipoib_cm_create_rx_qp>> return ib_create_qp(priv->pd, &attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_cm.c|1104| <<ipoib_cm_create_tx_qp>> tx_qp = ib_create_qp(priv->pd, &attr);
+ *   - drivers/infiniband/ulp/ipoib/ipoib_verbs.c|205| <<ipoib_transport_dev_init>> priv->qp = ib_create_qp(priv->pd, &init_attr);
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|527| <<srp_create_ch_ib>> qp = ib_create_qp(dev->pd, init_attr);
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1661| <<srpt_create_ch_ib>> ch->qp = ib_create_qp(sdev->pd, qp_init);
+ *   - net/rds/ib_cm.c|1853| <<rds_ib_setup_fastreg>> rds_ibdev->fastreg_qp = ib_create_qp(rds_ibdev->pd, &qp_init_attr);
+ *   - net/smc/smc_ib.c|251| <<smc_ib_create_queue_pair>> lnk->roce_qp = ib_create_qp(lnk->roce_pd, &qp_attr);
+ */
 struct ib_qp *ib_create_qp(struct ib_pd *pd,
 			   struct ib_qp_init_attr *qp_init_attr)
 {
diff --git a/drivers/infiniband/hw/mlx4/cq.c b/drivers/infiniband/hw/mlx4/cq.c
index 0f94287822a3..823b90556216 100644
--- a/drivers/infiniband/hw/mlx4/cq.c
+++ b/drivers/infiniband/hw/mlx4/cq.c
@@ -171,6 +171,19 @@ static int mlx4_ib_get_cq_umem(struct mlx4_ib_dev *dev, struct ib_ucontext *cont
 }
 
 #define CQ_CREATE_FLAGS_SUPPORTED IB_UVERBS_CQ_FLAGS_TIMESTAMP_COMPLETION
+/*
+ * used by:
+ *   - drivers/infiniband/hw/mlx4/main.c|2916| <<mlx4_ib_add>> ibdev->ib_dev.create_cq = mlx4_ib_create_cq
+ *
+ * called by:
+ *   - drivers/infiniband/core/cq.c|148| <<__ib_alloc_cq>> cq = dev->create_cq(dev, &cq_attr, NULL, NULL);
+ *   - drivers/infiniband/core/uverbs_cmd.c|1704| <<__ib_alloc_cq>> cq = ib_dev->create_cq(ib_dev, &attr, obj->uobject.context, uhw);
+ *   - drivers/infiniband/core/uverbs_cmd.c|1791| <<ib_uverbs_create_cq>> obj = create_cq(file, &ucore, &uhw, &cmd_ex,
+ *   - drivers/infiniband/core/uverbs_cmd.c|1839| <<ib_uverbs_ex_create_cq>> obj = create_cq(file, ucore, uhw, &cmd,
+ *   - drivers/infiniband/core/uverbs_std_types_cq.c|117| <<UVERBS_HANDLER>> cq = ib_dev->create_cq(ib_dev, &attr, obj->uobject.context, &uhw);
+ *   - drivers/infiniband/core/verbs.c|1551| <<UVERBS_HANDLER>> cq = device->create_cq(device, cq_attr, NULL, NULL);
+ *   - drivers/infiniband/hw/cxgb4/cq.c|960| <<c4iw_create_cq>> ret = create_cq(&rhp->rdev, &chp->cq,
+ */
 struct ib_cq *mlx4_ib_create_cq(struct ib_device *ibdev,
 				const struct ib_cq_init_attr *attr,
 				struct ib_ucontext *context,
diff --git a/drivers/infiniband/hw/mlx4/qp.c b/drivers/infiniband/hw/mlx4/qp.c
index 8d346cb407e5..5bfd359fa256 100644
--- a/drivers/infiniband/hw/mlx4/qp.c
+++ b/drivers/infiniband/hw/mlx4/qp.c
@@ -841,6 +841,25 @@ static void mlx4_ib_release_wqn(struct mlx4_ib_ucontext *context,
 	mutex_unlock(&context->wqn_ranges_mutex);
 }
 
+/*
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_initiate_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_work_handler [rdma_cm]  
+ * [0] process_one_work            
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/qp.c|1490| <<_mlx4_ib_create_qp>> err = create_qp_common(to_mdev(pd->device), pd, MLX4_IB_QP_SRC,
+ *   - drivers/infiniband/hw/mlx4/qp.c|1521| <<_mlx4_ib_create_qp>> err = create_qp_common(to_mdev(pd->device), pd, MLX4_IB_QP_SRC,
+ *   - drivers/infiniband/hw/mlx4/qp.c|4115| <<mlx4_ib_create_wq>> err = create_qp_common(dev, pd, MLX4_IB_RWQ_SRC, &ib_qp_init_attr,
+ */
 static int create_qp_common(struct mlx4_ib_dev *dev, struct ib_pd *pd,
 			    enum mlx4_ib_source_type src,
 			    struct ib_qp_init_attr *init_attr,
@@ -1421,6 +1440,10 @@ static u32 get_sqp_num(struct mlx4_ib_dev *dev, struct ib_qp_init_attr *attr)
 		return dev->dev->caps.spec_qps[attr->port_num - 1].qp1_proxy;
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/qp.c|1546| <<mlx4_ib_create_qp>> ibqp = _mlx4_ib_create_qp(pd, init_attr, udata);
+ */
 static struct ib_qp *_mlx4_ib_create_qp(struct ib_pd *pd,
 					struct ib_qp_init_attr *init_attr,
 					struct ib_udata *udata)
@@ -1536,6 +1559,47 @@ static struct ib_qp *_mlx4_ib_create_qp(struct ib_pd *pd,
 	return &qp->ibqp;
 }
 
+/*
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_initiate_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_work_handler [rdma_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 其他创建qp的例子:
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_handle_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_listen_handler [rdma_cm]
+ * [0] cma_req_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_req_handler [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * used by:
+ *   - drivers/infiniband/hw/mlx4/main.c|2910| <<mlx4_ib_add>> ibdev->ib_dev.create_qp = mlx4_ib_create_qp;
+ *
+ * called by:
+ *   - drivers/infiniband/core/core_priv.h|338| <<_ib_create_qp>> qp = dev->create_qp(pd, attr, udata);
+ *   - drivers/infiniband/core/uverbs_cmd.c|2345| <<ib_uverbs_create_qp>> err = create_qp(file, &ucore, &uhw, &cmd_ex,
+ *   - drivers/infiniband/core/uverbs_cmd.c|2392| <<ib_uverbs_ex_create_qp>> err = create_qp(file, ucore, uhw, &cmd,
+ *   - drivers/infiniband/hw/cxgb4/qp.c|1895| <<c4iw_create_qp>> ret = create_qp(&rhp->rdev, &qhp->wq, &schp->cq, &rchp->cq,
+ */
 struct ib_qp *mlx4_ib_create_qp(struct ib_pd *pd,
 				struct ib_qp_init_attr *init_attr,
 				struct ib_udata *udata) {
diff --git a/drivers/net/ethernet/mellanox/mlx4/cq.c b/drivers/net/ethernet/mellanox/mlx4/cq.c
index b80214e75599..9e492c839595 100644
--- a/drivers/net/ethernet/mellanox/mlx4/cq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/cq.c
@@ -101,6 +101,19 @@ static void mlx4_add_cq_to_tasklet(struct mlx4_cq *cq)
 	spin_unlock_irqrestore(&tasklet_ctx->lock, flags);
 }
 
+/*
+ * [0] mlx4_ib_cq_comp [mlx4_ib]
+ * [0] mlx4_cq_completion [mlx4_core]
+ * [0] mlx4_eq_int [mlx4_core]
+ * [0] mlx4_msi_x_interrupt [mlx4_core]
+ * [0] __handle_irq_event_percpu
+ * [0] handle_irq_event_percpu
+ * [0] handle_irq_event
+ * [0] handle_edge_irq
+ * [0] handle_irq
+ * [0] do_IRQ
+ * [0] common_interrupt
+ */
 void mlx4_cq_completion(struct mlx4_dev *dev, u32 cqn)
 {
 	struct mlx4_cq *cq;
@@ -120,6 +133,9 @@ void mlx4_cq_completion(struct mlx4_dev *dev, u32 cqn)
 	 */
 	++cq->arm_sn;
 
+	/*
+	 * 对于ib的mlx4_ib_cq_comp [mlx4_ib]
+	 */
 	cq->comp(cq);
 }
 
@@ -287,6 +303,11 @@ static void mlx4_cq_free_icm(struct mlx4_dev *dev, int cqn)
 		__mlx4_cq_free_icm(dev, cqn);
 }
 
+/*
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/cq.c|253| <<mlx4_ib_create_cq>> err = mlx4_cq_alloc(dev->dev, entries, &cq->buf.mtt, uar,
+ *   - drivers/net/ethernet/mellanox/mlx4/en_cq.c|144| <<mlx4_en_activate_cq>> err = mlx4_cq_alloc(mdev->dev, cq->size, &cq->wqres.mtt,
+ */
 int mlx4_cq_alloc(struct mlx4_dev *dev, int nent,
 		  struct mlx4_mtt *mtt, struct mlx4_uar *uar, u64 db_rec,
 		  struct mlx4_cq *cq, unsigned vector, int collapsed,
diff --git a/drivers/net/ethernet/mellanox/mlx4/eq.c b/drivers/net/ethernet/mellanox/mlx4/eq.c
index ac40f26a0b17..1cffcfdbd461 100644
--- a/drivers/net/ethernet/mellanox/mlx4/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx4/eq.c
@@ -490,6 +490,11 @@ void mlx4_master_handle_slave_flr(struct work_struct *work)
 	}
 }
 
+/*
+ * called by:
+ *   - drivers/net/ethernet/mellanox/mlx4/eq.c|856| <<mlx4_interrupt>> work |= mlx4_eq_int(dev, &priv->eq_table.eq[i]);
+ *   - drivers/net/ethernet/mellanox/mlx4/eq.c|866| <<mlx4_msi_x_interrupt>> mlx4_eq_int(dev, eq);
+ */
 static int mlx4_eq_int(struct mlx4_dev *dev, struct mlx4_eq *eq)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -1449,6 +1454,11 @@ struct cpu_rmap *mlx4_get_cpu_rmap(struct mlx4_dev *dev, int port)
 }
 EXPORT_SYMBOL(mlx4_get_cpu_rmap);
 
+/*
+ * called by:
+ *   - drivers/infiniband/hw/mlx4/main.c|2582| <<mlx4_ib_alloc_eqs>> if (!mlx4_assign_eq(dev, i,
+ *   - drivers/net/ethernet/mellanox/mlx4/en_cq.c|112| <<mlx4_en_activate_cq>> err = mlx4_assign_eq(mdev->dev, priv->port,
+ */
 int mlx4_assign_eq(struct mlx4_dev *dev, u8 port, int *vector)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
diff --git a/include/rdma/ib_verbs.h b/include/rdma/ib_verbs.h
index 2d8f287813ab..5a8b7d8f55c9 100644
--- a/include/rdma/ib_verbs.h
+++ b/include/rdma/ib_verbs.h
@@ -3474,6 +3474,24 @@ static inline int ib_post_recv(struct ib_qp *qp,
 struct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private,
 			    int nr_cqe, int comp_vector,
 			    enum ib_poll_context poll_ctx, const char *caller);
+/*
+ * called by:
+ *   - drivers/infiniband/core/mad.c|3199| <<ib_mad_port_open>> port_priv->cq = ib_alloc_cq(port_priv->device, port_priv, cq_size, 0,
+ *   - drivers/infiniband/hw/mlx5/gsi.c|171| <<mlx5_ib_gsi_create_qp>> gsi->cq = ib_alloc_cq(pd->device, gsi, init_attr->cap.max_send_wr, 0,
+ *   - drivers/infiniband/hw/mlx5/main.c|3241| <<create_umr_res>> cq = ib_alloc_cq(&dev->ib_dev, NULL, 128, 0, IB_POLL_SOFTIRQ);
+ *   - drivers/infiniband/ulp/iser/iser_verbs.c|99| <<iser_create_device_ib_res>> comp->cq = ib_alloc_cq(ib_dev, comp, max_cqe, i,
+ *   - drivers/infiniband/ulp/isert/ib_isert.c|279| <<isert_alloc_comps>> comp->cq = ib_alloc_cq(device->ib_device, comp, max_cqe, i,
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|503| <<srp_create_ch_ib>> recv_cq = ib_alloc_cq(dev->dev, ch, target->queue_size + 1,
+ *   - drivers/infiniband/ulp/srp/ib_srp.c|510| <<srp_create_ch_ib>> send_cq = ib_alloc_cq(dev->dev, ch, m * target->queue_size,
+ *   - drivers/infiniband/ulp/srpt/ib_srpt.c|1632| <<srpt_create_ch_ib>> ch->cq = ib_alloc_cq(sdev->device, ch, ch->rq_size + srp_sq_size,
+ *   - drivers/nvme/host/rdma.c|448| <<nvme_rdma_create_queue_ib>> queue->ib_cq = ib_alloc_cq(ibdev, queue,
+ *   - drivers/nvme/target/rdma.c|948| <<nvmet_rdma_create_queue_ib>> queue->cq = ib_alloc_cq(ndev->device, queue,
+ *   - net/9p/trans_rdma.c|703| <<rdma_create_trans>> rdma->cq = ib_alloc_cq(rdma->cm_id->device, client,
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|783| <<svc_rdma_accept>> newxprt->sc_sq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_sq_depth,
+ *   - net/sunrpc/xprtrdma/svc_rdma_transport.c|789| <<svc_rdma_accept>> newxprt->sc_rq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_rq_depth,
+ *   - net/sunrpc/xprtrdma/verbs.c|577| <<rpcrdma_ep_create>> sendcq = ib_alloc_cq(ia->ri_device, NULL,
+ *   - net/sunrpc/xprtrdma/verbs.c|587| <<rpcrdma_ep_create>> recvcq = ib_alloc_cq(ia->ri_device, NULL,
+ */
 #define ib_alloc_cq(device, priv, nr_cqe, comp_vect, poll_ctx) \
 	__ib_alloc_cq((device), (priv), (nr_cqe), (comp_vect), (poll_ctx), KBUILD_MODNAME)
 
diff --git a/net/netfilter/xt_hl.c b/net/netfilter/xt_hl.c
index 003951149c9e..1535e87ed9bd 100644
--- a/net/netfilter/xt_hl.c
+++ b/net/netfilter/xt_hl.c
@@ -1,96 +1,169 @@
 /*
- * IP tables module for matching the value of the TTL
- * (C) 2000,2001 by Harald Welte <laforge@netfilter.org>
+ * TTL modification target for IP tables
+ * (C) 2000,2005 by Harald Welte <laforge@netfilter.org>
  *
- * Hop Limit matching module
- * (C) 2001-2002 Maciej Soltysiak <solt@dns.toxicfilms.tv>
+ * Hop Limit modification target for ip6tables
+ * Maciej Soltysiak <solt@dns.toxicfilms.tv>
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
-
-#include <linux/ip.h>
-#include <linux/ipv6.h>
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 #include <linux/module.h>
 #include <linux/skbuff.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <net/checksum.h>
 
 #include <linux/netfilter/x_tables.h>
-#include <linux/netfilter_ipv4/ipt_ttl.h>
-#include <linux/netfilter_ipv6/ip6t_hl.h>
+#include <linux/netfilter_ipv4/ipt_TTL.h>
+#include <linux/netfilter_ipv6/ip6t_HL.h>
 
+MODULE_AUTHOR("Harald Welte <laforge@netfilter.org>");
 MODULE_AUTHOR("Maciej Soltysiak <solt@dns.toxicfilms.tv>");
-MODULE_DESCRIPTION("Xtables: Hoplimit/TTL field match");
+MODULE_DESCRIPTION("Xtables: Hoplimit/TTL Limit field modification target");
 MODULE_LICENSE("GPL");
-MODULE_ALIAS("ipt_ttl");
-MODULE_ALIAS("ip6t_hl");
 
-static bool ttl_mt(const struct sk_buff *skb, struct xt_action_param *par)
+static unsigned int
+ttl_tg(struct sk_buff *skb, const struct xt_action_param *par)
 {
-	const struct ipt_ttl_info *info = par->matchinfo;
-	const u8 ttl = ip_hdr(skb)->ttl;
+	struct iphdr *iph;
+	const struct ipt_TTL_info *info = par->targinfo;
+	int new_ttl;
+
+	if (!skb_make_writable(skb, skb->len))
+		return NF_DROP;
+
+	iph = ip_hdr(skb);
 
 	switch (info->mode) {
-	case IPT_TTL_EQ:
-		return ttl == info->ttl;
-	case IPT_TTL_NE:
-		return ttl != info->ttl;
-	case IPT_TTL_LT:
-		return ttl < info->ttl;
-	case IPT_TTL_GT:
-		return ttl > info->ttl;
+	case IPT_TTL_SET:
+		new_ttl = info->ttl;
+		break;
+	case IPT_TTL_INC:
+		new_ttl = iph->ttl + info->ttl;
+		if (new_ttl > 255)
+			new_ttl = 255;
+		break;
+	case IPT_TTL_DEC:
+		new_ttl = iph->ttl - info->ttl;
+		if (new_ttl < 0)
+			new_ttl = 0;
+		break;
+	default:
+		new_ttl = iph->ttl;
+		break;
+	}
+
+	if (new_ttl != iph->ttl) {
+		csum_replace2(&iph->check, htons(iph->ttl << 8),
+					   htons(new_ttl << 8));
+		iph->ttl = new_ttl;
 	}
 
-	return false;
+	return XT_CONTINUE;
 }
 
-static bool hl_mt6(const struct sk_buff *skb, struct xt_action_param *par)
+static unsigned int
+hl_tg6(struct sk_buff *skb, const struct xt_action_param *par)
 {
-	const struct ip6t_hl_info *info = par->matchinfo;
-	const struct ipv6hdr *ip6h = ipv6_hdr(skb);
+	struct ipv6hdr *ip6h;
+	const struct ip6t_HL_info *info = par->targinfo;
+	int new_hl;
+
+	if (!skb_make_writable(skb, skb->len))
+		return NF_DROP;
+
+	ip6h = ipv6_hdr(skb);
 
 	switch (info->mode) {
-	case IP6T_HL_EQ:
-		return ip6h->hop_limit == info->hop_limit;
-	case IP6T_HL_NE:
-		return ip6h->hop_limit != info->hop_limit;
-	case IP6T_HL_LT:
-		return ip6h->hop_limit < info->hop_limit;
-	case IP6T_HL_GT:
-		return ip6h->hop_limit > info->hop_limit;
+	case IP6T_HL_SET:
+		new_hl = info->hop_limit;
+		break;
+	case IP6T_HL_INC:
+		new_hl = ip6h->hop_limit + info->hop_limit;
+		if (new_hl > 255)
+			new_hl = 255;
+		break;
+	case IP6T_HL_DEC:
+		new_hl = ip6h->hop_limit - info->hop_limit;
+		if (new_hl < 0)
+			new_hl = 0;
+		break;
+	default:
+		new_hl = ip6h->hop_limit;
+		break;
 	}
 
-	return false;
+	ip6h->hop_limit = new_hl;
+
+	return XT_CONTINUE;
+}
+
+static int ttl_tg_check(const struct xt_tgchk_param *par)
+{
+	const struct ipt_TTL_info *info = par->targinfo;
+
+	if (info->mode > IPT_TTL_MAXMODE) {
+		pr_info("TTL: invalid or unknown mode %u\n", info->mode);
+		return -EINVAL;
+	}
+	if (info->mode != IPT_TTL_SET && info->ttl == 0)
+		return -EINVAL;
+	return 0;
+}
+
+static int hl_tg6_check(const struct xt_tgchk_param *par)
+{
+	const struct ip6t_HL_info *info = par->targinfo;
+
+	if (info->mode > IP6T_HL_MAXMODE) {
+		pr_info("invalid or unknown mode %u\n", info->mode);
+		return -EINVAL;
+	}
+	if (info->mode != IP6T_HL_SET && info->hop_limit == 0) {
+		pr_info("increment/decrement does not "
+			"make sense with value 0\n");
+		return -EINVAL;
+	}
+	return 0;
 }
 
-static struct xt_match hl_mt_reg[] __read_mostly = {
+static struct xt_target hl_tg_reg[] __read_mostly = {
 	{
-		.name       = "ttl",
+		.name       = "TTL",
 		.revision   = 0,
 		.family     = NFPROTO_IPV4,
-		.match      = ttl_mt,
-		.matchsize  = sizeof(struct ipt_ttl_info),
+		.target     = ttl_tg,
+		.targetsize = sizeof(struct ipt_TTL_info),
+		.table      = "mangle",
+		.checkentry = ttl_tg_check,
 		.me         = THIS_MODULE,
 	},
 	{
-		.name       = "hl",
+		.name       = "HL",
 		.revision   = 0,
 		.family     = NFPROTO_IPV6,
-		.match      = hl_mt6,
-		.matchsize  = sizeof(struct ip6t_hl_info),
+		.target     = hl_tg6,
+		.targetsize = sizeof(struct ip6t_HL_info),
+		.table      = "mangle",
+		.checkentry = hl_tg6_check,
 		.me         = THIS_MODULE,
 	},
 };
 
-static int __init hl_mt_init(void)
+static int __init hl_tg_init(void)
 {
-	return xt_register_matches(hl_mt_reg, ARRAY_SIZE(hl_mt_reg));
+	return xt_register_targets(hl_tg_reg, ARRAY_SIZE(hl_tg_reg));
 }
 
-static void __exit hl_mt_exit(void)
+static void __exit hl_tg_exit(void)
 {
-	xt_unregister_matches(hl_mt_reg, ARRAY_SIZE(hl_mt_reg));
+	xt_unregister_targets(hl_tg_reg, ARRAY_SIZE(hl_tg_reg));
 }
 
-module_init(hl_mt_init);
-module_exit(hl_mt_exit);
+module_init(hl_tg_init);
+module_exit(hl_tg_exit);
+MODULE_ALIAS("ipt_TTL");
+MODULE_ALIAS("ip6t_HL");
diff --git a/net/netfilter/xt_tcpmss.c b/net/netfilter/xt_tcpmss.c
index c53d4d18eadf..9dae4d665965 100644
--- a/net/netfilter/xt_tcpmss.c
+++ b/net/netfilter/xt_tcpmss.c
@@ -1,110 +1,353 @@
-/* Kernel module to match TCP MSS values. */
-
-/* Copyright (C) 2000 Marc Boucher <marc@mbsi.ca>
- * Portions (C) 2005 by Harald Welte <laforge@netfilter.org>
+/*
+ * This is a module which is used for setting the MSS option in TCP packets.
+ *
+ * Copyright (C) 2000 Marc Boucher <marc@mbsi.ca>
+ * Copyright (C) 2007 Patrick McHardy <kaber@trash.net>
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
-
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 #include <linux/module.h>
 #include <linux/skbuff.h>
+#include <linux/ip.h>
+#include <linux/gfp.h>
+#include <linux/ipv6.h>
+#include <linux/tcp.h>
+#include <net/dst.h>
+#include <net/flow.h>
+#include <net/ipv6.h>
+#include <net/route.h>
 #include <net/tcp.h>
 
-#include <linux/netfilter/xt_tcpmss.h>
-#include <linux/netfilter/x_tables.h>
-
 #include <linux/netfilter_ipv4/ip_tables.h>
 #include <linux/netfilter_ipv6/ip6_tables.h>
+#include <linux/netfilter/x_tables.h>
+#include <linux/netfilter/xt_tcpudp.h>
+#include <linux/netfilter/xt_TCPMSS.h>
 
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Marc Boucher <marc@mbsi.ca>");
-MODULE_DESCRIPTION("Xtables: TCP MSS match");
-MODULE_ALIAS("ipt_tcpmss");
-MODULE_ALIAS("ip6t_tcpmss");
+MODULE_DESCRIPTION("Xtables: TCP Maximum Segment Size (MSS) adjustment");
+MODULE_ALIAS("ipt_TCPMSS");
+MODULE_ALIAS("ip6t_TCPMSS");
+
+static inline unsigned int
+optlen(const u_int8_t *opt, unsigned int offset)
+{
+	/* Beware zero-length options: make finite progress */
+	if (opt[offset] <= TCPOPT_NOP || opt[offset+1] == 0)
+		return 1;
+	else
+		return opt[offset+1];
+}
+
+static u_int32_t tcpmss_reverse_mtu(struct net *net,
+				    const struct sk_buff *skb,
+				    unsigned int family)
+{
+	struct flowi fl;
+	const struct nf_afinfo *ai;
+	struct rtable *rt = NULL;
+	u_int32_t mtu     = ~0U;
+
+	if (family == PF_INET) {
+		struct flowi4 *fl4 = &fl.u.ip4;
+		memset(fl4, 0, sizeof(*fl4));
+		fl4->daddr = ip_hdr(skb)->saddr;
+	} else {
+		struct flowi6 *fl6 = &fl.u.ip6;
+
+		memset(fl6, 0, sizeof(*fl6));
+		fl6->daddr = ipv6_hdr(skb)->saddr;
+	}
+	ai = nf_get_afinfo(family);
+	if (ai != NULL)
+		ai->route(net, (struct dst_entry **)&rt, &fl, false);
+
+	if (rt != NULL) {
+		mtu = dst_mtu(&rt->dst);
+		dst_release(&rt->dst);
+	}
+	return mtu;
+}
 
-static bool
-tcpmss_mt(const struct sk_buff *skb, struct xt_action_param *par)
+static int
+tcpmss_mangle_packet(struct sk_buff *skb,
+		     const struct xt_action_param *par,
+		     unsigned int family,
+		     unsigned int tcphoff,
+		     unsigned int minlen)
 {
-	const struct xt_tcpmss_match_info *info = par->matchinfo;
-	const struct tcphdr *th;
-	struct tcphdr _tcph;
-	/* tcp.doff is only 4 bits, ie. max 15 * 4 bytes */
-	const u_int8_t *op;
-	u8 _opt[15 * 4 - sizeof(_tcph)];
-	unsigned int i, optlen;
-
-	/* If we don't have the whole header, drop packet. */
-	th = skb_header_pointer(skb, par->thoff, sizeof(_tcph), &_tcph);
-	if (th == NULL)
-		goto dropit;
-
-	/* Malformed. */
-	if (th->doff*4 < sizeof(*th))
-		goto dropit;
-
-	optlen = th->doff*4 - sizeof(*th);
-	if (!optlen)
-		goto out;
-
-	/* Truncated options. */
-	op = skb_header_pointer(skb, par->thoff + sizeof(*th), optlen, _opt);
-	if (op == NULL)
-		goto dropit;
-
-	for (i = 0; i < optlen; ) {
-		if (op[i] == TCPOPT_MSS
-		    && (optlen - i) >= TCPOLEN_MSS
-		    && op[i+1] == TCPOLEN_MSS) {
-			u_int16_t mssval;
-
-			mssval = (op[i+2] << 8) | op[i+3];
-
-			return (mssval >= info->mss_min &&
-				mssval <= info->mss_max) ^ info->invert;
+	const struct xt_tcpmss_info *info = par->targinfo;
+	struct tcphdr *tcph;
+	int len, tcp_hdrlen;
+	unsigned int i;
+	__be16 oldval;
+	u16 newmss;
+	u8 *opt;
+
+	/* This is a fragment, no TCP header is available */
+	if (par->fragoff != 0)
+		return 0;
+
+	if (!skb_make_writable(skb, skb->len))
+		return -1;
+
+	len = skb->len - tcphoff;
+	if (len < (int)sizeof(struct tcphdr))
+		return -1;
+
+	tcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);
+	tcp_hdrlen = tcph->doff * 4;
+
+	if (len < tcp_hdrlen || tcp_hdrlen < sizeof(struct tcphdr))
+		return -1;
+
+	if (info->mss == XT_TCPMSS_CLAMP_PMTU) {
+		struct net *net = xt_net(par);
+		unsigned int in_mtu = tcpmss_reverse_mtu(net, skb, family);
+		unsigned int min_mtu = min(dst_mtu(skb_dst(skb)), in_mtu);
+
+		if (min_mtu <= minlen) {
+			net_err_ratelimited("unknown or invalid path-MTU (%u)\n",
+					    min_mtu);
+			return -1;
+		}
+		newmss = min_mtu - minlen;
+	} else
+		newmss = info->mss;
+
+	opt = (u_int8_t *)tcph;
+	for (i = sizeof(struct tcphdr); i <= tcp_hdrlen - TCPOLEN_MSS; i += optlen(opt, i)) {
+		if (opt[i] == TCPOPT_MSS && opt[i+1] == TCPOLEN_MSS) {
+			u_int16_t oldmss;
+
+			oldmss = (opt[i+2] << 8) | opt[i+3];
+
+			/* Never increase MSS, even when setting it, as
+			 * doing so results in problems for hosts that rely
+			 * on MSS being set correctly.
+			 */
+			if (oldmss <= newmss)
+				return 0;
+
+			opt[i+2] = (newmss & 0xff00) >> 8;
+			opt[i+3] = newmss & 0x00ff;
+
+			inet_proto_csum_replace2(&tcph->check, skb,
+						 htons(oldmss), htons(newmss),
+						 false);
+			return 0;
 		}
-		if (op[i] < 2)
-			i++;
-		else
-			i += op[i+1] ? : 1;
 	}
-out:
-	return info->invert;
 
-dropit:
-	par->hotdrop = true;
+	/* There is data after the header so the option can't be added
+	 * without moving it, and doing so may make the SYN packet
+	 * itself too large. Accept the packet unmodified instead.
+	 */
+	if (len > tcp_hdrlen)
+		return 0;
+
+	/* tcph->doff has 4 bits, do not wrap it to 0 */
+	if (tcp_hdrlen >= 15 * 4)
+		return 0;
+
+	/*
+	 * MSS Option not found ?! add it..
+	 */
+	if (skb_tailroom(skb) < TCPOLEN_MSS) {
+		if (pskb_expand_head(skb, 0,
+				     TCPOLEN_MSS - skb_tailroom(skb),
+				     GFP_ATOMIC))
+			return -1;
+		tcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);
+	}
+
+	skb_put(skb, TCPOLEN_MSS);
+
+	/*
+	 * IPv4: RFC 1122 states "If an MSS option is not received at
+	 * connection setup, TCP MUST assume a default send MSS of 536".
+	 * IPv6: RFC 2460 states IPv6 has a minimum MTU of 1280 and a minimum
+	 * length IPv6 header of 60, ergo the default MSS value is 1220
+	 * Since no MSS was provided, we must use the default values
+	 */
+	if (xt_family(par) == NFPROTO_IPV4)
+		newmss = min(newmss, (u16)536);
+	else
+		newmss = min(newmss, (u16)1220);
+
+	opt = (u_int8_t *)tcph + sizeof(struct tcphdr);
+	memmove(opt + TCPOLEN_MSS, opt, len - sizeof(struct tcphdr));
+
+	inet_proto_csum_replace2(&tcph->check, skb,
+				 htons(len), htons(len + TCPOLEN_MSS), true);
+	opt[0] = TCPOPT_MSS;
+	opt[1] = TCPOLEN_MSS;
+	opt[2] = (newmss & 0xff00) >> 8;
+	opt[3] = newmss & 0x00ff;
+
+	inet_proto_csum_replace4(&tcph->check, skb, 0, *((__be32 *)opt), false);
+
+	oldval = ((__be16 *)tcph)[6];
+	tcph->doff += TCPOLEN_MSS/4;
+	inet_proto_csum_replace2(&tcph->check, skb,
+				 oldval, ((__be16 *)tcph)[6], false);
+	return TCPOLEN_MSS;
+}
+
+static unsigned int
+tcpmss_tg4(struct sk_buff *skb, const struct xt_action_param *par)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	__be16 newlen;
+	int ret;
+
+	ret = tcpmss_mangle_packet(skb, par,
+				   PF_INET,
+				   iph->ihl * 4,
+				   sizeof(*iph) + sizeof(struct tcphdr));
+	if (ret < 0)
+		return NF_DROP;
+	if (ret > 0) {
+		iph = ip_hdr(skb);
+		newlen = htons(ntohs(iph->tot_len) + ret);
+		csum_replace2(&iph->check, iph->tot_len, newlen);
+		iph->tot_len = newlen;
+	}
+	return XT_CONTINUE;
+}
+
+#if IS_ENABLED(CONFIG_IP6_NF_IPTABLES)
+static unsigned int
+tcpmss_tg6(struct sk_buff *skb, const struct xt_action_param *par)
+{
+	struct ipv6hdr *ipv6h = ipv6_hdr(skb);
+	u8 nexthdr;
+	__be16 frag_off, oldlen, newlen;
+	int tcphoff;
+	int ret;
+
+	nexthdr = ipv6h->nexthdr;
+	tcphoff = ipv6_skip_exthdr(skb, sizeof(*ipv6h), &nexthdr, &frag_off);
+	if (tcphoff < 0)
+		return NF_DROP;
+	ret = tcpmss_mangle_packet(skb, par,
+				   PF_INET6,
+				   tcphoff,
+				   sizeof(*ipv6h) + sizeof(struct tcphdr));
+	if (ret < 0)
+		return NF_DROP;
+	if (ret > 0) {
+		ipv6h = ipv6_hdr(skb);
+		oldlen = ipv6h->payload_len;
+		newlen = htons(ntohs(oldlen) + ret);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->csum = csum_add(csum_sub(skb->csum, oldlen),
+					     newlen);
+		ipv6h->payload_len = newlen;
+	}
+	return XT_CONTINUE;
+}
+#endif
+
+/* Must specify -p tcp --syn */
+static inline bool find_syn_match(const struct xt_entry_match *m)
+{
+	const struct xt_tcp *tcpinfo = (const struct xt_tcp *)m->data;
+
+	if (strcmp(m->u.kernel.match->name, "tcp") == 0 &&
+	    tcpinfo->flg_cmp & TCPHDR_SYN &&
+	    !(tcpinfo->invflags & XT_TCP_INV_FLAGS))
+		return true;
+
 	return false;
 }
 
-static struct xt_match tcpmss_mt_reg[] __read_mostly = {
+static int tcpmss_tg4_check(const struct xt_tgchk_param *par)
+{
+	const struct xt_tcpmss_info *info = par->targinfo;
+	const struct ipt_entry *e = par->entryinfo;
+	const struct xt_entry_match *ematch;
+
+	if (info->mss == XT_TCPMSS_CLAMP_PMTU &&
+	    (par->hook_mask & ~((1 << NF_INET_FORWARD) |
+			   (1 << NF_INET_LOCAL_OUT) |
+			   (1 << NF_INET_POST_ROUTING))) != 0) {
+		pr_info("path-MTU clamping only supported in "
+			"FORWARD, OUTPUT and POSTROUTING hooks\n");
+		return -EINVAL;
+	}
+	if (par->nft_compat)
+		return 0;
+
+	xt_ematch_foreach(ematch, e)
+		if (find_syn_match(ematch))
+			return 0;
+	pr_info("Only works on TCP SYN packets\n");
+	return -EINVAL;
+}
+
+#if IS_ENABLED(CONFIG_IP6_NF_IPTABLES)
+static int tcpmss_tg6_check(const struct xt_tgchk_param *par)
+{
+	const struct xt_tcpmss_info *info = par->targinfo;
+	const struct ip6t_entry *e = par->entryinfo;
+	const struct xt_entry_match *ematch;
+
+	if (info->mss == XT_TCPMSS_CLAMP_PMTU &&
+	    (par->hook_mask & ~((1 << NF_INET_FORWARD) |
+			   (1 << NF_INET_LOCAL_OUT) |
+			   (1 << NF_INET_POST_ROUTING))) != 0) {
+		pr_info("path-MTU clamping only supported in "
+			"FORWARD, OUTPUT and POSTROUTING hooks\n");
+		return -EINVAL;
+	}
+	if (par->nft_compat)
+		return 0;
+
+	xt_ematch_foreach(ematch, e)
+		if (find_syn_match(ematch))
+			return 0;
+	pr_info("Only works on TCP SYN packets\n");
+	return -EINVAL;
+}
+#endif
+
+static struct xt_target tcpmss_tg_reg[] __read_mostly = {
 	{
-		.name		= "tcpmss",
 		.family		= NFPROTO_IPV4,
-		.match		= tcpmss_mt,
-		.matchsize	= sizeof(struct xt_tcpmss_match_info),
+		.name		= "TCPMSS",
+		.checkentry	= tcpmss_tg4_check,
+		.target		= tcpmss_tg4,
+		.targetsize	= sizeof(struct xt_tcpmss_info),
 		.proto		= IPPROTO_TCP,
 		.me		= THIS_MODULE,
 	},
+#if IS_ENABLED(CONFIG_IP6_NF_IPTABLES)
 	{
-		.name		= "tcpmss",
 		.family		= NFPROTO_IPV6,
-		.match		= tcpmss_mt,
-		.matchsize	= sizeof(struct xt_tcpmss_match_info),
+		.name		= "TCPMSS",
+		.checkentry	= tcpmss_tg6_check,
+		.target		= tcpmss_tg6,
+		.targetsize	= sizeof(struct xt_tcpmss_info),
 		.proto		= IPPROTO_TCP,
 		.me		= THIS_MODULE,
 	},
+#endif
 };
 
-static int __init tcpmss_mt_init(void)
+static int __init tcpmss_tg_init(void)
 {
-	return xt_register_matches(tcpmss_mt_reg, ARRAY_SIZE(tcpmss_mt_reg));
+	return xt_register_targets(tcpmss_tg_reg, ARRAY_SIZE(tcpmss_tg_reg));
 }
 
-static void __exit tcpmss_mt_exit(void)
+static void __exit tcpmss_tg_exit(void)
 {
-	xt_unregister_matches(tcpmss_mt_reg, ARRAY_SIZE(tcpmss_mt_reg));
+	xt_unregister_targets(tcpmss_tg_reg, ARRAY_SIZE(tcpmss_tg_reg));
 }
 
-module_init(tcpmss_mt_init);
-module_exit(tcpmss_mt_exit);
+module_init(tcpmss_tg_init);
+module_exit(tcpmss_tg_exit);
diff --git a/net/rds/ib.h b/net/rds/ib.h
index 80228d14976d..9d446cce2106 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -452,6 +452,12 @@ static inline struct rds_connection *rds_ib_map_conn(struct rds_connection *conn
 	return (struct rds_connection *)(unsigned long)id;
 }
 
+/*
+ * called by:
+ *   - net/rds/ib.c|413| <<rds_ib_laddr_check_cm>> cm_id = rds_ib_rdma_create_id(net, rds_rdma_cm_event_handler, &dummy_ic,
+ *   - net/rds/ib_cm.c|1459| <<rds_ib_conn_path_connect>> ic->i_cm_id = rds_ib_rdma_create_id(rds_conn_net(conn),
+ *   - net/rds/rdma_transport.c|386| <<rds_rdma_listen_init_common>> cm_id = rds_ib_rdma_create_id(&init_net, handler, dummy_ic, NULL, RDMA_PS_TCP, IB_QPT_RC);
+ */
 static inline struct rdma_cm_id *rds_ib_rdma_create_id(struct net *net,
 						       rdma_cm_event_handler event_handler,
 						       struct rds_ib_connection *ic,
diff --git a/net/rds/ib_cm.c b/net/rds/ib_cm.c
index f801268091dd..9b5b57306cdc 100644
--- a/net/rds/ib_cm.c
+++ b/net/rds/ib_cm.c
@@ -808,6 +808,11 @@ void rds_dma_hdrs_free(struct dma_pool *pool, struct rds_header **hdrs,
  * This needs to be very careful to not leave IS_ERR pointers around for
  * cleanup to trip over.
  */
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|1299| <<rds_ib_cm_handle_connect>> err = rds_ib_setup_qp(conn);
+ *   - net/rds/ib_cm.c|1404| <<rds_ib_cm_initiate_connect>> ret = rds_ib_setup_qp(conn);
+ */
 static int rds_ib_setup_qp(struct rds_connection *conn)
 {
 	struct rds_ib_connection *ic = conn->c_transport_data;
@@ -1355,6 +1360,12 @@ void rds_ib_conn_destroy_init(struct rds_connection *conn)
 	queue_delayed_work(rds_aux_wq, &work->work, 0);
 }
 
+/*
+ * called by (处理RDMA_CM_EVENT_ROUTE_RESOLVED):
+ *   - net/rds/rdma_transport.c|212| <<rds_rdma_cm_event_handler_cmn>> ret = trans->cm_initiate_connect(cm_id, isv6);
+ *
+ * struct rds_transport rds_ib_transport.cm_initiate_connect = rds_ib_cm_initiate_connect()
+ */
 int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id, bool isv6)
 {
 	struct rds_connection *conn = rds_ib_get_conn(cm_id);
@@ -1435,6 +1446,12 @@ int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id, bool isv6)
 	return ret;
 }
 
+/*
+ * struct rds_transport rds_ib_transport.conn_path_connect = rds_ib_conn_path_connect()
+ *
+ * called by:
+ *   - net/rds/threads.c|245| <<rds_connect_worker>> ret = conn->c_trans->conn_path_connect(cp);
+ */
 int rds_ib_conn_path_connect(struct rds_conn_path *cp)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -1673,6 +1690,9 @@ void rds_ib_conn_path_shutdown(struct rds_conn_path *cp)
 	ic->i_active_side = 0;
 }
 
+/*
+ * struct rds_transport rds_ib_transport.conn_alloc = rds_ib_conn_alloc()
+ */
 int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp)
 {
 	struct rds_ib_connection *ic;
diff --git a/net/rds/ib_ring.c b/net/rds/ib_ring.c
index b66cd4244d30..b61d27f30379 100644
--- a/net/rds/ib_ring.c
+++ b/net/rds/ib_ring.c
@@ -63,6 +63,13 @@
  */
 DECLARE_WAIT_QUEUE_HEAD(rds_ib_ring_empty_wait);
 
+/*
+ * called by:
+ *   - net/rds/ib_cm.c|1655| <<rds_ib_conn_path_shutdown>> rds_ib_ring_init(&ic->i_send_ring, rds_ib_sysctl_max_send_wr);
+ *   - net/rds/ib_cm.c|1656| <<rds_ib_conn_path_shutdown>> rds_ib_ring_init(&ic->i_recv_ring, rds_ib_sysctl_max_recv_wr);
+ *   - net/rds/ib_cm.c|1707| <<rds_ib_conn_alloc>> rds_ib_ring_init(&ic->i_send_ring, rds_ib_sysctl_max_send_wr);
+ *   - net/rds/ib_cm.c|1708| <<rds_ib_conn_alloc>> rds_ib_ring_init(&ic->i_recv_ring, rds_ib_sysctl_max_recv_wr);
+ */
 void rds_ib_ring_init(struct rds_ib_work_ring *ring, u32 nr)
 {
 	memset(ring, 0, sizeof(*ring));
diff --git a/net/rds/ib_send.c b/net/rds/ib_send.c
index 475235f6d6f7..b2bf1b62fe88 100644
--- a/net/rds/ib_send.c
+++ b/net/rds/ib_send.c
@@ -564,6 +564,11 @@ static inline int rds_ib_set_wr_signal_state(struct rds_ib_connection *ic,
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off)
 {
+	/*
+	 * 在以下修改c_transport_data:
+	 *   - net/rds/ib_cm.c|1718| <<rds_ib_conn_alloc>> conn->c_transport_data = ic;
+	 *   - net/rds/loop.c|134| <<rds_loop_conn_alloc>> conn->c_transport_data = lc;
+	 */
 	struct rds_ib_connection *ic = conn->c_transport_data;
 	struct ib_device *dev = ic->i_cm_id->device;
 	struct rds_ib_send_work *send = NULL;
@@ -730,6 +735,7 @@ int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 	do {
 		unsigned int len = 0;
 
+		/* send是struct rds_ib_send_work */
 		/* Set up the header */
 		send->s_wr.send_flags = send_flags;
 		send->s_wr.opcode = IB_WR_SEND;
@@ -836,6 +842,12 @@ int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 
 	/* XXX need to worry about failed_wr and partial sends. */
 	failed_wr = &first->s_wr;
+	/*
+	 * conn是struct rds_connection
+	 * ic是struct rds_ib_connection *ic = conn->c_transport_data;
+	 *
+	 * qp似乎在rds_cond_queue_reconnect_work()创建, 调用rds_connect_worker()
+	 */
 	ret = ib_post_send(ic->i_cm_id->qp, &first->s_wr, &failed_wr);
 	rdsdebug("ic %p first %p (wr %p) ret %d wr %p\n", ic,
 		 first, &first->s_wr, ret, failed_wr);
diff --git a/net/rds/message.c b/net/rds/message.c
index c305f88c05e7..a0e0bc9b05bf 100644
--- a/net/rds/message.c
+++ b/net/rds/message.c
@@ -247,6 +247,13 @@ EXPORT_SYMBOL_GPL(rds_message_add_rdma_dest_extension);
  * rds ops will need. This is to minimize memory allocation count. Then, each rds op
  * can grab SGs when initializing its part of the rds_message.
  */
+/*
+ * called by:
+ *   - net/rds/cong.c|187| <<rds_cong_map_pages>> rm = rds_message_alloc(extra_bytes, GFP_NOWAIT);
+ *   - net/rds/send.c|1474| <<rds_sendmsg>> rm = rds_message_alloc(ret, GFP_KERNEL);
+ *   - net/rds/send.c|1706| <<rds_send_internal>> rm = rds_message_alloc(ret, gfp);
+ *   - net/rds/send.c|1825| <<rds_send_probe>> rm = rds_message_alloc(0, GFP_ATOMIC);
+ */
 struct rds_message *rds_message_alloc(unsigned int extra_len, gfp_t gfp)
 {
 	struct rds_message *rm;
diff --git a/net/rds/rds.h b/net/rds/rds.h
index 9ae01c72ef19..d29acaca3f5f 100644
--- a/net/rds/rds.h
+++ b/net/rds/rds.h
@@ -157,6 +157,24 @@ enum {
 	RDS_CONN_DOWN = 0,
 	RDS_CONN_CONNECTING,
 	RDS_CONN_DISCONNECTING,
+	/*
+	 * 使用RDS_CONN_UP的地方:
+	 *   - net/rds/connection.c|471| <<rds_conn_shutdown>> if (!rds_conn_path_transition(cp, RDS_CONN_UP,
+	 *   - net/rds/connection.c|888| <<rds_conn_info_visitor>> atomic_read(&cp->cp_state) == RDS_CONN_UP,
+	 *   - net/rds/connection.c|917| <<rds6_conn_info_visitor>> atomic_read(&cp->cp_state) == RDS_CONN_UP,
+	 *   - net/rds/connection.c|1058| <<rds_conn_path_drop>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 *   - net/rds/ib.c|272| <<rds_ib_conn_info_visitor>> if (rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/ib.c|320| <<rds6_ib_conn_info_visitor>> if (rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/ib_cm.c|1203| <<rds_ib_cm_handle_connect>> rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/ib_cm.c|1218| <<rds_ib_cm_handle_connect>> if (rds_conn_state(conn) == RDS_CONN_UP) {
+	 *   - net/rds/rds.h|172| <<conn_state_mnem>> CASE_RET(RDS_CONN_UP);
+	 *   - net/rds/rds.h|1174| <<rds_conn_path_up>> return atomic_read(&cp->cp_state) == RDS_CONN_UP;
+	 *   - net/rds/tcp_listen.c|210| <<rds_tcp_accept_one>> BUG_ON(conn_state == RDS_CONN_UP);
+	 *   - net/rds/threads.c|108| <<rds_connect_path_complete>> if (!rds_conn_path_transition(cp, curr, RDS_CONN_UP)) {
+	 *   - net/rds/threads.c|289| <<rds_send_worker>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 *   - net/rds/threads.c|320| <<rds_recv_worker>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 *   - net/rds/threads.c|353| <<rds_hb_worker>> if (rds_conn_path_state(cp) == RDS_CONN_UP) {
+	 */
 	RDS_CONN_UP,
 	RDS_CONN_RESETTING,
 	RDS_CONN_ERROR,
@@ -180,6 +198,11 @@ static inline const char *conn_state_mnem(int state)
 
 /* Bits for c_flags */
 #define RDS_LL_SEND_FULL	0
+/*
+ * 在以下使用RDS_RECONNECT_PENDING:
+ *   - net/rds/rds.h|1069| <<rds_cond_queue_reconnect_work>> if (!test_and_set_bit(RDS_RECONNECT_PENDING, &cp->cp_flags))
+ *   - net/rds/rds.h|1077| <<rds_clear_reconnect_pending_work_bit>> clear_bit(RDS_RECONNECT_PENDING, &cp->cp_flags);
+ */
 #define RDS_RECONNECT_PENDING	1
 #define RDS_IN_XMIT		2
 #define RDS_RECV_REFILL		3
@@ -271,6 +294,20 @@ struct rds_conn_path {
 
 	spinlock_t		cp_lock;		/* protect msg queues */
 	u64			cp_next_tx_seq;
+	/*
+	 * 在以下使用cp_send_queue:
+	 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+	 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+	 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+	 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+	 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+	 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+	 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+	 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+	 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+	 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 */
 	struct list_head	cp_send_queue;
 	struct list_head	cp_retrans;
 
@@ -842,6 +879,11 @@ struct rds_sock {
 	 * newline
 	 */
 	spinlock_t		rs_snd_lock;
+	/*
+	 * 在以下使用rs_send_queue:
+	 *   - net/rds/send.c|917| <<rds_send_drop_to>> list_for_each_entry_safe(rm, tmp, &rs->rs_send_queue, m_sock_item) {
+	 *   - net/rds/send.c|1062| <<rds_send_queue_rm>> list_add_tail(&rm->m_sock_item, &rs->rs_send_queue);
+	 */
 	struct list_head	rs_send_queue;
 	u32			rs_snd_bytes; /* Total bytes to all peers */
 	u32			rs_buf_info_dest_cnt;
@@ -1059,12 +1101,26 @@ static inline void rds_clear_shutdown_pending_work_bit(struct rds_conn_path *cp)
 	smp_mb__after_atomic();
 }
 
+/*
+ * called by:
+ *   - net/rds/send.c|1529| <<rds_sendmsg>> rds_cond_queue_reconnect_work(&conn->c_path[0],
+ *   - net/rds/threads.c|219| <<rds_queue_reconnect>> rds_cond_queue_reconnect_work(cp, delay);
+ */
 static inline void rds_cond_queue_reconnect_work(struct rds_conn_path *cp, unsigned long delay)
 {
+	/*
+	 * cp->cp_conn_w的使用的地方:
+	 *   - net/rds/connection.c|209| <<__rds_conn_path_init>> INIT_DELAYED_WORK(&cp->cp_conn_w, rds_connect_worker);
+	 *   - net/rds/connection.c|521| <<rds_conn_shutdown>> cancel_delayed_work_sync(&cp->cp_conn_w);
+	 *   - net/rds/rds.h|1070| <<rds_cond_queue_reconnect_work>> queue_delayed_work(cp->cp_wq, &cp->cp_conn_w, delay);
+	 */
 	if (!test_and_set_bit(RDS_RECONNECT_PENDING, &cp->cp_flags))
 		queue_delayed_work(cp->cp_wq, &cp->cp_conn_w, delay);
 }
 
+/*
+ * 清空rds_conn_path->cp_flags中的RDS_RECONNECT_PENDING
+ */
 static inline void rds_clear_reconnect_pending_work_bit(struct rds_conn_path *cp)
 {
 	/* clear_bit() does not imply a memory barrier */
diff --git a/net/rds/send.c b/net/rds/send.c
index 4648bcee9fcd..7813a057cd9b 100644
--- a/net/rds/send.c
+++ b/net/rds/send.c
@@ -319,6 +319,20 @@ int rds_send_xmit(struct rds_conn_path *cp)
 
 			spin_lock_irqsave(&cp->cp_lock, flags);
 
+			/*
+			 * 在以下使用cp_send_queue:
+			 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+			 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+			 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+			 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+			 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+			 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+			 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+			 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+			 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+			 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+			 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+			 */
 			if (!list_empty(&cp->cp_send_queue)) {
 				rm = list_entry(cp->cp_send_queue.next,
 						struct rds_message,
@@ -433,6 +447,9 @@ int rds_send_xmit(struct rds_conn_path *cp)
 
 		if (rm->data.op_active && !cp->cp_xmit_data_sent) {
 			rm->m_final_op = &rm->data;
+			/*
+			 * 一个例子是rds_ib_xmit()
+			 */
 			ret = conn->c_trans->xmit(conn, rm,
 						  cp->cp_xmit_hdr_off,
 						  cp->cp_xmit_sg,
@@ -902,6 +919,11 @@ void rds_send_drop_acked(struct rds_connection *conn, u64 ack,
 }
 EXPORT_SYMBOL_GPL(rds_send_drop_acked);
 
+/*
+ * called by:
+ *   - net/rds/af_rds.c|122| <<rds_release>> rds_send_drop_to(rs, NULL);
+ *   - net/rds/af_rds.c|399| <<rds_cancel_sent_to>> rds_send_drop_to(rs, &sin6);
+ */
 void rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in6 *dest)
 {
 	struct rds_message *rm, *tmp;
@@ -1019,6 +1041,12 @@ void rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in6 *dest)
  * possible that another thread can race with us and remove the
  * message from the flow with RDS_CANCEL_SENT_TO.
  */
+/*
+ * called by:
+ *   - net/rds/send.c|1609| <<rds_sendmsg>> while (!rds_send_queue_rm(rs, conn, cpath, rm, rs->rs_bound_port,
+ *   - net/rds/send.c|1623| <<rds_sendmsg>> rds_send_queue_rm(rs, conn, cpath, rm,
+ *   - net/rds/send.c|1772| <<rds_send_internal>> if (!rds_send_queue_rm(rs, conn, &conn->c_path[0], rm,
+ */
 static int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,
 			     struct rds_conn_path *cp,
 			     struct rds_message *rm, __be16 sport,
@@ -1078,6 +1106,20 @@ static int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,
 			goto out;
 		}
 		rm->m_inc.i_hdr.h_sequence = cpu_to_be64(cp->cp_next_tx_seq++);
+		/*
+		 * 在以下使用cp_send_queue:
+		 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+		 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+		 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+		 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+		 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+		 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+		 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+		 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+		 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+		 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+		 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+		 */
 		list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
 		set_bit(RDS_MSG_ON_CONN, &rm->m_flags);
 
@@ -1286,6 +1328,39 @@ static int rds_send_mprds_hash(struct rds_sock *rs, struct rds_connection *conn)
 	return hash;
 }
 
+/*
+ * 创建qp的例子:
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_initiate_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_work_handler [rdma_cm]  
+ * [0] process_one_work            
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ *
+ * 其他创建qp的例子:
+ * [0] mlx4_ib_create_qp [mlx4_ib]
+ * [0] ib_create_qp [ib_core]
+ * [0] rdma_create_qp [rdma_cm]
+ * [0] rds_ib_setup_qp [rds_rdma]
+ * [0] rds_ib_cm_handle_connect [rds_rdma]
+ * [0] rds_rdma_cm_event_handler_cmn [rds_rdma]
+ * [0] rds_rdma_cm_event_handler [rds_rdma]
+ * [0] cma_listen_handler [rdma_cm]
+ * [0] cma_req_handler [rdma_cm]
+ * [0] cm_process_work [ib_cm]
+ * [0] cm_req_handler [ib_cm]
+ * [0] cm_work_handler [ib_cm]
+ * [0] process_one_work
+ * [0] worker_thread
+ * [0] kthread
+ * [0] ret_from_fork
+ */
 int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 {
 	struct sock *sk = sock->sk;
@@ -1458,6 +1533,9 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 	 */
 	if (nonblock) {
 		spin_lock_irqsave(&rs->rs_snd_lock, flags);
+		/*
+		 * rs来自struct rds_sock *rs = rds_sk_to_rs(sk);
+		 */
 		no_space = bufi->rsbi_snd_bytes >= rds_sk_sndbuf(rs);
 		spin_unlock_irqrestore(&rs->rs_snd_lock, flags);
 		if (no_space) {
@@ -1500,8 +1578,17 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 
 	/* rds_conn_create has a spinlock that runs with IRQ off.
 	 * Caching the conn in the socket helps a lot. */
+	/*
+	 * rs来自struct rds_sock *rs = rds_sk_to_rs(sk);
+	 */
 	if (rs->rs_conn && ipv6_addr_equal(&rs->rs_conn->c_faddr, &daddr) &&
 	    rs->rs_tos == rs->rs_conn->c_tos) {
+		/*
+		 * struct rds_sock *rs = rds_sk_to_rs(sk)
+		 * conn是struct rds_connection
+		 *
+		 * rs->rs_conn的值是在下面的else部分设置的
+		 */
 		conn = rs->rs_conn;
 		cpath = rs->rs_conn_path;
 	} else {
@@ -1514,6 +1601,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 			ret = PTR_ERR(conn);
 			goto out;
 		}
+		/*
+		 * struct rds_connection conn
+		 *  -> int c_npaths;
+		 *  -> struct rds_transport *c_trans;
+		 *      -> unsigned int t_mp_capable:1;
+		 *  -> struct rds_conn_path *c_path;
+		 */
 		if (conn->c_trans->t_mp_capable) {
 			/* c_npaths == 0 if we have not talked to this peer
 			 * before.  Initiate a connection request to the
@@ -1631,6 +1725,13 @@ int rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)
 	if (!dport)
 		rds_stats_inc(s_send_ping);
 
+	/*
+	 * struct rds_connection conn
+	 *  -> int c_npaths;
+	 *  -> struct rds_transport *c_trans;
+	 *      -> unsigned int t_mp_capable:1;
+	 *  -> struct rds_conn_path *c_path;
+	 */
 	ret = rds_send_xmit(cpath);
 	if (ret == -ENOMEM || ret == -EAGAIN)
 		rds_cond_queue_send_work(cpath, 1);
@@ -1826,6 +1927,20 @@ static int rds_send_probe(struct rds_conn_path *cp, __be16 sport,
 		goto out;
 
 	spin_lock_irqsave(&cp->cp_lock, flags);
+	/*
+	 * 在以下使用cp_send_queue:
+	 *   - net/rds/connection.c|195| <<__rds_conn_path_init>> INIT_LIST_HEAD(&cp->cp_send_queue);
+	 *   - net/rds/connection.c|572| <<rds_conn_path_destroy>> &cp->cp_send_queue,
+	 *   - net/rds/connection.c|692| <<rds_conn_message_info_cmn>> list = &cp->cp_send_queue;
+	 *   - net/rds/rds_single_path.h|13| <<c_send_queue>> #define c_send_queue c_path[0].cp_send_queue
+	 *   - net/rds/send.c|109| <<rds_send_path_reset>> list_splice_init(&cp->cp_retrans, &cp->cp_send_queue);
+	 *   - net/rds/send.c|113| <<rds_send_path_reset>> list_for_each_entry_safe(rm, tmp, &cp->cp_send_queue,
+	 *   - net/rds/send.c|322| <<rds_send_xmit>> if (!list_empty(&cp->cp_send_queue)) {
+	 *   - net/rds/send.c|323| <<rds_send_xmit>> rm = list_entry(cp->cp_send_queue.next,
+	 *   - net/rds/send.c|535| <<rds_send_xmit>> !list_empty(&cp->cp_send_queue)) && !raced) {
+	 *   - net/rds/send.c|1081| <<rds_send_queue_rm>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 *   - net/rds/send.c|1842| <<rds_send_probe>> list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
+	 */
 	list_add_tail(&rm->m_conn_item, &cp->cp_send_queue);
 	set_bit(RDS_MSG_ON_CONN, &rm->m_flags);
 	rds_message_addref(rm);
diff --git a/net/rds/threads.c b/net/rds/threads.c
index 53d548c3d7ab..85c987c90bd6 100644
--- a/net/rds/threads.c
+++ b/net/rds/threads.c
@@ -101,6 +101,12 @@ static inline void rds_update_avg_connect_time(struct rds_conn_path *cp)
 	atomic64_set(&cp->cp_conn->c_trans->rds_avg_conn_jf, new_avg_jf);
 }
 
+/*
+ * called by:
+ *   - net/rds/tcp_connect.c|69| <<rds_tcp_state_change>> rds_connect_path_complete(cp, RDS_CONN_CONNECTING);
+ *   - net/rds/tcp_listen.c|219| <<rds_tcp_accept_one>> rds_connect_path_complete(cp, RDS_CONN_RESETTING);
+ *   - net/rds/tcp_listen.c|222| <<rds_tcp_accept_one>> rds_connect_path_complete(cp, RDS_CONN_CONNECTING);
+ */
 void rds_connect_path_complete(struct rds_conn_path *cp, int curr)
 {
 	struct rds_connection *conn = cp->cp_conn;
@@ -221,6 +227,10 @@ void rds_queue_reconnect(struct rds_conn_path *cp)
 				       rds_sysctl_reconnect_max_jiffies);
 }
 
+/*
+ * used by:
+ *   - net/rds/connection.c|209| <<__rds_conn_path_init>> INIT_DELAYED_WORK(&cp->cp_conn_w, rds_connect_worker);
+ */
 void rds_connect_worker(struct work_struct *work)
 {
 	struct rds_conn_path *cp = container_of(work,
@@ -242,6 +252,9 @@ void rds_connect_worker(struct work_struct *work)
 		cp->cp_connection_start = get_seconds();
 		cp->cp_drop_source = DR_DEFAULT;
 
+		/*
+		 * 一个例子是rds_ib_conn_path_connect()
+		 */
 		ret = conn->c_trans->conn_path_connect(cp);
 		rds_rtd_ptr(RDS_RTD_CM_EXT,
 			    "conn %p for <%pI6c,%pI6c,%d> dispatched, ret %d\n",
@@ -265,6 +278,9 @@ void rds_connect_worker(struct work_struct *work)
 			conn, conn_state_mnem(atomic_read(&cp->cp_state)));
 	}
 out:
+	/*
+	 * 清空rds_conn_path->cp_flags中的RDS_RECONNECT_PENDING
+	 */
 	rds_clear_reconnect_pending_work_bit(cp);
 }
 
-- 
2.20.1 (Apple Git-117)

